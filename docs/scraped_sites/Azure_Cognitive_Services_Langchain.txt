This toolkit is used to interact with the Azure Cognitive Services API to achieve some multimodal capabilities. Currently There are four tools bundled in this toolkit: First, you need to set up an Azure account and create a Cognitive Services resource. You can follow the instructions here to create a resource.  Then, you need to get the endpoint, key and region of your resource, and set them as environment variables. You can find them in the "Keys and Endpoint" page of your resource. IntegrationsCallbacksChat modelsChat loadersDocument loadersDocument transformersLLMsMemoryRetrieversText embedding modelsAgents & ToolkitsAINetworkAirbyte Question AnsweringAmadeusAzure Cognitive ServicesCSVDocument ComparisonGithubGitlabGmailGoogle Drive toolJiraJSONMultiOnOffice365OpenAPINatural Language APIsPandas DataframePlayWright BrowserPowerBI DatasetPythonSpark DataframeSpark SQLSQL DatabaseVectorstoreXorbitsToolsVector storesGrouped by provider Callbacks Chat models Chat loaders Document loaders Document transformers LLMs Memory Retrievers Text embedding models Agents & ToolkitsAINetworkAirbyte Question AnsweringAmadeusAzure Cognitive ServicesCSVDocument ComparisonGithubGitlabGmailGoogle Drive toolJiraJSONMultiOnOffice365OpenAPINatural Language APIsPandas DataframePlayWright BrowserPowerBI DatasetPythonSpark DataframeSpark SQLSQL DatabaseVectorstoreXorbits AINetwork Airbyte Question Answering Amadeus Azure Cognitive Services CSV Document Comparison Github Gitlab Gmail Google Drive tool Jira JSON MultiOn Office365 OpenAPI Natural Language APIs Pandas Dataframe PlayWright Browser PowerBI Dataset Python Spark Dataframe Spark SQL SQL Database Vectorstore Xorbits Tools Vector stores Grouped by provider  Integrations Agents & Toolkits Azure Cognitive Services AzureCogsImageAnalysisTool: used to extract caption, objects, tags, and text from images. (Note: this tool is not available on Mac OS yet, due to the dependency on azure-ai-vision package, which is only supported on Windows and Linux currently.) AzureCogsFormRecognizerTool: used to extract text, tables, and key-value pairs from documents. AzureCogsSpeech2TextTool: used to transcribe speech to text. AzureCogsText2SpeechTool: used to synthesize text to speech. AzureCognitiveServicesToolkit initialize_agent AgentType Create the Toolkit Use within an Agent Discord Twitter Python JS/TS Homepage Blog Skip to main contentü¶úÔ∏èüîó LangChainDocsUse casesIntegrationsAPILangSmithJS/TS DocsCTRLKIntegrationsCallbacksChat modelsChat loadersDocument loadersDocument transformersLLMsMemoryRetrieversText embedding modelsAgents & ToolkitsAINetworkAirbyte Question AnsweringAmadeusAzure Cognitive ServicesCSVDocument ComparisonGithubGitlabGmailGoogle Drive toolJiraJSONMultiOnOffice365OpenAPINatural Language APIsPandas DataframePlayWright BrowserPowerBI DatasetPythonSpark DataframeSpark SQLSQL DatabaseVectorstoreXorbitsToolsVector storesGrouped by providerIntegrationsAgents & ToolkitsAzure Cognitive ServicesOn this pageAzure Cognitive ServicesThis toolkit is used to interact with the Azure Cognitive Services API to achieve some multimodal capabilities.Currently There are four tools bundled in this toolkit:AzureCogsImageAnalysisTool: used to extract caption, objects, tags, and text from images. (Note: this tool is not available on Mac OS yet, due to the dependency on azure-ai-vision package, which is only supported on Windows and Linux currently.)AzureCogsFormRecognizerTool: used to extract text, tables, and key-value pairs from documents.AzureCogsSpeech2TextTool: used to transcribe speech to text.AzureCogsText2SpeechTool: used to synthesize text to speech.First, you need to set up an Azure account and create a Cognitive Services resource. You can follow the instructions here to create a resource. Then, you need to get the endpoint, key and region of your resource, and set them as environment variables. You can find them in the "Keys and Endpoint" page of your resource.# !pip install --upgrade azure-ai-formrecognizer > /dev/null# !pip install --upgrade azure-cognitiveservices-speech > /dev/null# For Windows/Linux# !pip install --upgrade azure-ai-vision > /dev/nullimport osos.environ["OPENAI_API_KEY"] = "sk-"os.environ["AZURE_COGS_KEY"] = ""os.environ["AZURE_COGS_ENDPOINT"] = ""os.environ["AZURE_COGS_REGION"] = ""Create the Toolkit‚Äãfrom langchain.agents.agent_toolkits import AzureCognitiveServicesToolkittoolkit = AzureCognitiveServicesToolkit()API Reference:AzureCognitiveServicesToolkit[tool.name for tool in toolkit.get_tools()]    ['Azure Cognitive Services Image Analysis',     'Azure Cognitive Services Form Recognizer',     'Azure Cognitive Services Speech2Text',     'Azure Cognitive Services Text2Speech']Use within an Agent‚Äãfrom langchain import OpenAIfrom langchain.agents import initialize_agent, AgentTypeAPI Reference:initialize_agentAgentTypellm = OpenAI(temperature=0)agent = initialize_agent(    tools=toolkit.get_tools(),    llm=llm,    agent=AgentType.STRUCTURED_CHAT_ZERO_SHOT_REACT_DESCRIPTION,    verbose=True,)agent.run(    "What can I make with these ingredients?"    "https://images.openai.com/blob/9ad5a2ab-041f-475f-ad6a-b51899c50182/ingredients.png")            > Entering new AgentExecutor chain...        Action:    ```    {      "action": "Azure Cognitive Services Image Analysis",      "action_input": "https://images.openai.com/blob/9ad5a2ab-041f-475f-ad6a-b51899c50182/ingredients.png"    }    ```            Observation: Caption: a group of eggs and flour in bowls    Objects: Egg, Egg, Food    Tags: dairy, ingredient, indoor, thickening agent, food, mixing bowl, powder, flour, egg, bowl    Thought: I can use the objects and tags to suggest recipes    Action:    ```    {      "action": "Final Answer",      "action_input": "You can make pancakes, omelettes, or quiches with these ingredients!"    }    ```        > Finished chain.    'You can make pancakes, omelettes, or quiches with these ingredients!'audio_file = agent.run("Tell me a joke and read it out for me.")            > Entering new AgentExecutor chain...    Action:    ```    {      "action": "Azure Cognitive Services Text2Speech",      "action_input": "Why did the chicken cross the playground? To get to the other slide!"    }    ```            Observation: /tmp/tmpa3uu_j6b.wav    Thought: I have the audio file of the joke    Action:    ```    {      "action": "Final Answer",      "action_input": "/tmp/tmpa3uu_j6b.wav"    }    ```        > Finished chain.    '/tmp/tmpa3uu_j6b.wav'from IPython import displayaudio = display.Audio(audio_file)display.display(audio)PreviousAmadeusNextCSVCreate the ToolkitUse within an AgentCommunityDiscordTwitterGitHubPythonJS/TSMoreHomepageBlogCopyright ¬© 2023 LangChain, Inc. Skip to main content ü¶úÔ∏èüîó LangChainDocsUse casesIntegrationsAPILangSmithJS/TS DocsCTRLK ü¶úÔ∏èüîó LangChainDocsUse casesIntegrationsAPI LangSmithJS/TS DocsCTRLK  CTRLK CTRLK  CTRLK   IntegrationsCallbacksChat modelsChat loadersDocument loadersDocument transformersLLMsMemoryRetrieversText embedding modelsAgents & ToolkitsAINetworkAirbyte Question AnsweringAmadeusAzure Cognitive ServicesCSVDocument ComparisonGithubGitlabGmailGoogle Drive toolJiraJSONMultiOnOffice365OpenAPINatural Language APIsPandas DataframePlayWright BrowserPowerBI DatasetPythonSpark DataframeSpark SQLSQL DatabaseVectorstoreXorbitsToolsVector storesGrouped by providerIntegrationsAgents & ToolkitsAzure Cognitive ServicesOn this pageAzure Cognitive ServicesThis toolkit is used to interact with the Azure Cognitive Services API to achieve some multimodal capabilities.Currently There are four tools bundled in this toolkit:AzureCogsImageAnalysisTool: used to extract caption, objects, tags, and text from images. (Note: this tool is not available on Mac OS yet, due to the dependency on azure-ai-vision package, which is only supported on Windows and Linux currently.)AzureCogsFormRecognizerTool: used to extract text, tables, and key-value pairs from documents.AzureCogsSpeech2TextTool: used to transcribe speech to text.AzureCogsText2SpeechTool: used to synthesize text to speech.First, you need to set up an Azure account and create a Cognitive Services resource. You can follow the instructions here to create a resource. Then, you need to get the endpoint, key and region of your resource, and set them as environment variables. You can find them in the "Keys and Endpoint" page of your resource.# !pip install --upgrade azure-ai-formrecognizer > /dev/null# !pip install --upgrade azure-cognitiveservices-speech > /dev/null# For Windows/Linux# !pip install --upgrade azure-ai-vision > /dev/nullimport osos.environ["OPENAI_API_KEY"] = "sk-"os.environ["AZURE_COGS_KEY"] = ""os.environ["AZURE_COGS_ENDPOINT"] = ""os.environ["AZURE_COGS_REGION"] = ""Create the Toolkit‚Äãfrom langchain.agents.agent_toolkits import AzureCognitiveServicesToolkittoolkit = AzureCognitiveServicesToolkit()API Reference:AzureCognitiveServicesToolkit[tool.name for tool in toolkit.get_tools()]    ['Azure Cognitive Services Image Analysis',     'Azure Cognitive Services Form Recognizer',     'Azure Cognitive Services Speech2Text',     'Azure Cognitive Services Text2Speech']Use within an Agent‚Äãfrom langchain import OpenAIfrom langchain.agents import initialize_agent, AgentTypeAPI Reference:initialize_agentAgentTypellm = OpenAI(temperature=0)agent = initialize_agent(    tools=toolkit.get_tools(),    llm=llm,    agent=AgentType.STRUCTURED_CHAT_ZERO_SHOT_REACT_DESCRIPTION,    verbose=True,)agent.run(    "What can I make with these ingredients?"    "https://images.openai.com/blob/9ad5a2ab-041f-475f-ad6a-b51899c50182/ingredients.png")            > Entering new AgentExecutor chain...        Action:    ```    {      "action": "Azure Cognitive Services Image Analysis",      "action_input": "https://images.openai.com/blob/9ad5a2ab-041f-475f-ad6a-b51899c50182/ingredients.png"    }    ```            Observation: Caption: a group of eggs and flour in bowls    Objects: Egg, Egg, Food    Tags: dairy, ingredient, indoor, thickening agent, food, mixing bowl, powder, flour, egg, bowl    Thought: I can use the objects and tags to suggest recipes    Action:    ```    {      "action": "Final Answer",      "action_input": "You can make pancakes, omelettes, or quiches with these ingredients!"    }    ```        > Finished chain.    'You can make pancakes, omelettes, or quiches with these ingredients!'audio_file = agent.run("Tell me a joke and read it out for me.")            > Entering new AgentExecutor chain...    Action:    ```    {      "action": "Azure Cognitive Services Text2Speech",      "action_input": "Why did the chicken cross the playground? To get to the other slide!"    }    ```            Observation: /tmp/tmpa3uu_j6b.wav    Thought: I have the audio file of the joke    Action:    ```    {      "action": "Final Answer",      "action_input": "/tmp/tmpa3uu_j6b.wav"    }    ```        > Finished chain.    '/tmp/tmpa3uu_j6b.wav'from IPython import displayaudio = display.Audio(audio_file)display.display(audio)PreviousAmadeusNextCSVCreate the ToolkitUse within an Agent IntegrationsCallbacksChat modelsChat loadersDocument loadersDocument transformersLLMsMemoryRetrieversText embedding modelsAgents & ToolkitsAINetworkAirbyte Question AnsweringAmadeusAzure Cognitive ServicesCSVDocument ComparisonGithubGitlabGmailGoogle Drive toolJiraJSONMultiOnOffice365OpenAPINatural Language APIsPandas DataframePlayWright BrowserPowerBI DatasetPythonSpark DataframeSpark SQLSQL DatabaseVectorstoreXorbitsToolsVector storesGrouped by providerIntegrationsAgents & ToolkitsAzure Cognitive ServicesOn this pageAzure Cognitive ServicesThis toolkit is used to interact with the Azure Cognitive Services API to achieve some multimodal capabilities.Currently There are four tools bundled in this toolkit:AzureCogsImageAnalysisTool: used to extract caption, objects, tags, and text from images. (Note: this tool is not available on Mac OS yet, due to the dependency on azure-ai-vision package, which is only supported on Windows and Linux currently.)AzureCogsFormRecognizerTool: used to extract text, tables, and key-value pairs from documents.AzureCogsSpeech2TextTool: used to transcribe speech to text.AzureCogsText2SpeechTool: used to synthesize text to speech.First, you need to set up an Azure account and create a Cognitive Services resource. You can follow the instructions here to create a resource. Then, you need to get the endpoint, key and region of your resource, and set them as environment variables. You can find them in the "Keys and Endpoint" page of your resource.# !pip install --upgrade azure-ai-formrecognizer > /dev/null# !pip install --upgrade azure-cognitiveservices-speech > /dev/null# For Windows/Linux# !pip install --upgrade azure-ai-vision > /dev/nullimport osos.environ["OPENAI_API_KEY"] = "sk-"os.environ["AZURE_COGS_KEY"] = ""os.environ["AZURE_COGS_ENDPOINT"] = ""os.environ["AZURE_COGS_REGION"] = ""Create the Toolkit‚Äãfrom langchain.agents.agent_toolkits import AzureCognitiveServicesToolkittoolkit = AzureCognitiveServicesToolkit()API Reference:AzureCognitiveServicesToolkit[tool.name for tool in toolkit.get_tools()]    ['Azure Cognitive Services Image Analysis',     'Azure Cognitive Services Form Recognizer',     'Azure Cognitive Services Speech2Text',     'Azure Cognitive Services Text2Speech']Use within an Agent‚Äãfrom langchain import OpenAIfrom langchain.agents import initialize_agent, AgentTypeAPI Reference:initialize_agentAgentTypellm = OpenAI(temperature=0)agent = initialize_agent(    tools=toolkit.get_tools(),    llm=llm,    agent=AgentType.STRUCTURED_CHAT_ZERO_SHOT_REACT_DESCRIPTION,    verbose=True,)agent.run(    "What can I make with these ingredients?"    "https://images.openai.com/blob/9ad5a2ab-041f-475f-ad6a-b51899c50182/ingredients.png")            > Entering new AgentExecutor chain...        Action:    ```    {      "action": "Azure Cognitive Services Image Analysis",      "action_input": "https://images.openai.com/blob/9ad5a2ab-041f-475f-ad6a-b51899c50182/ingredients.png"    }    ```            Observation: Caption: a group of eggs and flour in bowls    Objects: Egg, Egg, Food    Tags: dairy, ingredient, indoor, thickening agent, food, mixing bowl, powder, flour, egg, bowl    Thought: I can use the objects and tags to suggest recipes    Action:    ```    {      "action": "Final Answer",      "action_input": "You can make pancakes, omelettes, or quiches with these ingredients!"    }    ```        > Finished chain.    'You can make pancakes, omelettes, or quiches with these ingredients!'audio_file = agent.run("Tell me a joke and read it out for me.")            > Entering new AgentExecutor chain...    Action:    ```    {      "action": "Azure Cognitive Services Text2Speech",      "action_input": "Why did the chicken cross the playground? To get to the other slide!"    }    ```            Observation: /tmp/tmpa3uu_j6b.wav    Thought: I have the audio file of the joke    Action:    ```    {      "action": "Final Answer",      "action_input": "/tmp/tmpa3uu_j6b.wav"    }    ```        > Finished chain.    '/tmp/tmpa3uu_j6b.wav'from IPython import displayaudio = display.Audio(audio_file)display.display(audio)PreviousAmadeusNextCSVCreate the ToolkitUse within an Agent IntegrationsCallbacksChat modelsChat loadersDocument loadersDocument transformersLLMsMemoryRetrieversText embedding modelsAgents & ToolkitsAINetworkAirbyte Question AnsweringAmadeusAzure Cognitive ServicesCSVDocument ComparisonGithubGitlabGmailGoogle Drive toolJiraJSONMultiOnOffice365OpenAPINatural Language APIsPandas DataframePlayWright BrowserPowerBI DatasetPythonSpark DataframeSpark SQLSQL DatabaseVectorstoreXorbitsToolsVector storesGrouped by provider IntegrationsCallbacksChat modelsChat loadersDocument loadersDocument transformersLLMsMemoryRetrieversText embedding modelsAgents & ToolkitsAINetworkAirbyte Question AnsweringAmadeusAzure Cognitive ServicesCSVDocument ComparisonGithubGitlabGmailGoogle Drive toolJiraJSONMultiOnOffice365OpenAPINatural Language APIsPandas DataframePlayWright BrowserPowerBI DatasetPythonSpark DataframeSpark SQLSQL DatabaseVectorstoreXorbitsToolsVector storesGrouped by provider Integrations Callbacks Chat models Chat loaders Document loaders Document transformers LLMs Memory Retrievers Text embedding models Agents & Toolkits Tools Vector stores Grouped by provider IntegrationsAgents & ToolkitsAzure Cognitive ServicesOn this pageAzure Cognitive ServicesThis toolkit is used to interact with the Azure Cognitive Services API to achieve some multimodal capabilities.Currently There are four tools bundled in this toolkit:AzureCogsImageAnalysisTool: used to extract caption, objects, tags, and text from images. (Note: this tool is not available on Mac OS yet, due to the dependency on azure-ai-vision package, which is only supported on Windows and Linux currently.)AzureCogsFormRecognizerTool: used to extract text, tables, and key-value pairs from documents.AzureCogsSpeech2TextTool: used to transcribe speech to text.AzureCogsText2SpeechTool: used to synthesize text to speech.First, you need to set up an Azure account and create a Cognitive Services resource. You can follow the instructions here to create a resource. Then, you need to get the endpoint, key and region of your resource, and set them as environment variables. You can find them in the "Keys and Endpoint" page of your resource.# !pip install --upgrade azure-ai-formrecognizer > /dev/null# !pip install --upgrade azure-cognitiveservices-speech > /dev/null# For Windows/Linux# !pip install --upgrade azure-ai-vision > /dev/nullimport osos.environ["OPENAI_API_KEY"] = "sk-"os.environ["AZURE_COGS_KEY"] = ""os.environ["AZURE_COGS_ENDPOINT"] = ""os.environ["AZURE_COGS_REGION"] = ""Create the Toolkit‚Äãfrom langchain.agents.agent_toolkits import AzureCognitiveServicesToolkittoolkit = AzureCognitiveServicesToolkit()API Reference:AzureCognitiveServicesToolkit[tool.name for tool in toolkit.get_tools()]    ['Azure Cognitive Services Image Analysis',     'Azure Cognitive Services Form Recognizer',     'Azure Cognitive Services Speech2Text',     'Azure Cognitive Services Text2Speech']Use within an Agent‚Äãfrom langchain import OpenAIfrom langchain.agents import initialize_agent, AgentTypeAPI Reference:initialize_agentAgentTypellm = OpenAI(temperature=0)agent = initialize_agent(    tools=toolkit.get_tools(),    llm=llm,    agent=AgentType.STRUCTURED_CHAT_ZERO_SHOT_REACT_DESCRIPTION,    verbose=True,)agent.run(    "What can I make with these ingredients?"    "https://images.openai.com/blob/9ad5a2ab-041f-475f-ad6a-b51899c50182/ingredients.png")            > Entering new AgentExecutor chain...        Action:    ```    {      "action": "Azure Cognitive Services Image Analysis",      "action_input": "https://images.openai.com/blob/9ad5a2ab-041f-475f-ad6a-b51899c50182/ingredients.png"    }    ```            Observation: Caption: a group of eggs and flour in bowls    Objects: Egg, Egg, Food    Tags: dairy, ingredient, indoor, thickening agent, food, mixing bowl, powder, flour, egg, bowl    Thought: I can use the objects and tags to suggest recipes    Action:    ```    {      "action": "Final Answer",      "action_input": "You can make pancakes, omelettes, or quiches with these ingredients!"    }    ```        > Finished chain.    'You can make pancakes, omelettes, or quiches with these ingredients!'audio_file = agent.run("Tell me a joke and read it out for me.")            > Entering new AgentExecutor chain...    Action:    ```    {      "action": "Azure Cognitive Services Text2Speech",      "action_input": "Why did the chicken cross the playground? To get to the other slide!"    }    ```            Observation: /tmp/tmpa3uu_j6b.wav    Thought: I have the audio file of the joke    Action:    ```    {      "action": "Final Answer",      "action_input": "/tmp/tmpa3uu_j6b.wav"    }    ```        > Finished chain.    '/tmp/tmpa3uu_j6b.wav'from IPython import displayaudio = display.Audio(audio_file)display.display(audio)PreviousAmadeusNextCSVCreate the ToolkitUse within an Agent IntegrationsAgents & ToolkitsAzure Cognitive ServicesOn this pageAzure Cognitive ServicesThis toolkit is used to interact with the Azure Cognitive Services API to achieve some multimodal capabilities.Currently There are four tools bundled in this toolkit:AzureCogsImageAnalysisTool: used to extract caption, objects, tags, and text from images. (Note: this tool is not available on Mac OS yet, due to the dependency on azure-ai-vision package, which is only supported on Windows and Linux currently.)AzureCogsFormRecognizerTool: used to extract text, tables, and key-value pairs from documents.AzureCogsSpeech2TextTool: used to transcribe speech to text.AzureCogsText2SpeechTool: used to synthesize text to speech.First, you need to set up an Azure account and create a Cognitive Services resource. You can follow the instructions here to create a resource. Then, you need to get the endpoint, key and region of your resource, and set them as environment variables. You can find them in the "Keys and Endpoint" page of your resource.# !pip install --upgrade azure-ai-formrecognizer > /dev/null# !pip install --upgrade azure-cognitiveservices-speech > /dev/null# For Windows/Linux# !pip install --upgrade azure-ai-vision > /dev/nullimport osos.environ["OPENAI_API_KEY"] = "sk-"os.environ["AZURE_COGS_KEY"] = ""os.environ["AZURE_COGS_ENDPOINT"] = ""os.environ["AZURE_COGS_REGION"] = ""Create the Toolkit‚Äãfrom langchain.agents.agent_toolkits import AzureCognitiveServicesToolkittoolkit = AzureCognitiveServicesToolkit()API Reference:AzureCognitiveServicesToolkit[tool.name for tool in toolkit.get_tools()]    ['Azure Cognitive Services Image Analysis',     'Azure Cognitive Services Form Recognizer',     'Azure Cognitive Services Speech2Text',     'Azure Cognitive Services Text2Speech']Use within an Agent‚Äãfrom langchain import OpenAIfrom langchain.agents import initialize_agent, AgentTypeAPI Reference:initialize_agentAgentTypellm = OpenAI(temperature=0)agent = initialize_agent(    tools=toolkit.get_tools(),    llm=llm,    agent=AgentType.STRUCTURED_CHAT_ZERO_SHOT_REACT_DESCRIPTION,    verbose=True,)agent.run(    "What can I make with these ingredients?"    "https://images.openai.com/blob/9ad5a2ab-041f-475f-ad6a-b51899c50182/ingredients.png")            > Entering new AgentExecutor chain...        Action:    ```    {      "action": "Azure Cognitive Services Image Analysis",      "action_input": "https://images.openai.com/blob/9ad5a2ab-041f-475f-ad6a-b51899c50182/ingredients.png"    }    ```            Observation: Caption: a group of eggs and flour in bowls    Objects: Egg, Egg, Food    Tags: dairy, ingredient, indoor, thickening agent, food, mixing bowl, powder, flour, egg, bowl    Thought: I can use the objects and tags to suggest recipes    Action:    ```    {      "action": "Final Answer",      "action_input": "You can make pancakes, omelettes, or quiches with these ingredients!"    }    ```        > Finished chain.    'You can make pancakes, omelettes, or quiches with these ingredients!'audio_file = agent.run("Tell me a joke and read it out for me.")            > Entering new AgentExecutor chain...    Action:    ```    {      "action": "Azure Cognitive Services Text2Speech",      "action_input": "Why did the chicken cross the playground? To get to the other slide!"    }    ```            Observation: /tmp/tmpa3uu_j6b.wav    Thought: I have the audio file of the joke    Action:    ```    {      "action": "Final Answer",      "action_input": "/tmp/tmpa3uu_j6b.wav"    }    ```        > Finished chain.    '/tmp/tmpa3uu_j6b.wav'from IPython import displayaudio = display.Audio(audio_file)display.display(audio)PreviousAmadeusNextCSVCreate the ToolkitUse within an Agent IntegrationsAgents & ToolkitsAzure Cognitive ServicesOn this pageAzure Cognitive ServicesThis toolkit is used to interact with the Azure Cognitive Services API to achieve some multimodal capabilities.Currently There are four tools bundled in this toolkit:AzureCogsImageAnalysisTool: used to extract caption, objects, tags, and text from images. (Note: this tool is not available on Mac OS yet, due to the dependency on azure-ai-vision package, which is only supported on Windows and Linux currently.)AzureCogsFormRecognizerTool: used to extract text, tables, and key-value pairs from documents.AzureCogsSpeech2TextTool: used to transcribe speech to text.AzureCogsText2SpeechTool: used to synthesize text to speech.First, you need to set up an Azure account and create a Cognitive Services resource. You can follow the instructions here to create a resource. Then, you need to get the endpoint, key and region of your resource, and set them as environment variables. You can find them in the "Keys and Endpoint" page of your resource.# !pip install --upgrade azure-ai-formrecognizer > /dev/null# !pip install --upgrade azure-cognitiveservices-speech > /dev/null# For Windows/Linux# !pip install --upgrade azure-ai-vision > /dev/nullimport osos.environ["OPENAI_API_KEY"] = "sk-"os.environ["AZURE_COGS_KEY"] = ""os.environ["AZURE_COGS_ENDPOINT"] = ""os.environ["AZURE_COGS_REGION"] = ""Create the Toolkit‚Äãfrom langchain.agents.agent_toolkits import AzureCognitiveServicesToolkittoolkit = AzureCognitiveServicesToolkit()API Reference:AzureCognitiveServicesToolkit[tool.name for tool in toolkit.get_tools()]    ['Azure Cognitive Services Image Analysis',     'Azure Cognitive Services Form Recognizer',     'Azure Cognitive Services Speech2Text',     'Azure Cognitive Services Text2Speech']Use within an Agent‚Äãfrom langchain import OpenAIfrom langchain.agents import initialize_agent, AgentTypeAPI Reference:initialize_agentAgentTypellm = OpenAI(temperature=0)agent = initialize_agent(    tools=toolkit.get_tools(),    llm=llm,    agent=AgentType.STRUCTURED_CHAT_ZERO_SHOT_REACT_DESCRIPTION,    verbose=True,)agent.run(    "What can I make with these ingredients?"    "https://images.openai.com/blob/9ad5a2ab-041f-475f-ad6a-b51899c50182/ingredients.png")            > Entering new AgentExecutor chain...        Action:    ```    {      "action": "Azure Cognitive Services Image Analysis",      "action_input": "https://images.openai.com/blob/9ad5a2ab-041f-475f-ad6a-b51899c50182/ingredients.png"    }    ```            Observation: Caption: a group of eggs and flour in bowls    Objects: Egg, Egg, Food    Tags: dairy, ingredient, indoor, thickening agent, food, mixing bowl, powder, flour, egg, bowl    Thought: I can use the objects and tags to suggest recipes    Action:    ```    {      "action": "Final Answer",      "action_input": "You can make pancakes, omelettes, or quiches with these ingredients!"    }    ```        > Finished chain.    'You can make pancakes, omelettes, or quiches with these ingredients!'audio_file = agent.run("Tell me a joke and read it out for me.")            > Entering new AgentExecutor chain...    Action:    ```    {      "action": "Azure Cognitive Services Text2Speech",      "action_input": "Why did the chicken cross the playground? To get to the other slide!"    }    ```            Observation: /tmp/tmpa3uu_j6b.wav    Thought: I have the audio file of the joke    Action:    ```    {      "action": "Final Answer",      "action_input": "/tmp/tmpa3uu_j6b.wav"    }    ```        > Finished chain.    '/tmp/tmpa3uu_j6b.wav'from IPython import displayaudio = display.Audio(audio_file)display.display(audio)PreviousAmadeusNextCSV IntegrationsAgents & ToolkitsAzure Cognitive ServicesOn this pageAzure Cognitive ServicesThis toolkit is used to interact with the Azure Cognitive Services API to achieve some multimodal capabilities.Currently There are four tools bundled in this toolkit:AzureCogsImageAnalysisTool: used to extract caption, objects, tags, and text from images. (Note: this tool is not available on Mac OS yet, due to the dependency on azure-ai-vision package, which is only supported on Windows and Linux currently.)AzureCogsFormRecognizerTool: used to extract text, tables, and key-value pairs from documents.AzureCogsSpeech2TextTool: used to transcribe speech to text.AzureCogsText2SpeechTool: used to synthesize text to speech.First, you need to set up an Azure account and create a Cognitive Services resource. You can follow the instructions here to create a resource. Then, you need to get the endpoint, key and region of your resource, and set them as environment variables. You can find them in the "Keys and Endpoint" page of your resource.# !pip install --upgrade azure-ai-formrecognizer > /dev/null# !pip install --upgrade azure-cognitiveservices-speech > /dev/null# For Windows/Linux# !pip install --upgrade azure-ai-vision > /dev/nullimport osos.environ["OPENAI_API_KEY"] = "sk-"os.environ["AZURE_COGS_KEY"] = ""os.environ["AZURE_COGS_ENDPOINT"] = ""os.environ["AZURE_COGS_REGION"] = ""Create the Toolkit‚Äãfrom langchain.agents.agent_toolkits import AzureCognitiveServicesToolkittoolkit = AzureCognitiveServicesToolkit()API Reference:AzureCognitiveServicesToolkit[tool.name for tool in toolkit.get_tools()]    ['Azure Cognitive Services Image Analysis',     'Azure Cognitive Services Form Recognizer',     'Azure Cognitive Services Speech2Text',     'Azure Cognitive Services Text2Speech']Use within an Agent‚Äãfrom langchain import OpenAIfrom langchain.agents import initialize_agent, AgentTypeAPI Reference:initialize_agentAgentTypellm = OpenAI(temperature=0)agent = initialize_agent(    tools=toolkit.get_tools(),    llm=llm,    agent=AgentType.STRUCTURED_CHAT_ZERO_SHOT_REACT_DESCRIPTION,    verbose=True,)agent.run(    "What can I make with these ingredients?"    "https://images.openai.com/blob/9ad5a2ab-041f-475f-ad6a-b51899c50182/ingredients.png")            > Entering new AgentExecutor chain...        Action:    ```    {      "action": "Azure Cognitive Services Image Analysis",      "action_input": "https://images.openai.com/blob/9ad5a2ab-041f-475f-ad6a-b51899c50182/ingredients.png"    }    ```            Observation: Caption: a group of eggs and flour in bowls    Objects: Egg, Egg, Food    Tags: dairy, ingredient, indoor, thickening agent, food, mixing bowl, powder, flour, egg, bowl    Thought: I can use the objects and tags to suggest recipes    Action:    ```    {      "action": "Final Answer",      "action_input": "You can make pancakes, omelettes, or quiches with these ingredients!"    }    ```        > Finished chain.    'You can make pancakes, omelettes, or quiches with these ingredients!'audio_file = agent.run("Tell me a joke and read it out for me.")            > Entering new AgentExecutor chain...    Action:    ```    {      "action": "Azure Cognitive Services Text2Speech",      "action_input": "Why did the chicken cross the playground? To get to the other slide!"    }    ```            Observation: /tmp/tmpa3uu_j6b.wav    Thought: I have the audio file of the joke    Action:    ```    {      "action": "Final Answer",      "action_input": "/tmp/tmpa3uu_j6b.wav"    }    ```        > Finished chain.    '/tmp/tmpa3uu_j6b.wav'from IPython import displayaudio = display.Audio(audio_file)display.display(audio)PreviousAmadeusNextCSV On this page Azure Cognitive ServicesThis toolkit is used to interact with the Azure Cognitive Services API to achieve some multimodal capabilities.Currently There are four tools bundled in this toolkit:AzureCogsImageAnalysisTool: used to extract caption, objects, tags, and text from images. (Note: this tool is not available on Mac OS yet, due to the dependency on azure-ai-vision package, which is only supported on Windows and Linux currently.)AzureCogsFormRecognizerTool: used to extract text, tables, and key-value pairs from documents.AzureCogsSpeech2TextTool: used to transcribe speech to text.AzureCogsText2SpeechTool: used to synthesize text to speech.First, you need to set up an Azure account and create a Cognitive Services resource. You can follow the instructions here to create a resource. Then, you need to get the endpoint, key and region of your resource, and set them as environment variables. You can find them in the "Keys and Endpoint" page of your resource.# !pip install --upgrade azure-ai-formrecognizer > /dev/null# !pip install --upgrade azure-cognitiveservices-speech > /dev/null# For Windows/Linux# !pip install --upgrade azure-ai-vision > /dev/nullimport osos.environ["OPENAI_API_KEY"] = "sk-"os.environ["AZURE_COGS_KEY"] = ""os.environ["AZURE_COGS_ENDPOINT"] = ""os.environ["AZURE_COGS_REGION"] = ""Create the Toolkit‚Äãfrom langchain.agents.agent_toolkits import AzureCognitiveServicesToolkittoolkit = AzureCognitiveServicesToolkit()API Reference:AzureCognitiveServicesToolkit[tool.name for tool in toolkit.get_tools()]    ['Azure Cognitive Services Image Analysis',     'Azure Cognitive Services Form Recognizer',     'Azure Cognitive Services Speech2Text',     'Azure Cognitive Services Text2Speech']Use within an Agent‚Äãfrom langchain import OpenAIfrom langchain.agents import initialize_agent, AgentTypeAPI Reference:initialize_agentAgentTypellm = OpenAI(temperature=0)agent = initialize_agent(    tools=toolkit.get_tools(),    llm=llm,    agent=AgentType.STRUCTURED_CHAT_ZERO_SHOT_REACT_DESCRIPTION,    verbose=True,)agent.run(    "What can I make with these ingredients?"    "https://images.openai.com/blob/9ad5a2ab-041f-475f-ad6a-b51899c50182/ingredients.png")            > Entering new AgentExecutor chain...        Action:    ```    {      "action": "Azure Cognitive Services Image Analysis",      "action_input": "https://images.openai.com/blob/9ad5a2ab-041f-475f-ad6a-b51899c50182/ingredients.png"    }    ```            Observation: Caption: a group of eggs and flour in bowls    Objects: Egg, Egg, Food    Tags: dairy, ingredient, indoor, thickening agent, food, mixing bowl, powder, flour, egg, bowl    Thought: I can use the objects and tags to suggest recipes    Action:    ```    {      "action": "Final Answer",      "action_input": "You can make pancakes, omelettes, or quiches with these ingredients!"    }    ```        > Finished chain.    'You can make pancakes, omelettes, or quiches with these ingredients!'audio_file = agent.run("Tell me a joke and read it out for me.")            > Entering new AgentExecutor chain...    Action:    ```    {      "action": "Azure Cognitive Services Text2Speech",      "action_input": "Why did the chicken cross the playground? To get to the other slide!"    }    ```            Observation: /tmp/tmpa3uu_j6b.wav    Thought: I have the audio file of the joke    Action:    ```    {      "action": "Final Answer",      "action_input": "/tmp/tmpa3uu_j6b.wav"    }    ```        > Finished chain.    '/tmp/tmpa3uu_j6b.wav'from IPython import displayaudio = display.Audio(audio_file)display.display(audio) # !pip install --upgrade azure-ai-formrecognizer > /dev/null# !pip install --upgrade azure-cognitiveservices-speech > /dev/null# For Windows/Linux# !pip install --upgrade azure-ai-vision > /dev/null # !pip install --upgrade azure-ai-formrecognizer > /dev/null# !pip install --upgrade azure-cognitiveservices-speech > /dev/null# For Windows/Linux# !pip install --upgrade azure-ai-vision > /dev/null  import osos.environ["OPENAI_API_KEY"] = "sk-"os.environ["AZURE_COGS_KEY"] = ""os.environ["AZURE_COGS_ENDPOINT"] = ""os.environ["AZURE_COGS_REGION"] = "" import osos.environ["OPENAI_API_KEY"] = "sk-"os.environ["AZURE_COGS_KEY"] = ""os.environ["AZURE_COGS_ENDPOINT"] = ""os.environ["AZURE_COGS_REGION"] = ""  from langchain.agents.agent_toolkits import AzureCognitiveServicesToolkittoolkit = AzureCognitiveServicesToolkit() from langchain.agents.agent_toolkits import AzureCognitiveServicesToolkittoolkit = AzureCognitiveServicesToolkit()  API Reference:AzureCognitiveServicesToolkit [tool.name for tool in toolkit.get_tools()] [tool.name for tool in toolkit.get_tools()]      ['Azure Cognitive Services Image Analysis',     'Azure Cognitive Services Form Recognizer',     'Azure Cognitive Services Speech2Text',     'Azure Cognitive Services Text2Speech']     ['Azure Cognitive Services Image Analysis',     'Azure Cognitive Services Form Recognizer',     'Azure Cognitive Services Speech2Text',     'Azure Cognitive Services Text2Speech']     ['Azure Cognitive Services Image Analysis',     'Azure Cognitive Services Form Recognizer',     'Azure Cognitive Services Speech2Text',     'Azure Cognitive Services Text2Speech']  from langchain import OpenAIfrom langchain.agents import initialize_agent, AgentType from langchain import OpenAIfrom langchain.agents import initialize_agent, AgentType  API Reference:initialize_agentAgentType llm = OpenAI(temperature=0)agent = initialize_agent(    tools=toolkit.get_tools(),    llm=llm,    agent=AgentType.STRUCTURED_CHAT_ZERO_SHOT_REACT_DESCRIPTION,    verbose=True,) llm = OpenAI(temperature=0)agent = initialize_agent(    tools=toolkit.get_tools(),    llm=llm,    agent=AgentType.STRUCTURED_CHAT_ZERO_SHOT_REACT_DESCRIPTION,    verbose=True,)  agent.run(    "What can I make with these ingredients?"    "https://images.openai.com/blob/9ad5a2ab-041f-475f-ad6a-b51899c50182/ingredients.png") agent.run(    "What can I make with these ingredients?"    "https://images.openai.com/blob/9ad5a2ab-041f-475f-ad6a-b51899c50182/ingredients.png")              > Entering new AgentExecutor chain...        Action:    ```    {      "action": "Azure Cognitive Services Image Analysis",      "action_input": "https://images.openai.com/blob/9ad5a2ab-041f-475f-ad6a-b51899c50182/ingredients.png"    }    ```            Observation: Caption: a group of eggs and flour in bowls    Objects: Egg, Egg, Food    Tags: dairy, ingredient, indoor, thickening agent, food, mixing bowl, powder, flour, egg, bowl    Thought: I can use the objects and tags to suggest recipes    Action:    ```    {      "action": "Final Answer",      "action_input": "You can make pancakes, omelettes, or quiches with these ingredients!"    }    ```        > Finished chain.    'You can make pancakes, omelettes, or quiches with these ingredients!'             > Entering new AgentExecutor chain...        Action:    ```    {      "action": "Azure Cognitive Services Image Analysis",      "action_input": "https://images.openai.com/blob/9ad5a2ab-041f-475f-ad6a-b51899c50182/ingredients.png"    }    ```            Observation: Caption: a group of eggs and flour in bowls    Objects: Egg, Egg, Food    Tags: dairy, ingredient, indoor, thickening agent, food, mixing bowl, powder, flour, egg, bowl    Thought: I can use the objects and tags to suggest recipes    Action:    ```    {      "action": "Final Answer",      "action_input": "You can make pancakes, omelettes, or quiches with these ingredients!"    }    ```        > Finished chain.    'You can make pancakes, omelettes, or quiches with these ingredients!'             > Entering new AgentExecutor chain...        Action:    ```    {      "action": "Azure Cognitive Services Image Analysis",      "action_input": "https://images.openai.com/blob/9ad5a2ab-041f-475f-ad6a-b51899c50182/ingredients.png"    }    ```            Observation: Caption: a group of eggs and flour in bowls    Objects: Egg, Egg, Food    Tags: dairy, ingredient, indoor, thickening agent, food, mixing bowl, powder, flour, egg, bowl    Thought: I can use the objects and tags to suggest recipes    Action:    ```    {      "action": "Final Answer",      "action_input": "You can make pancakes, omelettes, or quiches with these ingredients!"    }    ```        > Finished chain.    'You can make pancakes, omelettes, or quiches with these ingredients!'  audio_file = agent.run("Tell me a joke and read it out for me.") audio_file = agent.run("Tell me a joke and read it out for me.")              > Entering new AgentExecutor chain...    Action:    ```    {      "action": "Azure Cognitive Services Text2Speech",      "action_input": "Why did the chicken cross the playground? To get to the other slide!"    }    ```            Observation: /tmp/tmpa3uu_j6b.wav    Thought: I have the audio file of the joke    Action:    ```    {      "action": "Final Answer",      "action_input": "/tmp/tmpa3uu_j6b.wav"    }    ```        > Finished chain.    '/tmp/tmpa3uu_j6b.wav'             > Entering new AgentExecutor chain...    Action:    ```    {      "action": "Azure Cognitive Services Text2Speech",      "action_input": "Why did the chicken cross the playground? To get to the other slide!"    }    ```            Observation: /tmp/tmpa3uu_j6b.wav    Thought: I have the audio file of the joke    Action:    ```    {      "action": "Final Answer",      "action_input": "/tmp/tmpa3uu_j6b.wav"    }    ```        > Finished chain.    '/tmp/tmpa3uu_j6b.wav'             > Entering new AgentExecutor chain...    Action:    ```    {      "action": "Azure Cognitive Services Text2Speech",      "action_input": "Why did the chicken cross the playground? To get to the other slide!"    }    ```            Observation: /tmp/tmpa3uu_j6b.wav    Thought: I have the audio file of the joke    Action:    ```    {      "action": "Final Answer",      "action_input": "/tmp/tmpa3uu_j6b.wav"    }    ```        > Finished chain.    '/tmp/tmpa3uu_j6b.wav'  from IPython import displayaudio = display.Audio(audio_file)display.display(audio) from IPython import displayaudio = display.Audio(audio_file)display.display(audio)  Previous Amadeus Next CSV Create the ToolkitUse within an Agent Create the ToolkitUse within an Agent CommunityDiscordTwitterGitHubPythonJS/TSMoreHomepageBlogCopyright ¬© 2023 LangChain, Inc. CommunityDiscordTwitterGitHubPythonJS/TSMoreHomepageBlog CommunityDiscordTwitter Community GitHubPythonJS/TS GitHub MoreHomepageBlog More Copyright ¬© 2023 LangChain, Inc. Copyright ¬© 2023 LangChain, Inc. Skip to main content (#docusaurus_skipToContent_fallback) ü¶úÔ∏èüîó LangChain (/) Docs (/docs/get_started/introduction) Use cases (/docs/use_cases) Integrations (/docs/integrations) API (https://api.python.langchain.com) LangSmith (https://smith.langchain.com) JS/TS Docs (https://js.langchain.com/docs)  (https://github.com/hwchase17/langchain) Integrations (/docs/integrations) Callbacks (/docs/integrations/callbacks/) Chat models (/docs/integrations/chat/) Chat loaders (/docs/integrations/chat_loaders/) Document loaders (/docs/integrations/document_loaders/) Document transformers (/docs/integrations/document_transformers/) LLMs (/docs/integrations/llms/) Memory (/docs/integrations/memory/) Retrievers (/docs/integrations/retrievers/) Text embedding models (/docs/integrations/text_embedding/) Agents & Toolkits (/docs/integrations/toolkits/) AINetwork (/docs/integrations/toolkits/ainetwork) Airbyte Question Answering (/docs/integrations/toolkits/airbyte_structured_qa) Amadeus (/docs/integrations/toolkits/amadeus) Azure Cognitive Services (/docs/integrations/toolkits/azure_cognitive_services) CSV (/docs/integrations/toolkits/csv) Document Comparison (/docs/integrations/toolkits/document_comparison_toolkit) Github (/docs/integrations/toolkits/github) Gitlab (/docs/integrations/toolkits/gitlab) Gmail (/docs/integrations/toolkits/gmail) Google Drive tool (/docs/integrations/toolkits/google_drive) Jira (/docs/integrations/toolkits/jira) JSON (/docs/integrations/toolkits/json) MultiOn (/docs/integrations/toolkits/multion) Office365 (/docs/integrations/toolkits/office365) OpenAPI (/docs/integrations/toolkits/openapi) Natural Language APIs (/docs/integrations/toolkits/openapi_nla) Pandas Dataframe (/docs/integrations/toolkits/pandas) PlayWright Browser (/docs/integrations/toolkits/playwright) PowerBI Dataset (/docs/integrations/toolkits/powerbi) Python (/docs/integrations/toolkits/python) Spark Dataframe (/docs/integrations/toolkits/spark) Spark SQL (/docs/integrations/toolkits/spark_sql) SQL Database (/docs/integrations/toolkits/sql_database) Vectorstore (/docs/integrations/toolkits/vectorstore) Xorbits (/docs/integrations/toolkits/xorbits) Tools (/docs/integrations/tools/) Vector stores (/docs/integrations/vectorstores/) Grouped by provider (/docs/integrations/providers/)  (/) Integrations (/docs/integrations) Agents & Toolkits (/docs/integrations/toolkits/) here (https://docs.microsoft.com/en-us/azure/cognitive-services/cognitive-services-apis-create-account?tabs=multiservice%2Cwindows) ‚Äã (#create-the-toolkit) AzureCognitiveServicesToolkit (https://api.python.langchain.com/en/latest/agents/langchain.agents.agent_toolkits.azure_cognitive_services.AzureCognitiveServicesToolkit.html) ‚Äã (#use-within-an-agent) initialize_agent (https://api.python.langchain.com/en/latest/agents/langchain.agents.initialize.initialize_agent.html) AgentType (https://api.python.langchain.com/en/latest/agents/langchain.agents.agent_types.AgentType.html) PreviousAmadeus (/docs/integrations/toolkits/amadeus) NextCSV (/docs/integrations/toolkits/csv) Create the Toolkit (#create-the-toolkit) Use within an Agent (#use-within-an-agent) Discord (https://discord.gg/cU2adEyC7w) Twitter (https://twitter.com/LangChainAI) Python (https://github.com/hwchase17/langchain) JS/TS (https://github.com/hwchase17/langchainjs) Homepage (https://langchain.com) Blog (https://blog.langchain.dev)