llama-cpp-python is a Python binding for llama.cpp.  It supports inference for many LLMs, which can be accessed on HuggingFace. This notebook goes over how to run llama-cpp-python within LangChain. Note: new versions of llama-cpp-python use GGUF model files (see here). This is a breaking change. To convert existing GGML models to GGUF you can run the following in llama.cpp: There are different options on how to install the llama-cpp package:  lama.cpp supports multiple BLAS backends for faster processing. Use the FORCE_CMAKE=1 environment variable to force the use of cmake and install the pip package for the desired BLAS backend (source). Example installation with cuBLAS backend: IMPORTANT: If you have already installed the CPU only version of the package, you need to reinstall it from scratch. Consider the following command:  llama.cpp supports Apple silicon first-class citizen - optimized via ARM NEON, Accelerate and Metal frameworks. Use the FORCE_CMAKE=1 environment variable to force the use of cmake and install the pip package for the Metal support (source). Example installation with Metal Support: IMPORTANT: If you have already installed a cpu only version of the package, you need to reinstall it from scratch: consider the following command:  It is stable to install the llama-cpp-python library by compiling from the source. You can follow most of the instructions in the repository itself but there are some windows specific instructions which might be useful. Requirements to install the llama-cpp-python, You can ignore the second environment variable if you have an NVIDIA GPU. In the same command prompt (anaconda prompt) you set the variables, you can cd into llama-cpp-python directory and run the following commands. Make sure you are following all instructions to install all necessary model files. You don't need an API_TOKEN as you will run the LLM locally. It is worth understanding which models are suitable to be used on the desired machine. Consider using a template that suits your model! Check the models page on HuggingFace etc. to get a correct prompting template. Example using a LLaMA 2 7B model Example using a LLaMA v1 model If the installation with BLAS backend was correct, you will see a BLAS = 1 indicator in model properties. Two of the most important parameters for use with GPU are: Setting these parameters correctly will dramatically improve the evaluation speed (see wrapper code for more details). If the installation with Metal was correct, you will see a NEON = 1 indicator in model properties. Two of the most important GPU parameters are: Setting these parameters correctly will dramatically improve the evaluation speed (see wrapper code for more details). The console log will show the following log to indicate Metal was enable properly. You also could check Activity Monitor by watching the GPU usage of the process, the CPU usage will drop dramatically after turn on n_gpu_layers=1.  For the first call to the LLM, the performance may be slow due to the model compilation in Metal GPU. We can specify grammars to constrain model outputs. This will sample tokens according to the grammar. For example, supply the path to the specifed json.gbnf file in order to produce JSON. We can also supply list.gbnf to return a list. IntegrationsCallbacksChat modelsChat loadersDocument loadersDocument transformersLLMsAI21Aleph AlphaAmazon API GatewayAnyscaleAzure MLAzure OpenAIBananaBasetenBeamBedrockBittensorCerebriumAIChatGLMClarifaiCohereC TransformersCTranslate2DatabricksDeepInfraDeepSparseEden AIFireworksForefrontAIGoogle Vertex AI PaLMGooseAIGPT4AllHugging Face HubHugging Face Local PipelinesHuggingface TextGen InferenceJSONFormerKoboldAI APILlama.cppLLM Caching integrationsManifestMinimaxModalMosaicMLNLP CloudOctoAIOllamaOpaquePromptsOpenAIOpenLLMOpenLMPetalsPipelineAIPredibasePrediction GuardPromptLayer OpenAIRELLMReplicateRunhouseSageMakerEndpointStochasticAINebula (Symbl.ai)TextGenTitan TakeoffTongyi QwenvLLMWriterXorbits Inference (Xinference)MemoryRetrieversText embedding modelsAgents & ToolkitsToolsVector storesGrouped by provider Callbacks Chat models Chat loaders Document loaders Document transformers LLMsAI21Aleph AlphaAmazon API GatewayAnyscaleAzure MLAzure OpenAIBananaBasetenBeamBedrockBittensorCerebriumAIChatGLMClarifaiCohereC TransformersCTranslate2DatabricksDeepInfraDeepSparseEden AIFireworksForefrontAIGoogle Vertex AI PaLMGooseAIGPT4AllHugging Face HubHugging Face Local PipelinesHuggingface TextGen InferenceJSONFormerKoboldAI APILlama.cppLLM Caching integrationsManifestMinimaxModalMosaicMLNLP CloudOctoAIOllamaOpaquePromptsOpenAIOpenLLMOpenLMPetalsPipelineAIPredibasePrediction GuardPromptLayer OpenAIRELLMReplicateRunhouseSageMakerEndpointStochasticAINebula (Symbl.ai)TextGenTitan TakeoffTongyi QwenvLLMWriterXorbits Inference (Xinference) AI21 Aleph Alpha Amazon API Gateway Anyscale Azure ML Azure OpenAI Banana Baseten Beam Bedrock Bittensor CerebriumAI ChatGLM Clarifai Cohere C Transformers CTranslate2 Databricks DeepInfra DeepSparse Eden AI Fireworks ForefrontAI Google Vertex AI PaLM GooseAI GPT4All Hugging Face Hub Hugging Face Local Pipelines Huggingface TextGen Inference JSONFormer KoboldAI API Llama.cpp LLM Caching integrations Manifest Minimax Modal MosaicML NLP Cloud OctoAI Ollama OpaquePrompts OpenAI OpenLLM OpenLM Petals PipelineAI Predibase Prediction Guard PromptLayer OpenAI RELLM Replicate Runhouse SageMakerEndpoint StochasticAI Nebula (Symbl.ai) TextGen Titan Takeoff Tongyi Qwen vLLM Writer Xorbits Inference (Xinference) Memory Retrievers Text embedding models Agents & Toolkits Tools Vector stores Grouped by provider  Integrations LLMs Llama.cpp CPU usage CPU + GPU (using one of many BLAS backends) Metal GPU (MacOS with Apple Silicon Chip)  git python cmake Visual Studio Community (make sure you install this with the following settings)Desktop development with C++Python developmentLinux embedded development with C++ Desktop development with C++ Python development Linux embedded development with C++ Clone git repository recursively to get llama.cpp submodule as well  Open up command Prompt (or anaconda prompt if you have it installed), set up environment variables to install. Follow this if you do not have a GPU, you must set both of the following variables. LlamaCpp CallbackManager StreamingStdOutCallbackHandler n_gpu_layers - determines how many layers of the model are offloaded to your GPU. n_batch - how many tokens are processed in parallel.  n_gpu_layers - determines how many layers of the model are offloaded to your Metal GPU, in the most case, set it to 1 is enough for Metal n_batch - how many tokens are processed in parallel, default is 8, set to bigger number. f16_kv - for some reason, Metal only support True, otherwise you will get error such as Asserting on type 0 GGML_ASSERT: .../ggml-metal.m:706: false && "not implemented" InstallationCPU only installationInstallation with OpenBLAS / cuBLAS / CLBlastInstallation with MetalInstallation with Windows CPU only installation Installation with OpenBLAS / cuBLAS / CLBlast Installation with Metal Installation with Windows UsageCPUGPUMetalGrammars CPU GPU Metal Grammars Discord Twitter Python JS/TS Homepage Blog Skip to main contentü¶úÔ∏èüîó LangChainDocsUse casesIntegrationsAPILangSmithJS/TS DocsCTRLKIntegrationsCallbacksChat modelsChat loadersDocument loadersDocument transformersLLMsAI21Aleph AlphaAmazon API GatewayAnyscaleAzure MLAzure OpenAIBananaBasetenBeamBedrockBittensorCerebriumAIChatGLMClarifaiCohereC TransformersCTranslate2DatabricksDeepInfraDeepSparseEden AIFireworksForefrontAIGoogle Vertex AI PaLMGooseAIGPT4AllHugging Face HubHugging Face Local PipelinesHuggingface TextGen InferenceJSONFormerKoboldAI APILlama.cppLLM Caching integrationsManifestMinimaxModalMosaicMLNLP CloudOctoAIOllamaOpaquePromptsOpenAIOpenLLMOpenLMPetalsPipelineAIPredibasePrediction GuardPromptLayer OpenAIRELLMReplicateRunhouseSageMakerEndpointStochasticAINebula (Symbl.ai)TextGenTitan TakeoffTongyi QwenvLLMWriterXorbits Inference (Xinference)MemoryRetrieversText embedding modelsAgents & ToolkitsToolsVector storesGrouped by providerIntegrationsLLMsLlama.cppOn this pageLlama.cppllama-cpp-python is a Python binding for llama.cpp. It supports inference for many LLMs, which can be accessed on HuggingFace.This notebook goes over how to run llama-cpp-python within LangChain.Note: new versions of llama-cpp-python use GGUF model files (see here).This is a breaking change.To convert existing GGML models to GGUF you can run the following in llama.cpp:python ./convert-llama-ggmlv3-to-gguf.py --eps 1e-5 --input models/openorca-platypus2-13b.ggmlv3.q4_0.bin --output models/openorca-platypus2-13b.gguf.q4_0.binInstallation‚ÄãThere are different options on how to install the llama-cpp package: CPU usageCPU + GPU (using one of many BLAS backends)Metal GPU (MacOS with Apple Silicon Chip) CPU only installation‚Äãpip install llama-cpp-pythonInstallation with OpenBLAS / cuBLAS / CLBlast‚Äãlama.cpp supports multiple BLAS backends for faster processing. Use the FORCE_CMAKE=1 environment variable to force the use of cmake and install the pip package for the desired BLAS backend (source).Example installation with cuBLAS backend:CMAKE_ARGS="-DLLAMA_CUBLAS=on" FORCE_CMAKE=1 pip install llama-cpp-pythonIMPORTANT: If you have already installed the CPU only version of the package, you need to reinstall it from scratch. Consider the following command: CMAKE_ARGS="-DLLAMA_CUBLAS=on" FORCE_CMAKE=1 pip install --upgrade --force-reinstall llama-cpp-python --no-cache-dirInstallation with Metal‚Äãllama.cpp supports Apple silicon first-class citizen - optimized via ARM NEON, Accelerate and Metal frameworks. Use the FORCE_CMAKE=1 environment variable to force the use of cmake and install the pip package for the Metal support (source).Example installation with Metal Support:CMAKE_ARGS="-DLLAMA_METAL=on" FORCE_CMAKE=1 pip install llama-cpp-pythonIMPORTANT: If you have already installed a cpu only version of the package, you need to reinstall it from scratch: consider the following command: CMAKE_ARGS="-DLLAMA_METAL=on" FORCE_CMAKE=1 pip install --upgrade --force-reinstall llama-cpp-python --no-cache-dirInstallation with Windows‚ÄãIt is stable to install the llama-cpp-python library by compiling from the source. You can follow most of the instructions in the repository itself but there are some windows specific instructions which might be useful.Requirements to install the llama-cpp-python,gitpythoncmakeVisual Studio Community (make sure you install this with the following settings)Desktop development with C++Python developmentLinux embedded development with C++Clone git repository recursively to get llama.cpp submodule as well git clone --recursive -j8 https://github.com/abetlen/llama-cpp-python.gitOpen up command Prompt (or anaconda prompt if you have it installed), set up environment variables to install. Follow this if you do not have a GPU, you must set both of the following variables.set FORCE_CMAKE=1set CMAKE_ARGS=-DLLAMA_CUBLAS=OFFYou can ignore the second environment variable if you have an NVIDIA GPU.Compiling and installing‚ÄãIn the same command prompt (anaconda prompt) you set the variables, you can cd into llama-cpp-python directory and run the following commands.python setup.py cleanpython setup.py installUsage‚ÄãMake sure you are following all instructions to install all necessary model files.You don't need an API_TOKEN as you will run the LLM locally.It is worth understanding which models are suitable to be used on the desired machine.from langchain.llms import LlamaCppfrom langchain import PromptTemplate, LLMChainfrom langchain.callbacks.manager import CallbackManagerfrom langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandlerAPI Reference:LlamaCppCallbackManagerStreamingStdOutCallbackHandlerConsider using a template that suits your model! Check the models page on HuggingFace etc. to get a correct prompting template.template = """Question: {question}Answer: Let's work this out in a step by step way to be sure we have the right answer."""prompt = PromptTemplate(template=template, input_variables=["question"])# Callbacks support token-wise streamingcallback_manager = CallbackManager([StreamingStdOutCallbackHandler()])CPU‚ÄãExample using a LLaMA 2 7B model# Make sure the model path is correct for your system!llm = LlamaCpp(    model_path="/Users/rlm/Desktop/Code/llama.cpp/models/openorca-platypus2-13b.gguf.q4_0.bin",    temperature=0.75,    max_tokens=2000,    top_p=1,    callback_manager=callback_manager,     verbose=True, # Verbose is required to pass to the callback manager)prompt = """Question: A rap battle between Stephen Colbert and John Oliver"""llm(prompt)        Stephen Colbert:    Yo, John, I heard you've been talkin' smack about me on your show.    Let me tell you somethin', pal, I'm the king of late-night TV    My satire is sharp as a razor, it cuts deeper than a knife    While you're just a british bloke tryin' to be funny with your accent and your wit.    John Oliver:    Oh Stephen, don't be ridiculous, you may have the ratings but I got the real talk.    My show is the one that people actually watch and listen to, not just for the laughs but for the facts.    While you're busy talkin' trash, I'm out here bringing the truth to light.    Stephen Colbert:    Truth? Ha! You think your show is about truth? Please, it's all just a joke to you.    You're just a fancy-pants british guy tryin' to be funny with your news and your jokes.    While I'm the one who's really makin' a difference, with my sat        llama_print_timings:        load time =   358.60 ms    llama_print_timings:      sample time =   172.55 ms /   256 runs   (    0.67 ms per token,  1483.59 tokens per second)    llama_print_timings: prompt eval time =   613.36 ms /    16 tokens (   38.33 ms per token,    26.09 tokens per second)    llama_print_timings:        eval time = 10151.17 ms /   255 runs   (   39.81 ms per token,    25.12 tokens per second)    llama_print_timings:       total time = 11332.41 ms    "\nStephen Colbert:\nYo, John, I heard you've been talkin' smack about me on your show.\nLet me tell you somethin', pal, I'm the king of late-night TV\nMy satire is sharp as a razor, it cuts deeper than a knife\nWhile you're just a british bloke tryin' to be funny with your accent and your wit.\nJohn Oliver:\nOh Stephen, don't be ridiculous, you may have the ratings but I got the real talk.\nMy show is the one that people actually watch and listen to, not just for the laughs but for the facts.\nWhile you're busy talkin' trash, I'm out here bringing the truth to light.\nStephen Colbert:\nTruth? Ha! You think your show is about truth? Please, it's all just a joke to you.\nYou're just a fancy-pants british guy tryin' to be funny with your news and your jokes.\nWhile I'm the one who's really makin' a difference, with my sat"Example using a LLaMA v1 model# Make sure the model path is correct for your system!llm = LlamaCpp(    model_path="./ggml-model-q4_0.bin", callback_manager=callback_manager, verbose=True)llm_chain = LLMChain(prompt=prompt, llm=llm)question = "What NFL team won the Super Bowl in the year Justin Bieber was born?"llm_chain.run(question)            1. First, find out when Justin Bieber was born.    2. We know that Justin Bieber was born on March 1, 1994.    3. Next, we need to look up when the Super Bowl was played in that year.    4. The Super Bowl was played on January 28, 1995.    5. Finally, we can use this information to answer the question. The NFL team that won the Super Bowl in the year Justin Bieber was born is the San Francisco 49ers.        llama_print_timings:        load time =   434.15 ms    llama_print_timings:      sample time =    41.81 ms /   121 runs   (    0.35 ms per token)    llama_print_timings: prompt eval time =  2523.78 ms /    48 tokens (   52.58 ms per token)    llama_print_timings:        eval time = 23971.57 ms /   121 runs   (  198.11 ms per token)    llama_print_timings:       total time = 28945.95 ms    '\n\n1. First, find out when Justin Bieber was born.\n2. We know that Justin Bieber was born on March 1, 1994.\n3. Next, we need to look up when the Super Bowl was played in that year.\n4. The Super Bowl was played on January 28, 1995.\n5. Finally, we can use this information to answer the question. The NFL team that won the Super Bowl in the year Justin Bieber was born is the San Francisco 49ers.'GPU‚ÄãIf the installation with BLAS backend was correct, you will see a BLAS = 1 indicator in model properties.Two of the most important parameters for use with GPU are:n_gpu_layers - determines how many layers of the model are offloaded to your GPU.n_batch - how many tokens are processed in parallel. Setting these parameters correctly will dramatically improve the evaluation speed (see wrapper code for more details).n_gpu_layers = 40  # Change this value based on your model and your GPU VRAM pool.n_batch = 512  # Should be between 1 and n_ctx, consider the amount of VRAM in your GPU.# Make sure the model path is correct for your system!llm = LlamaCpp(    model_path="/Users/rlm/Desktop/Code/llama.cpp/models/openorca-platypus2-13b.gguf.q4_0.bin",    n_gpu_layers=n_gpu_layers,    n_batch=n_batch,    callback_manager=callback_manager,    verbose=True, # Verbose is required to pass to the callback manager)llm_chain = LLMChain(prompt=prompt, llm=llm)question = "What NFL team won the Super Bowl in the year Justin Bieber was born?"llm_chain.run(question)            1. Identify Justin Bieber's birth date: Justin Bieber was born on March 1, 1994.        2. Find the Super Bowl winner of that year: The NFL season of 1993 with the Super Bowl being played in January or of 1994.        3. Determine which team won the game: The Dallas Cowboys faced the Buffalo Bills in Super Bowl XXVII on January 31, 1993 (as the year is mis-labelled due to a error). The Dallas Cowboys won this matchup.        So, Justin Bieber was born when the Dallas Cowboys were the reigning NFL Super Bowl.        llama_print_timings:        load time =   427.63 ms    llama_print_timings:      sample time =   115.85 ms /   164 runs   (    0.71 ms per token,  1415.67 tokens per second)    llama_print_timings: prompt eval time =   427.53 ms /    45 tokens (    9.50 ms per token,   105.26 tokens per second)    llama_print_timings:        eval time =  4526.53 ms /   163 runs   (   27.77 ms per token,    36.01 tokens per second)    llama_print_timings:       total time =  5293.77 ms    "\n\n1. Identify Justin Bieber's birth date: Justin Bieber was born on March 1, 1994.\n\n2. Find the Super Bowl winner of that year: The NFL season of 1993 with the Super Bowl being played in January or of 1994.\n\n3. Determine which team won the game: The Dallas Cowboys faced the Buffalo Bills in Super Bowl XXVII on January 31, 1993 (as the year is mis-labelled due to a error). The Dallas Cowboys won this matchup.\n\nSo, Justin Bieber was born when the Dallas Cowboys were the reigning NFL Super Bowl."Metal‚ÄãIf the installation with Metal was correct, you will see a NEON = 1 indicator in model properties.Two of the most important GPU parameters are:n_gpu_layers - determines how many layers of the model are offloaded to your Metal GPU, in the most case, set it to 1 is enough for Metaln_batch - how many tokens are processed in parallel, default is 8, set to bigger number.f16_kv - for some reason, Metal only support True, otherwise you will get error such as Asserting on type 0 GGML_ASSERT: .../ggml-metal.m:706: false && "not implemented"Setting these parameters correctly will dramatically improve the evaluation speed (see wrapper code for more details).n_gpu_layers = 1  # Metal set to 1 is enough.n_batch = 512  # Should be between 1 and n_ctx, consider the amount of RAM of your Apple Silicon Chip.# Make sure the model path is correct for your system!llm = LlamaCpp(    model_path="/Users/rlm/Desktop/Code/llama.cpp/models/openorca-platypus2-13b.gguf.q4_0.bin",    n_gpu_layers=n_gpu_layers,    n_batch=n_batch,    f16_kv=True,  # MUST set to True, otherwise you will run into problem after a couple of calls    callback_manager=callback_manager,    verbose=True, # Verbose is required to pass to the callback manager)The console log will show the following log to indicate Metal was enable properly.ggml_metal_init: allocatingggml_metal_init: using MPS...You also could check Activity Monitor by watching the GPU usage of the process, the CPU usage will drop dramatically after turn on n_gpu_layers=1. For the first call to the LLM, the performance may be slow due to the model compilation in Metal GPU.Grammars‚ÄãWe can specify grammars to constrain model outputs.This will sample tokens according to the grammar.For example, supply the path to the specifed json.gbnf file in order to produce JSON.n_gpu_layers = 1  # Metal set to 1 is enough.n_batch = 512  # Should be between 1 and n_ctx, consider the amount of RAM of your Apple Silicon Chip.# Make sure the model path is correct for your system!llm = LlamaCpp(    model_path="/Users/rlm/Desktop/Code/llama.cpp/models/openorca-platypus2-13b.gguf.q4_0.bin",    n_gpu_layers=n_gpu_layers,    n_batch=n_batch,    f16_kv=True,  # MUST set to True, otherwise you will run into problem after a couple of calls    callback_manager=callback_manager,    verbose=True, # Verbose is required to pass to the callback manager    grammar_path="/Users/rlm/Desktop/Code/langchain-main/langchain/libs/langchain/langchain/llms/grammars/json.gbnf",)result=llm("Describe a person in JSON format:")    {      "name": "John Doe",      "age": 34,      "": {        "title": "Software Developer",        "company": "Google"      },      "interests": [        "Sports",        "Music",        "Cooking"      ],      "address": {        "street_number": 123,        "street_name": "Oak Street",        "city": "Mountain View",        "state": "California",        "postal_code": 94040      }}        llama_print_timings:        load time =   357.51 ms    llama_print_timings:      sample time =  1213.30 ms /   144 runs   (    8.43 ms per token,   118.68 tokens per second)    llama_print_timings: prompt eval time =   356.78 ms /     9 tokens (   39.64 ms per token,    25.23 tokens per second)    llama_print_timings:        eval time =  3947.16 ms /   143 runs   (   27.60 ms per token,    36.23 tokens per second)    llama_print_timings:       total time =  5846.21 msWe can also supply list.gbnf to return a list.n_gpu_layers = 1 n_batch = 512llm = LlamaCpp(    model_path="/Users/rlm/Desktop/Code/llama.cpp/models/openorca-platypus2-13b.gguf.q4_0.bin",    n_gpu_layers=n_gpu_layers,    n_batch=n_batch,    f16_kv=True,  # MUST set to True, otherwise you will run into problem after a couple of calls    callback_manager=callback_manager,    verbose=True,    grammar_path="/Users/rlm/Desktop/Code/langchain-main/langchain/libs/langchain/langchain/llms/grammars/list.gbnf",)result=llm("List of top-3 my favourite books:")    ["The Catcher in the Rye", "Wuthering Heights", "Anna Karenina"]        llama_print_timings:        load time =   322.34 ms    llama_print_timings:      sample time =   232.60 ms /    26 runs   (    8.95 ms per token,   111.78 tokens per second)    llama_print_timings: prompt eval time =   321.90 ms /    11 tokens (   29.26 ms per token,    34.17 tokens per second)    llama_print_timings:        eval time =   680.82 ms /    25 runs   (   27.23 ms per token,    36.72 tokens per second)    llama_print_timings:       total time =  1295.27 msPreviousKoboldAI APINextLLM Caching integrationsInstallationCPU only installationInstallation with OpenBLAS / cuBLAS / CLBlastInstallation with MetalInstallation with WindowsUsageCPUGPUMetalGrammarsCommunityDiscordTwitterGitHubPythonJS/TSMoreHomepageBlogCopyright ¬© 2023 LangChain, Inc. Skip to main content ü¶úÔ∏èüîó LangChainDocsUse casesIntegrationsAPILangSmithJS/TS DocsCTRLK ü¶úÔ∏èüîó LangChainDocsUse casesIntegrationsAPI LangSmithJS/TS DocsCTRLK  CTRLK CTRLK  CTRLK   IntegrationsCallbacksChat modelsChat loadersDocument loadersDocument transformersLLMsAI21Aleph AlphaAmazon API GatewayAnyscaleAzure MLAzure OpenAIBananaBasetenBeamBedrockBittensorCerebriumAIChatGLMClarifaiCohereC TransformersCTranslate2DatabricksDeepInfraDeepSparseEden AIFireworksForefrontAIGoogle Vertex AI PaLMGooseAIGPT4AllHugging Face HubHugging Face Local PipelinesHuggingface TextGen InferenceJSONFormerKoboldAI APILlama.cppLLM Caching integrationsManifestMinimaxModalMosaicMLNLP CloudOctoAIOllamaOpaquePromptsOpenAIOpenLLMOpenLMPetalsPipelineAIPredibasePrediction GuardPromptLayer OpenAIRELLMReplicateRunhouseSageMakerEndpointStochasticAINebula (Symbl.ai)TextGenTitan TakeoffTongyi QwenvLLMWriterXorbits Inference (Xinference)MemoryRetrieversText embedding modelsAgents & ToolkitsToolsVector storesGrouped by providerIntegrationsLLMsLlama.cppOn this pageLlama.cppllama-cpp-python is a Python binding for llama.cpp. It supports inference for many LLMs, which can be accessed on HuggingFace.This notebook goes over how to run llama-cpp-python within LangChain.Note: new versions of llama-cpp-python use GGUF model files (see here).This is a breaking change.To convert existing GGML models to GGUF you can run the following in llama.cpp:python ./convert-llama-ggmlv3-to-gguf.py --eps 1e-5 --input models/openorca-platypus2-13b.ggmlv3.q4_0.bin --output models/openorca-platypus2-13b.gguf.q4_0.binInstallation‚ÄãThere are different options on how to install the llama-cpp package: CPU usageCPU + GPU (using one of many BLAS backends)Metal GPU (MacOS with Apple Silicon Chip) CPU only installation‚Äãpip install llama-cpp-pythonInstallation with OpenBLAS / cuBLAS / CLBlast‚Äãlama.cpp supports multiple BLAS backends for faster processing. Use the FORCE_CMAKE=1 environment variable to force the use of cmake and install the pip package for the desired BLAS backend (source).Example installation with cuBLAS backend:CMAKE_ARGS="-DLLAMA_CUBLAS=on" FORCE_CMAKE=1 pip install llama-cpp-pythonIMPORTANT: If you have already installed the CPU only version of the package, you need to reinstall it from scratch. Consider the following command: CMAKE_ARGS="-DLLAMA_CUBLAS=on" FORCE_CMAKE=1 pip install --upgrade --force-reinstall llama-cpp-python --no-cache-dirInstallation with Metal‚Äãllama.cpp supports Apple silicon first-class citizen - optimized via ARM NEON, Accelerate and Metal frameworks. Use the FORCE_CMAKE=1 environment variable to force the use of cmake and install the pip package for the Metal support (source).Example installation with Metal Support:CMAKE_ARGS="-DLLAMA_METAL=on" FORCE_CMAKE=1 pip install llama-cpp-pythonIMPORTANT: If you have already installed a cpu only version of the package, you need to reinstall it from scratch: consider the following command: CMAKE_ARGS="-DLLAMA_METAL=on" FORCE_CMAKE=1 pip install --upgrade --force-reinstall llama-cpp-python --no-cache-dirInstallation with Windows‚ÄãIt is stable to install the llama-cpp-python library by compiling from the source. You can follow most of the instructions in the repository itself but there are some windows specific instructions which might be useful.Requirements to install the llama-cpp-python,gitpythoncmakeVisual Studio Community (make sure you install this with the following settings)Desktop development with C++Python developmentLinux embedded development with C++Clone git repository recursively to get llama.cpp submodule as well git clone --recursive -j8 https://github.com/abetlen/llama-cpp-python.gitOpen up command Prompt (or anaconda prompt if you have it installed), set up environment variables to install. Follow this if you do not have a GPU, you must set both of the following variables.set FORCE_CMAKE=1set CMAKE_ARGS=-DLLAMA_CUBLAS=OFFYou can ignore the second environment variable if you have an NVIDIA GPU.Compiling and installing‚ÄãIn the same command prompt (anaconda prompt) you set the variables, you can cd into llama-cpp-python directory and run the following commands.python setup.py cleanpython setup.py installUsage‚ÄãMake sure you are following all instructions to install all necessary model files.You don't need an API_TOKEN as you will run the LLM locally.It is worth understanding which models are suitable to be used on the desired machine.from langchain.llms import LlamaCppfrom langchain import PromptTemplate, LLMChainfrom langchain.callbacks.manager import CallbackManagerfrom langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandlerAPI Reference:LlamaCppCallbackManagerStreamingStdOutCallbackHandlerConsider using a template that suits your model! Check the models page on HuggingFace etc. to get a correct prompting template.template = """Question: {question}Answer: Let's work this out in a step by step way to be sure we have the right answer."""prompt = PromptTemplate(template=template, input_variables=["question"])# Callbacks support token-wise streamingcallback_manager = CallbackManager([StreamingStdOutCallbackHandler()])CPU‚ÄãExample using a LLaMA 2 7B model# Make sure the model path is correct for your system!llm = LlamaCpp(    model_path="/Users/rlm/Desktop/Code/llama.cpp/models/openorca-platypus2-13b.gguf.q4_0.bin",    temperature=0.75,    max_tokens=2000,    top_p=1,    callback_manager=callback_manager,     verbose=True, # Verbose is required to pass to the callback manager)prompt = """Question: A rap battle between Stephen Colbert and John Oliver"""llm(prompt)        Stephen Colbert:    Yo, John, I heard you've been talkin' smack about me on your show.    Let me tell you somethin', pal, I'm the king of late-night TV    My satire is sharp as a razor, it cuts deeper than a knife    While you're just a british bloke tryin' to be funny with your accent and your wit.    John Oliver:    Oh Stephen, don't be ridiculous, you may have the ratings but I got the real talk.    My show is the one that people actually watch and listen to, not just for the laughs but for the facts.    While you're busy talkin' trash, I'm out here bringing the truth to light.    Stephen Colbert:    Truth? Ha! You think your show is about truth? Please, it's all just a joke to you.    You're just a fancy-pants british guy tryin' to be funny with your news and your jokes.    While I'm the one who's really makin' a difference, with my sat        llama_print_timings:        load time =   358.60 ms    llama_print_timings:      sample time =   172.55 ms /   256 runs   (    0.67 ms per token,  1483.59 tokens per second)    llama_print_timings: prompt eval time =   613.36 ms /    16 tokens (   38.33 ms per token,    26.09 tokens per second)    llama_print_timings:        eval time = 10151.17 ms /   255 runs   (   39.81 ms per token,    25.12 tokens per second)    llama_print_timings:       total time = 11332.41 ms    "\nStephen Colbert:\nYo, John, I heard you've been talkin' smack about me on your show.\nLet me tell you somethin', pal, I'm the king of late-night TV\nMy satire is sharp as a razor, it cuts deeper than a knife\nWhile you're just a british bloke tryin' to be funny with your accent and your wit.\nJohn Oliver:\nOh Stephen, don't be ridiculous, you may have the ratings but I got the real talk.\nMy show is the one that people actually watch and listen to, not just for the laughs but for the facts.\nWhile you're busy talkin' trash, I'm out here bringing the truth to light.\nStephen Colbert:\nTruth? Ha! You think your show is about truth? Please, it's all just a joke to you.\nYou're just a fancy-pants british guy tryin' to be funny with your news and your jokes.\nWhile I'm the one who's really makin' a difference, with my sat"Example using a LLaMA v1 model# Make sure the model path is correct for your system!llm = LlamaCpp(    model_path="./ggml-model-q4_0.bin", callback_manager=callback_manager, verbose=True)llm_chain = LLMChain(prompt=prompt, llm=llm)question = "What NFL team won the Super Bowl in the year Justin Bieber was born?"llm_chain.run(question)            1. First, find out when Justin Bieber was born.    2. We know that Justin Bieber was born on March 1, 1994.    3. Next, we need to look up when the Super Bowl was played in that year.    4. The Super Bowl was played on January 28, 1995.    5. Finally, we can use this information to answer the question. The NFL team that won the Super Bowl in the year Justin Bieber was born is the San Francisco 49ers.        llama_print_timings:        load time =   434.15 ms    llama_print_timings:      sample time =    41.81 ms /   121 runs   (    0.35 ms per token)    llama_print_timings: prompt eval time =  2523.78 ms /    48 tokens (   52.58 ms per token)    llama_print_timings:        eval time = 23971.57 ms /   121 runs   (  198.11 ms per token)    llama_print_timings:       total time = 28945.95 ms    '\n\n1. First, find out when Justin Bieber was born.\n2. We know that Justin Bieber was born on March 1, 1994.\n3. Next, we need to look up when the Super Bowl was played in that year.\n4. The Super Bowl was played on January 28, 1995.\n5. Finally, we can use this information to answer the question. The NFL team that won the Super Bowl in the year Justin Bieber was born is the San Francisco 49ers.'GPU‚ÄãIf the installation with BLAS backend was correct, you will see a BLAS = 1 indicator in model properties.Two of the most important parameters for use with GPU are:n_gpu_layers - determines how many layers of the model are offloaded to your GPU.n_batch - how many tokens are processed in parallel. Setting these parameters correctly will dramatically improve the evaluation speed (see wrapper code for more details).n_gpu_layers = 40  # Change this value based on your model and your GPU VRAM pool.n_batch = 512  # Should be between 1 and n_ctx, consider the amount of VRAM in your GPU.# Make sure the model path is correct for your system!llm = LlamaCpp(    model_path="/Users/rlm/Desktop/Code/llama.cpp/models/openorca-platypus2-13b.gguf.q4_0.bin",    n_gpu_layers=n_gpu_layers,    n_batch=n_batch,    callback_manager=callback_manager,    verbose=True, # Verbose is required to pass to the callback manager)llm_chain = LLMChain(prompt=prompt, llm=llm)question = "What NFL team won the Super Bowl in the year Justin Bieber was born?"llm_chain.run(question)            1. Identify Justin Bieber's birth date: Justin Bieber was born on March 1, 1994.        2. Find the Super Bowl winner of that year: The NFL season of 1993 with the Super Bowl being played in January or of 1994.        3. Determine which team won the game: The Dallas Cowboys faced the Buffalo Bills in Super Bowl XXVII on January 31, 1993 (as the year is mis-labelled due to a error). The Dallas Cowboys won this matchup.        So, Justin Bieber was born when the Dallas Cowboys were the reigning NFL Super Bowl.        llama_print_timings:        load time =   427.63 ms    llama_print_timings:      sample time =   115.85 ms /   164 runs   (    0.71 ms per token,  1415.67 tokens per second)    llama_print_timings: prompt eval time =   427.53 ms /    45 tokens (    9.50 ms per token,   105.26 tokens per second)    llama_print_timings:        eval time =  4526.53 ms /   163 runs   (   27.77 ms per token,    36.01 tokens per second)    llama_print_timings:       total time =  5293.77 ms    "\n\n1. Identify Justin Bieber's birth date: Justin Bieber was born on March 1, 1994.\n\n2. Find the Super Bowl winner of that year: The NFL season of 1993 with the Super Bowl being played in January or of 1994.\n\n3. Determine which team won the game: The Dallas Cowboys faced the Buffalo Bills in Super Bowl XXVII on January 31, 1993 (as the year is mis-labelled due to a error). The Dallas Cowboys won this matchup.\n\nSo, Justin Bieber was born when the Dallas Cowboys were the reigning NFL Super Bowl."Metal‚ÄãIf the installation with Metal was correct, you will see a NEON = 1 indicator in model properties.Two of the most important GPU parameters are:n_gpu_layers - determines how many layers of the model are offloaded to your Metal GPU, in the most case, set it to 1 is enough for Metaln_batch - how many tokens are processed in parallel, default is 8, set to bigger number.f16_kv - for some reason, Metal only support True, otherwise you will get error such as Asserting on type 0 GGML_ASSERT: .../ggml-metal.m:706: false && "not implemented"Setting these parameters correctly will dramatically improve the evaluation speed (see wrapper code for more details).n_gpu_layers = 1  # Metal set to 1 is enough.n_batch = 512  # Should be between 1 and n_ctx, consider the amount of RAM of your Apple Silicon Chip.# Make sure the model path is correct for your system!llm = LlamaCpp(    model_path="/Users/rlm/Desktop/Code/llama.cpp/models/openorca-platypus2-13b.gguf.q4_0.bin",    n_gpu_layers=n_gpu_layers,    n_batch=n_batch,    f16_kv=True,  # MUST set to True, otherwise you will run into problem after a couple of calls    callback_manager=callback_manager,    verbose=True, # Verbose is required to pass to the callback manager)The console log will show the following log to indicate Metal was enable properly.ggml_metal_init: allocatingggml_metal_init: using MPS...You also could check Activity Monitor by watching the GPU usage of the process, the CPU usage will drop dramatically after turn on n_gpu_layers=1. For the first call to the LLM, the performance may be slow due to the model compilation in Metal GPU.Grammars‚ÄãWe can specify grammars to constrain model outputs.This will sample tokens according to the grammar.For example, supply the path to the specifed json.gbnf file in order to produce JSON.n_gpu_layers = 1  # Metal set to 1 is enough.n_batch = 512  # Should be between 1 and n_ctx, consider the amount of RAM of your Apple Silicon Chip.# Make sure the model path is correct for your system!llm = LlamaCpp(    model_path="/Users/rlm/Desktop/Code/llama.cpp/models/openorca-platypus2-13b.gguf.q4_0.bin",    n_gpu_layers=n_gpu_layers,    n_batch=n_batch,    f16_kv=True,  # MUST set to True, otherwise you will run into problem after a couple of calls    callback_manager=callback_manager,    verbose=True, # Verbose is required to pass to the callback manager    grammar_path="/Users/rlm/Desktop/Code/langchain-main/langchain/libs/langchain/langchain/llms/grammars/json.gbnf",)result=llm("Describe a person in JSON format:")    {      "name": "John Doe",      "age": 34,      "": {        "title": "Software Developer",        "company": "Google"      },      "interests": [        "Sports",        "Music",        "Cooking"      ],      "address": {        "street_number": 123,        "street_name": "Oak Street",        "city": "Mountain View",        "state": "California",        "postal_code": 94040      }}        llama_print_timings:        load time =   357.51 ms    llama_print_timings:      sample time =  1213.30 ms /   144 runs   (    8.43 ms per token,   118.68 tokens per second)    llama_print_timings: prompt eval time =   356.78 ms /     9 tokens (   39.64 ms per token,    25.23 tokens per second)    llama_print_timings:        eval time =  3947.16 ms /   143 runs   (   27.60 ms per token,    36.23 tokens per second)    llama_print_timings:       total time =  5846.21 msWe can also supply list.gbnf to return a list.n_gpu_layers = 1 n_batch = 512llm = LlamaCpp(    model_path="/Users/rlm/Desktop/Code/llama.cpp/models/openorca-platypus2-13b.gguf.q4_0.bin",    n_gpu_layers=n_gpu_layers,    n_batch=n_batch,    f16_kv=True,  # MUST set to True, otherwise you will run into problem after a couple of calls    callback_manager=callback_manager,    verbose=True,    grammar_path="/Users/rlm/Desktop/Code/langchain-main/langchain/libs/langchain/langchain/llms/grammars/list.gbnf",)result=llm("List of top-3 my favourite books:")    ["The Catcher in the Rye", "Wuthering Heights", "Anna Karenina"]        llama_print_timings:        load time =   322.34 ms    llama_print_timings:      sample time =   232.60 ms /    26 runs   (    8.95 ms per token,   111.78 tokens per second)    llama_print_timings: prompt eval time =   321.90 ms /    11 tokens (   29.26 ms per token,    34.17 tokens per second)    llama_print_timings:        eval time =   680.82 ms /    25 runs   (   27.23 ms per token,    36.72 tokens per second)    llama_print_timings:       total time =  1295.27 msPreviousKoboldAI APINextLLM Caching integrationsInstallationCPU only installationInstallation with OpenBLAS / cuBLAS / CLBlastInstallation with MetalInstallation with WindowsUsageCPUGPUMetalGrammars IntegrationsCallbacksChat modelsChat loadersDocument loadersDocument transformersLLMsAI21Aleph AlphaAmazon API GatewayAnyscaleAzure MLAzure OpenAIBananaBasetenBeamBedrockBittensorCerebriumAIChatGLMClarifaiCohereC TransformersCTranslate2DatabricksDeepInfraDeepSparseEden AIFireworksForefrontAIGoogle Vertex AI PaLMGooseAIGPT4AllHugging Face HubHugging Face Local PipelinesHuggingface TextGen InferenceJSONFormerKoboldAI APILlama.cppLLM Caching integrationsManifestMinimaxModalMosaicMLNLP CloudOctoAIOllamaOpaquePromptsOpenAIOpenLLMOpenLMPetalsPipelineAIPredibasePrediction GuardPromptLayer OpenAIRELLMReplicateRunhouseSageMakerEndpointStochasticAINebula (Symbl.ai)TextGenTitan TakeoffTongyi QwenvLLMWriterXorbits Inference (Xinference)MemoryRetrieversText embedding modelsAgents & ToolkitsToolsVector storesGrouped by providerIntegrationsLLMsLlama.cppOn this pageLlama.cppllama-cpp-python is a Python binding for llama.cpp. It supports inference for many LLMs, which can be accessed on HuggingFace.This notebook goes over how to run llama-cpp-python within LangChain.Note: new versions of llama-cpp-python use GGUF model files (see here).This is a breaking change.To convert existing GGML models to GGUF you can run the following in llama.cpp:python ./convert-llama-ggmlv3-to-gguf.py --eps 1e-5 --input models/openorca-platypus2-13b.ggmlv3.q4_0.bin --output models/openorca-platypus2-13b.gguf.q4_0.binInstallation‚ÄãThere are different options on how to install the llama-cpp package: CPU usageCPU + GPU (using one of many BLAS backends)Metal GPU (MacOS with Apple Silicon Chip) CPU only installation‚Äãpip install llama-cpp-pythonInstallation with OpenBLAS / cuBLAS / CLBlast‚Äãlama.cpp supports multiple BLAS backends for faster processing. Use the FORCE_CMAKE=1 environment variable to force the use of cmake and install the pip package for the desired BLAS backend (source).Example installation with cuBLAS backend:CMAKE_ARGS="-DLLAMA_CUBLAS=on" FORCE_CMAKE=1 pip install llama-cpp-pythonIMPORTANT: If you have already installed the CPU only version of the package, you need to reinstall it from scratch. Consider the following command: CMAKE_ARGS="-DLLAMA_CUBLAS=on" FORCE_CMAKE=1 pip install --upgrade --force-reinstall llama-cpp-python --no-cache-dirInstallation with Metal‚Äãllama.cpp supports Apple silicon first-class citizen - optimized via ARM NEON, Accelerate and Metal frameworks. Use the FORCE_CMAKE=1 environment variable to force the use of cmake and install the pip package for the Metal support (source).Example installation with Metal Support:CMAKE_ARGS="-DLLAMA_METAL=on" FORCE_CMAKE=1 pip install llama-cpp-pythonIMPORTANT: If you have already installed a cpu only version of the package, you need to reinstall it from scratch: consider the following command: CMAKE_ARGS="-DLLAMA_METAL=on" FORCE_CMAKE=1 pip install --upgrade --force-reinstall llama-cpp-python --no-cache-dirInstallation with Windows‚ÄãIt is stable to install the llama-cpp-python library by compiling from the source. You can follow most of the instructions in the repository itself but there are some windows specific instructions which might be useful.Requirements to install the llama-cpp-python,gitpythoncmakeVisual Studio Community (make sure you install this with the following settings)Desktop development with C++Python developmentLinux embedded development with C++Clone git repository recursively to get llama.cpp submodule as well git clone --recursive -j8 https://github.com/abetlen/llama-cpp-python.gitOpen up command Prompt (or anaconda prompt if you have it installed), set up environment variables to install. Follow this if you do not have a GPU, you must set both of the following variables.set FORCE_CMAKE=1set CMAKE_ARGS=-DLLAMA_CUBLAS=OFFYou can ignore the second environment variable if you have an NVIDIA GPU.Compiling and installing‚ÄãIn the same command prompt (anaconda prompt) you set the variables, you can cd into llama-cpp-python directory and run the following commands.python setup.py cleanpython setup.py installUsage‚ÄãMake sure you are following all instructions to install all necessary model files.You don't need an API_TOKEN as you will run the LLM locally.It is worth understanding which models are suitable to be used on the desired machine.from langchain.llms import LlamaCppfrom langchain import PromptTemplate, LLMChainfrom langchain.callbacks.manager import CallbackManagerfrom langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandlerAPI Reference:LlamaCppCallbackManagerStreamingStdOutCallbackHandlerConsider using a template that suits your model! Check the models page on HuggingFace etc. to get a correct prompting template.template = """Question: {question}Answer: Let's work this out in a step by step way to be sure we have the right answer."""prompt = PromptTemplate(template=template, input_variables=["question"])# Callbacks support token-wise streamingcallback_manager = CallbackManager([StreamingStdOutCallbackHandler()])CPU‚ÄãExample using a LLaMA 2 7B model# Make sure the model path is correct for your system!llm = LlamaCpp(    model_path="/Users/rlm/Desktop/Code/llama.cpp/models/openorca-platypus2-13b.gguf.q4_0.bin",    temperature=0.75,    max_tokens=2000,    top_p=1,    callback_manager=callback_manager,     verbose=True, # Verbose is required to pass to the callback manager)prompt = """Question: A rap battle between Stephen Colbert and John Oliver"""llm(prompt)        Stephen Colbert:    Yo, John, I heard you've been talkin' smack about me on your show.    Let me tell you somethin', pal, I'm the king of late-night TV    My satire is sharp as a razor, it cuts deeper than a knife    While you're just a british bloke tryin' to be funny with your accent and your wit.    John Oliver:    Oh Stephen, don't be ridiculous, you may have the ratings but I got the real talk.    My show is the one that people actually watch and listen to, not just for the laughs but for the facts.    While you're busy talkin' trash, I'm out here bringing the truth to light.    Stephen Colbert:    Truth? Ha! You think your show is about truth? Please, it's all just a joke to you.    You're just a fancy-pants british guy tryin' to be funny with your news and your jokes.    While I'm the one who's really makin' a difference, with my sat        llama_print_timings:        load time =   358.60 ms    llama_print_timings:      sample time =   172.55 ms /   256 runs   (    0.67 ms per token,  1483.59 tokens per second)    llama_print_timings: prompt eval time =   613.36 ms /    16 tokens (   38.33 ms per token,    26.09 tokens per second)    llama_print_timings:        eval time = 10151.17 ms /   255 runs   (   39.81 ms per token,    25.12 tokens per second)    llama_print_timings:       total time = 11332.41 ms    "\nStephen Colbert:\nYo, John, I heard you've been talkin' smack about me on your show.\nLet me tell you somethin', pal, I'm the king of late-night TV\nMy satire is sharp as a razor, it cuts deeper than a knife\nWhile you're just a british bloke tryin' to be funny with your accent and your wit.\nJohn Oliver:\nOh Stephen, don't be ridiculous, you may have the ratings but I got the real talk.\nMy show is the one that people actually watch and listen to, not just for the laughs but for the facts.\nWhile you're busy talkin' trash, I'm out here bringing the truth to light.\nStephen Colbert:\nTruth? Ha! You think your show is about truth? Please, it's all just a joke to you.\nYou're just a fancy-pants british guy tryin' to be funny with your news and your jokes.\nWhile I'm the one who's really makin' a difference, with my sat"Example using a LLaMA v1 model# Make sure the model path is correct for your system!llm = LlamaCpp(    model_path="./ggml-model-q4_0.bin", callback_manager=callback_manager, verbose=True)llm_chain = LLMChain(prompt=prompt, llm=llm)question = "What NFL team won the Super Bowl in the year Justin Bieber was born?"llm_chain.run(question)            1. First, find out when Justin Bieber was born.    2. We know that Justin Bieber was born on March 1, 1994.    3. Next, we need to look up when the Super Bowl was played in that year.    4. The Super Bowl was played on January 28, 1995.    5. Finally, we can use this information to answer the question. The NFL team that won the Super Bowl in the year Justin Bieber was born is the San Francisco 49ers.        llama_print_timings:        load time =   434.15 ms    llama_print_timings:      sample time =    41.81 ms /   121 runs   (    0.35 ms per token)    llama_print_timings: prompt eval time =  2523.78 ms /    48 tokens (   52.58 ms per token)    llama_print_timings:        eval time = 23971.57 ms /   121 runs   (  198.11 ms per token)    llama_print_timings:       total time = 28945.95 ms    '\n\n1. First, find out when Justin Bieber was born.\n2. We know that Justin Bieber was born on March 1, 1994.\n3. Next, we need to look up when the Super Bowl was played in that year.\n4. The Super Bowl was played on January 28, 1995.\n5. Finally, we can use this information to answer the question. The NFL team that won the Super Bowl in the year Justin Bieber was born is the San Francisco 49ers.'GPU‚ÄãIf the installation with BLAS backend was correct, you will see a BLAS = 1 indicator in model properties.Two of the most important parameters for use with GPU are:n_gpu_layers - determines how many layers of the model are offloaded to your GPU.n_batch - how many tokens are processed in parallel. Setting these parameters correctly will dramatically improve the evaluation speed (see wrapper code for more details).n_gpu_layers = 40  # Change this value based on your model and your GPU VRAM pool.n_batch = 512  # Should be between 1 and n_ctx, consider the amount of VRAM in your GPU.# Make sure the model path is correct for your system!llm = LlamaCpp(    model_path="/Users/rlm/Desktop/Code/llama.cpp/models/openorca-platypus2-13b.gguf.q4_0.bin",    n_gpu_layers=n_gpu_layers,    n_batch=n_batch,    callback_manager=callback_manager,    verbose=True, # Verbose is required to pass to the callback manager)llm_chain = LLMChain(prompt=prompt, llm=llm)question = "What NFL team won the Super Bowl in the year Justin Bieber was born?"llm_chain.run(question)            1. Identify Justin Bieber's birth date: Justin Bieber was born on March 1, 1994.        2. Find the Super Bowl winner of that year: The NFL season of 1993 with the Super Bowl being played in January or of 1994.        3. Determine which team won the game: The Dallas Cowboys faced the Buffalo Bills in Super Bowl XXVII on January 31, 1993 (as the year is mis-labelled due to a error). The Dallas Cowboys won this matchup.        So, Justin Bieber was born when the Dallas Cowboys were the reigning NFL Super Bowl.        llama_print_timings:        load time =   427.63 ms    llama_print_timings:      sample time =   115.85 ms /   164 runs   (    0.71 ms per token,  1415.67 tokens per second)    llama_print_timings: prompt eval time =   427.53 ms /    45 tokens (    9.50 ms per token,   105.26 tokens per second)    llama_print_timings:        eval time =  4526.53 ms /   163 runs   (   27.77 ms per token,    36.01 tokens per second)    llama_print_timings:       total time =  5293.77 ms    "\n\n1. Identify Justin Bieber's birth date: Justin Bieber was born on March 1, 1994.\n\n2. Find the Super Bowl winner of that year: The NFL season of 1993 with the Super Bowl being played in January or of 1994.\n\n3. Determine which team won the game: The Dallas Cowboys faced the Buffalo Bills in Super Bowl XXVII on January 31, 1993 (as the year is mis-labelled due to a error). The Dallas Cowboys won this matchup.\n\nSo, Justin Bieber was born when the Dallas Cowboys were the reigning NFL Super Bowl."Metal‚ÄãIf the installation with Metal was correct, you will see a NEON = 1 indicator in model properties.Two of the most important GPU parameters are:n_gpu_layers - determines how many layers of the model are offloaded to your Metal GPU, in the most case, set it to 1 is enough for Metaln_batch - how many tokens are processed in parallel, default is 8, set to bigger number.f16_kv - for some reason, Metal only support True, otherwise you will get error such as Asserting on type 0 GGML_ASSERT: .../ggml-metal.m:706: false && "not implemented"Setting these parameters correctly will dramatically improve the evaluation speed (see wrapper code for more details).n_gpu_layers = 1  # Metal set to 1 is enough.n_batch = 512  # Should be between 1 and n_ctx, consider the amount of RAM of your Apple Silicon Chip.# Make sure the model path is correct for your system!llm = LlamaCpp(    model_path="/Users/rlm/Desktop/Code/llama.cpp/models/openorca-platypus2-13b.gguf.q4_0.bin",    n_gpu_layers=n_gpu_layers,    n_batch=n_batch,    f16_kv=True,  # MUST set to True, otherwise you will run into problem after a couple of calls    callback_manager=callback_manager,    verbose=True, # Verbose is required to pass to the callback manager)The console log will show the following log to indicate Metal was enable properly.ggml_metal_init: allocatingggml_metal_init: using MPS...You also could check Activity Monitor by watching the GPU usage of the process, the CPU usage will drop dramatically after turn on n_gpu_layers=1. For the first call to the LLM, the performance may be slow due to the model compilation in Metal GPU.Grammars‚ÄãWe can specify grammars to constrain model outputs.This will sample tokens according to the grammar.For example, supply the path to the specifed json.gbnf file in order to produce JSON.n_gpu_layers = 1  # Metal set to 1 is enough.n_batch = 512  # Should be between 1 and n_ctx, consider the amount of RAM of your Apple Silicon Chip.# Make sure the model path is correct for your system!llm = LlamaCpp(    model_path="/Users/rlm/Desktop/Code/llama.cpp/models/openorca-platypus2-13b.gguf.q4_0.bin",    n_gpu_layers=n_gpu_layers,    n_batch=n_batch,    f16_kv=True,  # MUST set to True, otherwise you will run into problem after a couple of calls    callback_manager=callback_manager,    verbose=True, # Verbose is required to pass to the callback manager    grammar_path="/Users/rlm/Desktop/Code/langchain-main/langchain/libs/langchain/langchain/llms/grammars/json.gbnf",)result=llm("Describe a person in JSON format:")    {      "name": "John Doe",      "age": 34,      "": {        "title": "Software Developer",        "company": "Google"      },      "interests": [        "Sports",        "Music",        "Cooking"      ],      "address": {        "street_number": 123,        "street_name": "Oak Street",        "city": "Mountain View",        "state": "California",        "postal_code": 94040      }}        llama_print_timings:        load time =   357.51 ms    llama_print_timings:      sample time =  1213.30 ms /   144 runs   (    8.43 ms per token,   118.68 tokens per second)    llama_print_timings: prompt eval time =   356.78 ms /     9 tokens (   39.64 ms per token,    25.23 tokens per second)    llama_print_timings:        eval time =  3947.16 ms /   143 runs   (   27.60 ms per token,    36.23 tokens per second)    llama_print_timings:       total time =  5846.21 msWe can also supply list.gbnf to return a list.n_gpu_layers = 1 n_batch = 512llm = LlamaCpp(    model_path="/Users/rlm/Desktop/Code/llama.cpp/models/openorca-platypus2-13b.gguf.q4_0.bin",    n_gpu_layers=n_gpu_layers,    n_batch=n_batch,    f16_kv=True,  # MUST set to True, otherwise you will run into problem after a couple of calls    callback_manager=callback_manager,    verbose=True,    grammar_path="/Users/rlm/Desktop/Code/langchain-main/langchain/libs/langchain/langchain/llms/grammars/list.gbnf",)result=llm("List of top-3 my favourite books:")    ["The Catcher in the Rye", "Wuthering Heights", "Anna Karenina"]        llama_print_timings:        load time =   322.34 ms    llama_print_timings:      sample time =   232.60 ms /    26 runs   (    8.95 ms per token,   111.78 tokens per second)    llama_print_timings: prompt eval time =   321.90 ms /    11 tokens (   29.26 ms per token,    34.17 tokens per second)    llama_print_timings:        eval time =   680.82 ms /    25 runs   (   27.23 ms per token,    36.72 tokens per second)    llama_print_timings:       total time =  1295.27 msPreviousKoboldAI APINextLLM Caching integrationsInstallationCPU only installationInstallation with OpenBLAS / cuBLAS / CLBlastInstallation with MetalInstallation with WindowsUsageCPUGPUMetalGrammars IntegrationsCallbacksChat modelsChat loadersDocument loadersDocument transformersLLMsAI21Aleph AlphaAmazon API GatewayAnyscaleAzure MLAzure OpenAIBananaBasetenBeamBedrockBittensorCerebriumAIChatGLMClarifaiCohereC TransformersCTranslate2DatabricksDeepInfraDeepSparseEden AIFireworksForefrontAIGoogle Vertex AI PaLMGooseAIGPT4AllHugging Face HubHugging Face Local PipelinesHuggingface TextGen InferenceJSONFormerKoboldAI APILlama.cppLLM Caching integrationsManifestMinimaxModalMosaicMLNLP CloudOctoAIOllamaOpaquePromptsOpenAIOpenLLMOpenLMPetalsPipelineAIPredibasePrediction GuardPromptLayer OpenAIRELLMReplicateRunhouseSageMakerEndpointStochasticAINebula (Symbl.ai)TextGenTitan TakeoffTongyi QwenvLLMWriterXorbits Inference (Xinference)MemoryRetrieversText embedding modelsAgents & ToolkitsToolsVector storesGrouped by provider IntegrationsCallbacksChat modelsChat loadersDocument loadersDocument transformersLLMsAI21Aleph AlphaAmazon API GatewayAnyscaleAzure MLAzure OpenAIBananaBasetenBeamBedrockBittensorCerebriumAIChatGLMClarifaiCohereC TransformersCTranslate2DatabricksDeepInfraDeepSparseEden AIFireworksForefrontAIGoogle Vertex AI PaLMGooseAIGPT4AllHugging Face HubHugging Face Local PipelinesHuggingface TextGen InferenceJSONFormerKoboldAI APILlama.cppLLM Caching integrationsManifestMinimaxModalMosaicMLNLP CloudOctoAIOllamaOpaquePromptsOpenAIOpenLLMOpenLMPetalsPipelineAIPredibasePrediction GuardPromptLayer OpenAIRELLMReplicateRunhouseSageMakerEndpointStochasticAINebula (Symbl.ai)TextGenTitan TakeoffTongyi QwenvLLMWriterXorbits Inference (Xinference)MemoryRetrieversText embedding modelsAgents & ToolkitsToolsVector storesGrouped by provider Integrations Callbacks Chat models Chat loaders Document loaders Document transformers LLMs Memory Retrievers Text embedding models Agents & Toolkits Tools Vector stores Grouped by provider IntegrationsLLMsLlama.cppOn this pageLlama.cppllama-cpp-python is a Python binding for llama.cpp. It supports inference for many LLMs, which can be accessed on HuggingFace.This notebook goes over how to run llama-cpp-python within LangChain.Note: new versions of llama-cpp-python use GGUF model files (see here).This is a breaking change.To convert existing GGML models to GGUF you can run the following in llama.cpp:python ./convert-llama-ggmlv3-to-gguf.py --eps 1e-5 --input models/openorca-platypus2-13b.ggmlv3.q4_0.bin --output models/openorca-platypus2-13b.gguf.q4_0.binInstallation‚ÄãThere are different options on how to install the llama-cpp package: CPU usageCPU + GPU (using one of many BLAS backends)Metal GPU (MacOS with Apple Silicon Chip) CPU only installation‚Äãpip install llama-cpp-pythonInstallation with OpenBLAS / cuBLAS / CLBlast‚Äãlama.cpp supports multiple BLAS backends for faster processing. Use the FORCE_CMAKE=1 environment variable to force the use of cmake and install the pip package for the desired BLAS backend (source).Example installation with cuBLAS backend:CMAKE_ARGS="-DLLAMA_CUBLAS=on" FORCE_CMAKE=1 pip install llama-cpp-pythonIMPORTANT: If you have already installed the CPU only version of the package, you need to reinstall it from scratch. Consider the following command: CMAKE_ARGS="-DLLAMA_CUBLAS=on" FORCE_CMAKE=1 pip install --upgrade --force-reinstall llama-cpp-python --no-cache-dirInstallation with Metal‚Äãllama.cpp supports Apple silicon first-class citizen - optimized via ARM NEON, Accelerate and Metal frameworks. Use the FORCE_CMAKE=1 environment variable to force the use of cmake and install the pip package for the Metal support (source).Example installation with Metal Support:CMAKE_ARGS="-DLLAMA_METAL=on" FORCE_CMAKE=1 pip install llama-cpp-pythonIMPORTANT: If you have already installed a cpu only version of the package, you need to reinstall it from scratch: consider the following command: CMAKE_ARGS="-DLLAMA_METAL=on" FORCE_CMAKE=1 pip install --upgrade --force-reinstall llama-cpp-python --no-cache-dirInstallation with Windows‚ÄãIt is stable to install the llama-cpp-python library by compiling from the source. You can follow most of the instructions in the repository itself but there are some windows specific instructions which might be useful.Requirements to install the llama-cpp-python,gitpythoncmakeVisual Studio Community (make sure you install this with the following settings)Desktop development with C++Python developmentLinux embedded development with C++Clone git repository recursively to get llama.cpp submodule as well git clone --recursive -j8 https://github.com/abetlen/llama-cpp-python.gitOpen up command Prompt (or anaconda prompt if you have it installed), set up environment variables to install. Follow this if you do not have a GPU, you must set both of the following variables.set FORCE_CMAKE=1set CMAKE_ARGS=-DLLAMA_CUBLAS=OFFYou can ignore the second environment variable if you have an NVIDIA GPU.Compiling and installing‚ÄãIn the same command prompt (anaconda prompt) you set the variables, you can cd into llama-cpp-python directory and run the following commands.python setup.py cleanpython setup.py installUsage‚ÄãMake sure you are following all instructions to install all necessary model files.You don't need an API_TOKEN as you will run the LLM locally.It is worth understanding which models are suitable to be used on the desired machine.from langchain.llms import LlamaCppfrom langchain import PromptTemplate, LLMChainfrom langchain.callbacks.manager import CallbackManagerfrom langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandlerAPI Reference:LlamaCppCallbackManagerStreamingStdOutCallbackHandlerConsider using a template that suits your model! Check the models page on HuggingFace etc. to get a correct prompting template.template = """Question: {question}Answer: Let's work this out in a step by step way to be sure we have the right answer."""prompt = PromptTemplate(template=template, input_variables=["question"])# Callbacks support token-wise streamingcallback_manager = CallbackManager([StreamingStdOutCallbackHandler()])CPU‚ÄãExample using a LLaMA 2 7B model# Make sure the model path is correct for your system!llm = LlamaCpp(    model_path="/Users/rlm/Desktop/Code/llama.cpp/models/openorca-platypus2-13b.gguf.q4_0.bin",    temperature=0.75,    max_tokens=2000,    top_p=1,    callback_manager=callback_manager,     verbose=True, # Verbose is required to pass to the callback manager)prompt = """Question: A rap battle between Stephen Colbert and John Oliver"""llm(prompt)        Stephen Colbert:    Yo, John, I heard you've been talkin' smack about me on your show.    Let me tell you somethin', pal, I'm the king of late-night TV    My satire is sharp as a razor, it cuts deeper than a knife    While you're just a british bloke tryin' to be funny with your accent and your wit.    John Oliver:    Oh Stephen, don't be ridiculous, you may have the ratings but I got the real talk.    My show is the one that people actually watch and listen to, not just for the laughs but for the facts.    While you're busy talkin' trash, I'm out here bringing the truth to light.    Stephen Colbert:    Truth? Ha! You think your show is about truth? Please, it's all just a joke to you.    You're just a fancy-pants british guy tryin' to be funny with your news and your jokes.    While I'm the one who's really makin' a difference, with my sat        llama_print_timings:        load time =   358.60 ms    llama_print_timings:      sample time =   172.55 ms /   256 runs   (    0.67 ms per token,  1483.59 tokens per second)    llama_print_timings: prompt eval time =   613.36 ms /    16 tokens (   38.33 ms per token,    26.09 tokens per second)    llama_print_timings:        eval time = 10151.17 ms /   255 runs   (   39.81 ms per token,    25.12 tokens per second)    llama_print_timings:       total time = 11332.41 ms    "\nStephen Colbert:\nYo, John, I heard you've been talkin' smack about me on your show.\nLet me tell you somethin', pal, I'm the king of late-night TV\nMy satire is sharp as a razor, it cuts deeper than a knife\nWhile you're just a british bloke tryin' to be funny with your accent and your wit.\nJohn Oliver:\nOh Stephen, don't be ridiculous, you may have the ratings but I got the real talk.\nMy show is the one that people actually watch and listen to, not just for the laughs but for the facts.\nWhile you're busy talkin' trash, I'm out here bringing the truth to light.\nStephen Colbert:\nTruth? Ha! You think your show is about truth? Please, it's all just a joke to you.\nYou're just a fancy-pants british guy tryin' to be funny with your news and your jokes.\nWhile I'm the one who's really makin' a difference, with my sat"Example using a LLaMA v1 model# Make sure the model path is correct for your system!llm = LlamaCpp(    model_path="./ggml-model-q4_0.bin", callback_manager=callback_manager, verbose=True)llm_chain = LLMChain(prompt=prompt, llm=llm)question = "What NFL team won the Super Bowl in the year Justin Bieber was born?"llm_chain.run(question)            1. First, find out when Justin Bieber was born.    2. We know that Justin Bieber was born on March 1, 1994.    3. Next, we need to look up when the Super Bowl was played in that year.    4. The Super Bowl was played on January 28, 1995.    5. Finally, we can use this information to answer the question. The NFL team that won the Super Bowl in the year Justin Bieber was born is the San Francisco 49ers.        llama_print_timings:        load time =   434.15 ms    llama_print_timings:      sample time =    41.81 ms /   121 runs   (    0.35 ms per token)    llama_print_timings: prompt eval time =  2523.78 ms /    48 tokens (   52.58 ms per token)    llama_print_timings:        eval time = 23971.57 ms /   121 runs   (  198.11 ms per token)    llama_print_timings:       total time = 28945.95 ms    '\n\n1. First, find out when Justin Bieber was born.\n2. We know that Justin Bieber was born on March 1, 1994.\n3. Next, we need to look up when the Super Bowl was played in that year.\n4. The Super Bowl was played on January 28, 1995.\n5. Finally, we can use this information to answer the question. The NFL team that won the Super Bowl in the year Justin Bieber was born is the San Francisco 49ers.'GPU‚ÄãIf the installation with BLAS backend was correct, you will see a BLAS = 1 indicator in model properties.Two of the most important parameters for use with GPU are:n_gpu_layers - determines how many layers of the model are offloaded to your GPU.n_batch - how many tokens are processed in parallel. Setting these parameters correctly will dramatically improve the evaluation speed (see wrapper code for more details).n_gpu_layers = 40  # Change this value based on your model and your GPU VRAM pool.n_batch = 512  # Should be between 1 and n_ctx, consider the amount of VRAM in your GPU.# Make sure the model path is correct for your system!llm = LlamaCpp(    model_path="/Users/rlm/Desktop/Code/llama.cpp/models/openorca-platypus2-13b.gguf.q4_0.bin",    n_gpu_layers=n_gpu_layers,    n_batch=n_batch,    callback_manager=callback_manager,    verbose=True, # Verbose is required to pass to the callback manager)llm_chain = LLMChain(prompt=prompt, llm=llm)question = "What NFL team won the Super Bowl in the year Justin Bieber was born?"llm_chain.run(question)            1. Identify Justin Bieber's birth date: Justin Bieber was born on March 1, 1994.        2. Find the Super Bowl winner of that year: The NFL season of 1993 with the Super Bowl being played in January or of 1994.        3. Determine which team won the game: The Dallas Cowboys faced the Buffalo Bills in Super Bowl XXVII on January 31, 1993 (as the year is mis-labelled due to a error). The Dallas Cowboys won this matchup.        So, Justin Bieber was born when the Dallas Cowboys were the reigning NFL Super Bowl.        llama_print_timings:        load time =   427.63 ms    llama_print_timings:      sample time =   115.85 ms /   164 runs   (    0.71 ms per token,  1415.67 tokens per second)    llama_print_timings: prompt eval time =   427.53 ms /    45 tokens (    9.50 ms per token,   105.26 tokens per second)    llama_print_timings:        eval time =  4526.53 ms /   163 runs   (   27.77 ms per token,    36.01 tokens per second)    llama_print_timings:       total time =  5293.77 ms    "\n\n1. Identify Justin Bieber's birth date: Justin Bieber was born on March 1, 1994.\n\n2. Find the Super Bowl winner of that year: The NFL season of 1993 with the Super Bowl being played in January or of 1994.\n\n3. Determine which team won the game: The Dallas Cowboys faced the Buffalo Bills in Super Bowl XXVII on January 31, 1993 (as the year is mis-labelled due to a error). The Dallas Cowboys won this matchup.\n\nSo, Justin Bieber was born when the Dallas Cowboys were the reigning NFL Super Bowl."Metal‚ÄãIf the installation with Metal was correct, you will see a NEON = 1 indicator in model properties.Two of the most important GPU parameters are:n_gpu_layers - determines how many layers of the model are offloaded to your Metal GPU, in the most case, set it to 1 is enough for Metaln_batch - how many tokens are processed in parallel, default is 8, set to bigger number.f16_kv - for some reason, Metal only support True, otherwise you will get error such as Asserting on type 0 GGML_ASSERT: .../ggml-metal.m:706: false && "not implemented"Setting these parameters correctly will dramatically improve the evaluation speed (see wrapper code for more details).n_gpu_layers = 1  # Metal set to 1 is enough.n_batch = 512  # Should be between 1 and n_ctx, consider the amount of RAM of your Apple Silicon Chip.# Make sure the model path is correct for your system!llm = LlamaCpp(    model_path="/Users/rlm/Desktop/Code/llama.cpp/models/openorca-platypus2-13b.gguf.q4_0.bin",    n_gpu_layers=n_gpu_layers,    n_batch=n_batch,    f16_kv=True,  # MUST set to True, otherwise you will run into problem after a couple of calls    callback_manager=callback_manager,    verbose=True, # Verbose is required to pass to the callback manager)The console log will show the following log to indicate Metal was enable properly.ggml_metal_init: allocatingggml_metal_init: using MPS...You also could check Activity Monitor by watching the GPU usage of the process, the CPU usage will drop dramatically after turn on n_gpu_layers=1. For the first call to the LLM, the performance may be slow due to the model compilation in Metal GPU.Grammars‚ÄãWe can specify grammars to constrain model outputs.This will sample tokens according to the grammar.For example, supply the path to the specifed json.gbnf file in order to produce JSON.n_gpu_layers = 1  # Metal set to 1 is enough.n_batch = 512  # Should be between 1 and n_ctx, consider the amount of RAM of your Apple Silicon Chip.# Make sure the model path is correct for your system!llm = LlamaCpp(    model_path="/Users/rlm/Desktop/Code/llama.cpp/models/openorca-platypus2-13b.gguf.q4_0.bin",    n_gpu_layers=n_gpu_layers,    n_batch=n_batch,    f16_kv=True,  # MUST set to True, otherwise you will run into problem after a couple of calls    callback_manager=callback_manager,    verbose=True, # Verbose is required to pass to the callback manager    grammar_path="/Users/rlm/Desktop/Code/langchain-main/langchain/libs/langchain/langchain/llms/grammars/json.gbnf",)result=llm("Describe a person in JSON format:")    {      "name": "John Doe",      "age": 34,      "": {        "title": "Software Developer",        "company": "Google"      },      "interests": [        "Sports",        "Music",        "Cooking"      ],      "address": {        "street_number": 123,        "street_name": "Oak Street",        "city": "Mountain View",        "state": "California",        "postal_code": 94040      }}        llama_print_timings:        load time =   357.51 ms    llama_print_timings:      sample time =  1213.30 ms /   144 runs   (    8.43 ms per token,   118.68 tokens per second)    llama_print_timings: prompt eval time =   356.78 ms /     9 tokens (   39.64 ms per token,    25.23 tokens per second)    llama_print_timings:        eval time =  3947.16 ms /   143 runs   (   27.60 ms per token,    36.23 tokens per second)    llama_print_timings:       total time =  5846.21 msWe can also supply list.gbnf to return a list.n_gpu_layers = 1 n_batch = 512llm = LlamaCpp(    model_path="/Users/rlm/Desktop/Code/llama.cpp/models/openorca-platypus2-13b.gguf.q4_0.bin",    n_gpu_layers=n_gpu_layers,    n_batch=n_batch,    f16_kv=True,  # MUST set to True, otherwise you will run into problem after a couple of calls    callback_manager=callback_manager,    verbose=True,    grammar_path="/Users/rlm/Desktop/Code/langchain-main/langchain/libs/langchain/langchain/llms/grammars/list.gbnf",)result=llm("List of top-3 my favourite books:")    ["The Catcher in the Rye", "Wuthering Heights", "Anna Karenina"]        llama_print_timings:        load time =   322.34 ms    llama_print_timings:      sample time =   232.60 ms /    26 runs   (    8.95 ms per token,   111.78 tokens per second)    llama_print_timings: prompt eval time =   321.90 ms /    11 tokens (   29.26 ms per token,    34.17 tokens per second)    llama_print_timings:        eval time =   680.82 ms /    25 runs   (   27.23 ms per token,    36.72 tokens per second)    llama_print_timings:       total time =  1295.27 msPreviousKoboldAI APINextLLM Caching integrationsInstallationCPU only installationInstallation with OpenBLAS / cuBLAS / CLBlastInstallation with MetalInstallation with WindowsUsageCPUGPUMetalGrammars IntegrationsLLMsLlama.cppOn this pageLlama.cppllama-cpp-python is a Python binding for llama.cpp. It supports inference for many LLMs, which can be accessed on HuggingFace.This notebook goes over how to run llama-cpp-python within LangChain.Note: new versions of llama-cpp-python use GGUF model files (see here).This is a breaking change.To convert existing GGML models to GGUF you can run the following in llama.cpp:python ./convert-llama-ggmlv3-to-gguf.py --eps 1e-5 --input models/openorca-platypus2-13b.ggmlv3.q4_0.bin --output models/openorca-platypus2-13b.gguf.q4_0.binInstallation‚ÄãThere are different options on how to install the llama-cpp package: CPU usageCPU + GPU (using one of many BLAS backends)Metal GPU (MacOS with Apple Silicon Chip) CPU only installation‚Äãpip install llama-cpp-pythonInstallation with OpenBLAS / cuBLAS / CLBlast‚Äãlama.cpp supports multiple BLAS backends for faster processing. Use the FORCE_CMAKE=1 environment variable to force the use of cmake and install the pip package for the desired BLAS backend (source).Example installation with cuBLAS backend:CMAKE_ARGS="-DLLAMA_CUBLAS=on" FORCE_CMAKE=1 pip install llama-cpp-pythonIMPORTANT: If you have already installed the CPU only version of the package, you need to reinstall it from scratch. Consider the following command: CMAKE_ARGS="-DLLAMA_CUBLAS=on" FORCE_CMAKE=1 pip install --upgrade --force-reinstall llama-cpp-python --no-cache-dirInstallation with Metal‚Äãllama.cpp supports Apple silicon first-class citizen - optimized via ARM NEON, Accelerate and Metal frameworks. Use the FORCE_CMAKE=1 environment variable to force the use of cmake and install the pip package for the Metal support (source).Example installation with Metal Support:CMAKE_ARGS="-DLLAMA_METAL=on" FORCE_CMAKE=1 pip install llama-cpp-pythonIMPORTANT: If you have already installed a cpu only version of the package, you need to reinstall it from scratch: consider the following command: CMAKE_ARGS="-DLLAMA_METAL=on" FORCE_CMAKE=1 pip install --upgrade --force-reinstall llama-cpp-python --no-cache-dirInstallation with Windows‚ÄãIt is stable to install the llama-cpp-python library by compiling from the source. You can follow most of the instructions in the repository itself but there are some windows specific instructions which might be useful.Requirements to install the llama-cpp-python,gitpythoncmakeVisual Studio Community (make sure you install this with the following settings)Desktop development with C++Python developmentLinux embedded development with C++Clone git repository recursively to get llama.cpp submodule as well git clone --recursive -j8 https://github.com/abetlen/llama-cpp-python.gitOpen up command Prompt (or anaconda prompt if you have it installed), set up environment variables to install. Follow this if you do not have a GPU, you must set both of the following variables.set FORCE_CMAKE=1set CMAKE_ARGS=-DLLAMA_CUBLAS=OFFYou can ignore the second environment variable if you have an NVIDIA GPU.Compiling and installing‚ÄãIn the same command prompt (anaconda prompt) you set the variables, you can cd into llama-cpp-python directory and run the following commands.python setup.py cleanpython setup.py installUsage‚ÄãMake sure you are following all instructions to install all necessary model files.You don't need an API_TOKEN as you will run the LLM locally.It is worth understanding which models are suitable to be used on the desired machine.from langchain.llms import LlamaCppfrom langchain import PromptTemplate, LLMChainfrom langchain.callbacks.manager import CallbackManagerfrom langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandlerAPI Reference:LlamaCppCallbackManagerStreamingStdOutCallbackHandlerConsider using a template that suits your model! Check the models page on HuggingFace etc. to get a correct prompting template.template = """Question: {question}Answer: Let's work this out in a step by step way to be sure we have the right answer."""prompt = PromptTemplate(template=template, input_variables=["question"])# Callbacks support token-wise streamingcallback_manager = CallbackManager([StreamingStdOutCallbackHandler()])CPU‚ÄãExample using a LLaMA 2 7B model# Make sure the model path is correct for your system!llm = LlamaCpp(    model_path="/Users/rlm/Desktop/Code/llama.cpp/models/openorca-platypus2-13b.gguf.q4_0.bin",    temperature=0.75,    max_tokens=2000,    top_p=1,    callback_manager=callback_manager,     verbose=True, # Verbose is required to pass to the callback manager)prompt = """Question: A rap battle between Stephen Colbert and John Oliver"""llm(prompt)        Stephen Colbert:    Yo, John, I heard you've been talkin' smack about me on your show.    Let me tell you somethin', pal, I'm the king of late-night TV    My satire is sharp as a razor, it cuts deeper than a knife    While you're just a british bloke tryin' to be funny with your accent and your wit.    John Oliver:    Oh Stephen, don't be ridiculous, you may have the ratings but I got the real talk.    My show is the one that people actually watch and listen to, not just for the laughs but for the facts.    While you're busy talkin' trash, I'm out here bringing the truth to light.    Stephen Colbert:    Truth? Ha! You think your show is about truth? Please, it's all just a joke to you.    You're just a fancy-pants british guy tryin' to be funny with your news and your jokes.    While I'm the one who's really makin' a difference, with my sat        llama_print_timings:        load time =   358.60 ms    llama_print_timings:      sample time =   172.55 ms /   256 runs   (    0.67 ms per token,  1483.59 tokens per second)    llama_print_timings: prompt eval time =   613.36 ms /    16 tokens (   38.33 ms per token,    26.09 tokens per second)    llama_print_timings:        eval time = 10151.17 ms /   255 runs   (   39.81 ms per token,    25.12 tokens per second)    llama_print_timings:       total time = 11332.41 ms    "\nStephen Colbert:\nYo, John, I heard you've been talkin' smack about me on your show.\nLet me tell you somethin', pal, I'm the king of late-night TV\nMy satire is sharp as a razor, it cuts deeper than a knife\nWhile you're just a british bloke tryin' to be funny with your accent and your wit.\nJohn Oliver:\nOh Stephen, don't be ridiculous, you may have the ratings but I got the real talk.\nMy show is the one that people actually watch and listen to, not just for the laughs but for the facts.\nWhile you're busy talkin' trash, I'm out here bringing the truth to light.\nStephen Colbert:\nTruth? Ha! You think your show is about truth? Please, it's all just a joke to you.\nYou're just a fancy-pants british guy tryin' to be funny with your news and your jokes.\nWhile I'm the one who's really makin' a difference, with my sat"Example using a LLaMA v1 model# Make sure the model path is correct for your system!llm = LlamaCpp(    model_path="./ggml-model-q4_0.bin", callback_manager=callback_manager, verbose=True)llm_chain = LLMChain(prompt=prompt, llm=llm)question = "What NFL team won the Super Bowl in the year Justin Bieber was born?"llm_chain.run(question)            1. First, find out when Justin Bieber was born.    2. We know that Justin Bieber was born on March 1, 1994.    3. Next, we need to look up when the Super Bowl was played in that year.    4. The Super Bowl was played on January 28, 1995.    5. Finally, we can use this information to answer the question. The NFL team that won the Super Bowl in the year Justin Bieber was born is the San Francisco 49ers.        llama_print_timings:        load time =   434.15 ms    llama_print_timings:      sample time =    41.81 ms /   121 runs   (    0.35 ms per token)    llama_print_timings: prompt eval time =  2523.78 ms /    48 tokens (   52.58 ms per token)    llama_print_timings:        eval time = 23971.57 ms /   121 runs   (  198.11 ms per token)    llama_print_timings:       total time = 28945.95 ms    '\n\n1. First, find out when Justin Bieber was born.\n2. We know that Justin Bieber was born on March 1, 1994.\n3. Next, we need to look up when the Super Bowl was played in that year.\n4. The Super Bowl was played on January 28, 1995.\n5. Finally, we can use this information to answer the question. The NFL team that won the Super Bowl in the year Justin Bieber was born is the San Francisco 49ers.'GPU‚ÄãIf the installation with BLAS backend was correct, you will see a BLAS = 1 indicator in model properties.Two of the most important parameters for use with GPU are:n_gpu_layers - determines how many layers of the model are offloaded to your GPU.n_batch - how many tokens are processed in parallel. Setting these parameters correctly will dramatically improve the evaluation speed (see wrapper code for more details).n_gpu_layers = 40  # Change this value based on your model and your GPU VRAM pool.n_batch = 512  # Should be between 1 and n_ctx, consider the amount of VRAM in your GPU.# Make sure the model path is correct for your system!llm = LlamaCpp(    model_path="/Users/rlm/Desktop/Code/llama.cpp/models/openorca-platypus2-13b.gguf.q4_0.bin",    n_gpu_layers=n_gpu_layers,    n_batch=n_batch,    callback_manager=callback_manager,    verbose=True, # Verbose is required to pass to the callback manager)llm_chain = LLMChain(prompt=prompt, llm=llm)question = "What NFL team won the Super Bowl in the year Justin Bieber was born?"llm_chain.run(question)            1. Identify Justin Bieber's birth date: Justin Bieber was born on March 1, 1994.        2. Find the Super Bowl winner of that year: The NFL season of 1993 with the Super Bowl being played in January or of 1994.        3. Determine which team won the game: The Dallas Cowboys faced the Buffalo Bills in Super Bowl XXVII on January 31, 1993 (as the year is mis-labelled due to a error). The Dallas Cowboys won this matchup.        So, Justin Bieber was born when the Dallas Cowboys were the reigning NFL Super Bowl.        llama_print_timings:        load time =   427.63 ms    llama_print_timings:      sample time =   115.85 ms /   164 runs   (    0.71 ms per token,  1415.67 tokens per second)    llama_print_timings: prompt eval time =   427.53 ms /    45 tokens (    9.50 ms per token,   105.26 tokens per second)    llama_print_timings:        eval time =  4526.53 ms /   163 runs   (   27.77 ms per token,    36.01 tokens per second)    llama_print_timings:       total time =  5293.77 ms    "\n\n1. Identify Justin Bieber's birth date: Justin Bieber was born on March 1, 1994.\n\n2. Find the Super Bowl winner of that year: The NFL season of 1993 with the Super Bowl being played in January or of 1994.\n\n3. Determine which team won the game: The Dallas Cowboys faced the Buffalo Bills in Super Bowl XXVII on January 31, 1993 (as the year is mis-labelled due to a error). The Dallas Cowboys won this matchup.\n\nSo, Justin Bieber was born when the Dallas Cowboys were the reigning NFL Super Bowl."Metal‚ÄãIf the installation with Metal was correct, you will see a NEON = 1 indicator in model properties.Two of the most important GPU parameters are:n_gpu_layers - determines how many layers of the model are offloaded to your Metal GPU, in the most case, set it to 1 is enough for Metaln_batch - how many tokens are processed in parallel, default is 8, set to bigger number.f16_kv - for some reason, Metal only support True, otherwise you will get error such as Asserting on type 0 GGML_ASSERT: .../ggml-metal.m:706: false && "not implemented"Setting these parameters correctly will dramatically improve the evaluation speed (see wrapper code for more details).n_gpu_layers = 1  # Metal set to 1 is enough.n_batch = 512  # Should be between 1 and n_ctx, consider the amount of RAM of your Apple Silicon Chip.# Make sure the model path is correct for your system!llm = LlamaCpp(    model_path="/Users/rlm/Desktop/Code/llama.cpp/models/openorca-platypus2-13b.gguf.q4_0.bin",    n_gpu_layers=n_gpu_layers,    n_batch=n_batch,    f16_kv=True,  # MUST set to True, otherwise you will run into problem after a couple of calls    callback_manager=callback_manager,    verbose=True, # Verbose is required to pass to the callback manager)The console log will show the following log to indicate Metal was enable properly.ggml_metal_init: allocatingggml_metal_init: using MPS...You also could check Activity Monitor by watching the GPU usage of the process, the CPU usage will drop dramatically after turn on n_gpu_layers=1. For the first call to the LLM, the performance may be slow due to the model compilation in Metal GPU.Grammars‚ÄãWe can specify grammars to constrain model outputs.This will sample tokens according to the grammar.For example, supply the path to the specifed json.gbnf file in order to produce JSON.n_gpu_layers = 1  # Metal set to 1 is enough.n_batch = 512  # Should be between 1 and n_ctx, consider the amount of RAM of your Apple Silicon Chip.# Make sure the model path is correct for your system!llm = LlamaCpp(    model_path="/Users/rlm/Desktop/Code/llama.cpp/models/openorca-platypus2-13b.gguf.q4_0.bin",    n_gpu_layers=n_gpu_layers,    n_batch=n_batch,    f16_kv=True,  # MUST set to True, otherwise you will run into problem after a couple of calls    callback_manager=callback_manager,    verbose=True, # Verbose is required to pass to the callback manager    grammar_path="/Users/rlm/Desktop/Code/langchain-main/langchain/libs/langchain/langchain/llms/grammars/json.gbnf",)result=llm("Describe a person in JSON format:")    {      "name": "John Doe",      "age": 34,      "": {        "title": "Software Developer",        "company": "Google"      },      "interests": [        "Sports",        "Music",        "Cooking"      ],      "address": {        "street_number": 123,        "street_name": "Oak Street",        "city": "Mountain View",        "state": "California",        "postal_code": 94040      }}        llama_print_timings:        load time =   357.51 ms    llama_print_timings:      sample time =  1213.30 ms /   144 runs   (    8.43 ms per token,   118.68 tokens per second)    llama_print_timings: prompt eval time =   356.78 ms /     9 tokens (   39.64 ms per token,    25.23 tokens per second)    llama_print_timings:        eval time =  3947.16 ms /   143 runs   (   27.60 ms per token,    36.23 tokens per second)    llama_print_timings:       total time =  5846.21 msWe can also supply list.gbnf to return a list.n_gpu_layers = 1 n_batch = 512llm = LlamaCpp(    model_path="/Users/rlm/Desktop/Code/llama.cpp/models/openorca-platypus2-13b.gguf.q4_0.bin",    n_gpu_layers=n_gpu_layers,    n_batch=n_batch,    f16_kv=True,  # MUST set to True, otherwise you will run into problem after a couple of calls    callback_manager=callback_manager,    verbose=True,    grammar_path="/Users/rlm/Desktop/Code/langchain-main/langchain/libs/langchain/langchain/llms/grammars/list.gbnf",)result=llm("List of top-3 my favourite books:")    ["The Catcher in the Rye", "Wuthering Heights", "Anna Karenina"]        llama_print_timings:        load time =   322.34 ms    llama_print_timings:      sample time =   232.60 ms /    26 runs   (    8.95 ms per token,   111.78 tokens per second)    llama_print_timings: prompt eval time =   321.90 ms /    11 tokens (   29.26 ms per token,    34.17 tokens per second)    llama_print_timings:        eval time =   680.82 ms /    25 runs   (   27.23 ms per token,    36.72 tokens per second)    llama_print_timings:       total time =  1295.27 msPreviousKoboldAI APINextLLM Caching integrations IntegrationsLLMsLlama.cppOn this pageLlama.cppllama-cpp-python is a Python binding for llama.cpp. It supports inference for many LLMs, which can be accessed on HuggingFace.This notebook goes over how to run llama-cpp-python within LangChain.Note: new versions of llama-cpp-python use GGUF model files (see here).This is a breaking change.To convert existing GGML models to GGUF you can run the following in llama.cpp:python ./convert-llama-ggmlv3-to-gguf.py --eps 1e-5 --input models/openorca-platypus2-13b.ggmlv3.q4_0.bin --output models/openorca-platypus2-13b.gguf.q4_0.binInstallation‚ÄãThere are different options on how to install the llama-cpp package: CPU usageCPU + GPU (using one of many BLAS backends)Metal GPU (MacOS with Apple Silicon Chip) CPU only installation‚Äãpip install llama-cpp-pythonInstallation with OpenBLAS / cuBLAS / CLBlast‚Äãlama.cpp supports multiple BLAS backends for faster processing. Use the FORCE_CMAKE=1 environment variable to force the use of cmake and install the pip package for the desired BLAS backend (source).Example installation with cuBLAS backend:CMAKE_ARGS="-DLLAMA_CUBLAS=on" FORCE_CMAKE=1 pip install llama-cpp-pythonIMPORTANT: If you have already installed the CPU only version of the package, you need to reinstall it from scratch. Consider the following command: CMAKE_ARGS="-DLLAMA_CUBLAS=on" FORCE_CMAKE=1 pip install --upgrade --force-reinstall llama-cpp-python --no-cache-dirInstallation with Metal‚Äãllama.cpp supports Apple silicon first-class citizen - optimized via ARM NEON, Accelerate and Metal frameworks. Use the FORCE_CMAKE=1 environment variable to force the use of cmake and install the pip package for the Metal support (source).Example installation with Metal Support:CMAKE_ARGS="-DLLAMA_METAL=on" FORCE_CMAKE=1 pip install llama-cpp-pythonIMPORTANT: If you have already installed a cpu only version of the package, you need to reinstall it from scratch: consider the following command: CMAKE_ARGS="-DLLAMA_METAL=on" FORCE_CMAKE=1 pip install --upgrade --force-reinstall llama-cpp-python --no-cache-dirInstallation with Windows‚ÄãIt is stable to install the llama-cpp-python library by compiling from the source. You can follow most of the instructions in the repository itself but there are some windows specific instructions which might be useful.Requirements to install the llama-cpp-python,gitpythoncmakeVisual Studio Community (make sure you install this with the following settings)Desktop development with C++Python developmentLinux embedded development with C++Clone git repository recursively to get llama.cpp submodule as well git clone --recursive -j8 https://github.com/abetlen/llama-cpp-python.gitOpen up command Prompt (or anaconda prompt if you have it installed), set up environment variables to install. Follow this if you do not have a GPU, you must set both of the following variables.set FORCE_CMAKE=1set CMAKE_ARGS=-DLLAMA_CUBLAS=OFFYou can ignore the second environment variable if you have an NVIDIA GPU.Compiling and installing‚ÄãIn the same command prompt (anaconda prompt) you set the variables, you can cd into llama-cpp-python directory and run the following commands.python setup.py cleanpython setup.py installUsage‚ÄãMake sure you are following all instructions to install all necessary model files.You don't need an API_TOKEN as you will run the LLM locally.It is worth understanding which models are suitable to be used on the desired machine.from langchain.llms import LlamaCppfrom langchain import PromptTemplate, LLMChainfrom langchain.callbacks.manager import CallbackManagerfrom langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandlerAPI Reference:LlamaCppCallbackManagerStreamingStdOutCallbackHandlerConsider using a template that suits your model! Check the models page on HuggingFace etc. to get a correct prompting template.template = """Question: {question}Answer: Let's work this out in a step by step way to be sure we have the right answer."""prompt = PromptTemplate(template=template, input_variables=["question"])# Callbacks support token-wise streamingcallback_manager = CallbackManager([StreamingStdOutCallbackHandler()])CPU‚ÄãExample using a LLaMA 2 7B model# Make sure the model path is correct for your system!llm = LlamaCpp(    model_path="/Users/rlm/Desktop/Code/llama.cpp/models/openorca-platypus2-13b.gguf.q4_0.bin",    temperature=0.75,    max_tokens=2000,    top_p=1,    callback_manager=callback_manager,     verbose=True, # Verbose is required to pass to the callback manager)prompt = """Question: A rap battle between Stephen Colbert and John Oliver"""llm(prompt)        Stephen Colbert:    Yo, John, I heard you've been talkin' smack about me on your show.    Let me tell you somethin', pal, I'm the king of late-night TV    My satire is sharp as a razor, it cuts deeper than a knife    While you're just a british bloke tryin' to be funny with your accent and your wit.    John Oliver:    Oh Stephen, don't be ridiculous, you may have the ratings but I got the real talk.    My show is the one that people actually watch and listen to, not just for the laughs but for the facts.    While you're busy talkin' trash, I'm out here bringing the truth to light.    Stephen Colbert:    Truth? Ha! You think your show is about truth? Please, it's all just a joke to you.    You're just a fancy-pants british guy tryin' to be funny with your news and your jokes.    While I'm the one who's really makin' a difference, with my sat        llama_print_timings:        load time =   358.60 ms    llama_print_timings:      sample time =   172.55 ms /   256 runs   (    0.67 ms per token,  1483.59 tokens per second)    llama_print_timings: prompt eval time =   613.36 ms /    16 tokens (   38.33 ms per token,    26.09 tokens per second)    llama_print_timings:        eval time = 10151.17 ms /   255 runs   (   39.81 ms per token,    25.12 tokens per second)    llama_print_timings:       total time = 11332.41 ms    "\nStephen Colbert:\nYo, John, I heard you've been talkin' smack about me on your show.\nLet me tell you somethin', pal, I'm the king of late-night TV\nMy satire is sharp as a razor, it cuts deeper than a knife\nWhile you're just a british bloke tryin' to be funny with your accent and your wit.\nJohn Oliver:\nOh Stephen, don't be ridiculous, you may have the ratings but I got the real talk.\nMy show is the one that people actually watch and listen to, not just for the laughs but for the facts.\nWhile you're busy talkin' trash, I'm out here bringing the truth to light.\nStephen Colbert:\nTruth? Ha! You think your show is about truth? Please, it's all just a joke to you.\nYou're just a fancy-pants british guy tryin' to be funny with your news and your jokes.\nWhile I'm the one who's really makin' a difference, with my sat"Example using a LLaMA v1 model# Make sure the model path is correct for your system!llm = LlamaCpp(    model_path="./ggml-model-q4_0.bin", callback_manager=callback_manager, verbose=True)llm_chain = LLMChain(prompt=prompt, llm=llm)question = "What NFL team won the Super Bowl in the year Justin Bieber was born?"llm_chain.run(question)            1. First, find out when Justin Bieber was born.    2. We know that Justin Bieber was born on March 1, 1994.    3. Next, we need to look up when the Super Bowl was played in that year.    4. The Super Bowl was played on January 28, 1995.    5. Finally, we can use this information to answer the question. The NFL team that won the Super Bowl in the year Justin Bieber was born is the San Francisco 49ers.        llama_print_timings:        load time =   434.15 ms    llama_print_timings:      sample time =    41.81 ms /   121 runs   (    0.35 ms per token)    llama_print_timings: prompt eval time =  2523.78 ms /    48 tokens (   52.58 ms per token)    llama_print_timings:        eval time = 23971.57 ms /   121 runs   (  198.11 ms per token)    llama_print_timings:       total time = 28945.95 ms    '\n\n1. First, find out when Justin Bieber was born.\n2. We know that Justin Bieber was born on March 1, 1994.\n3. Next, we need to look up when the Super Bowl was played in that year.\n4. The Super Bowl was played on January 28, 1995.\n5. Finally, we can use this information to answer the question. The NFL team that won the Super Bowl in the year Justin Bieber was born is the San Francisco 49ers.'GPU‚ÄãIf the installation with BLAS backend was correct, you will see a BLAS = 1 indicator in model properties.Two of the most important parameters for use with GPU are:n_gpu_layers - determines how many layers of the model are offloaded to your GPU.n_batch - how many tokens are processed in parallel. Setting these parameters correctly will dramatically improve the evaluation speed (see wrapper code for more details).n_gpu_layers = 40  # Change this value based on your model and your GPU VRAM pool.n_batch = 512  # Should be between 1 and n_ctx, consider the amount of VRAM in your GPU.# Make sure the model path is correct for your system!llm = LlamaCpp(    model_path="/Users/rlm/Desktop/Code/llama.cpp/models/openorca-platypus2-13b.gguf.q4_0.bin",    n_gpu_layers=n_gpu_layers,    n_batch=n_batch,    callback_manager=callback_manager,    verbose=True, # Verbose is required to pass to the callback manager)llm_chain = LLMChain(prompt=prompt, llm=llm)question = "What NFL team won the Super Bowl in the year Justin Bieber was born?"llm_chain.run(question)            1. Identify Justin Bieber's birth date: Justin Bieber was born on March 1, 1994.        2. Find the Super Bowl winner of that year: The NFL season of 1993 with the Super Bowl being played in January or of 1994.        3. Determine which team won the game: The Dallas Cowboys faced the Buffalo Bills in Super Bowl XXVII on January 31, 1993 (as the year is mis-labelled due to a error). The Dallas Cowboys won this matchup.        So, Justin Bieber was born when the Dallas Cowboys were the reigning NFL Super Bowl.        llama_print_timings:        load time =   427.63 ms    llama_print_timings:      sample time =   115.85 ms /   164 runs   (    0.71 ms per token,  1415.67 tokens per second)    llama_print_timings: prompt eval time =   427.53 ms /    45 tokens (    9.50 ms per token,   105.26 tokens per second)    llama_print_timings:        eval time =  4526.53 ms /   163 runs   (   27.77 ms per token,    36.01 tokens per second)    llama_print_timings:       total time =  5293.77 ms    "\n\n1. Identify Justin Bieber's birth date: Justin Bieber was born on March 1, 1994.\n\n2. Find the Super Bowl winner of that year: The NFL season of 1993 with the Super Bowl being played in January or of 1994.\n\n3. Determine which team won the game: The Dallas Cowboys faced the Buffalo Bills in Super Bowl XXVII on January 31, 1993 (as the year is mis-labelled due to a error). The Dallas Cowboys won this matchup.\n\nSo, Justin Bieber was born when the Dallas Cowboys were the reigning NFL Super Bowl."Metal‚ÄãIf the installation with Metal was correct, you will see a NEON = 1 indicator in model properties.Two of the most important GPU parameters are:n_gpu_layers - determines how many layers of the model are offloaded to your Metal GPU, in the most case, set it to 1 is enough for Metaln_batch - how many tokens are processed in parallel, default is 8, set to bigger number.f16_kv - for some reason, Metal only support True, otherwise you will get error such as Asserting on type 0 GGML_ASSERT: .../ggml-metal.m:706: false && "not implemented"Setting these parameters correctly will dramatically improve the evaluation speed (see wrapper code for more details).n_gpu_layers = 1  # Metal set to 1 is enough.n_batch = 512  # Should be between 1 and n_ctx, consider the amount of RAM of your Apple Silicon Chip.# Make sure the model path is correct for your system!llm = LlamaCpp(    model_path="/Users/rlm/Desktop/Code/llama.cpp/models/openorca-platypus2-13b.gguf.q4_0.bin",    n_gpu_layers=n_gpu_layers,    n_batch=n_batch,    f16_kv=True,  # MUST set to True, otherwise you will run into problem after a couple of calls    callback_manager=callback_manager,    verbose=True, # Verbose is required to pass to the callback manager)The console log will show the following log to indicate Metal was enable properly.ggml_metal_init: allocatingggml_metal_init: using MPS...You also could check Activity Monitor by watching the GPU usage of the process, the CPU usage will drop dramatically after turn on n_gpu_layers=1. For the first call to the LLM, the performance may be slow due to the model compilation in Metal GPU.Grammars‚ÄãWe can specify grammars to constrain model outputs.This will sample tokens according to the grammar.For example, supply the path to the specifed json.gbnf file in order to produce JSON.n_gpu_layers = 1  # Metal set to 1 is enough.n_batch = 512  # Should be between 1 and n_ctx, consider the amount of RAM of your Apple Silicon Chip.# Make sure the model path is correct for your system!llm = LlamaCpp(    model_path="/Users/rlm/Desktop/Code/llama.cpp/models/openorca-platypus2-13b.gguf.q4_0.bin",    n_gpu_layers=n_gpu_layers,    n_batch=n_batch,    f16_kv=True,  # MUST set to True, otherwise you will run into problem after a couple of calls    callback_manager=callback_manager,    verbose=True, # Verbose is required to pass to the callback manager    grammar_path="/Users/rlm/Desktop/Code/langchain-main/langchain/libs/langchain/langchain/llms/grammars/json.gbnf",)result=llm("Describe a person in JSON format:")    {      "name": "John Doe",      "age": 34,      "": {        "title": "Software Developer",        "company": "Google"      },      "interests": [        "Sports",        "Music",        "Cooking"      ],      "address": {        "street_number": 123,        "street_name": "Oak Street",        "city": "Mountain View",        "state": "California",        "postal_code": 94040      }}        llama_print_timings:        load time =   357.51 ms    llama_print_timings:      sample time =  1213.30 ms /   144 runs   (    8.43 ms per token,   118.68 tokens per second)    llama_print_timings: prompt eval time =   356.78 ms /     9 tokens (   39.64 ms per token,    25.23 tokens per second)    llama_print_timings:        eval time =  3947.16 ms /   143 runs   (   27.60 ms per token,    36.23 tokens per second)    llama_print_timings:       total time =  5846.21 msWe can also supply list.gbnf to return a list.n_gpu_layers = 1 n_batch = 512llm = LlamaCpp(    model_path="/Users/rlm/Desktop/Code/llama.cpp/models/openorca-platypus2-13b.gguf.q4_0.bin",    n_gpu_layers=n_gpu_layers,    n_batch=n_batch,    f16_kv=True,  # MUST set to True, otherwise you will run into problem after a couple of calls    callback_manager=callback_manager,    verbose=True,    grammar_path="/Users/rlm/Desktop/Code/langchain-main/langchain/libs/langchain/langchain/llms/grammars/list.gbnf",)result=llm("List of top-3 my favourite books:")    ["The Catcher in the Rye", "Wuthering Heights", "Anna Karenina"]        llama_print_timings:        load time =   322.34 ms    llama_print_timings:      sample time =   232.60 ms /    26 runs   (    8.95 ms per token,   111.78 tokens per second)    llama_print_timings: prompt eval time =   321.90 ms /    11 tokens (   29.26 ms per token,    34.17 tokens per second)    llama_print_timings:        eval time =   680.82 ms /    25 runs   (   27.23 ms per token,    36.72 tokens per second)    llama_print_timings:       total time =  1295.27 msPreviousKoboldAI APINextLLM Caching integrations On this page Llama.cppllama-cpp-python is a Python binding for llama.cpp. It supports inference for many LLMs, which can be accessed on HuggingFace.This notebook goes over how to run llama-cpp-python within LangChain.Note: new versions of llama-cpp-python use GGUF model files (see here).This is a breaking change.To convert existing GGML models to GGUF you can run the following in llama.cpp:python ./convert-llama-ggmlv3-to-gguf.py --eps 1e-5 --input models/openorca-platypus2-13b.ggmlv3.q4_0.bin --output models/openorca-platypus2-13b.gguf.q4_0.binInstallation‚ÄãThere are different options on how to install the llama-cpp package: CPU usageCPU + GPU (using one of many BLAS backends)Metal GPU (MacOS with Apple Silicon Chip) CPU only installation‚Äãpip install llama-cpp-pythonInstallation with OpenBLAS / cuBLAS / CLBlast‚Äãlama.cpp supports multiple BLAS backends for faster processing. Use the FORCE_CMAKE=1 environment variable to force the use of cmake and install the pip package for the desired BLAS backend (source).Example installation with cuBLAS backend:CMAKE_ARGS="-DLLAMA_CUBLAS=on" FORCE_CMAKE=1 pip install llama-cpp-pythonIMPORTANT: If you have already installed the CPU only version of the package, you need to reinstall it from scratch. Consider the following command: CMAKE_ARGS="-DLLAMA_CUBLAS=on" FORCE_CMAKE=1 pip install --upgrade --force-reinstall llama-cpp-python --no-cache-dirInstallation with Metal‚Äãllama.cpp supports Apple silicon first-class citizen - optimized via ARM NEON, Accelerate and Metal frameworks. Use the FORCE_CMAKE=1 environment variable to force the use of cmake and install the pip package for the Metal support (source).Example installation with Metal Support:CMAKE_ARGS="-DLLAMA_METAL=on" FORCE_CMAKE=1 pip install llama-cpp-pythonIMPORTANT: If you have already installed a cpu only version of the package, you need to reinstall it from scratch: consider the following command: CMAKE_ARGS="-DLLAMA_METAL=on" FORCE_CMAKE=1 pip install --upgrade --force-reinstall llama-cpp-python --no-cache-dirInstallation with Windows‚ÄãIt is stable to install the llama-cpp-python library by compiling from the source. You can follow most of the instructions in the repository itself but there are some windows specific instructions which might be useful.Requirements to install the llama-cpp-python,gitpythoncmakeVisual Studio Community (make sure you install this with the following settings)Desktop development with C++Python developmentLinux embedded development with C++Clone git repository recursively to get llama.cpp submodule as well git clone --recursive -j8 https://github.com/abetlen/llama-cpp-python.gitOpen up command Prompt (or anaconda prompt if you have it installed), set up environment variables to install. Follow this if you do not have a GPU, you must set both of the following variables.set FORCE_CMAKE=1set CMAKE_ARGS=-DLLAMA_CUBLAS=OFFYou can ignore the second environment variable if you have an NVIDIA GPU.Compiling and installing‚ÄãIn the same command prompt (anaconda prompt) you set the variables, you can cd into llama-cpp-python directory and run the following commands.python setup.py cleanpython setup.py installUsage‚ÄãMake sure you are following all instructions to install all necessary model files.You don't need an API_TOKEN as you will run the LLM locally.It is worth understanding which models are suitable to be used on the desired machine.from langchain.llms import LlamaCppfrom langchain import PromptTemplate, LLMChainfrom langchain.callbacks.manager import CallbackManagerfrom langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandlerAPI Reference:LlamaCppCallbackManagerStreamingStdOutCallbackHandlerConsider using a template that suits your model! Check the models page on HuggingFace etc. to get a correct prompting template.template = """Question: {question}Answer: Let's work this out in a step by step way to be sure we have the right answer."""prompt = PromptTemplate(template=template, input_variables=["question"])# Callbacks support token-wise streamingcallback_manager = CallbackManager([StreamingStdOutCallbackHandler()])CPU‚ÄãExample using a LLaMA 2 7B model# Make sure the model path is correct for your system!llm = LlamaCpp(    model_path="/Users/rlm/Desktop/Code/llama.cpp/models/openorca-platypus2-13b.gguf.q4_0.bin",    temperature=0.75,    max_tokens=2000,    top_p=1,    callback_manager=callback_manager,     verbose=True, # Verbose is required to pass to the callback manager)prompt = """Question: A rap battle between Stephen Colbert and John Oliver"""llm(prompt)        Stephen Colbert:    Yo, John, I heard you've been talkin' smack about me on your show.    Let me tell you somethin', pal, I'm the king of late-night TV    My satire is sharp as a razor, it cuts deeper than a knife    While you're just a british bloke tryin' to be funny with your accent and your wit.    John Oliver:    Oh Stephen, don't be ridiculous, you may have the ratings but I got the real talk.    My show is the one that people actually watch and listen to, not just for the laughs but for the facts.    While you're busy talkin' trash, I'm out here bringing the truth to light.    Stephen Colbert:    Truth? Ha! You think your show is about truth? Please, it's all just a joke to you.    You're just a fancy-pants british guy tryin' to be funny with your news and your jokes.    While I'm the one who's really makin' a difference, with my sat        llama_print_timings:        load time =   358.60 ms    llama_print_timings:      sample time =   172.55 ms /   256 runs   (    0.67 ms per token,  1483.59 tokens per second)    llama_print_timings: prompt eval time =   613.36 ms /    16 tokens (   38.33 ms per token,    26.09 tokens per second)    llama_print_timings:        eval time = 10151.17 ms /   255 runs   (   39.81 ms per token,    25.12 tokens per second)    llama_print_timings:       total time = 11332.41 ms    "\nStephen Colbert:\nYo, John, I heard you've been talkin' smack about me on your show.\nLet me tell you somethin', pal, I'm the king of late-night TV\nMy satire is sharp as a razor, it cuts deeper than a knife\nWhile you're just a british bloke tryin' to be funny with your accent and your wit.\nJohn Oliver:\nOh Stephen, don't be ridiculous, you may have the ratings but I got the real talk.\nMy show is the one that people actually watch and listen to, not just for the laughs but for the facts.\nWhile you're busy talkin' trash, I'm out here bringing the truth to light.\nStephen Colbert:\nTruth? Ha! You think your show is about truth? Please, it's all just a joke to you.\nYou're just a fancy-pants british guy tryin' to be funny with your news and your jokes.\nWhile I'm the one who's really makin' a difference, with my sat"Example using a LLaMA v1 model# Make sure the model path is correct for your system!llm = LlamaCpp(    model_path="./ggml-model-q4_0.bin", callback_manager=callback_manager, verbose=True)llm_chain = LLMChain(prompt=prompt, llm=llm)question = "What NFL team won the Super Bowl in the year Justin Bieber was born?"llm_chain.run(question)            1. First, find out when Justin Bieber was born.    2. We know that Justin Bieber was born on March 1, 1994.    3. Next, we need to look up when the Super Bowl was played in that year.    4. The Super Bowl was played on January 28, 1995.    5. Finally, we can use this information to answer the question. The NFL team that won the Super Bowl in the year Justin Bieber was born is the San Francisco 49ers.        llama_print_timings:        load time =   434.15 ms    llama_print_timings:      sample time =    41.81 ms /   121 runs   (    0.35 ms per token)    llama_print_timings: prompt eval time =  2523.78 ms /    48 tokens (   52.58 ms per token)    llama_print_timings:        eval time = 23971.57 ms /   121 runs   (  198.11 ms per token)    llama_print_timings:       total time = 28945.95 ms    '\n\n1. First, find out when Justin Bieber was born.\n2. We know that Justin Bieber was born on March 1, 1994.\n3. Next, we need to look up when the Super Bowl was played in that year.\n4. The Super Bowl was played on January 28, 1995.\n5. Finally, we can use this information to answer the question. The NFL team that won the Super Bowl in the year Justin Bieber was born is the San Francisco 49ers.'GPU‚ÄãIf the installation with BLAS backend was correct, you will see a BLAS = 1 indicator in model properties.Two of the most important parameters for use with GPU are:n_gpu_layers - determines how many layers of the model are offloaded to your GPU.n_batch - how many tokens are processed in parallel. Setting these parameters correctly will dramatically improve the evaluation speed (see wrapper code for more details).n_gpu_layers = 40  # Change this value based on your model and your GPU VRAM pool.n_batch = 512  # Should be between 1 and n_ctx, consider the amount of VRAM in your GPU.# Make sure the model path is correct for your system!llm = LlamaCpp(    model_path="/Users/rlm/Desktop/Code/llama.cpp/models/openorca-platypus2-13b.gguf.q4_0.bin",    n_gpu_layers=n_gpu_layers,    n_batch=n_batch,    callback_manager=callback_manager,    verbose=True, # Verbose is required to pass to the callback manager)llm_chain = LLMChain(prompt=prompt, llm=llm)question = "What NFL team won the Super Bowl in the year Justin Bieber was born?"llm_chain.run(question)            1. Identify Justin Bieber's birth date: Justin Bieber was born on March 1, 1994.        2. Find the Super Bowl winner of that year: The NFL season of 1993 with the Super Bowl being played in January or of 1994.        3. Determine which team won the game: The Dallas Cowboys faced the Buffalo Bills in Super Bowl XXVII on January 31, 1993 (as the year is mis-labelled due to a error). The Dallas Cowboys won this matchup.        So, Justin Bieber was born when the Dallas Cowboys were the reigning NFL Super Bowl.        llama_print_timings:        load time =   427.63 ms    llama_print_timings:      sample time =   115.85 ms /   164 runs   (    0.71 ms per token,  1415.67 tokens per second)    llama_print_timings: prompt eval time =   427.53 ms /    45 tokens (    9.50 ms per token,   105.26 tokens per second)    llama_print_timings:        eval time =  4526.53 ms /   163 runs   (   27.77 ms per token,    36.01 tokens per second)    llama_print_timings:       total time =  5293.77 ms    "\n\n1. Identify Justin Bieber's birth date: Justin Bieber was born on March 1, 1994.\n\n2. Find the Super Bowl winner of that year: The NFL season of 1993 with the Super Bowl being played in January or of 1994.\n\n3. Determine which team won the game: The Dallas Cowboys faced the Buffalo Bills in Super Bowl XXVII on January 31, 1993 (as the year is mis-labelled due to a error). The Dallas Cowboys won this matchup.\n\nSo, Justin Bieber was born when the Dallas Cowboys were the reigning NFL Super Bowl."Metal‚ÄãIf the installation with Metal was correct, you will see a NEON = 1 indicator in model properties.Two of the most important GPU parameters are:n_gpu_layers - determines how many layers of the model are offloaded to your Metal GPU, in the most case, set it to 1 is enough for Metaln_batch - how many tokens are processed in parallel, default is 8, set to bigger number.f16_kv - for some reason, Metal only support True, otherwise you will get error such as Asserting on type 0 GGML_ASSERT: .../ggml-metal.m:706: false && "not implemented"Setting these parameters correctly will dramatically improve the evaluation speed (see wrapper code for more details).n_gpu_layers = 1  # Metal set to 1 is enough.n_batch = 512  # Should be between 1 and n_ctx, consider the amount of RAM of your Apple Silicon Chip.# Make sure the model path is correct for your system!llm = LlamaCpp(    model_path="/Users/rlm/Desktop/Code/llama.cpp/models/openorca-platypus2-13b.gguf.q4_0.bin",    n_gpu_layers=n_gpu_layers,    n_batch=n_batch,    f16_kv=True,  # MUST set to True, otherwise you will run into problem after a couple of calls    callback_manager=callback_manager,    verbose=True, # Verbose is required to pass to the callback manager)The console log will show the following log to indicate Metal was enable properly.ggml_metal_init: allocatingggml_metal_init: using MPS...You also could check Activity Monitor by watching the GPU usage of the process, the CPU usage will drop dramatically after turn on n_gpu_layers=1. For the first call to the LLM, the performance may be slow due to the model compilation in Metal GPU.Grammars‚ÄãWe can specify grammars to constrain model outputs.This will sample tokens according to the grammar.For example, supply the path to the specifed json.gbnf file in order to produce JSON.n_gpu_layers = 1  # Metal set to 1 is enough.n_batch = 512  # Should be between 1 and n_ctx, consider the amount of RAM of your Apple Silicon Chip.# Make sure the model path is correct for your system!llm = LlamaCpp(    model_path="/Users/rlm/Desktop/Code/llama.cpp/models/openorca-platypus2-13b.gguf.q4_0.bin",    n_gpu_layers=n_gpu_layers,    n_batch=n_batch,    f16_kv=True,  # MUST set to True, otherwise you will run into problem after a couple of calls    callback_manager=callback_manager,    verbose=True, # Verbose is required to pass to the callback manager    grammar_path="/Users/rlm/Desktop/Code/langchain-main/langchain/libs/langchain/langchain/llms/grammars/json.gbnf",)result=llm("Describe a person in JSON format:")    {      "name": "John Doe",      "age": 34,      "": {        "title": "Software Developer",        "company": "Google"      },      "interests": [        "Sports",        "Music",        "Cooking"      ],      "address": {        "street_number": 123,        "street_name": "Oak Street",        "city": "Mountain View",        "state": "California",        "postal_code": 94040      }}        llama_print_timings:        load time =   357.51 ms    llama_print_timings:      sample time =  1213.30 ms /   144 runs   (    8.43 ms per token,   118.68 tokens per second)    llama_print_timings: prompt eval time =   356.78 ms /     9 tokens (   39.64 ms per token,    25.23 tokens per second)    llama_print_timings:        eval time =  3947.16 ms /   143 runs   (   27.60 ms per token,    36.23 tokens per second)    llama_print_timings:       total time =  5846.21 msWe can also supply list.gbnf to return a list.n_gpu_layers = 1 n_batch = 512llm = LlamaCpp(    model_path="/Users/rlm/Desktop/Code/llama.cpp/models/openorca-platypus2-13b.gguf.q4_0.bin",    n_gpu_layers=n_gpu_layers,    n_batch=n_batch,    f16_kv=True,  # MUST set to True, otherwise you will run into problem after a couple of calls    callback_manager=callback_manager,    verbose=True,    grammar_path="/Users/rlm/Desktop/Code/langchain-main/langchain/libs/langchain/langchain/llms/grammars/list.gbnf",)result=llm("List of top-3 my favourite books:")    ["The Catcher in the Rye", "Wuthering Heights", "Anna Karenina"]        llama_print_timings:        load time =   322.34 ms    llama_print_timings:      sample time =   232.60 ms /    26 runs   (    8.95 ms per token,   111.78 tokens per second)    llama_print_timings: prompt eval time =   321.90 ms /    11 tokens (   29.26 ms per token,    34.17 tokens per second)    llama_print_timings:        eval time =   680.82 ms /    25 runs   (   27.23 ms per token,    36.72 tokens per second)    llama_print_timings:       total time =  1295.27 ms python ./convert-llama-ggmlv3-to-gguf.py --eps 1e-5 --input models/openorca-platypus2-13b.ggmlv3.q4_0.bin --output models/openorca-platypus2-13b.gguf.q4_0.bin python ./convert-llama-ggmlv3-to-gguf.py --eps 1e-5 --input models/openorca-platypus2-13b.ggmlv3.q4_0.bin --output models/openorca-platypus2-13b.gguf.q4_0.bin  pip install llama-cpp-python pip install llama-cpp-python  CMAKE_ARGS="-DLLAMA_CUBLAS=on" FORCE_CMAKE=1 pip install llama-cpp-python CMAKE_ARGS="-DLLAMA_CUBLAS=on" FORCE_CMAKE=1 pip install llama-cpp-python  CMAKE_ARGS="-DLLAMA_CUBLAS=on" FORCE_CMAKE=1 pip install --upgrade --force-reinstall llama-cpp-python --no-cache-dir CMAKE_ARGS="-DLLAMA_CUBLAS=on" FORCE_CMAKE=1 pip install --upgrade --force-reinstall llama-cpp-python --no-cache-dir  CMAKE_ARGS="-DLLAMA_METAL=on" FORCE_CMAKE=1 pip install llama-cpp-python CMAKE_ARGS="-DLLAMA_METAL=on" FORCE_CMAKE=1 pip install llama-cpp-python  CMAKE_ARGS="-DLLAMA_METAL=on" FORCE_CMAKE=1 pip install --upgrade --force-reinstall llama-cpp-python --no-cache-dir CMAKE_ARGS="-DLLAMA_METAL=on" FORCE_CMAKE=1 pip install --upgrade --force-reinstall llama-cpp-python --no-cache-dir  git clone --recursive -j8 https://github.com/abetlen/llama-cpp-python.git git clone --recursive -j8 https://github.com/abetlen/llama-cpp-python.git  set FORCE_CMAKE=1set CMAKE_ARGS=-DLLAMA_CUBLAS=OFF set FORCE_CMAKE=1set CMAKE_ARGS=-DLLAMA_CUBLAS=OFF  python setup.py cleanpython setup.py install python setup.py cleanpython setup.py install  from langchain.llms import LlamaCppfrom langchain import PromptTemplate, LLMChainfrom langchain.callbacks.manager import CallbackManagerfrom langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler from langchain.llms import LlamaCppfrom langchain import PromptTemplate, LLMChainfrom langchain.callbacks.manager import CallbackManagerfrom langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler  API Reference:LlamaCppCallbackManagerStreamingStdOutCallbackHandler template = """Question: {question}Answer: Let's work this out in a step by step way to be sure we have the right answer."""prompt = PromptTemplate(template=template, input_variables=["question"]) template = """Question: {question}Answer: Let's work this out in a step by step way to be sure we have the right answer."""prompt = PromptTemplate(template=template, input_variables=["question"])  # Callbacks support token-wise streamingcallback_manager = CallbackManager([StreamingStdOutCallbackHandler()]) # Callbacks support token-wise streamingcallback_manager = CallbackManager([StreamingStdOutCallbackHandler()])  # Make sure the model path is correct for your system!llm = LlamaCpp(    model_path="/Users/rlm/Desktop/Code/llama.cpp/models/openorca-platypus2-13b.gguf.q4_0.bin",    temperature=0.75,    max_tokens=2000,    top_p=1,    callback_manager=callback_manager,     verbose=True, # Verbose is required to pass to the callback manager) # Make sure the model path is correct for your system!llm = LlamaCpp(    model_path="/Users/rlm/Desktop/Code/llama.cpp/models/openorca-platypus2-13b.gguf.q4_0.bin",    temperature=0.75,    max_tokens=2000,    top_p=1,    callback_manager=callback_manager,     verbose=True, # Verbose is required to pass to the callback manager)  prompt = """Question: A rap battle between Stephen Colbert and John Oliver"""llm(prompt) prompt = """Question: A rap battle between Stephen Colbert and John Oliver"""llm(prompt)          Stephen Colbert:    Yo, John, I heard you've been talkin' smack about me on your show.    Let me tell you somethin', pal, I'm the king of late-night TV    My satire is sharp as a razor, it cuts deeper than a knife    While you're just a british bloke tryin' to be funny with your accent and your wit.    John Oliver:    Oh Stephen, don't be ridiculous, you may have the ratings but I got the real talk.    My show is the one that people actually watch and listen to, not just for the laughs but for the facts.    While you're busy talkin' trash, I'm out here bringing the truth to light.    Stephen Colbert:    Truth? Ha! You think your show is about truth? Please, it's all just a joke to you.    You're just a fancy-pants british guy tryin' to be funny with your news and your jokes.    While I'm the one who's really makin' a difference, with my sat        llama_print_timings:        load time =   358.60 ms    llama_print_timings:      sample time =   172.55 ms /   256 runs   (    0.67 ms per token,  1483.59 tokens per second)    llama_print_timings: prompt eval time =   613.36 ms /    16 tokens (   38.33 ms per token,    26.09 tokens per second)    llama_print_timings:        eval time = 10151.17 ms /   255 runs   (   39.81 ms per token,    25.12 tokens per second)    llama_print_timings:       total time = 11332.41 ms    "\nStephen Colbert:\nYo, John, I heard you've been talkin' smack about me on your show.\nLet me tell you somethin', pal, I'm the king of late-night TV\nMy satire is sharp as a razor, it cuts deeper than a knife\nWhile you're just a british bloke tryin' to be funny with your accent and your wit.\nJohn Oliver:\nOh Stephen, don't be ridiculous, you may have the ratings but I got the real talk.\nMy show is the one that people actually watch and listen to, not just for the laughs but for the facts.\nWhile you're busy talkin' trash, I'm out here bringing the truth to light.\nStephen Colbert:\nTruth? Ha! You think your show is about truth? Please, it's all just a joke to you.\nYou're just a fancy-pants british guy tryin' to be funny with your news and your jokes.\nWhile I'm the one who's really makin' a difference, with my sat"         Stephen Colbert:    Yo, John, I heard you've been talkin' smack about me on your show.    Let me tell you somethin', pal, I'm the king of late-night TV    My satire is sharp as a razor, it cuts deeper than a knife    While you're just a british bloke tryin' to be funny with your accent and your wit.    John Oliver:    Oh Stephen, don't be ridiculous, you may have the ratings but I got the real talk.    My show is the one that people actually watch and listen to, not just for the laughs but for the facts.    While you're busy talkin' trash, I'm out here bringing the truth to light.    Stephen Colbert:    Truth? Ha! You think your show is about truth? Please, it's all just a joke to you.    You're just a fancy-pants british guy tryin' to be funny with your news and your jokes.    While I'm the one who's really makin' a difference, with my sat        llama_print_timings:        load time =   358.60 ms    llama_print_timings:      sample time =   172.55 ms /   256 runs   (    0.67 ms per token,  1483.59 tokens per second)    llama_print_timings: prompt eval time =   613.36 ms /    16 tokens (   38.33 ms per token,    26.09 tokens per second)    llama_print_timings:        eval time = 10151.17 ms /   255 runs   (   39.81 ms per token,    25.12 tokens per second)    llama_print_timings:       total time = 11332.41 ms    "\nStephen Colbert:\nYo, John, I heard you've been talkin' smack about me on your show.\nLet me tell you somethin', pal, I'm the king of late-night TV\nMy satire is sharp as a razor, it cuts deeper than a knife\nWhile you're just a british bloke tryin' to be funny with your accent and your wit.\nJohn Oliver:\nOh Stephen, don't be ridiculous, you may have the ratings but I got the real talk.\nMy show is the one that people actually watch and listen to, not just for the laughs but for the facts.\nWhile you're busy talkin' trash, I'm out here bringing the truth to light.\nStephen Colbert:\nTruth? Ha! You think your show is about truth? Please, it's all just a joke to you.\nYou're just a fancy-pants british guy tryin' to be funny with your news and your jokes.\nWhile I'm the one who's really makin' a difference, with my sat"         Stephen Colbert:    Yo, John, I heard you've been talkin' smack about me on your show.    Let me tell you somethin', pal, I'm the king of late-night TV    My satire is sharp as a razor, it cuts deeper than a knife    While you're just a british bloke tryin' to be funny with your accent and your wit.    John Oliver:    Oh Stephen, don't be ridiculous, you may have the ratings but I got the real talk.    My show is the one that people actually watch and listen to, not just for the laughs but for the facts.    While you're busy talkin' trash, I'm out here bringing the truth to light.    Stephen Colbert:    Truth? Ha! You think your show is about truth? Please, it's all just a joke to you.    You're just a fancy-pants british guy tryin' to be funny with your news and your jokes.    While I'm the one who's really makin' a difference, with my sat        llama_print_timings:        load time =   358.60 ms    llama_print_timings:      sample time =   172.55 ms /   256 runs   (    0.67 ms per token,  1483.59 tokens per second)    llama_print_timings: prompt eval time =   613.36 ms /    16 tokens (   38.33 ms per token,    26.09 tokens per second)    llama_print_timings:        eval time = 10151.17 ms /   255 runs   (   39.81 ms per token,    25.12 tokens per second)    llama_print_timings:       total time = 11332.41 ms    "\nStephen Colbert:\nYo, John, I heard you've been talkin' smack about me on your show.\nLet me tell you somethin', pal, I'm the king of late-night TV\nMy satire is sharp as a razor, it cuts deeper than a knife\nWhile you're just a british bloke tryin' to be funny with your accent and your wit.\nJohn Oliver:\nOh Stephen, don't be ridiculous, you may have the ratings but I got the real talk.\nMy show is the one that people actually watch and listen to, not just for the laughs but for the facts.\nWhile you're busy talkin' trash, I'm out here bringing the truth to light.\nStephen Colbert:\nTruth? Ha! You think your show is about truth? Please, it's all just a joke to you.\nYou're just a fancy-pants british guy tryin' to be funny with your news and your jokes.\nWhile I'm the one who's really makin' a difference, with my sat"  # Make sure the model path is correct for your system!llm = LlamaCpp(    model_path="./ggml-model-q4_0.bin", callback_manager=callback_manager, verbose=True) # Make sure the model path is correct for your system!llm = LlamaCpp(    model_path="./ggml-model-q4_0.bin", callback_manager=callback_manager, verbose=True)  llm_chain = LLMChain(prompt=prompt, llm=llm) llm_chain = LLMChain(prompt=prompt, llm=llm)  question = "What NFL team won the Super Bowl in the year Justin Bieber was born?"llm_chain.run(question) question = "What NFL team won the Super Bowl in the year Justin Bieber was born?"llm_chain.run(question)              1. First, find out when Justin Bieber was born.    2. We know that Justin Bieber was born on March 1, 1994.    3. Next, we need to look up when the Super Bowl was played in that year.    4. The Super Bowl was played on January 28, 1995.    5. Finally, we can use this information to answer the question. The NFL team that won the Super Bowl in the year Justin Bieber was born is the San Francisco 49ers.        llama_print_timings:        load time =   434.15 ms    llama_print_timings:      sample time =    41.81 ms /   121 runs   (    0.35 ms per token)    llama_print_timings: prompt eval time =  2523.78 ms /    48 tokens (   52.58 ms per token)    llama_print_timings:        eval time = 23971.57 ms /   121 runs   (  198.11 ms per token)    llama_print_timings:       total time = 28945.95 ms    '\n\n1. First, find out when Justin Bieber was born.\n2. We know that Justin Bieber was born on March 1, 1994.\n3. Next, we need to look up when the Super Bowl was played in that year.\n4. The Super Bowl was played on January 28, 1995.\n5. Finally, we can use this information to answer the question. The NFL team that won the Super Bowl in the year Justin Bieber was born is the San Francisco 49ers.'             1. First, find out when Justin Bieber was born.    2. We know that Justin Bieber was born on March 1, 1994.    3. Next, we need to look up when the Super Bowl was played in that year.    4. The Super Bowl was played on January 28, 1995.    5. Finally, we can use this information to answer the question. The NFL team that won the Super Bowl in the year Justin Bieber was born is the San Francisco 49ers.        llama_print_timings:        load time =   434.15 ms    llama_print_timings:      sample time =    41.81 ms /   121 runs   (    0.35 ms per token)    llama_print_timings: prompt eval time =  2523.78 ms /    48 tokens (   52.58 ms per token)    llama_print_timings:        eval time = 23971.57 ms /   121 runs   (  198.11 ms per token)    llama_print_timings:       total time = 28945.95 ms    '\n\n1. First, find out when Justin Bieber was born.\n2. We know that Justin Bieber was born on March 1, 1994.\n3. Next, we need to look up when the Super Bowl was played in that year.\n4. The Super Bowl was played on January 28, 1995.\n5. Finally, we can use this information to answer the question. The NFL team that won the Super Bowl in the year Justin Bieber was born is the San Francisco 49ers.'             1. First, find out when Justin Bieber was born.    2. We know that Justin Bieber was born on March 1, 1994.    3. Next, we need to look up when the Super Bowl was played in that year.    4. The Super Bowl was played on January 28, 1995.    5. Finally, we can use this information to answer the question. The NFL team that won the Super Bowl in the year Justin Bieber was born is the San Francisco 49ers.        llama_print_timings:        load time =   434.15 ms    llama_print_timings:      sample time =    41.81 ms /   121 runs   (    0.35 ms per token)    llama_print_timings: prompt eval time =  2523.78 ms /    48 tokens (   52.58 ms per token)    llama_print_timings:        eval time = 23971.57 ms /   121 runs   (  198.11 ms per token)    llama_print_timings:       total time = 28945.95 ms    '\n\n1. First, find out when Justin Bieber was born.\n2. We know that Justin Bieber was born on March 1, 1994.\n3. Next, we need to look up when the Super Bowl was played in that year.\n4. The Super Bowl was played on January 28, 1995.\n5. Finally, we can use this information to answer the question. The NFL team that won the Super Bowl in the year Justin Bieber was born is the San Francisco 49ers.'  n_gpu_layers = 40  # Change this value based on your model and your GPU VRAM pool.n_batch = 512  # Should be between 1 and n_ctx, consider the amount of VRAM in your GPU.# Make sure the model path is correct for your system!llm = LlamaCpp(    model_path="/Users/rlm/Desktop/Code/llama.cpp/models/openorca-platypus2-13b.gguf.q4_0.bin",    n_gpu_layers=n_gpu_layers,    n_batch=n_batch,    callback_manager=callback_manager,    verbose=True, # Verbose is required to pass to the callback manager) n_gpu_layers = 40  # Change this value based on your model and your GPU VRAM pool.n_batch = 512  # Should be between 1 and n_ctx, consider the amount of VRAM in your GPU.# Make sure the model path is correct for your system!llm = LlamaCpp(    model_path="/Users/rlm/Desktop/Code/llama.cpp/models/openorca-platypus2-13b.gguf.q4_0.bin",    n_gpu_layers=n_gpu_layers,    n_batch=n_batch,    callback_manager=callback_manager,    verbose=True, # Verbose is required to pass to the callback manager)  llm_chain = LLMChain(prompt=prompt, llm=llm)question = "What NFL team won the Super Bowl in the year Justin Bieber was born?"llm_chain.run(question) llm_chain = LLMChain(prompt=prompt, llm=llm)question = "What NFL team won the Super Bowl in the year Justin Bieber was born?"llm_chain.run(question)              1. Identify Justin Bieber's birth date: Justin Bieber was born on March 1, 1994.        2. Find the Super Bowl winner of that year: The NFL season of 1993 with the Super Bowl being played in January or of 1994.        3. Determine which team won the game: The Dallas Cowboys faced the Buffalo Bills in Super Bowl XXVII on January 31, 1993 (as the year is mis-labelled due to a error). The Dallas Cowboys won this matchup.        So, Justin Bieber was born when the Dallas Cowboys were the reigning NFL Super Bowl.        llama_print_timings:        load time =   427.63 ms    llama_print_timings:      sample time =   115.85 ms /   164 runs   (    0.71 ms per token,  1415.67 tokens per second)    llama_print_timings: prompt eval time =   427.53 ms /    45 tokens (    9.50 ms per token,   105.26 tokens per second)    llama_print_timings:        eval time =  4526.53 ms /   163 runs   (   27.77 ms per token,    36.01 tokens per second)    llama_print_timings:       total time =  5293.77 ms    "\n\n1. Identify Justin Bieber's birth date: Justin Bieber was born on March 1, 1994.\n\n2. Find the Super Bowl winner of that year: The NFL season of 1993 with the Super Bowl being played in January or of 1994.\n\n3. Determine which team won the game: The Dallas Cowboys faced the Buffalo Bills in Super Bowl XXVII on January 31, 1993 (as the year is mis-labelled due to a error). The Dallas Cowboys won this matchup.\n\nSo, Justin Bieber was born when the Dallas Cowboys were the reigning NFL Super Bowl."             1. Identify Justin Bieber's birth date: Justin Bieber was born on March 1, 1994.        2. Find the Super Bowl winner of that year: The NFL season of 1993 with the Super Bowl being played in January or of 1994.        3. Determine which team won the game: The Dallas Cowboys faced the Buffalo Bills in Super Bowl XXVII on January 31, 1993 (as the year is mis-labelled due to a error). The Dallas Cowboys won this matchup.        So, Justin Bieber was born when the Dallas Cowboys were the reigning NFL Super Bowl.        llama_print_timings:        load time =   427.63 ms    llama_print_timings:      sample time =   115.85 ms /   164 runs   (    0.71 ms per token,  1415.67 tokens per second)    llama_print_timings: prompt eval time =   427.53 ms /    45 tokens (    9.50 ms per token,   105.26 tokens per second)    llama_print_timings:        eval time =  4526.53 ms /   163 runs   (   27.77 ms per token,    36.01 tokens per second)    llama_print_timings:       total time =  5293.77 ms    "\n\n1. Identify Justin Bieber's birth date: Justin Bieber was born on March 1, 1994.\n\n2. Find the Super Bowl winner of that year: The NFL season of 1993 with the Super Bowl being played in January or of 1994.\n\n3. Determine which team won the game: The Dallas Cowboys faced the Buffalo Bills in Super Bowl XXVII on January 31, 1993 (as the year is mis-labelled due to a error). The Dallas Cowboys won this matchup.\n\nSo, Justin Bieber was born when the Dallas Cowboys were the reigning NFL Super Bowl."             1. Identify Justin Bieber's birth date: Justin Bieber was born on March 1, 1994.        2. Find the Super Bowl winner of that year: The NFL season of 1993 with the Super Bowl being played in January or of 1994.        3. Determine which team won the game: The Dallas Cowboys faced the Buffalo Bills in Super Bowl XXVII on January 31, 1993 (as the year is mis-labelled due to a error). The Dallas Cowboys won this matchup.        So, Justin Bieber was born when the Dallas Cowboys were the reigning NFL Super Bowl.        llama_print_timings:        load time =   427.63 ms    llama_print_timings:      sample time =   115.85 ms /   164 runs   (    0.71 ms per token,  1415.67 tokens per second)    llama_print_timings: prompt eval time =   427.53 ms /    45 tokens (    9.50 ms per token,   105.26 tokens per second)    llama_print_timings:        eval time =  4526.53 ms /   163 runs   (   27.77 ms per token,    36.01 tokens per second)    llama_print_timings:       total time =  5293.77 ms    "\n\n1. Identify Justin Bieber's birth date: Justin Bieber was born on March 1, 1994.\n\n2. Find the Super Bowl winner of that year: The NFL season of 1993 with the Super Bowl being played in January or of 1994.\n\n3. Determine which team won the game: The Dallas Cowboys faced the Buffalo Bills in Super Bowl XXVII on January 31, 1993 (as the year is mis-labelled due to a error). The Dallas Cowboys won this matchup.\n\nSo, Justin Bieber was born when the Dallas Cowboys were the reigning NFL Super Bowl."  n_gpu_layers = 1  # Metal set to 1 is enough.n_batch = 512  # Should be between 1 and n_ctx, consider the amount of RAM of your Apple Silicon Chip.# Make sure the model path is correct for your system!llm = LlamaCpp(    model_path="/Users/rlm/Desktop/Code/llama.cpp/models/openorca-platypus2-13b.gguf.q4_0.bin",    n_gpu_layers=n_gpu_layers,    n_batch=n_batch,    f16_kv=True,  # MUST set to True, otherwise you will run into problem after a couple of calls    callback_manager=callback_manager,    verbose=True, # Verbose is required to pass to the callback manager) n_gpu_layers = 1  # Metal set to 1 is enough.n_batch = 512  # Should be between 1 and n_ctx, consider the amount of RAM of your Apple Silicon Chip.# Make sure the model path is correct for your system!llm = LlamaCpp(    model_path="/Users/rlm/Desktop/Code/llama.cpp/models/openorca-platypus2-13b.gguf.q4_0.bin",    n_gpu_layers=n_gpu_layers,    n_batch=n_batch,    f16_kv=True,  # MUST set to True, otherwise you will run into problem after a couple of calls    callback_manager=callback_manager,    verbose=True, # Verbose is required to pass to the callback manager)  ggml_metal_init: allocatingggml_metal_init: using MPS... ggml_metal_init: allocatingggml_metal_init: using MPS...  n_gpu_layers = 1  # Metal set to 1 is enough.n_batch = 512  # Should be between 1 and n_ctx, consider the amount of RAM of your Apple Silicon Chip.# Make sure the model path is correct for your system!llm = LlamaCpp(    model_path="/Users/rlm/Desktop/Code/llama.cpp/models/openorca-platypus2-13b.gguf.q4_0.bin",    n_gpu_layers=n_gpu_layers,    n_batch=n_batch,    f16_kv=True,  # MUST set to True, otherwise you will run into problem after a couple of calls    callback_manager=callback_manager,    verbose=True, # Verbose is required to pass to the callback manager    grammar_path="/Users/rlm/Desktop/Code/langchain-main/langchain/libs/langchain/langchain/llms/grammars/json.gbnf",) n_gpu_layers = 1  # Metal set to 1 is enough.n_batch = 512  # Should be between 1 and n_ctx, consider the amount of RAM of your Apple Silicon Chip.# Make sure the model path is correct for your system!llm = LlamaCpp(    model_path="/Users/rlm/Desktop/Code/llama.cpp/models/openorca-platypus2-13b.gguf.q4_0.bin",    n_gpu_layers=n_gpu_layers,    n_batch=n_batch,    f16_kv=True,  # MUST set to True, otherwise you will run into problem after a couple of calls    callback_manager=callback_manager,    verbose=True, # Verbose is required to pass to the callback manager    grammar_path="/Users/rlm/Desktop/Code/langchain-main/langchain/libs/langchain/langchain/llms/grammars/json.gbnf",)  result=llm("Describe a person in JSON format:") result=llm("Describe a person in JSON format:")      {      "name": "John Doe",      "age": 34,      "": {        "title": "Software Developer",        "company": "Google"      },      "interests": [        "Sports",        "Music",        "Cooking"      ],      "address": {        "street_number": 123,        "street_name": "Oak Street",        "city": "Mountain View",        "state": "California",        "postal_code": 94040      }}        llama_print_timings:        load time =   357.51 ms    llama_print_timings:      sample time =  1213.30 ms /   144 runs   (    8.43 ms per token,   118.68 tokens per second)    llama_print_timings: prompt eval time =   356.78 ms /     9 tokens (   39.64 ms per token,    25.23 tokens per second)    llama_print_timings:        eval time =  3947.16 ms /   143 runs   (   27.60 ms per token,    36.23 tokens per second)    llama_print_timings:       total time =  5846.21 ms     {      "name": "John Doe",      "age": 34,      "": {        "title": "Software Developer",        "company": "Google"      },      "interests": [        "Sports",        "Music",        "Cooking"      ],      "address": {        "street_number": 123,        "street_name": "Oak Street",        "city": "Mountain View",        "state": "California",        "postal_code": 94040      }}        llama_print_timings:        load time =   357.51 ms    llama_print_timings:      sample time =  1213.30 ms /   144 runs   (    8.43 ms per token,   118.68 tokens per second)    llama_print_timings: prompt eval time =   356.78 ms /     9 tokens (   39.64 ms per token,    25.23 tokens per second)    llama_print_timings:        eval time =  3947.16 ms /   143 runs   (   27.60 ms per token,    36.23 tokens per second)    llama_print_timings:       total time =  5846.21 ms     {      "name": "John Doe",      "age": 34,      "": {        "title": "Software Developer",        "company": "Google"      },      "interests": [        "Sports",        "Music",        "Cooking"      ],      "address": {        "street_number": 123,        "street_name": "Oak Street",        "city": "Mountain View",        "state": "California",        "postal_code": 94040      }}        llama_print_timings:        load time =   357.51 ms    llama_print_timings:      sample time =  1213.30 ms /   144 runs   (    8.43 ms per token,   118.68 tokens per second)    llama_print_timings: prompt eval time =   356.78 ms /     9 tokens (   39.64 ms per token,    25.23 tokens per second)    llama_print_timings:        eval time =  3947.16 ms /   143 runs   (   27.60 ms per token,    36.23 tokens per second)    llama_print_timings:       total time =  5846.21 ms  n_gpu_layers = 1 n_batch = 512llm = LlamaCpp(    model_path="/Users/rlm/Desktop/Code/llama.cpp/models/openorca-platypus2-13b.gguf.q4_0.bin",    n_gpu_layers=n_gpu_layers,    n_batch=n_batch,    f16_kv=True,  # MUST set to True, otherwise you will run into problem after a couple of calls    callback_manager=callback_manager,    verbose=True,    grammar_path="/Users/rlm/Desktop/Code/langchain-main/langchain/libs/langchain/langchain/llms/grammars/list.gbnf",) n_gpu_layers = 1 n_batch = 512llm = LlamaCpp(    model_path="/Users/rlm/Desktop/Code/llama.cpp/models/openorca-platypus2-13b.gguf.q4_0.bin",    n_gpu_layers=n_gpu_layers,    n_batch=n_batch,    f16_kv=True,  # MUST set to True, otherwise you will run into problem after a couple of calls    callback_manager=callback_manager,    verbose=True,    grammar_path="/Users/rlm/Desktop/Code/langchain-main/langchain/libs/langchain/langchain/llms/grammars/list.gbnf",)  result=llm("List of top-3 my favourite books:") result=llm("List of top-3 my favourite books:")      ["The Catcher in the Rye", "Wuthering Heights", "Anna Karenina"]        llama_print_timings:        load time =   322.34 ms    llama_print_timings:      sample time =   232.60 ms /    26 runs   (    8.95 ms per token,   111.78 tokens per second)    llama_print_timings: prompt eval time =   321.90 ms /    11 tokens (   29.26 ms per token,    34.17 tokens per second)    llama_print_timings:        eval time =   680.82 ms /    25 runs   (   27.23 ms per token,    36.72 tokens per second)    llama_print_timings:       total time =  1295.27 ms     ["The Catcher in the Rye", "Wuthering Heights", "Anna Karenina"]        llama_print_timings:        load time =   322.34 ms    llama_print_timings:      sample time =   232.60 ms /    26 runs   (    8.95 ms per token,   111.78 tokens per second)    llama_print_timings: prompt eval time =   321.90 ms /    11 tokens (   29.26 ms per token,    34.17 tokens per second)    llama_print_timings:        eval time =   680.82 ms /    25 runs   (   27.23 ms per token,    36.72 tokens per second)    llama_print_timings:       total time =  1295.27 ms     ["The Catcher in the Rye", "Wuthering Heights", "Anna Karenina"]        llama_print_timings:        load time =   322.34 ms    llama_print_timings:      sample time =   232.60 ms /    26 runs   (    8.95 ms per token,   111.78 tokens per second)    llama_print_timings: prompt eval time =   321.90 ms /    11 tokens (   29.26 ms per token,    34.17 tokens per second)    llama_print_timings:        eval time =   680.82 ms /    25 runs   (   27.23 ms per token,    36.72 tokens per second)    llama_print_timings:       total time =  1295.27 ms  Previous KoboldAI API Next LLM Caching integrations InstallationCPU only installationInstallation with OpenBLAS / cuBLAS / CLBlastInstallation with MetalInstallation with WindowsUsageCPUGPUMetalGrammars InstallationCPU only installationInstallation with OpenBLAS / cuBLAS / CLBlastInstallation with MetalInstallation with WindowsUsageCPUGPUMetalGrammars CommunityDiscordTwitterGitHubPythonJS/TSMoreHomepageBlogCopyright ¬© 2023 LangChain, Inc. CommunityDiscordTwitterGitHubPythonJS/TSMoreHomepageBlog CommunityDiscordTwitter Community GitHubPythonJS/TS GitHub MoreHomepageBlog More Copyright ¬© 2023 LangChain, Inc. Copyright ¬© 2023 LangChain, Inc. Skip to main content (#docusaurus_skipToContent_fallback) ü¶úÔ∏èüîó LangChain (/) Docs (/docs/get_started/introduction) Use cases (/docs/use_cases) Integrations (/docs/integrations) API (https://api.python.langchain.com) LangSmith (https://smith.langchain.com) JS/TS Docs (https://js.langchain.com/docs)  (https://github.com/hwchase17/langchain) Integrations (/docs/integrations) Callbacks (/docs/integrations/callbacks/) Chat models (/docs/integrations/chat/) Chat loaders (/docs/integrations/chat_loaders/) Document loaders (/docs/integrations/document_loaders/) Document transformers (/docs/integrations/document_transformers/) LLMs (/docs/integrations/llms/) AI21 (/docs/integrations/llms/ai21) Aleph Alpha (/docs/integrations/llms/aleph_alpha) Amazon API Gateway (/docs/integrations/llms/amazon_api_gateway) Anyscale (/docs/integrations/llms/anyscale) Azure ML (/docs/integrations/llms/azure_ml) Azure OpenAI (/docs/integrations/llms/azure_openai) Banana (/docs/integrations/llms/banana) Baseten (/docs/integrations/llms/baseten) Beam (/docs/integrations/llms/beam) Bedrock (/docs/integrations/llms/bedrock) Bittensor (/docs/integrations/llms/bittensor) CerebriumAI (/docs/integrations/llms/cerebriumai) ChatGLM (/docs/integrations/llms/chatglm) Clarifai (/docs/integrations/llms/clarifai) Cohere (/docs/integrations/llms/cohere) C Transformers (/docs/integrations/llms/ctransformers) CTranslate2 (/docs/integrations/llms/ctranslate2) Databricks (/docs/integrations/llms/databricks) DeepInfra (/docs/integrations/llms/deepinfra) DeepSparse (/docs/integrations/llms/deepsparse) Eden AI (/docs/integrations/llms/edenai) Fireworks (/docs/integrations/llms/fireworks) ForefrontAI (/docs/integrations/llms/forefrontai) Google Vertex AI PaLM (/docs/integrations/llms/google_vertex_ai_palm) GooseAI (/docs/integrations/llms/gooseai) GPT4All (/docs/integrations/llms/gpt4all) Hugging Face Hub (/docs/integrations/llms/huggingface_hub) Hugging Face Local Pipelines (/docs/integrations/llms/huggingface_pipelines) Huggingface TextGen Inference (/docs/integrations/llms/huggingface_textgen_inference) JSONFormer (/docs/integrations/llms/jsonformer_experimental) KoboldAI API (/docs/integrations/llms/koboldai) Llama.cpp (/docs/integrations/llms/llamacpp) LLM Caching integrations (/docs/integrations/llms/llm_caching) Manifest (/docs/integrations/llms/manifest) Minimax (/docs/integrations/llms/minimax) Modal (/docs/integrations/llms/modal) MosaicML (/docs/integrations/llms/mosaicml) NLP Cloud (/docs/integrations/llms/nlpcloud) OctoAI (/docs/integrations/llms/octoai) Ollama (/docs/integrations/llms/ollama) OpaquePrompts (/docs/integrations/llms/opaqueprompts) OpenAI (/docs/integrations/llms/openai) OpenLLM (/docs/integrations/llms/openllm) OpenLM (/docs/integrations/llms/openlm) Petals (/docs/integrations/llms/petals) PipelineAI (/docs/integrations/llms/pipelineai) Predibase (/docs/integrations/llms/predibase) Prediction Guard (/docs/integrations/llms/predictionguard) PromptLayer OpenAI (/docs/integrations/llms/promptlayer_openai) RELLM (/docs/integrations/llms/rellm_experimental) Replicate (/docs/integrations/llms/replicate) Runhouse (/docs/integrations/llms/runhouse) SageMakerEndpoint (/docs/integrations/llms/sagemaker) StochasticAI (/docs/integrations/llms/stochasticai) Nebula (Symbl.ai) (/docs/integrations/llms/symblai_nebula) TextGen (/docs/integrations/llms/textgen) Titan Takeoff (/docs/integrations/llms/titan_takeoff) Tongyi Qwen (/docs/integrations/llms/tongyi) vLLM (/docs/integrations/llms/vllm) Writer (/docs/integrations/llms/writer) Xorbits Inference (Xinference) (/docs/integrations/llms/xinference) Memory (/docs/integrations/memory/) Retrievers (/docs/integrations/retrievers/) Text embedding models (/docs/integrations/text_embedding/) Agents & Toolkits (/docs/integrations/toolkits/) Tools (/docs/integrations/tools/) Vector stores (/docs/integrations/vectorstores/) Grouped by provider (/docs/integrations/providers/)  (/) Integrations (/docs/integrations) LLMs (/docs/integrations/llms/) llama-cpp-python (https://github.com/abetlen/llama-cpp-python) llama.cpp (https://github.com/ggerganov/llama.cpp) many LLMs (https://github.com/ggerganov/llama.cpp) HuggingFace (https://huggingface.co/TheBloke) here (https://github.com/abetlen/llama-cpp-python/pull/633) llama.cpp (https://github.com/ggerganov/llama.cpp) ‚Äã (#installation) ‚Äã (#cpu-only-installation) ‚Äã (#installation-with-openblas--cublas--clblast) source (https://github.com/abetlen/llama-cpp-python#installation-with-openblas--cublas--clblast) ‚Äã (#installation-with-metal) source (https://github.com/abetlen/llama-cpp-python/blob/main/docs/install/macos.md) ‚Äã (#installation-with-windows) ‚Äã (#compiling-and-installing) ‚Äã (#usage) install all necessary model files (https://github.com/ggerganov/llama.cpp) LlamaCpp (https://api.python.langchain.com/en/latest/llms/langchain.llms.llamacpp.LlamaCpp.html) CallbackManager (https://api.python.langchain.com/en/latest/callbacks/langchain.callbacks.manager.CallbackManager.html) StreamingStdOutCallbackHandler (https://api.python.langchain.com/en/latest/callbacks/langchain.callbacks.streaming_stdout.StreamingStdOutCallbackHandler.html) ‚Äã (#cpu) ‚Äã (#gpu) wrapper code (https://github.com/mmagnesium/langchain/blob/master/langchain/llms/llamacpp.py) ‚Äã (#metal) wrapper code (https://github.com/mmagnesium/langchain/blob/master/langchain/llms/llamacpp.py) ‚Äã (#grammars) grammars (https://github.com/ggerganov/llama.cpp/blob/master/grammars/README.md) PreviousKoboldAI API (/docs/integrations/llms/koboldai) NextLLM Caching integrations (/docs/integrations/llms/llm_caching) Installation (#installation) CPU only installation (#cpu-only-installation) Installation with OpenBLAS / cuBLAS / CLBlast (#installation-with-openblas--cublas--clblast) Installation with Metal (#installation-with-metal) Installation with Windows (#installation-with-windows) Usage (#usage) CPU (#cpu) GPU (#gpu) Metal (#metal) Grammars (#grammars) Discord (https://discord.gg/cU2adEyC7w) Twitter (https://twitter.com/LangChainAI) Python (https://github.com/hwchase17/langchain) JS/TS (https://github.com/hwchase17/langchainjs) Homepage (https://langchain.com) Blog (https://blog.langchain.dev)