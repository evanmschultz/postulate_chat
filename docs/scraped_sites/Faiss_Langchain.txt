Facebook AI Similarity Search (Faiss) is a library for efficient similarity search and clustering of dense vectors. It contains algorithms that search in sets of vectors of any size, up to ones that possibly do not fit in RAM. It also contains supporting code for evaluation and parameter tuning. Faiss documentation. This notebook shows how to use functionality related to the FAISS vector database. We want to use OpenAIEmbeddings so we have to get the OpenAI API Key.  There are some FAISS specific methods. One of them is similarity_search_with_score, which allows you to return not only the documents but also the distance score of the query to them. The returned distance score is L2 distance. Therefore, a lower score is better. It is also possible to do a search for documents similar to a given embedding vector using similarity_search_by_vector which accepts an embedding vector as a parameter instead of a string. You can also save and load a FAISS index. This is useful so you don't have to recreate it everytime you use it. you can pickle the FAISS Index by these functions. If you use embeddings model which is of 90 mb (sentence-transformers/all-MiniLM-L6-v2 or any other model), the resultant pickle size would be more than 90 mb. the size of the model is also included in the overall size. To overcome this, use the below functions. These functions only serializes FAISS index and size would be much lesser. this can be helpful if you wish to store the index in database like sql. You can also merge two FAISS vectorstores FAISS vectorstore can also support filtering, since the FAISS does not natively support filtering we have to do it manually. This is done by first fetching more results than k and then filtering them. You can filter the documents based on metadata. You can also set the fetch_k parameter when calling any search method to set how many documents you want to fetch before filtering. Here is a small example: Now we make the same query call but we filter for only page = 1  Same thing can be done with the max_marginal_relevance_search as well. Here is an example of how to set fetch_k parameter when calling similarity_search. Usually you would want the fetch_k parameter >> k parameter. This is because the fetch_k parameter is the number of documents that will be fetched before filtering. If you set fetch_k to a low number, you might not get enough documents to filter from. You can also delete ids. Note that the ids to delete should be the ids in the docstore. IntegrationsCallbacksChat modelsChat loadersDocument loadersDocument transformersLLMsMemoryRetrieversText embedding modelsAgents & ToolkitsToolsVector storesActiveloop Deep LakeAlibaba Cloud OpenSearchAnalyticDBAnnoyAtlasAwaDBAzure Cognitive SearchBagelDBCassandraChromaClickHouseDashVectorDingoDocArray HnswSearchDocArray InMemorySearchElasticsearchEpsillaFaissHologresLanceDBMarqoGoogle Vertex AI MatchingEngineMeilisearchMilvusMongoDB AtlasMyScaleNeo4j Vector IndexNucliaDBOpenSearchPostgres EmbeddingPGVectorPineconeQdrantRedisRocksetScaNNSingleStoreDBscikit-learnsqlite-vssStarRocksSupabase (Postgres)TairTencent Cloud VectorDBTigrisTypesenseUSearchvearchVectaraWeaviateXataZepZillizGrouped by provider Callbacks Chat models Chat loaders Document loaders Document transformers LLMs Memory Retrievers Text embedding models Agents & Toolkits Tools Vector storesActiveloop Deep LakeAlibaba Cloud OpenSearchAnalyticDBAnnoyAtlasAwaDBAzure Cognitive SearchBagelDBCassandraChromaClickHouseDashVectorDingoDocArray HnswSearchDocArray InMemorySearchElasticsearchEpsillaFaissHologresLanceDBMarqoGoogle Vertex AI MatchingEngineMeilisearchMilvusMongoDB AtlasMyScaleNeo4j Vector IndexNucliaDBOpenSearchPostgres EmbeddingPGVectorPineconeQdrantRedisRocksetScaNNSingleStoreDBscikit-learnsqlite-vssStarRocksSupabase (Postgres)TairTencent Cloud VectorDBTigrisTypesenseUSearchvearchVectaraWeaviateXataZepZilliz Activeloop Deep Lake Alibaba Cloud OpenSearch AnalyticDB Annoy Atlas AwaDB Azure Cognitive Search BagelDB Cassandra Chroma ClickHouse DashVector Dingo DocArray HnswSearch DocArray InMemorySearch Elasticsearch Epsilla Faiss Hologres LanceDB Marqo Google Vertex AI MatchingEngine Meilisearch Milvus MongoDB Atlas MyScale Neo4j Vector Index NucliaDB OpenSearch Postgres Embedding PGVector Pinecone Qdrant Redis Rockset ScaNN SingleStoreDB scikit-learn sqlite-vss StarRocks Supabase (Postgres) Tair Tencent Cloud VectorDB Tigris Typesense USearch vearch Vectara Weaviate Xata Zep Zilliz Grouped by provider  Integrations Vector stores Faiss OpenAIEmbeddings CharacterTextSplitter FAISS TextLoader TextLoader Document Similarity Search with score Saving and loading Merging Similarity Search with filtering Delete Discord Twitter Python JS/TS Homepage Blog Skip to main contentðŸ¦œï¸ðŸ”— LangChainDocsUse casesIntegrationsAPILangSmithJS/TS DocsCTRLKIntegrationsCallbacksChat modelsChat loadersDocument loadersDocument transformersLLMsMemoryRetrieversText embedding modelsAgents & ToolkitsToolsVector storesActiveloop Deep LakeAlibaba Cloud OpenSearchAnalyticDBAnnoyAtlasAwaDBAzure Cognitive SearchBagelDBCassandraChromaClickHouseDashVectorDingoDocArray HnswSearchDocArray InMemorySearchElasticsearchEpsillaFaissHologresLanceDBMarqoGoogle Vertex AI MatchingEngineMeilisearchMilvusMongoDB AtlasMyScaleNeo4j Vector IndexNucliaDBOpenSearchPostgres EmbeddingPGVectorPineconeQdrantRedisRocksetScaNNSingleStoreDBscikit-learnsqlite-vssStarRocksSupabase (Postgres)TairTencent Cloud VectorDBTigrisTypesenseUSearchvearchVectaraWeaviateXataZepZillizGrouped by providerIntegrationsVector storesFaissOn this pageFaissFacebook AI Similarity Search (Faiss) is a library for efficient similarity search and clustering of dense vectors. It contains algorithms that search in sets of vectors of any size, up to ones that possibly do not fit in RAM. It also contains supporting code for evaluation and parameter tuning.Faiss documentation.This notebook shows how to use functionality related to the FAISS vector database.pip install faiss-gpu # For CUDA 7.5+ Supported GPU's.# ORpip install faiss-cpu # For CPU InstallationWe want to use OpenAIEmbeddings so we have to get the OpenAI API Key. import osimport getpassos.environ["OPENAI_API_KEY"] = getpass.getpass("OpenAI API Key:")# Uncomment the following line if you need to initialize FAISS with no AVX2 optimization# os.environ['FAISS_NO_AVX2'] = '1'from langchain.embeddings.openai import OpenAIEmbeddingsfrom langchain.text_splitter import CharacterTextSplitterfrom langchain.vectorstores import FAISSfrom langchain.document_loaders import TextLoaderAPI Reference:OpenAIEmbeddingsCharacterTextSplitterFAISSTextLoaderfrom langchain.document_loaders import TextLoaderloader = TextLoader("../../../extras/modules/state_of_the_union.txt")documents = loader.load()text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)docs = text_splitter.split_documents(documents)embeddings = OpenAIEmbeddings()API Reference:TextLoaderdb = FAISS.from_documents(docs, embeddings)query = "What did the president say about Ketanji Brown Jackson"docs = db.similarity_search(query)print(docs[0].page_content)    Tonight. I call on the Senate to: Pass the Freedom to Vote Act. Pass the John Lewis Voting Rights Act. And while youâ€™re at it, pass the Disclose Act so Americans can know who is funding our elections.         Tonight, Iâ€™d like to honor someone who has dedicated his life to serve this country: Justice Stephen Breyerâ€”an Army veteran, Constitutional scholar, and retiring Justice of the United States Supreme Court. Justice Breyer, thank you for your service.         One of the most serious constitutional responsibilities a President has is nominating someone to serve on the United States Supreme Court.         And I did that 4 days ago, when I nominated Circuit Court of Appeals Judge Ketanji Brown Jackson. One of our nationâ€™s top legal minds, who will continue Justice Breyerâ€™s legacy of excellence.Similarity Search with scoreâ€‹There are some FAISS specific methods. One of them is similarity_search_with_score, which allows you to return not only the documents but also the distance score of the query to them. The returned distance score is L2 distance. Therefore, a lower score is better.docs_and_scores = db.similarity_search_with_score(query)docs_and_scores[0]    (Document(page_content='Tonight. I call on the Senate to: Pass the Freedom to Vote Act. Pass the John Lewis Voting Rights Act. And while youâ€™re at it, pass the Disclose Act so Americans can know who is funding our elections. \n\nTonight, Iâ€™d like to honor someone who has dedicated his life to serve this country: Justice Stephen Breyerâ€”an Army veteran, Constitutional scholar, and retiring Justice of the United States Supreme Court. Justice Breyer, thank you for your service. \n\nOne of the most serious constitutional responsibilities a President has is nominating someone to serve on the United States Supreme Court. \n\nAnd I did that 4 days ago, when I nominated Circuit Court of Appeals Judge Ketanji Brown Jackson. One of our nationâ€™s top legal minds, who will continue Justice Breyerâ€™s legacy of excellence.', metadata={'source': '../../../state_of_the_union.txt'}),     0.36913747)It is also possible to do a search for documents similar to a given embedding vector using similarity_search_by_vector which accepts an embedding vector as a parameter instead of a string.embedding_vector = embeddings.embed_query(query)docs_and_scores = db.similarity_search_by_vector(embedding_vector)Saving and loadingâ€‹You can also save and load a FAISS index. This is useful so you don't have to recreate it everytime you use it.db.save_local("faiss_index")new_db = FAISS.load_local("faiss_index", embeddings)docs = new_db.similarity_search(query)docs[0]    Document(page_content='Tonight. I call on the Senate to: Pass the Freedom to Vote Act. Pass the John Lewis Voting Rights Act. And while youâ€™re at it, pass the Disclose Act so Americans can know who is funding our elections. \n\nTonight, Iâ€™d like to honor someone who has dedicated his life to serve this country: Justice Stephen Breyerâ€”an Army veteran, Constitutional scholar, and retiring Justice of the United States Supreme Court. Justice Breyer, thank you for your service. \n\nOne of the most serious constitutional responsibilities a President has is nominating someone to serve on the United States Supreme Court. \n\nAnd I did that 4 days ago, when I nominated Circuit Court of Appeals Judge Ketanji Brown Jackson. One of our nationâ€™s top legal minds, who will continue Justice Breyerâ€™s legacy of excellence.', metadata={'source': '../../../state_of_the_union.txt'})Serializing and De-Serializing to bytesyou can pickle the FAISS Index by these functions. If you use embeddings model which is of 90 mb (sentence-transformers/all-MiniLM-L6-v2 or any other model), the resultant pickle size would be more than 90 mb. the size of the model is also included in the overall size. To overcome this, use the below functions. These functions only serializes FAISS index and size would be much lesser. this can be helpful if you wish to store the index in database like sql.pkl = db.serialize_to_bytes() # serializes the faiss indexembeddings = HuggingFaceEmbeddings(model_name="all-MiniLM-L6-v2")db = FAISS.deserialize_from_bytes(embeddings = embeddings, serialized = pkl) # Load the indexMergingâ€‹You can also merge two FAISS vectorstoresdb1 = FAISS.from_texts(["foo"], embeddings)db2 = FAISS.from_texts(["bar"], embeddings)db1.docstore._dict    {'068c473b-d420-487a-806b-fb0ccea7f711': Document(page_content='foo', metadata={})}db2.docstore._dict    {'807e0c63-13f6-4070-9774-5c6f0fbb9866': Document(page_content='bar', metadata={})}db1.merge_from(db2)db1.docstore._dict    {'068c473b-d420-487a-806b-fb0ccea7f711': Document(page_content='foo', metadata={}),     '807e0c63-13f6-4070-9774-5c6f0fbb9866': Document(page_content='bar', metadata={})}Similarity Search with filteringâ€‹FAISS vectorstore can also support filtering, since the FAISS does not natively support filtering we have to do it manually. This is done by first fetching more results than k and then filtering them. You can filter the documents based on metadata. You can also set the fetch_k parameter when calling any search method to set how many documents you want to fetch before filtering. Here is a small example:from langchain.schema import Documentlist_of_documents = [    Document(page_content="foo", metadata=dict(page=1)),    Document(page_content="bar", metadata=dict(page=1)),    Document(page_content="foo", metadata=dict(page=2)),    Document(page_content="barbar", metadata=dict(page=2)),    Document(page_content="foo", metadata=dict(page=3)),    Document(page_content="bar burr", metadata=dict(page=3)),    Document(page_content="foo", metadata=dict(page=4)),    Document(page_content="bar bruh", metadata=dict(page=4)),]db = FAISS.from_documents(list_of_documents, embeddings)results_with_scores = db.similarity_search_with_score("foo")for doc, score in results_with_scores:    print(f"Content: {doc.page_content}, Metadata: {doc.metadata}, Score: {score}")API Reference:Document    Content: foo, Metadata: {'page': 1}, Score: 5.159960813797904e-15    Content: foo, Metadata: {'page': 2}, Score: 5.159960813797904e-15    Content: foo, Metadata: {'page': 3}, Score: 5.159960813797904e-15    Content: foo, Metadata: {'page': 4}, Score: 5.159960813797904e-15Now we make the same query call but we filter for only page = 1 results_with_scores = db.similarity_search_with_score("foo", filter=dict(page=1))for doc, score in results_with_scores:    print(f"Content: {doc.page_content}, Metadata: {doc.metadata}, Score: {score}")    Content: foo, Metadata: {'page': 1}, Score: 5.159960813797904e-15    Content: bar, Metadata: {'page': 1}, Score: 0.3131446838378906Same thing can be done with the max_marginal_relevance_search as well.results = db.max_marginal_relevance_search("foo", filter=dict(page=1))for doc in results:    print(f"Content: {doc.page_content}, Metadata: {doc.metadata}")    Content: foo, Metadata: {'page': 1}    Content: bar, Metadata: {'page': 1}Here is an example of how to set fetch_k parameter when calling similarity_search. Usually you would want the fetch_k parameter >> k parameter. This is because the fetch_k parameter is the number of documents that will be fetched before filtering. If you set fetch_k to a low number, you might not get enough documents to filter from.results = db.similarity_search("foo", filter=dict(page=1), k=1, fetch_k=4)for doc in results:    print(f"Content: {doc.page_content}, Metadata: {doc.metadata}")    Content: foo, Metadata: {'page': 1}Deleteâ€‹You can also delete ids. Note that the ids to delete should be the ids in the docstore.db.delete([db.index_to_docstore_id[0]])    True# Is now missing0 in db.index_to_docstore_id    FalsePreviousEpsillaNextHologresSimilarity Search with scoreSaving and loadingMergingSimilarity Search with filteringDeleteCommunityDiscordTwitterGitHubPythonJS/TSMoreHomepageBlogCopyright Â© 2023 LangChain, Inc. Skip to main content ðŸ¦œï¸ðŸ”— LangChainDocsUse casesIntegrationsAPILangSmithJS/TS DocsCTRLK ðŸ¦œï¸ðŸ”— LangChainDocsUse casesIntegrationsAPI LangSmithJS/TS DocsCTRLK  CTRLK CTRLK  CTRLK   IntegrationsCallbacksChat modelsChat loadersDocument loadersDocument transformersLLMsMemoryRetrieversText embedding modelsAgents & ToolkitsToolsVector storesActiveloop Deep LakeAlibaba Cloud OpenSearchAnalyticDBAnnoyAtlasAwaDBAzure Cognitive SearchBagelDBCassandraChromaClickHouseDashVectorDingoDocArray HnswSearchDocArray InMemorySearchElasticsearchEpsillaFaissHologresLanceDBMarqoGoogle Vertex AI MatchingEngineMeilisearchMilvusMongoDB AtlasMyScaleNeo4j Vector IndexNucliaDBOpenSearchPostgres EmbeddingPGVectorPineconeQdrantRedisRocksetScaNNSingleStoreDBscikit-learnsqlite-vssStarRocksSupabase (Postgres)TairTencent Cloud VectorDBTigrisTypesenseUSearchvearchVectaraWeaviateXataZepZillizGrouped by providerIntegrationsVector storesFaissOn this pageFaissFacebook AI Similarity Search (Faiss) is a library for efficient similarity search and clustering of dense vectors. It contains algorithms that search in sets of vectors of any size, up to ones that possibly do not fit in RAM. It also contains supporting code for evaluation and parameter tuning.Faiss documentation.This notebook shows how to use functionality related to the FAISS vector database.pip install faiss-gpu # For CUDA 7.5+ Supported GPU's.# ORpip install faiss-cpu # For CPU InstallationWe want to use OpenAIEmbeddings so we have to get the OpenAI API Key. import osimport getpassos.environ["OPENAI_API_KEY"] = getpass.getpass("OpenAI API Key:")# Uncomment the following line if you need to initialize FAISS with no AVX2 optimization# os.environ['FAISS_NO_AVX2'] = '1'from langchain.embeddings.openai import OpenAIEmbeddingsfrom langchain.text_splitter import CharacterTextSplitterfrom langchain.vectorstores import FAISSfrom langchain.document_loaders import TextLoaderAPI Reference:OpenAIEmbeddingsCharacterTextSplitterFAISSTextLoaderfrom langchain.document_loaders import TextLoaderloader = TextLoader("../../../extras/modules/state_of_the_union.txt")documents = loader.load()text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)docs = text_splitter.split_documents(documents)embeddings = OpenAIEmbeddings()API Reference:TextLoaderdb = FAISS.from_documents(docs, embeddings)query = "What did the president say about Ketanji Brown Jackson"docs = db.similarity_search(query)print(docs[0].page_content)    Tonight. I call on the Senate to: Pass the Freedom to Vote Act. Pass the John Lewis Voting Rights Act. And while youâ€™re at it, pass the Disclose Act so Americans can know who is funding our elections.         Tonight, Iâ€™d like to honor someone who has dedicated his life to serve this country: Justice Stephen Breyerâ€”an Army veteran, Constitutional scholar, and retiring Justice of the United States Supreme Court. Justice Breyer, thank you for your service.         One of the most serious constitutional responsibilities a President has is nominating someone to serve on the United States Supreme Court.         And I did that 4 days ago, when I nominated Circuit Court of Appeals Judge Ketanji Brown Jackson. One of our nationâ€™s top legal minds, who will continue Justice Breyerâ€™s legacy of excellence.Similarity Search with scoreâ€‹There are some FAISS specific methods. One of them is similarity_search_with_score, which allows you to return not only the documents but also the distance score of the query to them. The returned distance score is L2 distance. Therefore, a lower score is better.docs_and_scores = db.similarity_search_with_score(query)docs_and_scores[0]    (Document(page_content='Tonight. I call on the Senate to: Pass the Freedom to Vote Act. Pass the John Lewis Voting Rights Act. And while youâ€™re at it, pass the Disclose Act so Americans can know who is funding our elections. \n\nTonight, Iâ€™d like to honor someone who has dedicated his life to serve this country: Justice Stephen Breyerâ€”an Army veteran, Constitutional scholar, and retiring Justice of the United States Supreme Court. Justice Breyer, thank you for your service. \n\nOne of the most serious constitutional responsibilities a President has is nominating someone to serve on the United States Supreme Court. \n\nAnd I did that 4 days ago, when I nominated Circuit Court of Appeals Judge Ketanji Brown Jackson. One of our nationâ€™s top legal minds, who will continue Justice Breyerâ€™s legacy of excellence.', metadata={'source': '../../../state_of_the_union.txt'}),     0.36913747)It is also possible to do a search for documents similar to a given embedding vector using similarity_search_by_vector which accepts an embedding vector as a parameter instead of a string.embedding_vector = embeddings.embed_query(query)docs_and_scores = db.similarity_search_by_vector(embedding_vector)Saving and loadingâ€‹You can also save and load a FAISS index. This is useful so you don't have to recreate it everytime you use it.db.save_local("faiss_index")new_db = FAISS.load_local("faiss_index", embeddings)docs = new_db.similarity_search(query)docs[0]    Document(page_content='Tonight. I call on the Senate to: Pass the Freedom to Vote Act. Pass the John Lewis Voting Rights Act. And while youâ€™re at it, pass the Disclose Act so Americans can know who is funding our elections. \n\nTonight, Iâ€™d like to honor someone who has dedicated his life to serve this country: Justice Stephen Breyerâ€”an Army veteran, Constitutional scholar, and retiring Justice of the United States Supreme Court. Justice Breyer, thank you for your service. \n\nOne of the most serious constitutional responsibilities a President has is nominating someone to serve on the United States Supreme Court. \n\nAnd I did that 4 days ago, when I nominated Circuit Court of Appeals Judge Ketanji Brown Jackson. One of our nationâ€™s top legal minds, who will continue Justice Breyerâ€™s legacy of excellence.', metadata={'source': '../../../state_of_the_union.txt'})Serializing and De-Serializing to bytesyou can pickle the FAISS Index by these functions. If you use embeddings model which is of 90 mb (sentence-transformers/all-MiniLM-L6-v2 or any other model), the resultant pickle size would be more than 90 mb. the size of the model is also included in the overall size. To overcome this, use the below functions. These functions only serializes FAISS index and size would be much lesser. this can be helpful if you wish to store the index in database like sql.pkl = db.serialize_to_bytes() # serializes the faiss indexembeddings = HuggingFaceEmbeddings(model_name="all-MiniLM-L6-v2")db = FAISS.deserialize_from_bytes(embeddings = embeddings, serialized = pkl) # Load the indexMergingâ€‹You can also merge two FAISS vectorstoresdb1 = FAISS.from_texts(["foo"], embeddings)db2 = FAISS.from_texts(["bar"], embeddings)db1.docstore._dict    {'068c473b-d420-487a-806b-fb0ccea7f711': Document(page_content='foo', metadata={})}db2.docstore._dict    {'807e0c63-13f6-4070-9774-5c6f0fbb9866': Document(page_content='bar', metadata={})}db1.merge_from(db2)db1.docstore._dict    {'068c473b-d420-487a-806b-fb0ccea7f711': Document(page_content='foo', metadata={}),     '807e0c63-13f6-4070-9774-5c6f0fbb9866': Document(page_content='bar', metadata={})}Similarity Search with filteringâ€‹FAISS vectorstore can also support filtering, since the FAISS does not natively support filtering we have to do it manually. This is done by first fetching more results than k and then filtering them. You can filter the documents based on metadata. You can also set the fetch_k parameter when calling any search method to set how many documents you want to fetch before filtering. Here is a small example:from langchain.schema import Documentlist_of_documents = [    Document(page_content="foo", metadata=dict(page=1)),    Document(page_content="bar", metadata=dict(page=1)),    Document(page_content="foo", metadata=dict(page=2)),    Document(page_content="barbar", metadata=dict(page=2)),    Document(page_content="foo", metadata=dict(page=3)),    Document(page_content="bar burr", metadata=dict(page=3)),    Document(page_content="foo", metadata=dict(page=4)),    Document(page_content="bar bruh", metadata=dict(page=4)),]db = FAISS.from_documents(list_of_documents, embeddings)results_with_scores = db.similarity_search_with_score("foo")for doc, score in results_with_scores:    print(f"Content: {doc.page_content}, Metadata: {doc.metadata}, Score: {score}")API Reference:Document    Content: foo, Metadata: {'page': 1}, Score: 5.159960813797904e-15    Content: foo, Metadata: {'page': 2}, Score: 5.159960813797904e-15    Content: foo, Metadata: {'page': 3}, Score: 5.159960813797904e-15    Content: foo, Metadata: {'page': 4}, Score: 5.159960813797904e-15Now we make the same query call but we filter for only page = 1 results_with_scores = db.similarity_search_with_score("foo", filter=dict(page=1))for doc, score in results_with_scores:    print(f"Content: {doc.page_content}, Metadata: {doc.metadata}, Score: {score}")    Content: foo, Metadata: {'page': 1}, Score: 5.159960813797904e-15    Content: bar, Metadata: {'page': 1}, Score: 0.3131446838378906Same thing can be done with the max_marginal_relevance_search as well.results = db.max_marginal_relevance_search("foo", filter=dict(page=1))for doc in results:    print(f"Content: {doc.page_content}, Metadata: {doc.metadata}")    Content: foo, Metadata: {'page': 1}    Content: bar, Metadata: {'page': 1}Here is an example of how to set fetch_k parameter when calling similarity_search. Usually you would want the fetch_k parameter >> k parameter. This is because the fetch_k parameter is the number of documents that will be fetched before filtering. If you set fetch_k to a low number, you might not get enough documents to filter from.results = db.similarity_search("foo", filter=dict(page=1), k=1, fetch_k=4)for doc in results:    print(f"Content: {doc.page_content}, Metadata: {doc.metadata}")    Content: foo, Metadata: {'page': 1}Deleteâ€‹You can also delete ids. Note that the ids to delete should be the ids in the docstore.db.delete([db.index_to_docstore_id[0]])    True# Is now missing0 in db.index_to_docstore_id    FalsePreviousEpsillaNextHologresSimilarity Search with scoreSaving and loadingMergingSimilarity Search with filteringDelete IntegrationsCallbacksChat modelsChat loadersDocument loadersDocument transformersLLMsMemoryRetrieversText embedding modelsAgents & ToolkitsToolsVector storesActiveloop Deep LakeAlibaba Cloud OpenSearchAnalyticDBAnnoyAtlasAwaDBAzure Cognitive SearchBagelDBCassandraChromaClickHouseDashVectorDingoDocArray HnswSearchDocArray InMemorySearchElasticsearchEpsillaFaissHologresLanceDBMarqoGoogle Vertex AI MatchingEngineMeilisearchMilvusMongoDB AtlasMyScaleNeo4j Vector IndexNucliaDBOpenSearchPostgres EmbeddingPGVectorPineconeQdrantRedisRocksetScaNNSingleStoreDBscikit-learnsqlite-vssStarRocksSupabase (Postgres)TairTencent Cloud VectorDBTigrisTypesenseUSearchvearchVectaraWeaviateXataZepZillizGrouped by providerIntegrationsVector storesFaissOn this pageFaissFacebook AI Similarity Search (Faiss) is a library for efficient similarity search and clustering of dense vectors. It contains algorithms that search in sets of vectors of any size, up to ones that possibly do not fit in RAM. It also contains supporting code for evaluation and parameter tuning.Faiss documentation.This notebook shows how to use functionality related to the FAISS vector database.pip install faiss-gpu # For CUDA 7.5+ Supported GPU's.# ORpip install faiss-cpu # For CPU InstallationWe want to use OpenAIEmbeddings so we have to get the OpenAI API Key. import osimport getpassos.environ["OPENAI_API_KEY"] = getpass.getpass("OpenAI API Key:")# Uncomment the following line if you need to initialize FAISS with no AVX2 optimization# os.environ['FAISS_NO_AVX2'] = '1'from langchain.embeddings.openai import OpenAIEmbeddingsfrom langchain.text_splitter import CharacterTextSplitterfrom langchain.vectorstores import FAISSfrom langchain.document_loaders import TextLoaderAPI Reference:OpenAIEmbeddingsCharacterTextSplitterFAISSTextLoaderfrom langchain.document_loaders import TextLoaderloader = TextLoader("../../../extras/modules/state_of_the_union.txt")documents = loader.load()text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)docs = text_splitter.split_documents(documents)embeddings = OpenAIEmbeddings()API Reference:TextLoaderdb = FAISS.from_documents(docs, embeddings)query = "What did the president say about Ketanji Brown Jackson"docs = db.similarity_search(query)print(docs[0].page_content)    Tonight. I call on the Senate to: Pass the Freedom to Vote Act. Pass the John Lewis Voting Rights Act. And while youâ€™re at it, pass the Disclose Act so Americans can know who is funding our elections.         Tonight, Iâ€™d like to honor someone who has dedicated his life to serve this country: Justice Stephen Breyerâ€”an Army veteran, Constitutional scholar, and retiring Justice of the United States Supreme Court. Justice Breyer, thank you for your service.         One of the most serious constitutional responsibilities a President has is nominating someone to serve on the United States Supreme Court.         And I did that 4 days ago, when I nominated Circuit Court of Appeals Judge Ketanji Brown Jackson. One of our nationâ€™s top legal minds, who will continue Justice Breyerâ€™s legacy of excellence.Similarity Search with scoreâ€‹There are some FAISS specific methods. One of them is similarity_search_with_score, which allows you to return not only the documents but also the distance score of the query to them. The returned distance score is L2 distance. Therefore, a lower score is better.docs_and_scores = db.similarity_search_with_score(query)docs_and_scores[0]    (Document(page_content='Tonight. I call on the Senate to: Pass the Freedom to Vote Act. Pass the John Lewis Voting Rights Act. And while youâ€™re at it, pass the Disclose Act so Americans can know who is funding our elections. \n\nTonight, Iâ€™d like to honor someone who has dedicated his life to serve this country: Justice Stephen Breyerâ€”an Army veteran, Constitutional scholar, and retiring Justice of the United States Supreme Court. Justice Breyer, thank you for your service. \n\nOne of the most serious constitutional responsibilities a President has is nominating someone to serve on the United States Supreme Court. \n\nAnd I did that 4 days ago, when I nominated Circuit Court of Appeals Judge Ketanji Brown Jackson. One of our nationâ€™s top legal minds, who will continue Justice Breyerâ€™s legacy of excellence.', metadata={'source': '../../../state_of_the_union.txt'}),     0.36913747)It is also possible to do a search for documents similar to a given embedding vector using similarity_search_by_vector which accepts an embedding vector as a parameter instead of a string.embedding_vector = embeddings.embed_query(query)docs_and_scores = db.similarity_search_by_vector(embedding_vector)Saving and loadingâ€‹You can also save and load a FAISS index. This is useful so you don't have to recreate it everytime you use it.db.save_local("faiss_index")new_db = FAISS.load_local("faiss_index", embeddings)docs = new_db.similarity_search(query)docs[0]    Document(page_content='Tonight. I call on the Senate to: Pass the Freedom to Vote Act. Pass the John Lewis Voting Rights Act. And while youâ€™re at it, pass the Disclose Act so Americans can know who is funding our elections. \n\nTonight, Iâ€™d like to honor someone who has dedicated his life to serve this country: Justice Stephen Breyerâ€”an Army veteran, Constitutional scholar, and retiring Justice of the United States Supreme Court. Justice Breyer, thank you for your service. \n\nOne of the most serious constitutional responsibilities a President has is nominating someone to serve on the United States Supreme Court. \n\nAnd I did that 4 days ago, when I nominated Circuit Court of Appeals Judge Ketanji Brown Jackson. One of our nationâ€™s top legal minds, who will continue Justice Breyerâ€™s legacy of excellence.', metadata={'source': '../../../state_of_the_union.txt'})Serializing and De-Serializing to bytesyou can pickle the FAISS Index by these functions. If you use embeddings model which is of 90 mb (sentence-transformers/all-MiniLM-L6-v2 or any other model), the resultant pickle size would be more than 90 mb. the size of the model is also included in the overall size. To overcome this, use the below functions. These functions only serializes FAISS index and size would be much lesser. this can be helpful if you wish to store the index in database like sql.pkl = db.serialize_to_bytes() # serializes the faiss indexembeddings = HuggingFaceEmbeddings(model_name="all-MiniLM-L6-v2")db = FAISS.deserialize_from_bytes(embeddings = embeddings, serialized = pkl) # Load the indexMergingâ€‹You can also merge two FAISS vectorstoresdb1 = FAISS.from_texts(["foo"], embeddings)db2 = FAISS.from_texts(["bar"], embeddings)db1.docstore._dict    {'068c473b-d420-487a-806b-fb0ccea7f711': Document(page_content='foo', metadata={})}db2.docstore._dict    {'807e0c63-13f6-4070-9774-5c6f0fbb9866': Document(page_content='bar', metadata={})}db1.merge_from(db2)db1.docstore._dict    {'068c473b-d420-487a-806b-fb0ccea7f711': Document(page_content='foo', metadata={}),     '807e0c63-13f6-4070-9774-5c6f0fbb9866': Document(page_content='bar', metadata={})}Similarity Search with filteringâ€‹FAISS vectorstore can also support filtering, since the FAISS does not natively support filtering we have to do it manually. This is done by first fetching more results than k and then filtering them. You can filter the documents based on metadata. You can also set the fetch_k parameter when calling any search method to set how many documents you want to fetch before filtering. Here is a small example:from langchain.schema import Documentlist_of_documents = [    Document(page_content="foo", metadata=dict(page=1)),    Document(page_content="bar", metadata=dict(page=1)),    Document(page_content="foo", metadata=dict(page=2)),    Document(page_content="barbar", metadata=dict(page=2)),    Document(page_content="foo", metadata=dict(page=3)),    Document(page_content="bar burr", metadata=dict(page=3)),    Document(page_content="foo", metadata=dict(page=4)),    Document(page_content="bar bruh", metadata=dict(page=4)),]db = FAISS.from_documents(list_of_documents, embeddings)results_with_scores = db.similarity_search_with_score("foo")for doc, score in results_with_scores:    print(f"Content: {doc.page_content}, Metadata: {doc.metadata}, Score: {score}")API Reference:Document    Content: foo, Metadata: {'page': 1}, Score: 5.159960813797904e-15    Content: foo, Metadata: {'page': 2}, Score: 5.159960813797904e-15    Content: foo, Metadata: {'page': 3}, Score: 5.159960813797904e-15    Content: foo, Metadata: {'page': 4}, Score: 5.159960813797904e-15Now we make the same query call but we filter for only page = 1 results_with_scores = db.similarity_search_with_score("foo", filter=dict(page=1))for doc, score in results_with_scores:    print(f"Content: {doc.page_content}, Metadata: {doc.metadata}, Score: {score}")    Content: foo, Metadata: {'page': 1}, Score: 5.159960813797904e-15    Content: bar, Metadata: {'page': 1}, Score: 0.3131446838378906Same thing can be done with the max_marginal_relevance_search as well.results = db.max_marginal_relevance_search("foo", filter=dict(page=1))for doc in results:    print(f"Content: {doc.page_content}, Metadata: {doc.metadata}")    Content: foo, Metadata: {'page': 1}    Content: bar, Metadata: {'page': 1}Here is an example of how to set fetch_k parameter when calling similarity_search. Usually you would want the fetch_k parameter >> k parameter. This is because the fetch_k parameter is the number of documents that will be fetched before filtering. If you set fetch_k to a low number, you might not get enough documents to filter from.results = db.similarity_search("foo", filter=dict(page=1), k=1, fetch_k=4)for doc in results:    print(f"Content: {doc.page_content}, Metadata: {doc.metadata}")    Content: foo, Metadata: {'page': 1}Deleteâ€‹You can also delete ids. Note that the ids to delete should be the ids in the docstore.db.delete([db.index_to_docstore_id[0]])    True# Is now missing0 in db.index_to_docstore_id    FalsePreviousEpsillaNextHologresSimilarity Search with scoreSaving and loadingMergingSimilarity Search with filteringDelete IntegrationsCallbacksChat modelsChat loadersDocument loadersDocument transformersLLMsMemoryRetrieversText embedding modelsAgents & ToolkitsToolsVector storesActiveloop Deep LakeAlibaba Cloud OpenSearchAnalyticDBAnnoyAtlasAwaDBAzure Cognitive SearchBagelDBCassandraChromaClickHouseDashVectorDingoDocArray HnswSearchDocArray InMemorySearchElasticsearchEpsillaFaissHologresLanceDBMarqoGoogle Vertex AI MatchingEngineMeilisearchMilvusMongoDB AtlasMyScaleNeo4j Vector IndexNucliaDBOpenSearchPostgres EmbeddingPGVectorPineconeQdrantRedisRocksetScaNNSingleStoreDBscikit-learnsqlite-vssStarRocksSupabase (Postgres)TairTencent Cloud VectorDBTigrisTypesenseUSearchvearchVectaraWeaviateXataZepZillizGrouped by provider IntegrationsCallbacksChat modelsChat loadersDocument loadersDocument transformersLLMsMemoryRetrieversText embedding modelsAgents & ToolkitsToolsVector storesActiveloop Deep LakeAlibaba Cloud OpenSearchAnalyticDBAnnoyAtlasAwaDBAzure Cognitive SearchBagelDBCassandraChromaClickHouseDashVectorDingoDocArray HnswSearchDocArray InMemorySearchElasticsearchEpsillaFaissHologresLanceDBMarqoGoogle Vertex AI MatchingEngineMeilisearchMilvusMongoDB AtlasMyScaleNeo4j Vector IndexNucliaDBOpenSearchPostgres EmbeddingPGVectorPineconeQdrantRedisRocksetScaNNSingleStoreDBscikit-learnsqlite-vssStarRocksSupabase (Postgres)TairTencent Cloud VectorDBTigrisTypesenseUSearchvearchVectaraWeaviateXataZepZillizGrouped by provider Integrations Callbacks Chat models Chat loaders Document loaders Document transformers LLMs Memory Retrievers Text embedding models Agents & Toolkits Tools Vector stores Grouped by provider IntegrationsVector storesFaissOn this pageFaissFacebook AI Similarity Search (Faiss) is a library for efficient similarity search and clustering of dense vectors. It contains algorithms that search in sets of vectors of any size, up to ones that possibly do not fit in RAM. It also contains supporting code for evaluation and parameter tuning.Faiss documentation.This notebook shows how to use functionality related to the FAISS vector database.pip install faiss-gpu # For CUDA 7.5+ Supported GPU's.# ORpip install faiss-cpu # For CPU InstallationWe want to use OpenAIEmbeddings so we have to get the OpenAI API Key. import osimport getpassos.environ["OPENAI_API_KEY"] = getpass.getpass("OpenAI API Key:")# Uncomment the following line if you need to initialize FAISS with no AVX2 optimization# os.environ['FAISS_NO_AVX2'] = '1'from langchain.embeddings.openai import OpenAIEmbeddingsfrom langchain.text_splitter import CharacterTextSplitterfrom langchain.vectorstores import FAISSfrom langchain.document_loaders import TextLoaderAPI Reference:OpenAIEmbeddingsCharacterTextSplitterFAISSTextLoaderfrom langchain.document_loaders import TextLoaderloader = TextLoader("../../../extras/modules/state_of_the_union.txt")documents = loader.load()text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)docs = text_splitter.split_documents(documents)embeddings = OpenAIEmbeddings()API Reference:TextLoaderdb = FAISS.from_documents(docs, embeddings)query = "What did the president say about Ketanji Brown Jackson"docs = db.similarity_search(query)print(docs[0].page_content)    Tonight. I call on the Senate to: Pass the Freedom to Vote Act. Pass the John Lewis Voting Rights Act. And while youâ€™re at it, pass the Disclose Act so Americans can know who is funding our elections.         Tonight, Iâ€™d like to honor someone who has dedicated his life to serve this country: Justice Stephen Breyerâ€”an Army veteran, Constitutional scholar, and retiring Justice of the United States Supreme Court. Justice Breyer, thank you for your service.         One of the most serious constitutional responsibilities a President has is nominating someone to serve on the United States Supreme Court.         And I did that 4 days ago, when I nominated Circuit Court of Appeals Judge Ketanji Brown Jackson. One of our nationâ€™s top legal minds, who will continue Justice Breyerâ€™s legacy of excellence.Similarity Search with scoreâ€‹There are some FAISS specific methods. One of them is similarity_search_with_score, which allows you to return not only the documents but also the distance score of the query to them. The returned distance score is L2 distance. Therefore, a lower score is better.docs_and_scores = db.similarity_search_with_score(query)docs_and_scores[0]    (Document(page_content='Tonight. I call on the Senate to: Pass the Freedom to Vote Act. Pass the John Lewis Voting Rights Act. And while youâ€™re at it, pass the Disclose Act so Americans can know who is funding our elections. \n\nTonight, Iâ€™d like to honor someone who has dedicated his life to serve this country: Justice Stephen Breyerâ€”an Army veteran, Constitutional scholar, and retiring Justice of the United States Supreme Court. Justice Breyer, thank you for your service. \n\nOne of the most serious constitutional responsibilities a President has is nominating someone to serve on the United States Supreme Court. \n\nAnd I did that 4 days ago, when I nominated Circuit Court of Appeals Judge Ketanji Brown Jackson. One of our nationâ€™s top legal minds, who will continue Justice Breyerâ€™s legacy of excellence.', metadata={'source': '../../../state_of_the_union.txt'}),     0.36913747)It is also possible to do a search for documents similar to a given embedding vector using similarity_search_by_vector which accepts an embedding vector as a parameter instead of a string.embedding_vector = embeddings.embed_query(query)docs_and_scores = db.similarity_search_by_vector(embedding_vector)Saving and loadingâ€‹You can also save and load a FAISS index. This is useful so you don't have to recreate it everytime you use it.db.save_local("faiss_index")new_db = FAISS.load_local("faiss_index", embeddings)docs = new_db.similarity_search(query)docs[0]    Document(page_content='Tonight. I call on the Senate to: Pass the Freedom to Vote Act. Pass the John Lewis Voting Rights Act. And while youâ€™re at it, pass the Disclose Act so Americans can know who is funding our elections. \n\nTonight, Iâ€™d like to honor someone who has dedicated his life to serve this country: Justice Stephen Breyerâ€”an Army veteran, Constitutional scholar, and retiring Justice of the United States Supreme Court. Justice Breyer, thank you for your service. \n\nOne of the most serious constitutional responsibilities a President has is nominating someone to serve on the United States Supreme Court. \n\nAnd I did that 4 days ago, when I nominated Circuit Court of Appeals Judge Ketanji Brown Jackson. One of our nationâ€™s top legal minds, who will continue Justice Breyerâ€™s legacy of excellence.', metadata={'source': '../../../state_of_the_union.txt'})Serializing and De-Serializing to bytesyou can pickle the FAISS Index by these functions. If you use embeddings model which is of 90 mb (sentence-transformers/all-MiniLM-L6-v2 or any other model), the resultant pickle size would be more than 90 mb. the size of the model is also included in the overall size. To overcome this, use the below functions. These functions only serializes FAISS index and size would be much lesser. this can be helpful if you wish to store the index in database like sql.pkl = db.serialize_to_bytes() # serializes the faiss indexembeddings = HuggingFaceEmbeddings(model_name="all-MiniLM-L6-v2")db = FAISS.deserialize_from_bytes(embeddings = embeddings, serialized = pkl) # Load the indexMergingâ€‹You can also merge two FAISS vectorstoresdb1 = FAISS.from_texts(["foo"], embeddings)db2 = FAISS.from_texts(["bar"], embeddings)db1.docstore._dict    {'068c473b-d420-487a-806b-fb0ccea7f711': Document(page_content='foo', metadata={})}db2.docstore._dict    {'807e0c63-13f6-4070-9774-5c6f0fbb9866': Document(page_content='bar', metadata={})}db1.merge_from(db2)db1.docstore._dict    {'068c473b-d420-487a-806b-fb0ccea7f711': Document(page_content='foo', metadata={}),     '807e0c63-13f6-4070-9774-5c6f0fbb9866': Document(page_content='bar', metadata={})}Similarity Search with filteringâ€‹FAISS vectorstore can also support filtering, since the FAISS does not natively support filtering we have to do it manually. This is done by first fetching more results than k and then filtering them. You can filter the documents based on metadata. You can also set the fetch_k parameter when calling any search method to set how many documents you want to fetch before filtering. Here is a small example:from langchain.schema import Documentlist_of_documents = [    Document(page_content="foo", metadata=dict(page=1)),    Document(page_content="bar", metadata=dict(page=1)),    Document(page_content="foo", metadata=dict(page=2)),    Document(page_content="barbar", metadata=dict(page=2)),    Document(page_content="foo", metadata=dict(page=3)),    Document(page_content="bar burr", metadata=dict(page=3)),    Document(page_content="foo", metadata=dict(page=4)),    Document(page_content="bar bruh", metadata=dict(page=4)),]db = FAISS.from_documents(list_of_documents, embeddings)results_with_scores = db.similarity_search_with_score("foo")for doc, score in results_with_scores:    print(f"Content: {doc.page_content}, Metadata: {doc.metadata}, Score: {score}")API Reference:Document    Content: foo, Metadata: {'page': 1}, Score: 5.159960813797904e-15    Content: foo, Metadata: {'page': 2}, Score: 5.159960813797904e-15    Content: foo, Metadata: {'page': 3}, Score: 5.159960813797904e-15    Content: foo, Metadata: {'page': 4}, Score: 5.159960813797904e-15Now we make the same query call but we filter for only page = 1 results_with_scores = db.similarity_search_with_score("foo", filter=dict(page=1))for doc, score in results_with_scores:    print(f"Content: {doc.page_content}, Metadata: {doc.metadata}, Score: {score}")    Content: foo, Metadata: {'page': 1}, Score: 5.159960813797904e-15    Content: bar, Metadata: {'page': 1}, Score: 0.3131446838378906Same thing can be done with the max_marginal_relevance_search as well.results = db.max_marginal_relevance_search("foo", filter=dict(page=1))for doc in results:    print(f"Content: {doc.page_content}, Metadata: {doc.metadata}")    Content: foo, Metadata: {'page': 1}    Content: bar, Metadata: {'page': 1}Here is an example of how to set fetch_k parameter when calling similarity_search. Usually you would want the fetch_k parameter >> k parameter. This is because the fetch_k parameter is the number of documents that will be fetched before filtering. If you set fetch_k to a low number, you might not get enough documents to filter from.results = db.similarity_search("foo", filter=dict(page=1), k=1, fetch_k=4)for doc in results:    print(f"Content: {doc.page_content}, Metadata: {doc.metadata}")    Content: foo, Metadata: {'page': 1}Deleteâ€‹You can also delete ids. Note that the ids to delete should be the ids in the docstore.db.delete([db.index_to_docstore_id[0]])    True# Is now missing0 in db.index_to_docstore_id    FalsePreviousEpsillaNextHologresSimilarity Search with scoreSaving and loadingMergingSimilarity Search with filteringDelete IntegrationsVector storesFaissOn this pageFaissFacebook AI Similarity Search (Faiss) is a library for efficient similarity search and clustering of dense vectors. It contains algorithms that search in sets of vectors of any size, up to ones that possibly do not fit in RAM. It also contains supporting code for evaluation and parameter tuning.Faiss documentation.This notebook shows how to use functionality related to the FAISS vector database.pip install faiss-gpu # For CUDA 7.5+ Supported GPU's.# ORpip install faiss-cpu # For CPU InstallationWe want to use OpenAIEmbeddings so we have to get the OpenAI API Key. import osimport getpassos.environ["OPENAI_API_KEY"] = getpass.getpass("OpenAI API Key:")# Uncomment the following line if you need to initialize FAISS with no AVX2 optimization# os.environ['FAISS_NO_AVX2'] = '1'from langchain.embeddings.openai import OpenAIEmbeddingsfrom langchain.text_splitter import CharacterTextSplitterfrom langchain.vectorstores import FAISSfrom langchain.document_loaders import TextLoaderAPI Reference:OpenAIEmbeddingsCharacterTextSplitterFAISSTextLoaderfrom langchain.document_loaders import TextLoaderloader = TextLoader("../../../extras/modules/state_of_the_union.txt")documents = loader.load()text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)docs = text_splitter.split_documents(documents)embeddings = OpenAIEmbeddings()API Reference:TextLoaderdb = FAISS.from_documents(docs, embeddings)query = "What did the president say about Ketanji Brown Jackson"docs = db.similarity_search(query)print(docs[0].page_content)    Tonight. I call on the Senate to: Pass the Freedom to Vote Act. Pass the John Lewis Voting Rights Act. And while youâ€™re at it, pass the Disclose Act so Americans can know who is funding our elections.         Tonight, Iâ€™d like to honor someone who has dedicated his life to serve this country: Justice Stephen Breyerâ€”an Army veteran, Constitutional scholar, and retiring Justice of the United States Supreme Court. Justice Breyer, thank you for your service.         One of the most serious constitutional responsibilities a President has is nominating someone to serve on the United States Supreme Court.         And I did that 4 days ago, when I nominated Circuit Court of Appeals Judge Ketanji Brown Jackson. One of our nationâ€™s top legal minds, who will continue Justice Breyerâ€™s legacy of excellence.Similarity Search with scoreâ€‹There are some FAISS specific methods. One of them is similarity_search_with_score, which allows you to return not only the documents but also the distance score of the query to them. The returned distance score is L2 distance. Therefore, a lower score is better.docs_and_scores = db.similarity_search_with_score(query)docs_and_scores[0]    (Document(page_content='Tonight. I call on the Senate to: Pass the Freedom to Vote Act. Pass the John Lewis Voting Rights Act. And while youâ€™re at it, pass the Disclose Act so Americans can know who is funding our elections. \n\nTonight, Iâ€™d like to honor someone who has dedicated his life to serve this country: Justice Stephen Breyerâ€”an Army veteran, Constitutional scholar, and retiring Justice of the United States Supreme Court. Justice Breyer, thank you for your service. \n\nOne of the most serious constitutional responsibilities a President has is nominating someone to serve on the United States Supreme Court. \n\nAnd I did that 4 days ago, when I nominated Circuit Court of Appeals Judge Ketanji Brown Jackson. One of our nationâ€™s top legal minds, who will continue Justice Breyerâ€™s legacy of excellence.', metadata={'source': '../../../state_of_the_union.txt'}),     0.36913747)It is also possible to do a search for documents similar to a given embedding vector using similarity_search_by_vector which accepts an embedding vector as a parameter instead of a string.embedding_vector = embeddings.embed_query(query)docs_and_scores = db.similarity_search_by_vector(embedding_vector)Saving and loadingâ€‹You can also save and load a FAISS index. This is useful so you don't have to recreate it everytime you use it.db.save_local("faiss_index")new_db = FAISS.load_local("faiss_index", embeddings)docs = new_db.similarity_search(query)docs[0]    Document(page_content='Tonight. I call on the Senate to: Pass the Freedom to Vote Act. Pass the John Lewis Voting Rights Act. And while youâ€™re at it, pass the Disclose Act so Americans can know who is funding our elections. \n\nTonight, Iâ€™d like to honor someone who has dedicated his life to serve this country: Justice Stephen Breyerâ€”an Army veteran, Constitutional scholar, and retiring Justice of the United States Supreme Court. Justice Breyer, thank you for your service. \n\nOne of the most serious constitutional responsibilities a President has is nominating someone to serve on the United States Supreme Court. \n\nAnd I did that 4 days ago, when I nominated Circuit Court of Appeals Judge Ketanji Brown Jackson. One of our nationâ€™s top legal minds, who will continue Justice Breyerâ€™s legacy of excellence.', metadata={'source': '../../../state_of_the_union.txt'})Serializing and De-Serializing to bytesyou can pickle the FAISS Index by these functions. If you use embeddings model which is of 90 mb (sentence-transformers/all-MiniLM-L6-v2 or any other model), the resultant pickle size would be more than 90 mb. the size of the model is also included in the overall size. To overcome this, use the below functions. These functions only serializes FAISS index and size would be much lesser. this can be helpful if you wish to store the index in database like sql.pkl = db.serialize_to_bytes() # serializes the faiss indexembeddings = HuggingFaceEmbeddings(model_name="all-MiniLM-L6-v2")db = FAISS.deserialize_from_bytes(embeddings = embeddings, serialized = pkl) # Load the indexMergingâ€‹You can also merge two FAISS vectorstoresdb1 = FAISS.from_texts(["foo"], embeddings)db2 = FAISS.from_texts(["bar"], embeddings)db1.docstore._dict    {'068c473b-d420-487a-806b-fb0ccea7f711': Document(page_content='foo', metadata={})}db2.docstore._dict    {'807e0c63-13f6-4070-9774-5c6f0fbb9866': Document(page_content='bar', metadata={})}db1.merge_from(db2)db1.docstore._dict    {'068c473b-d420-487a-806b-fb0ccea7f711': Document(page_content='foo', metadata={}),     '807e0c63-13f6-4070-9774-5c6f0fbb9866': Document(page_content='bar', metadata={})}Similarity Search with filteringâ€‹FAISS vectorstore can also support filtering, since the FAISS does not natively support filtering we have to do it manually. This is done by first fetching more results than k and then filtering them. You can filter the documents based on metadata. You can also set the fetch_k parameter when calling any search method to set how many documents you want to fetch before filtering. Here is a small example:from langchain.schema import Documentlist_of_documents = [    Document(page_content="foo", metadata=dict(page=1)),    Document(page_content="bar", metadata=dict(page=1)),    Document(page_content="foo", metadata=dict(page=2)),    Document(page_content="barbar", metadata=dict(page=2)),    Document(page_content="foo", metadata=dict(page=3)),    Document(page_content="bar burr", metadata=dict(page=3)),    Document(page_content="foo", metadata=dict(page=4)),    Document(page_content="bar bruh", metadata=dict(page=4)),]db = FAISS.from_documents(list_of_documents, embeddings)results_with_scores = db.similarity_search_with_score("foo")for doc, score in results_with_scores:    print(f"Content: {doc.page_content}, Metadata: {doc.metadata}, Score: {score}")API Reference:Document    Content: foo, Metadata: {'page': 1}, Score: 5.159960813797904e-15    Content: foo, Metadata: {'page': 2}, Score: 5.159960813797904e-15    Content: foo, Metadata: {'page': 3}, Score: 5.159960813797904e-15    Content: foo, Metadata: {'page': 4}, Score: 5.159960813797904e-15Now we make the same query call but we filter for only page = 1 results_with_scores = db.similarity_search_with_score("foo", filter=dict(page=1))for doc, score in results_with_scores:    print(f"Content: {doc.page_content}, Metadata: {doc.metadata}, Score: {score}")    Content: foo, Metadata: {'page': 1}, Score: 5.159960813797904e-15    Content: bar, Metadata: {'page': 1}, Score: 0.3131446838378906Same thing can be done with the max_marginal_relevance_search as well.results = db.max_marginal_relevance_search("foo", filter=dict(page=1))for doc in results:    print(f"Content: {doc.page_content}, Metadata: {doc.metadata}")    Content: foo, Metadata: {'page': 1}    Content: bar, Metadata: {'page': 1}Here is an example of how to set fetch_k parameter when calling similarity_search. Usually you would want the fetch_k parameter >> k parameter. This is because the fetch_k parameter is the number of documents that will be fetched before filtering. If you set fetch_k to a low number, you might not get enough documents to filter from.results = db.similarity_search("foo", filter=dict(page=1), k=1, fetch_k=4)for doc in results:    print(f"Content: {doc.page_content}, Metadata: {doc.metadata}")    Content: foo, Metadata: {'page': 1}Deleteâ€‹You can also delete ids. Note that the ids to delete should be the ids in the docstore.db.delete([db.index_to_docstore_id[0]])    True# Is now missing0 in db.index_to_docstore_id    FalsePreviousEpsillaNextHologresSimilarity Search with scoreSaving and loadingMergingSimilarity Search with filteringDelete IntegrationsVector storesFaissOn this pageFaissFacebook AI Similarity Search (Faiss) is a library for efficient similarity search and clustering of dense vectors. It contains algorithms that search in sets of vectors of any size, up to ones that possibly do not fit in RAM. It also contains supporting code for evaluation and parameter tuning.Faiss documentation.This notebook shows how to use functionality related to the FAISS vector database.pip install faiss-gpu # For CUDA 7.5+ Supported GPU's.# ORpip install faiss-cpu # For CPU InstallationWe want to use OpenAIEmbeddings so we have to get the OpenAI API Key. import osimport getpassos.environ["OPENAI_API_KEY"] = getpass.getpass("OpenAI API Key:")# Uncomment the following line if you need to initialize FAISS with no AVX2 optimization# os.environ['FAISS_NO_AVX2'] = '1'from langchain.embeddings.openai import OpenAIEmbeddingsfrom langchain.text_splitter import CharacterTextSplitterfrom langchain.vectorstores import FAISSfrom langchain.document_loaders import TextLoaderAPI Reference:OpenAIEmbeddingsCharacterTextSplitterFAISSTextLoaderfrom langchain.document_loaders import TextLoaderloader = TextLoader("../../../extras/modules/state_of_the_union.txt")documents = loader.load()text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)docs = text_splitter.split_documents(documents)embeddings = OpenAIEmbeddings()API Reference:TextLoaderdb = FAISS.from_documents(docs, embeddings)query = "What did the president say about Ketanji Brown Jackson"docs = db.similarity_search(query)print(docs[0].page_content)    Tonight. I call on the Senate to: Pass the Freedom to Vote Act. Pass the John Lewis Voting Rights Act. And while youâ€™re at it, pass the Disclose Act so Americans can know who is funding our elections.         Tonight, Iâ€™d like to honor someone who has dedicated his life to serve this country: Justice Stephen Breyerâ€”an Army veteran, Constitutional scholar, and retiring Justice of the United States Supreme Court. Justice Breyer, thank you for your service.         One of the most serious constitutional responsibilities a President has is nominating someone to serve on the United States Supreme Court.         And I did that 4 days ago, when I nominated Circuit Court of Appeals Judge Ketanji Brown Jackson. One of our nationâ€™s top legal minds, who will continue Justice Breyerâ€™s legacy of excellence.Similarity Search with scoreâ€‹There are some FAISS specific methods. One of them is similarity_search_with_score, which allows you to return not only the documents but also the distance score of the query to them. The returned distance score is L2 distance. Therefore, a lower score is better.docs_and_scores = db.similarity_search_with_score(query)docs_and_scores[0]    (Document(page_content='Tonight. I call on the Senate to: Pass the Freedom to Vote Act. Pass the John Lewis Voting Rights Act. And while youâ€™re at it, pass the Disclose Act so Americans can know who is funding our elections. \n\nTonight, Iâ€™d like to honor someone who has dedicated his life to serve this country: Justice Stephen Breyerâ€”an Army veteran, Constitutional scholar, and retiring Justice of the United States Supreme Court. Justice Breyer, thank you for your service. \n\nOne of the most serious constitutional responsibilities a President has is nominating someone to serve on the United States Supreme Court. \n\nAnd I did that 4 days ago, when I nominated Circuit Court of Appeals Judge Ketanji Brown Jackson. One of our nationâ€™s top legal minds, who will continue Justice Breyerâ€™s legacy of excellence.', metadata={'source': '../../../state_of_the_union.txt'}),     0.36913747)It is also possible to do a search for documents similar to a given embedding vector using similarity_search_by_vector which accepts an embedding vector as a parameter instead of a string.embedding_vector = embeddings.embed_query(query)docs_and_scores = db.similarity_search_by_vector(embedding_vector)Saving and loadingâ€‹You can also save and load a FAISS index. This is useful so you don't have to recreate it everytime you use it.db.save_local("faiss_index")new_db = FAISS.load_local("faiss_index", embeddings)docs = new_db.similarity_search(query)docs[0]    Document(page_content='Tonight. I call on the Senate to: Pass the Freedom to Vote Act. Pass the John Lewis Voting Rights Act. And while youâ€™re at it, pass the Disclose Act so Americans can know who is funding our elections. \n\nTonight, Iâ€™d like to honor someone who has dedicated his life to serve this country: Justice Stephen Breyerâ€”an Army veteran, Constitutional scholar, and retiring Justice of the United States Supreme Court. Justice Breyer, thank you for your service. \n\nOne of the most serious constitutional responsibilities a President has is nominating someone to serve on the United States Supreme Court. \n\nAnd I did that 4 days ago, when I nominated Circuit Court of Appeals Judge Ketanji Brown Jackson. One of our nationâ€™s top legal minds, who will continue Justice Breyerâ€™s legacy of excellence.', metadata={'source': '../../../state_of_the_union.txt'})Serializing and De-Serializing to bytesyou can pickle the FAISS Index by these functions. If you use embeddings model which is of 90 mb (sentence-transformers/all-MiniLM-L6-v2 or any other model), the resultant pickle size would be more than 90 mb. the size of the model is also included in the overall size. To overcome this, use the below functions. These functions only serializes FAISS index and size would be much lesser. this can be helpful if you wish to store the index in database like sql.pkl = db.serialize_to_bytes() # serializes the faiss indexembeddings = HuggingFaceEmbeddings(model_name="all-MiniLM-L6-v2")db = FAISS.deserialize_from_bytes(embeddings = embeddings, serialized = pkl) # Load the indexMergingâ€‹You can also merge two FAISS vectorstoresdb1 = FAISS.from_texts(["foo"], embeddings)db2 = FAISS.from_texts(["bar"], embeddings)db1.docstore._dict    {'068c473b-d420-487a-806b-fb0ccea7f711': Document(page_content='foo', metadata={})}db2.docstore._dict    {'807e0c63-13f6-4070-9774-5c6f0fbb9866': Document(page_content='bar', metadata={})}db1.merge_from(db2)db1.docstore._dict    {'068c473b-d420-487a-806b-fb0ccea7f711': Document(page_content='foo', metadata={}),     '807e0c63-13f6-4070-9774-5c6f0fbb9866': Document(page_content='bar', metadata={})}Similarity Search with filteringâ€‹FAISS vectorstore can also support filtering, since the FAISS does not natively support filtering we have to do it manually. This is done by first fetching more results than k and then filtering them. You can filter the documents based on metadata. You can also set the fetch_k parameter when calling any search method to set how many documents you want to fetch before filtering. Here is a small example:from langchain.schema import Documentlist_of_documents = [    Document(page_content="foo", metadata=dict(page=1)),    Document(page_content="bar", metadata=dict(page=1)),    Document(page_content="foo", metadata=dict(page=2)),    Document(page_content="barbar", metadata=dict(page=2)),    Document(page_content="foo", metadata=dict(page=3)),    Document(page_content="bar burr", metadata=dict(page=3)),    Document(page_content="foo", metadata=dict(page=4)),    Document(page_content="bar bruh", metadata=dict(page=4)),]db = FAISS.from_documents(list_of_documents, embeddings)results_with_scores = db.similarity_search_with_score("foo")for doc, score in results_with_scores:    print(f"Content: {doc.page_content}, Metadata: {doc.metadata}, Score: {score}")API Reference:Document    Content: foo, Metadata: {'page': 1}, Score: 5.159960813797904e-15    Content: foo, Metadata: {'page': 2}, Score: 5.159960813797904e-15    Content: foo, Metadata: {'page': 3}, Score: 5.159960813797904e-15    Content: foo, Metadata: {'page': 4}, Score: 5.159960813797904e-15Now we make the same query call but we filter for only page = 1 results_with_scores = db.similarity_search_with_score("foo", filter=dict(page=1))for doc, score in results_with_scores:    print(f"Content: {doc.page_content}, Metadata: {doc.metadata}, Score: {score}")    Content: foo, Metadata: {'page': 1}, Score: 5.159960813797904e-15    Content: bar, Metadata: {'page': 1}, Score: 0.3131446838378906Same thing can be done with the max_marginal_relevance_search as well.results = db.max_marginal_relevance_search("foo", filter=dict(page=1))for doc in results:    print(f"Content: {doc.page_content}, Metadata: {doc.metadata}")    Content: foo, Metadata: {'page': 1}    Content: bar, Metadata: {'page': 1}Here is an example of how to set fetch_k parameter when calling similarity_search. Usually you would want the fetch_k parameter >> k parameter. This is because the fetch_k parameter is the number of documents that will be fetched before filtering. If you set fetch_k to a low number, you might not get enough documents to filter from.results = db.similarity_search("foo", filter=dict(page=1), k=1, fetch_k=4)for doc in results:    print(f"Content: {doc.page_content}, Metadata: {doc.metadata}")    Content: foo, Metadata: {'page': 1}Deleteâ€‹You can also delete ids. Note that the ids to delete should be the ids in the docstore.db.delete([db.index_to_docstore_id[0]])    True# Is now missing0 in db.index_to_docstore_id    FalsePreviousEpsillaNextHologres IntegrationsVector storesFaissOn this pageFaissFacebook AI Similarity Search (Faiss) is a library for efficient similarity search and clustering of dense vectors. It contains algorithms that search in sets of vectors of any size, up to ones that possibly do not fit in RAM. It also contains supporting code for evaluation and parameter tuning.Faiss documentation.This notebook shows how to use functionality related to the FAISS vector database.pip install faiss-gpu # For CUDA 7.5+ Supported GPU's.# ORpip install faiss-cpu # For CPU InstallationWe want to use OpenAIEmbeddings so we have to get the OpenAI API Key. import osimport getpassos.environ["OPENAI_API_KEY"] = getpass.getpass("OpenAI API Key:")# Uncomment the following line if you need to initialize FAISS with no AVX2 optimization# os.environ['FAISS_NO_AVX2'] = '1'from langchain.embeddings.openai import OpenAIEmbeddingsfrom langchain.text_splitter import CharacterTextSplitterfrom langchain.vectorstores import FAISSfrom langchain.document_loaders import TextLoaderAPI Reference:OpenAIEmbeddingsCharacterTextSplitterFAISSTextLoaderfrom langchain.document_loaders import TextLoaderloader = TextLoader("../../../extras/modules/state_of_the_union.txt")documents = loader.load()text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)docs = text_splitter.split_documents(documents)embeddings = OpenAIEmbeddings()API Reference:TextLoaderdb = FAISS.from_documents(docs, embeddings)query = "What did the president say about Ketanji Brown Jackson"docs = db.similarity_search(query)print(docs[0].page_content)    Tonight. I call on the Senate to: Pass the Freedom to Vote Act. Pass the John Lewis Voting Rights Act. And while youâ€™re at it, pass the Disclose Act so Americans can know who is funding our elections.         Tonight, Iâ€™d like to honor someone who has dedicated his life to serve this country: Justice Stephen Breyerâ€”an Army veteran, Constitutional scholar, and retiring Justice of the United States Supreme Court. Justice Breyer, thank you for your service.         One of the most serious constitutional responsibilities a President has is nominating someone to serve on the United States Supreme Court.         And I did that 4 days ago, when I nominated Circuit Court of Appeals Judge Ketanji Brown Jackson. One of our nationâ€™s top legal minds, who will continue Justice Breyerâ€™s legacy of excellence.Similarity Search with scoreâ€‹There are some FAISS specific methods. One of them is similarity_search_with_score, which allows you to return not only the documents but also the distance score of the query to them. The returned distance score is L2 distance. Therefore, a lower score is better.docs_and_scores = db.similarity_search_with_score(query)docs_and_scores[0]    (Document(page_content='Tonight. I call on the Senate to: Pass the Freedom to Vote Act. Pass the John Lewis Voting Rights Act. And while youâ€™re at it, pass the Disclose Act so Americans can know who is funding our elections. \n\nTonight, Iâ€™d like to honor someone who has dedicated his life to serve this country: Justice Stephen Breyerâ€”an Army veteran, Constitutional scholar, and retiring Justice of the United States Supreme Court. Justice Breyer, thank you for your service. \n\nOne of the most serious constitutional responsibilities a President has is nominating someone to serve on the United States Supreme Court. \n\nAnd I did that 4 days ago, when I nominated Circuit Court of Appeals Judge Ketanji Brown Jackson. One of our nationâ€™s top legal minds, who will continue Justice Breyerâ€™s legacy of excellence.', metadata={'source': '../../../state_of_the_union.txt'}),     0.36913747)It is also possible to do a search for documents similar to a given embedding vector using similarity_search_by_vector which accepts an embedding vector as a parameter instead of a string.embedding_vector = embeddings.embed_query(query)docs_and_scores = db.similarity_search_by_vector(embedding_vector)Saving and loadingâ€‹You can also save and load a FAISS index. This is useful so you don't have to recreate it everytime you use it.db.save_local("faiss_index")new_db = FAISS.load_local("faiss_index", embeddings)docs = new_db.similarity_search(query)docs[0]    Document(page_content='Tonight. I call on the Senate to: Pass the Freedom to Vote Act. Pass the John Lewis Voting Rights Act. And while youâ€™re at it, pass the Disclose Act so Americans can know who is funding our elections. \n\nTonight, Iâ€™d like to honor someone who has dedicated his life to serve this country: Justice Stephen Breyerâ€”an Army veteran, Constitutional scholar, and retiring Justice of the United States Supreme Court. Justice Breyer, thank you for your service. \n\nOne of the most serious constitutional responsibilities a President has is nominating someone to serve on the United States Supreme Court. \n\nAnd I did that 4 days ago, when I nominated Circuit Court of Appeals Judge Ketanji Brown Jackson. One of our nationâ€™s top legal minds, who will continue Justice Breyerâ€™s legacy of excellence.', metadata={'source': '../../../state_of_the_union.txt'})Serializing and De-Serializing to bytesyou can pickle the FAISS Index by these functions. If you use embeddings model which is of 90 mb (sentence-transformers/all-MiniLM-L6-v2 or any other model), the resultant pickle size would be more than 90 mb. the size of the model is also included in the overall size. To overcome this, use the below functions. These functions only serializes FAISS index and size would be much lesser. this can be helpful if you wish to store the index in database like sql.pkl = db.serialize_to_bytes() # serializes the faiss indexembeddings = HuggingFaceEmbeddings(model_name="all-MiniLM-L6-v2")db = FAISS.deserialize_from_bytes(embeddings = embeddings, serialized = pkl) # Load the indexMergingâ€‹You can also merge two FAISS vectorstoresdb1 = FAISS.from_texts(["foo"], embeddings)db2 = FAISS.from_texts(["bar"], embeddings)db1.docstore._dict    {'068c473b-d420-487a-806b-fb0ccea7f711': Document(page_content='foo', metadata={})}db2.docstore._dict    {'807e0c63-13f6-4070-9774-5c6f0fbb9866': Document(page_content='bar', metadata={})}db1.merge_from(db2)db1.docstore._dict    {'068c473b-d420-487a-806b-fb0ccea7f711': Document(page_content='foo', metadata={}),     '807e0c63-13f6-4070-9774-5c6f0fbb9866': Document(page_content='bar', metadata={})}Similarity Search with filteringâ€‹FAISS vectorstore can also support filtering, since the FAISS does not natively support filtering we have to do it manually. This is done by first fetching more results than k and then filtering them. You can filter the documents based on metadata. You can also set the fetch_k parameter when calling any search method to set how many documents you want to fetch before filtering. Here is a small example:from langchain.schema import Documentlist_of_documents = [    Document(page_content="foo", metadata=dict(page=1)),    Document(page_content="bar", metadata=dict(page=1)),    Document(page_content="foo", metadata=dict(page=2)),    Document(page_content="barbar", metadata=dict(page=2)),    Document(page_content="foo", metadata=dict(page=3)),    Document(page_content="bar burr", metadata=dict(page=3)),    Document(page_content="foo", metadata=dict(page=4)),    Document(page_content="bar bruh", metadata=dict(page=4)),]db = FAISS.from_documents(list_of_documents, embeddings)results_with_scores = db.similarity_search_with_score("foo")for doc, score in results_with_scores:    print(f"Content: {doc.page_content}, Metadata: {doc.metadata}, Score: {score}")API Reference:Document    Content: foo, Metadata: {'page': 1}, Score: 5.159960813797904e-15    Content: foo, Metadata: {'page': 2}, Score: 5.159960813797904e-15    Content: foo, Metadata: {'page': 3}, Score: 5.159960813797904e-15    Content: foo, Metadata: {'page': 4}, Score: 5.159960813797904e-15Now we make the same query call but we filter for only page = 1 results_with_scores = db.similarity_search_with_score("foo", filter=dict(page=1))for doc, score in results_with_scores:    print(f"Content: {doc.page_content}, Metadata: {doc.metadata}, Score: {score}")    Content: foo, Metadata: {'page': 1}, Score: 5.159960813797904e-15    Content: bar, Metadata: {'page': 1}, Score: 0.3131446838378906Same thing can be done with the max_marginal_relevance_search as well.results = db.max_marginal_relevance_search("foo", filter=dict(page=1))for doc in results:    print(f"Content: {doc.page_content}, Metadata: {doc.metadata}")    Content: foo, Metadata: {'page': 1}    Content: bar, Metadata: {'page': 1}Here is an example of how to set fetch_k parameter when calling similarity_search. Usually you would want the fetch_k parameter >> k parameter. This is because the fetch_k parameter is the number of documents that will be fetched before filtering. If you set fetch_k to a low number, you might not get enough documents to filter from.results = db.similarity_search("foo", filter=dict(page=1), k=1, fetch_k=4)for doc in results:    print(f"Content: {doc.page_content}, Metadata: {doc.metadata}")    Content: foo, Metadata: {'page': 1}Deleteâ€‹You can also delete ids. Note that the ids to delete should be the ids in the docstore.db.delete([db.index_to_docstore_id[0]])    True# Is now missing0 in db.index_to_docstore_id    FalsePreviousEpsillaNextHologres On this page FaissFacebook AI Similarity Search (Faiss) is a library for efficient similarity search and clustering of dense vectors. It contains algorithms that search in sets of vectors of any size, up to ones that possibly do not fit in RAM. It also contains supporting code for evaluation and parameter tuning.Faiss documentation.This notebook shows how to use functionality related to the FAISS vector database.pip install faiss-gpu # For CUDA 7.5+ Supported GPU's.# ORpip install faiss-cpu # For CPU InstallationWe want to use OpenAIEmbeddings so we have to get the OpenAI API Key. import osimport getpassos.environ["OPENAI_API_KEY"] = getpass.getpass("OpenAI API Key:")# Uncomment the following line if you need to initialize FAISS with no AVX2 optimization# os.environ['FAISS_NO_AVX2'] = '1'from langchain.embeddings.openai import OpenAIEmbeddingsfrom langchain.text_splitter import CharacterTextSplitterfrom langchain.vectorstores import FAISSfrom langchain.document_loaders import TextLoaderAPI Reference:OpenAIEmbeddingsCharacterTextSplitterFAISSTextLoaderfrom langchain.document_loaders import TextLoaderloader = TextLoader("../../../extras/modules/state_of_the_union.txt")documents = loader.load()text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)docs = text_splitter.split_documents(documents)embeddings = OpenAIEmbeddings()API Reference:TextLoaderdb = FAISS.from_documents(docs, embeddings)query = "What did the president say about Ketanji Brown Jackson"docs = db.similarity_search(query)print(docs[0].page_content)    Tonight. I call on the Senate to: Pass the Freedom to Vote Act. Pass the John Lewis Voting Rights Act. And while youâ€™re at it, pass the Disclose Act so Americans can know who is funding our elections.         Tonight, Iâ€™d like to honor someone who has dedicated his life to serve this country: Justice Stephen Breyerâ€”an Army veteran, Constitutional scholar, and retiring Justice of the United States Supreme Court. Justice Breyer, thank you for your service.         One of the most serious constitutional responsibilities a President has is nominating someone to serve on the United States Supreme Court.         And I did that 4 days ago, when I nominated Circuit Court of Appeals Judge Ketanji Brown Jackson. One of our nationâ€™s top legal minds, who will continue Justice Breyerâ€™s legacy of excellence.Similarity Search with scoreâ€‹There are some FAISS specific methods. One of them is similarity_search_with_score, which allows you to return not only the documents but also the distance score of the query to them. The returned distance score is L2 distance. Therefore, a lower score is better.docs_and_scores = db.similarity_search_with_score(query)docs_and_scores[0]    (Document(page_content='Tonight. I call on the Senate to: Pass the Freedom to Vote Act. Pass the John Lewis Voting Rights Act. And while youâ€™re at it, pass the Disclose Act so Americans can know who is funding our elections. \n\nTonight, Iâ€™d like to honor someone who has dedicated his life to serve this country: Justice Stephen Breyerâ€”an Army veteran, Constitutional scholar, and retiring Justice of the United States Supreme Court. Justice Breyer, thank you for your service. \n\nOne of the most serious constitutional responsibilities a President has is nominating someone to serve on the United States Supreme Court. \n\nAnd I did that 4 days ago, when I nominated Circuit Court of Appeals Judge Ketanji Brown Jackson. One of our nationâ€™s top legal minds, who will continue Justice Breyerâ€™s legacy of excellence.', metadata={'source': '../../../state_of_the_union.txt'}),     0.36913747)It is also possible to do a search for documents similar to a given embedding vector using similarity_search_by_vector which accepts an embedding vector as a parameter instead of a string.embedding_vector = embeddings.embed_query(query)docs_and_scores = db.similarity_search_by_vector(embedding_vector)Saving and loadingâ€‹You can also save and load a FAISS index. This is useful so you don't have to recreate it everytime you use it.db.save_local("faiss_index")new_db = FAISS.load_local("faiss_index", embeddings)docs = new_db.similarity_search(query)docs[0]    Document(page_content='Tonight. I call on the Senate to: Pass the Freedom to Vote Act. Pass the John Lewis Voting Rights Act. And while youâ€™re at it, pass the Disclose Act so Americans can know who is funding our elections. \n\nTonight, Iâ€™d like to honor someone who has dedicated his life to serve this country: Justice Stephen Breyerâ€”an Army veteran, Constitutional scholar, and retiring Justice of the United States Supreme Court. Justice Breyer, thank you for your service. \n\nOne of the most serious constitutional responsibilities a President has is nominating someone to serve on the United States Supreme Court. \n\nAnd I did that 4 days ago, when I nominated Circuit Court of Appeals Judge Ketanji Brown Jackson. One of our nationâ€™s top legal minds, who will continue Justice Breyerâ€™s legacy of excellence.', metadata={'source': '../../../state_of_the_union.txt'})Serializing and De-Serializing to bytesyou can pickle the FAISS Index by these functions. If you use embeddings model which is of 90 mb (sentence-transformers/all-MiniLM-L6-v2 or any other model), the resultant pickle size would be more than 90 mb. the size of the model is also included in the overall size. To overcome this, use the below functions. These functions only serializes FAISS index and size would be much lesser. this can be helpful if you wish to store the index in database like sql.pkl = db.serialize_to_bytes() # serializes the faiss indexembeddings = HuggingFaceEmbeddings(model_name="all-MiniLM-L6-v2")db = FAISS.deserialize_from_bytes(embeddings = embeddings, serialized = pkl) # Load the indexMergingâ€‹You can also merge two FAISS vectorstoresdb1 = FAISS.from_texts(["foo"], embeddings)db2 = FAISS.from_texts(["bar"], embeddings)db1.docstore._dict    {'068c473b-d420-487a-806b-fb0ccea7f711': Document(page_content='foo', metadata={})}db2.docstore._dict    {'807e0c63-13f6-4070-9774-5c6f0fbb9866': Document(page_content='bar', metadata={})}db1.merge_from(db2)db1.docstore._dict    {'068c473b-d420-487a-806b-fb0ccea7f711': Document(page_content='foo', metadata={}),     '807e0c63-13f6-4070-9774-5c6f0fbb9866': Document(page_content='bar', metadata={})}Similarity Search with filteringâ€‹FAISS vectorstore can also support filtering, since the FAISS does not natively support filtering we have to do it manually. This is done by first fetching more results than k and then filtering them. You can filter the documents based on metadata. You can also set the fetch_k parameter when calling any search method to set how many documents you want to fetch before filtering. Here is a small example:from langchain.schema import Documentlist_of_documents = [    Document(page_content="foo", metadata=dict(page=1)),    Document(page_content="bar", metadata=dict(page=1)),    Document(page_content="foo", metadata=dict(page=2)),    Document(page_content="barbar", metadata=dict(page=2)),    Document(page_content="foo", metadata=dict(page=3)),    Document(page_content="bar burr", metadata=dict(page=3)),    Document(page_content="foo", metadata=dict(page=4)),    Document(page_content="bar bruh", metadata=dict(page=4)),]db = FAISS.from_documents(list_of_documents, embeddings)results_with_scores = db.similarity_search_with_score("foo")for doc, score in results_with_scores:    print(f"Content: {doc.page_content}, Metadata: {doc.metadata}, Score: {score}")API Reference:Document    Content: foo, Metadata: {'page': 1}, Score: 5.159960813797904e-15    Content: foo, Metadata: {'page': 2}, Score: 5.159960813797904e-15    Content: foo, Metadata: {'page': 3}, Score: 5.159960813797904e-15    Content: foo, Metadata: {'page': 4}, Score: 5.159960813797904e-15Now we make the same query call but we filter for only page = 1 results_with_scores = db.similarity_search_with_score("foo", filter=dict(page=1))for doc, score in results_with_scores:    print(f"Content: {doc.page_content}, Metadata: {doc.metadata}, Score: {score}")    Content: foo, Metadata: {'page': 1}, Score: 5.159960813797904e-15    Content: bar, Metadata: {'page': 1}, Score: 0.3131446838378906Same thing can be done with the max_marginal_relevance_search as well.results = db.max_marginal_relevance_search("foo", filter=dict(page=1))for doc in results:    print(f"Content: {doc.page_content}, Metadata: {doc.metadata}")    Content: foo, Metadata: {'page': 1}    Content: bar, Metadata: {'page': 1}Here is an example of how to set fetch_k parameter when calling similarity_search. Usually you would want the fetch_k parameter >> k parameter. This is because the fetch_k parameter is the number of documents that will be fetched before filtering. If you set fetch_k to a low number, you might not get enough documents to filter from.results = db.similarity_search("foo", filter=dict(page=1), k=1, fetch_k=4)for doc in results:    print(f"Content: {doc.page_content}, Metadata: {doc.metadata}")    Content: foo, Metadata: {'page': 1}Deleteâ€‹You can also delete ids. Note that the ids to delete should be the ids in the docstore.db.delete([db.index_to_docstore_id[0]])    True# Is now missing0 in db.index_to_docstore_id    False pip install faiss-gpu # For CUDA 7.5+ Supported GPU's.# ORpip install faiss-cpu # For CPU Installation pip install faiss-gpu # For CUDA 7.5+ Supported GPU's.# ORpip install faiss-cpu # For CPU Installation  import osimport getpassos.environ["OPENAI_API_KEY"] = getpass.getpass("OpenAI API Key:")# Uncomment the following line if you need to initialize FAISS with no AVX2 optimization# os.environ['FAISS_NO_AVX2'] = '1' import osimport getpassos.environ["OPENAI_API_KEY"] = getpass.getpass("OpenAI API Key:")# Uncomment the following line if you need to initialize FAISS with no AVX2 optimization# os.environ['FAISS_NO_AVX2'] = '1'  from langchain.embeddings.openai import OpenAIEmbeddingsfrom langchain.text_splitter import CharacterTextSplitterfrom langchain.vectorstores import FAISSfrom langchain.document_loaders import TextLoader from langchain.embeddings.openai import OpenAIEmbeddingsfrom langchain.text_splitter import CharacterTextSplitterfrom langchain.vectorstores import FAISSfrom langchain.document_loaders import TextLoader  API Reference:OpenAIEmbeddingsCharacterTextSplitterFAISSTextLoader from langchain.document_loaders import TextLoaderloader = TextLoader("../../../extras/modules/state_of_the_union.txt")documents = loader.load()text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)docs = text_splitter.split_documents(documents)embeddings = OpenAIEmbeddings() from langchain.document_loaders import TextLoaderloader = TextLoader("../../../extras/modules/state_of_the_union.txt")documents = loader.load()text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)docs = text_splitter.split_documents(documents)embeddings = OpenAIEmbeddings()  API Reference:TextLoader db = FAISS.from_documents(docs, embeddings)query = "What did the president say about Ketanji Brown Jackson"docs = db.similarity_search(query) db = FAISS.from_documents(docs, embeddings)query = "What did the president say about Ketanji Brown Jackson"docs = db.similarity_search(query)  print(docs[0].page_content) print(docs[0].page_content)      Tonight. I call on the Senate to: Pass the Freedom to Vote Act. Pass the John Lewis Voting Rights Act. And while youâ€™re at it, pass the Disclose Act so Americans can know who is funding our elections.         Tonight, Iâ€™d like to honor someone who has dedicated his life to serve this country: Justice Stephen Breyerâ€”an Army veteran, Constitutional scholar, and retiring Justice of the United States Supreme Court. Justice Breyer, thank you for your service.         One of the most serious constitutional responsibilities a President has is nominating someone to serve on the United States Supreme Court.         And I did that 4 days ago, when I nominated Circuit Court of Appeals Judge Ketanji Brown Jackson. One of our nationâ€™s top legal minds, who will continue Justice Breyerâ€™s legacy of excellence.     Tonight. I call on the Senate to: Pass the Freedom to Vote Act. Pass the John Lewis Voting Rights Act. And while youâ€™re at it, pass the Disclose Act so Americans can know who is funding our elections.         Tonight, Iâ€™d like to honor someone who has dedicated his life to serve this country: Justice Stephen Breyerâ€”an Army veteran, Constitutional scholar, and retiring Justice of the United States Supreme Court. Justice Breyer, thank you for your service.         One of the most serious constitutional responsibilities a President has is nominating someone to serve on the United States Supreme Court.         And I did that 4 days ago, when I nominated Circuit Court of Appeals Judge Ketanji Brown Jackson. One of our nationâ€™s top legal minds, who will continue Justice Breyerâ€™s legacy of excellence.     Tonight. I call on the Senate to: Pass the Freedom to Vote Act. Pass the John Lewis Voting Rights Act. And while youâ€™re at it, pass the Disclose Act so Americans can know who is funding our elections.         Tonight, Iâ€™d like to honor someone who has dedicated his life to serve this country: Justice Stephen Breyerâ€”an Army veteran, Constitutional scholar, and retiring Justice of the United States Supreme Court. Justice Breyer, thank you for your service.         One of the most serious constitutional responsibilities a President has is nominating someone to serve on the United States Supreme Court.         And I did that 4 days ago, when I nominated Circuit Court of Appeals Judge Ketanji Brown Jackson. One of our nationâ€™s top legal minds, who will continue Justice Breyerâ€™s legacy of excellence.  docs_and_scores = db.similarity_search_with_score(query) docs_and_scores = db.similarity_search_with_score(query)  docs_and_scores[0] docs_and_scores[0]      (Document(page_content='Tonight. I call on the Senate to: Pass the Freedom to Vote Act. Pass the John Lewis Voting Rights Act. And while youâ€™re at it, pass the Disclose Act so Americans can know who is funding our elections. \n\nTonight, Iâ€™d like to honor someone who has dedicated his life to serve this country: Justice Stephen Breyerâ€”an Army veteran, Constitutional scholar, and retiring Justice of the United States Supreme Court. Justice Breyer, thank you for your service. \n\nOne of the most serious constitutional responsibilities a President has is nominating someone to serve on the United States Supreme Court. \n\nAnd I did that 4 days ago, when I nominated Circuit Court of Appeals Judge Ketanji Brown Jackson. One of our nationâ€™s top legal minds, who will continue Justice Breyerâ€™s legacy of excellence.', metadata={'source': '../../../state_of_the_union.txt'}),     0.36913747)     (Document(page_content='Tonight. I call on the Senate to: Pass the Freedom to Vote Act. Pass the John Lewis Voting Rights Act. And while youâ€™re at it, pass the Disclose Act so Americans can know who is funding our elections. \n\nTonight, Iâ€™d like to honor someone who has dedicated his life to serve this country: Justice Stephen Breyerâ€”an Army veteran, Constitutional scholar, and retiring Justice of the United States Supreme Court. Justice Breyer, thank you for your service. \n\nOne of the most serious constitutional responsibilities a President has is nominating someone to serve on the United States Supreme Court. \n\nAnd I did that 4 days ago, when I nominated Circuit Court of Appeals Judge Ketanji Brown Jackson. One of our nationâ€™s top legal minds, who will continue Justice Breyerâ€™s legacy of excellence.', metadata={'source': '../../../state_of_the_union.txt'}),     0.36913747)     (Document(page_content='Tonight. I call on the Senate to: Pass the Freedom to Vote Act. Pass the John Lewis Voting Rights Act. And while youâ€™re at it, pass the Disclose Act so Americans can know who is funding our elections. \n\nTonight, Iâ€™d like to honor someone who has dedicated his life to serve this country: Justice Stephen Breyerâ€”an Army veteran, Constitutional scholar, and retiring Justice of the United States Supreme Court. Justice Breyer, thank you for your service. \n\nOne of the most serious constitutional responsibilities a President has is nominating someone to serve on the United States Supreme Court. \n\nAnd I did that 4 days ago, when I nominated Circuit Court of Appeals Judge Ketanji Brown Jackson. One of our nationâ€™s top legal minds, who will continue Justice Breyerâ€™s legacy of excellence.', metadata={'source': '../../../state_of_the_union.txt'}),     0.36913747)  embedding_vector = embeddings.embed_query(query)docs_and_scores = db.similarity_search_by_vector(embedding_vector) embedding_vector = embeddings.embed_query(query)docs_and_scores = db.similarity_search_by_vector(embedding_vector)  db.save_local("faiss_index") db.save_local("faiss_index")  new_db = FAISS.load_local("faiss_index", embeddings) new_db = FAISS.load_local("faiss_index", embeddings)  docs = new_db.similarity_search(query) docs = new_db.similarity_search(query)  docs[0] docs[0]      Document(page_content='Tonight. I call on the Senate to: Pass the Freedom to Vote Act. Pass the John Lewis Voting Rights Act. And while youâ€™re at it, pass the Disclose Act so Americans can know who is funding our elections. \n\nTonight, Iâ€™d like to honor someone who has dedicated his life to serve this country: Justice Stephen Breyerâ€”an Army veteran, Constitutional scholar, and retiring Justice of the United States Supreme Court. Justice Breyer, thank you for your service. \n\nOne of the most serious constitutional responsibilities a President has is nominating someone to serve on the United States Supreme Court. \n\nAnd I did that 4 days ago, when I nominated Circuit Court of Appeals Judge Ketanji Brown Jackson. One of our nationâ€™s top legal minds, who will continue Justice Breyerâ€™s legacy of excellence.', metadata={'source': '../../../state_of_the_union.txt'})     Document(page_content='Tonight. I call on the Senate to: Pass the Freedom to Vote Act. Pass the John Lewis Voting Rights Act. And while youâ€™re at it, pass the Disclose Act so Americans can know who is funding our elections. \n\nTonight, Iâ€™d like to honor someone who has dedicated his life to serve this country: Justice Stephen Breyerâ€”an Army veteran, Constitutional scholar, and retiring Justice of the United States Supreme Court. Justice Breyer, thank you for your service. \n\nOne of the most serious constitutional responsibilities a President has is nominating someone to serve on the United States Supreme Court. \n\nAnd I did that 4 days ago, when I nominated Circuit Court of Appeals Judge Ketanji Brown Jackson. One of our nationâ€™s top legal minds, who will continue Justice Breyerâ€™s legacy of excellence.', metadata={'source': '../../../state_of_the_union.txt'})     Document(page_content='Tonight. I call on the Senate to: Pass the Freedom to Vote Act. Pass the John Lewis Voting Rights Act. And while youâ€™re at it, pass the Disclose Act so Americans can know who is funding our elections. \n\nTonight, Iâ€™d like to honor someone who has dedicated his life to serve this country: Justice Stephen Breyerâ€”an Army veteran, Constitutional scholar, and retiring Justice of the United States Supreme Court. Justice Breyer, thank you for your service. \n\nOne of the most serious constitutional responsibilities a President has is nominating someone to serve on the United States Supreme Court. \n\nAnd I did that 4 days ago, when I nominated Circuit Court of Appeals Judge Ketanji Brown Jackson. One of our nationâ€™s top legal minds, who will continue Justice Breyerâ€™s legacy of excellence.', metadata={'source': '../../../state_of_the_union.txt'})  pkl = db.serialize_to_bytes() # serializes the faiss index pkl = db.serialize_to_bytes() # serializes the faiss index  embeddings = HuggingFaceEmbeddings(model_name="all-MiniLM-L6-v2") embeddings = HuggingFaceEmbeddings(model_name="all-MiniLM-L6-v2")  db = FAISS.deserialize_from_bytes(embeddings = embeddings, serialized = pkl) # Load the index db = FAISS.deserialize_from_bytes(embeddings = embeddings, serialized = pkl) # Load the index  db1 = FAISS.from_texts(["foo"], embeddings)db2 = FAISS.from_texts(["bar"], embeddings) db1 = FAISS.from_texts(["foo"], embeddings)db2 = FAISS.from_texts(["bar"], embeddings)  db1.docstore._dict db1.docstore._dict      {'068c473b-d420-487a-806b-fb0ccea7f711': Document(page_content='foo', metadata={})}     {'068c473b-d420-487a-806b-fb0ccea7f711': Document(page_content='foo', metadata={})}     {'068c473b-d420-487a-806b-fb0ccea7f711': Document(page_content='foo', metadata={})}  db2.docstore._dict db2.docstore._dict      {'807e0c63-13f6-4070-9774-5c6f0fbb9866': Document(page_content='bar', metadata={})}     {'807e0c63-13f6-4070-9774-5c6f0fbb9866': Document(page_content='bar', metadata={})}     {'807e0c63-13f6-4070-9774-5c6f0fbb9866': Document(page_content='bar', metadata={})}  db1.merge_from(db2) db1.merge_from(db2)  db1.docstore._dict db1.docstore._dict      {'068c473b-d420-487a-806b-fb0ccea7f711': Document(page_content='foo', metadata={}),     '807e0c63-13f6-4070-9774-5c6f0fbb9866': Document(page_content='bar', metadata={})}     {'068c473b-d420-487a-806b-fb0ccea7f711': Document(page_content='foo', metadata={}),     '807e0c63-13f6-4070-9774-5c6f0fbb9866': Document(page_content='bar', metadata={})}     {'068c473b-d420-487a-806b-fb0ccea7f711': Document(page_content='foo', metadata={}),     '807e0c63-13f6-4070-9774-5c6f0fbb9866': Document(page_content='bar', metadata={})}  from langchain.schema import Documentlist_of_documents = [    Document(page_content="foo", metadata=dict(page=1)),    Document(page_content="bar", metadata=dict(page=1)),    Document(page_content="foo", metadata=dict(page=2)),    Document(page_content="barbar", metadata=dict(page=2)),    Document(page_content="foo", metadata=dict(page=3)),    Document(page_content="bar burr", metadata=dict(page=3)),    Document(page_content="foo", metadata=dict(page=4)),    Document(page_content="bar bruh", metadata=dict(page=4)),]db = FAISS.from_documents(list_of_documents, embeddings)results_with_scores = db.similarity_search_with_score("foo")for doc, score in results_with_scores:    print(f"Content: {doc.page_content}, Metadata: {doc.metadata}, Score: {score}") from langchain.schema import Documentlist_of_documents = [    Document(page_content="foo", metadata=dict(page=1)),    Document(page_content="bar", metadata=dict(page=1)),    Document(page_content="foo", metadata=dict(page=2)),    Document(page_content="barbar", metadata=dict(page=2)),    Document(page_content="foo", metadata=dict(page=3)),    Document(page_content="bar burr", metadata=dict(page=3)),    Document(page_content="foo", metadata=dict(page=4)),    Document(page_content="bar bruh", metadata=dict(page=4)),]db = FAISS.from_documents(list_of_documents, embeddings)results_with_scores = db.similarity_search_with_score("foo")for doc, score in results_with_scores:    print(f"Content: {doc.page_content}, Metadata: {doc.metadata}, Score: {score}")  API Reference:Document     Content: foo, Metadata: {'page': 1}, Score: 5.159960813797904e-15    Content: foo, Metadata: {'page': 2}, Score: 5.159960813797904e-15    Content: foo, Metadata: {'page': 3}, Score: 5.159960813797904e-15    Content: foo, Metadata: {'page': 4}, Score: 5.159960813797904e-15     Content: foo, Metadata: {'page': 1}, Score: 5.159960813797904e-15    Content: foo, Metadata: {'page': 2}, Score: 5.159960813797904e-15    Content: foo, Metadata: {'page': 3}, Score: 5.159960813797904e-15    Content: foo, Metadata: {'page': 4}, Score: 5.159960813797904e-15     Content: foo, Metadata: {'page': 1}, Score: 5.159960813797904e-15    Content: foo, Metadata: {'page': 2}, Score: 5.159960813797904e-15    Content: foo, Metadata: {'page': 3}, Score: 5.159960813797904e-15    Content: foo, Metadata: {'page': 4}, Score: 5.159960813797904e-15  results_with_scores = db.similarity_search_with_score("foo", filter=dict(page=1))for doc, score in results_with_scores:    print(f"Content: {doc.page_content}, Metadata: {doc.metadata}, Score: {score}") results_with_scores = db.similarity_search_with_score("foo", filter=dict(page=1))for doc, score in results_with_scores:    print(f"Content: {doc.page_content}, Metadata: {doc.metadata}, Score: {score}")      Content: foo, Metadata: {'page': 1}, Score: 5.159960813797904e-15    Content: bar, Metadata: {'page': 1}, Score: 0.3131446838378906     Content: foo, Metadata: {'page': 1}, Score: 5.159960813797904e-15    Content: bar, Metadata: {'page': 1}, Score: 0.3131446838378906     Content: foo, Metadata: {'page': 1}, Score: 5.159960813797904e-15    Content: bar, Metadata: {'page': 1}, Score: 0.3131446838378906  results = db.max_marginal_relevance_search("foo", filter=dict(page=1))for doc in results:    print(f"Content: {doc.page_content}, Metadata: {doc.metadata}") results = db.max_marginal_relevance_search("foo", filter=dict(page=1))for doc in results:    print(f"Content: {doc.page_content}, Metadata: {doc.metadata}")      Content: foo, Metadata: {'page': 1}    Content: bar, Metadata: {'page': 1}     Content: foo, Metadata: {'page': 1}    Content: bar, Metadata: {'page': 1}     Content: foo, Metadata: {'page': 1}    Content: bar, Metadata: {'page': 1}  results = db.similarity_search("foo", filter=dict(page=1), k=1, fetch_k=4)for doc in results:    print(f"Content: {doc.page_content}, Metadata: {doc.metadata}") results = db.similarity_search("foo", filter=dict(page=1), k=1, fetch_k=4)for doc in results:    print(f"Content: {doc.page_content}, Metadata: {doc.metadata}")      Content: foo, Metadata: {'page': 1}     Content: foo, Metadata: {'page': 1}     Content: foo, Metadata: {'page': 1}  db.delete([db.index_to_docstore_id[0]]) db.delete([db.index_to_docstore_id[0]])      True     True     True  # Is now missing0 in db.index_to_docstore_id # Is now missing0 in db.index_to_docstore_id      False     False     False  Previous Epsilla Next Hologres Similarity Search with scoreSaving and loadingMergingSimilarity Search with filteringDelete Similarity Search with scoreSaving and loadingMergingSimilarity Search with filteringDelete CommunityDiscordTwitterGitHubPythonJS/TSMoreHomepageBlogCopyright Â© 2023 LangChain, Inc. CommunityDiscordTwitterGitHubPythonJS/TSMoreHomepageBlog CommunityDiscordTwitter Community GitHubPythonJS/TS GitHub MoreHomepageBlog More Copyright Â© 2023 LangChain, Inc. Copyright Â© 2023 LangChain, Inc. Skip to main content (#docusaurus_skipToContent_fallback) ðŸ¦œï¸ðŸ”— LangChain (/) Docs (/docs/get_started/introduction) Use cases (/docs/use_cases) Integrations (/docs/integrations) API (https://api.python.langchain.com) LangSmith (https://smith.langchain.com) JS/TS Docs (https://js.langchain.com/docs)  (https://github.com/hwchase17/langchain) Integrations (/docs/integrations) Callbacks (/docs/integrations/callbacks/) Chat models (/docs/integrations/chat/) Chat loaders (/docs/integrations/chat_loaders/) Document loaders (/docs/integrations/document_loaders/) Document transformers (/docs/integrations/document_transformers/) LLMs (/docs/integrations/llms/) Memory (/docs/integrations/memory/) Retrievers (/docs/integrations/retrievers/) Text embedding models (/docs/integrations/text_embedding/) Agents & Toolkits (/docs/integrations/toolkits/) Tools (/docs/integrations/tools/) Vector stores (/docs/integrations/vectorstores/) Activeloop Deep Lake (/docs/integrations/vectorstores/activeloop_deeplake) Alibaba Cloud OpenSearch (/docs/integrations/vectorstores/alibabacloud_opensearch) AnalyticDB (/docs/integrations/vectorstores/analyticdb) Annoy (/docs/integrations/vectorstores/annoy) Atlas (/docs/integrations/vectorstores/atlas) AwaDB (/docs/integrations/vectorstores/awadb) Azure Cognitive Search (/docs/integrations/vectorstores/azuresearch) BagelDB (/docs/integrations/vectorstores/bageldb) Cassandra (/docs/integrations/vectorstores/cassandra) Chroma (/docs/integrations/vectorstores/chroma) ClickHouse (/docs/integrations/vectorstores/clickhouse) DashVector (/docs/integrations/vectorstores/dashvector) Dingo (/docs/integrations/vectorstores/dingo) DocArray HnswSearch (/docs/integrations/vectorstores/docarray_hnsw) DocArray InMemorySearch (/docs/integrations/vectorstores/docarray_in_memory) Elasticsearch (/docs/integrations/vectorstores/elasticsearch) Epsilla (/docs/integrations/vectorstores/epsilla) Faiss (/docs/integrations/vectorstores/faiss) Hologres (/docs/integrations/vectorstores/hologres) LanceDB (/docs/integrations/vectorstores/lancedb) Marqo (/docs/integrations/vectorstores/marqo) Google Vertex AI MatchingEngine (/docs/integrations/vectorstores/matchingengine) Meilisearch (/docs/integrations/vectorstores/meilisearch) Milvus (/docs/integrations/vectorstores/milvus) MongoDB Atlas (/docs/integrations/vectorstores/mongodb_atlas) MyScale (/docs/integrations/vectorstores/myscale) Neo4j Vector Index (/docs/integrations/vectorstores/neo4jvector) NucliaDB (/docs/integrations/vectorstores/nucliadb) OpenSearch (/docs/integrations/vectorstores/opensearch) Postgres Embedding (/docs/integrations/vectorstores/pgembedding) PGVector (/docs/integrations/vectorstores/pgvector) Pinecone (/docs/integrations/vectorstores/pinecone) Qdrant (/docs/integrations/vectorstores/qdrant) Redis (/docs/integrations/vectorstores/redis) Rockset (/docs/integrations/vectorstores/rockset) ScaNN (/docs/integrations/vectorstores/scann) SingleStoreDB (/docs/integrations/vectorstores/singlestoredb) scikit-learn (/docs/integrations/vectorstores/sklearn) sqlite-vss (/docs/integrations/vectorstores/sqlitevss) StarRocks (/docs/integrations/vectorstores/starrocks) Supabase (Postgres) (/docs/integrations/vectorstores/supabase) Tair (/docs/integrations/vectorstores/tair) Tencent Cloud VectorDB (/docs/integrations/vectorstores/tencentvectordb) Tigris (/docs/integrations/vectorstores/tigris) Typesense (/docs/integrations/vectorstores/typesense) USearch (/docs/integrations/vectorstores/usearch) vearch (/docs/integrations/vectorstores/vearch) Vectara (/docs/integrations/vectorstores/vectara) Weaviate (/docs/integrations/vectorstores/weaviate) Xata (/docs/integrations/vectorstores/xata) Zep (/docs/integrations/vectorstores/zep) Zilliz (/docs/integrations/vectorstores/zilliz) Grouped by provider (/docs/integrations/providers/)  (/) Integrations (/docs/integrations) Vector stores (/docs/integrations/vectorstores/) Facebook AI Similarity Search (Faiss) (https://engineering.fb.com/2017/03/29/data-infrastructure/faiss-a-library-for-efficient-similarity-search/) Faiss documentation (https://faiss.ai/) OpenAIEmbeddings (https://api.python.langchain.com/en/latest/embeddings/langchain.embeddings.openai.OpenAIEmbeddings.html) CharacterTextSplitter (https://api.python.langchain.com/en/latest/text_splitter/langchain.text_splitter.CharacterTextSplitter.html) FAISS (https://api.python.langchain.com/en/latest/vectorstores/langchain.vectorstores.faiss.FAISS.html) TextLoader (https://api.python.langchain.com/en/latest/document_loaders/langchain.document_loaders.text.TextLoader.html) TextLoader (https://api.python.langchain.com/en/latest/document_loaders/langchain.document_loaders.text.TextLoader.html) â€‹ (#similarity-search-with-score) â€‹ (#saving-and-loading) â€‹ (#merging) â€‹ (#similarity-search-with-filtering) Document (https://api.python.langchain.com/en/latest/schema/langchain.schema.document.Document.html) â€‹ (#delete) PreviousEpsilla (/docs/integrations/vectorstores/epsilla) NextHologres (/docs/integrations/vectorstores/hologres) Similarity Search with score (#similarity-search-with-score) Saving and loading (#saving-and-loading) Merging (#merging) Similarity Search with filtering (#similarity-search-with-filtering) Delete (#delete) Discord (https://discord.gg/cU2adEyC7w) Twitter (https://twitter.com/LangChainAI) Python (https://github.com/hwchase17/langchain) JS/TS (https://github.com/hwchase17/langchainjs) Homepage (https://langchain.com) Blog (https://blog.langchain.dev)