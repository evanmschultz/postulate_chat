Note: This is seperate from the Google PaLM integration, it exposes Vertex AI PaLM API on Google Cloud.  By default, Google Cloud does not use customer data to train its foundation models as part of Google Cloud's AI/ML Privacy Commitment. More details about how Google processes data can also be found in Google's Customer Data Processing Addendum (CDPA). To use Vertex AI PaLM you must have the google-cloud-aiplatform Python package installed and either: This codebase uses the google.auth library which first looks for the application credentials variable mentioned above, and then looks for system-level auth. For more information, see:  You can now leverage the Codey API for code generation within Vertex AI.  The model names are: Vertex Model Garden exposes open-sourced models that can be deployed and served on Vertex AI. If you have successfully deployed a model from Vertex Model Garden, you can find a corresponding Vertex AI endpoint in the console or via API. Like all LLMs, we can then compose it with other components: IntegrationsCallbacksChat modelsChat loadersDocument loadersDocument transformersLLMsAI21Aleph AlphaAmazon API GatewayAnyscaleAzure MLAzure OpenAIBananaBasetenBeamBedrockBittensorCerebriumAIChatGLMClarifaiCohereC TransformersCTranslate2DatabricksDeepInfraDeepSparseEden AIFireworksForefrontAIGoogle Vertex AI PaLMGooseAIGPT4AllHugging Face HubHugging Face Local PipelinesHuggingface TextGen InferenceJSONFormerKoboldAI APILlama.cppLLM Caching integrationsManifestMinimaxModalMosaicMLNLP CloudOctoAIOllamaOpaquePromptsOpenAIOpenLLMOpenLMPetalsPipelineAIPredibasePrediction GuardPromptLayer OpenAIRELLMReplicateRunhouseSageMakerEndpointStochasticAINebula (Symbl.ai)TextGenTitan TakeoffTongyi QwenvLLMWriterXorbits Inference (Xinference)MemoryRetrieversText embedding modelsAgents & ToolkitsToolsVector storesGrouped by provider Callbacks Chat models Chat loaders Document loaders Document transformers LLMsAI21Aleph AlphaAmazon API GatewayAnyscaleAzure MLAzure OpenAIBananaBasetenBeamBedrockBittensorCerebriumAIChatGLMClarifaiCohereC TransformersCTranslate2DatabricksDeepInfraDeepSparseEden AIFireworksForefrontAIGoogle Vertex AI PaLMGooseAIGPT4AllHugging Face HubHugging Face Local PipelinesHuggingface TextGen InferenceJSONFormerKoboldAI APILlama.cppLLM Caching integrationsManifestMinimaxModalMosaicMLNLP CloudOctoAIOllamaOpaquePromptsOpenAIOpenLLMOpenLMPetalsPipelineAIPredibasePrediction GuardPromptLayer OpenAIRELLMReplicateRunhouseSageMakerEndpointStochasticAINebula (Symbl.ai)TextGenTitan TakeoffTongyi QwenvLLMWriterXorbits Inference (Xinference) AI21 Aleph Alpha Amazon API Gateway Anyscale Azure ML Azure OpenAI Banana Baseten Beam Bedrock Bittensor CerebriumAI ChatGLM Clarifai Cohere C Transformers CTranslate2 Databricks DeepInfra DeepSparse Eden AI Fireworks ForefrontAI Google Vertex AI PaLM GooseAI GPT4All Hugging Face Hub Hugging Face Local Pipelines Huggingface TextGen Inference JSONFormer KoboldAI API Llama.cpp LLM Caching integrations Manifest Minimax Modal MosaicML NLP Cloud OctoAI Ollama OpaquePrompts OpenAI OpenLLM OpenLM Petals PipelineAI Predibase Prediction Guard PromptLayer OpenAI RELLM Replicate Runhouse SageMakerEndpoint StochasticAI Nebula (Symbl.ai) TextGen Titan Takeoff Tongyi Qwen vLLM Writer Xorbits Inference (Xinference) Memory Retrievers Text embedding models Agents & Toolkits Tools Vector stores Grouped by provider  Integrations LLMs Google Vertex AI PaLM Have credentials configured for your environment (gcloud, workload identity, etc...) Store the path to a service account JSON file as the GOOGLE_APPLICATION_CREDENTIALS environment variable https://cloud.google.com/docs/authentication/application-default-credentials#GAC https://googleapis.dev/python/google-auth/latest/reference/google.auth.html#module-google.auth VertexAI code-bison: for code suggestion code-gecko: for code completion VertexAIModelGarden PromptTemplate Setting up Question-answering example Code generation example Using models deployed on Vertex Model Garden Discord Twitter Python JS/TS Homepage Blog Skip to main contentü¶úÔ∏èüîó LangChainDocsUse casesIntegrationsAPILangSmithJS/TS DocsCTRLKIntegrationsCallbacksChat modelsChat loadersDocument loadersDocument transformersLLMsAI21Aleph AlphaAmazon API GatewayAnyscaleAzure MLAzure OpenAIBananaBasetenBeamBedrockBittensorCerebriumAIChatGLMClarifaiCohereC TransformersCTranslate2DatabricksDeepInfraDeepSparseEden AIFireworksForefrontAIGoogle Vertex AI PaLMGooseAIGPT4AllHugging Face HubHugging Face Local PipelinesHuggingface TextGen InferenceJSONFormerKoboldAI APILlama.cppLLM Caching integrationsManifestMinimaxModalMosaicMLNLP CloudOctoAIOllamaOpaquePromptsOpenAIOpenLLMOpenLMPetalsPipelineAIPredibasePrediction GuardPromptLayer OpenAIRELLMReplicateRunhouseSageMakerEndpointStochasticAINebula (Symbl.ai)TextGenTitan TakeoffTongyi QwenvLLMWriterXorbits Inference (Xinference)MemoryRetrieversText embedding modelsAgents & ToolkitsToolsVector storesGrouped by providerIntegrationsLLMsGoogle Vertex AI PaLMOn this pageGoogle Vertex AI PaLMNote: This is seperate from the Google PaLM integration, it exposes Vertex AI PaLM API on Google Cloud. Setting up‚ÄãBy default, Google Cloud does not use customer data to train its foundation models as part of Google Cloud's AI/ML Privacy Commitment. More details about how Google processes data can also be found in Google's Customer Data Processing Addendum (CDPA).To use Vertex AI PaLM you must have the google-cloud-aiplatform Python package installed and either:Have credentials configured for your environment (gcloud, workload identity, etc...)Store the path to a service account JSON file as the GOOGLE_APPLICATION_CREDENTIALS environment variableThis codebase uses the google.auth library which first looks for the application credentials variable mentioned above, and then looks for system-level auth.For more information, see: https://cloud.google.com/docs/authentication/application-default-credentials#GAChttps://googleapis.dev/python/google-auth/latest/reference/google.auth.html#module-google.auth#!pip install google-cloud-aiplatformfrom langchain.llms import VertexAIAPI Reference:VertexAIQuestion-answering example‚Äãfrom langchain import PromptTemplate, LLMChaintemplate = """Question: {question}Answer: Let's think step by step."""prompt = PromptTemplate(template=template, input_variables=["question"])llm = VertexAI()llm_chain = LLMChain(prompt=prompt, llm=llm)question = "What NFL team won the Super Bowl in the year Justin Beiber was born?"llm_chain.run(question)    'Justin Bieber was born on March 1, 1994. The Super Bowl in 1994 was won by the San Francisco 49ers.\nThe final answer: San Francisco 49ers.'Code generation example‚ÄãYou can now leverage the Codey API for code generation within Vertex AI. The model names are:code-bison: for code suggestioncode-gecko: for code completionllm = VertexAI(model_name="code-bison")llm_chain = LLMChain(prompt=prompt, llm=llm)question = "Write a python function that identifies if the number is a prime number?"llm_chain.run(question)    '```python\ndef is_prime(n):\n  """\n  Determines if a number is prime.\n\n  Args:\n    n: The number to be tested.\n\n  Returns:\n    True if the number is prime, False otherwise.\n  """\n\n  # Check if the number is 1.\n  if n == 1:\n    return False\n\n  # Check if the number is 2.\n  if n == 2:\n    return True\n\n'Using models deployed on Vertex Model Garden‚ÄãVertex Model Garden exposes open-sourced models that can be deployed and served on Vertex AI. If you have successfully deployed a model from Vertex Model Garden, you can find a corresponding Vertex AI endpoint in the console or via API.from langchain.llms import VertexAIModelGardenAPI Reference:VertexAIModelGardenllm = VertexAIModelGarden(    project="YOUR PROJECT",    endpoint_id="YOUR ENDPOINT_ID")llm("What is the meaning of life?")Like all LLMs, we can then compose it with other components:from langchain.prompts import PromptTemplateprompt = PromptTemplate.from_template("What is the meaning of {thing}?")API Reference:PromptTemplatellm_oss_chain = prompt | llmllm_oss_chain.invoke({"thing": "life"})PreviousForefrontAINextGooseAISetting upQuestion-answering exampleCode generation exampleUsing models deployed on Vertex Model GardenCommunityDiscordTwitterGitHubPythonJS/TSMoreHomepageBlogCopyright ¬© 2023 LangChain, Inc. Skip to main content ü¶úÔ∏èüîó LangChainDocsUse casesIntegrationsAPILangSmithJS/TS DocsCTRLK ü¶úÔ∏èüîó LangChainDocsUse casesIntegrationsAPI LangSmithJS/TS DocsCTRLK  CTRLK CTRLK  CTRLK   IntegrationsCallbacksChat modelsChat loadersDocument loadersDocument transformersLLMsAI21Aleph AlphaAmazon API GatewayAnyscaleAzure MLAzure OpenAIBananaBasetenBeamBedrockBittensorCerebriumAIChatGLMClarifaiCohereC TransformersCTranslate2DatabricksDeepInfraDeepSparseEden AIFireworksForefrontAIGoogle Vertex AI PaLMGooseAIGPT4AllHugging Face HubHugging Face Local PipelinesHuggingface TextGen InferenceJSONFormerKoboldAI APILlama.cppLLM Caching integrationsManifestMinimaxModalMosaicMLNLP CloudOctoAIOllamaOpaquePromptsOpenAIOpenLLMOpenLMPetalsPipelineAIPredibasePrediction GuardPromptLayer OpenAIRELLMReplicateRunhouseSageMakerEndpointStochasticAINebula (Symbl.ai)TextGenTitan TakeoffTongyi QwenvLLMWriterXorbits Inference (Xinference)MemoryRetrieversText embedding modelsAgents & ToolkitsToolsVector storesGrouped by providerIntegrationsLLMsGoogle Vertex AI PaLMOn this pageGoogle Vertex AI PaLMNote: This is seperate from the Google PaLM integration, it exposes Vertex AI PaLM API on Google Cloud. Setting up‚ÄãBy default, Google Cloud does not use customer data to train its foundation models as part of Google Cloud's AI/ML Privacy Commitment. More details about how Google processes data can also be found in Google's Customer Data Processing Addendum (CDPA).To use Vertex AI PaLM you must have the google-cloud-aiplatform Python package installed and either:Have credentials configured for your environment (gcloud, workload identity, etc...)Store the path to a service account JSON file as the GOOGLE_APPLICATION_CREDENTIALS environment variableThis codebase uses the google.auth library which first looks for the application credentials variable mentioned above, and then looks for system-level auth.For more information, see: https://cloud.google.com/docs/authentication/application-default-credentials#GAChttps://googleapis.dev/python/google-auth/latest/reference/google.auth.html#module-google.auth#!pip install google-cloud-aiplatformfrom langchain.llms import VertexAIAPI Reference:VertexAIQuestion-answering example‚Äãfrom langchain import PromptTemplate, LLMChaintemplate = """Question: {question}Answer: Let's think step by step."""prompt = PromptTemplate(template=template, input_variables=["question"])llm = VertexAI()llm_chain = LLMChain(prompt=prompt, llm=llm)question = "What NFL team won the Super Bowl in the year Justin Beiber was born?"llm_chain.run(question)    'Justin Bieber was born on March 1, 1994. The Super Bowl in 1994 was won by the San Francisco 49ers.\nThe final answer: San Francisco 49ers.'Code generation example‚ÄãYou can now leverage the Codey API for code generation within Vertex AI. The model names are:code-bison: for code suggestioncode-gecko: for code completionllm = VertexAI(model_name="code-bison")llm_chain = LLMChain(prompt=prompt, llm=llm)question = "Write a python function that identifies if the number is a prime number?"llm_chain.run(question)    '```python\ndef is_prime(n):\n  """\n  Determines if a number is prime.\n\n  Args:\n    n: The number to be tested.\n\n  Returns:\n    True if the number is prime, False otherwise.\n  """\n\n  # Check if the number is 1.\n  if n == 1:\n    return False\n\n  # Check if the number is 2.\n  if n == 2:\n    return True\n\n'Using models deployed on Vertex Model Garden‚ÄãVertex Model Garden exposes open-sourced models that can be deployed and served on Vertex AI. If you have successfully deployed a model from Vertex Model Garden, you can find a corresponding Vertex AI endpoint in the console or via API.from langchain.llms import VertexAIModelGardenAPI Reference:VertexAIModelGardenllm = VertexAIModelGarden(    project="YOUR PROJECT",    endpoint_id="YOUR ENDPOINT_ID")llm("What is the meaning of life?")Like all LLMs, we can then compose it with other components:from langchain.prompts import PromptTemplateprompt = PromptTemplate.from_template("What is the meaning of {thing}?")API Reference:PromptTemplatellm_oss_chain = prompt | llmllm_oss_chain.invoke({"thing": "life"})PreviousForefrontAINextGooseAISetting upQuestion-answering exampleCode generation exampleUsing models deployed on Vertex Model Garden IntegrationsCallbacksChat modelsChat loadersDocument loadersDocument transformersLLMsAI21Aleph AlphaAmazon API GatewayAnyscaleAzure MLAzure OpenAIBananaBasetenBeamBedrockBittensorCerebriumAIChatGLMClarifaiCohereC TransformersCTranslate2DatabricksDeepInfraDeepSparseEden AIFireworksForefrontAIGoogle Vertex AI PaLMGooseAIGPT4AllHugging Face HubHugging Face Local PipelinesHuggingface TextGen InferenceJSONFormerKoboldAI APILlama.cppLLM Caching integrationsManifestMinimaxModalMosaicMLNLP CloudOctoAIOllamaOpaquePromptsOpenAIOpenLLMOpenLMPetalsPipelineAIPredibasePrediction GuardPromptLayer OpenAIRELLMReplicateRunhouseSageMakerEndpointStochasticAINebula (Symbl.ai)TextGenTitan TakeoffTongyi QwenvLLMWriterXorbits Inference (Xinference)MemoryRetrieversText embedding modelsAgents & ToolkitsToolsVector storesGrouped by providerIntegrationsLLMsGoogle Vertex AI PaLMOn this pageGoogle Vertex AI PaLMNote: This is seperate from the Google PaLM integration, it exposes Vertex AI PaLM API on Google Cloud. Setting up‚ÄãBy default, Google Cloud does not use customer data to train its foundation models as part of Google Cloud's AI/ML Privacy Commitment. More details about how Google processes data can also be found in Google's Customer Data Processing Addendum (CDPA).To use Vertex AI PaLM you must have the google-cloud-aiplatform Python package installed and either:Have credentials configured for your environment (gcloud, workload identity, etc...)Store the path to a service account JSON file as the GOOGLE_APPLICATION_CREDENTIALS environment variableThis codebase uses the google.auth library which first looks for the application credentials variable mentioned above, and then looks for system-level auth.For more information, see: https://cloud.google.com/docs/authentication/application-default-credentials#GAChttps://googleapis.dev/python/google-auth/latest/reference/google.auth.html#module-google.auth#!pip install google-cloud-aiplatformfrom langchain.llms import VertexAIAPI Reference:VertexAIQuestion-answering example‚Äãfrom langchain import PromptTemplate, LLMChaintemplate = """Question: {question}Answer: Let's think step by step."""prompt = PromptTemplate(template=template, input_variables=["question"])llm = VertexAI()llm_chain = LLMChain(prompt=prompt, llm=llm)question = "What NFL team won the Super Bowl in the year Justin Beiber was born?"llm_chain.run(question)    'Justin Bieber was born on March 1, 1994. The Super Bowl in 1994 was won by the San Francisco 49ers.\nThe final answer: San Francisco 49ers.'Code generation example‚ÄãYou can now leverage the Codey API for code generation within Vertex AI. The model names are:code-bison: for code suggestioncode-gecko: for code completionllm = VertexAI(model_name="code-bison")llm_chain = LLMChain(prompt=prompt, llm=llm)question = "Write a python function that identifies if the number is a prime number?"llm_chain.run(question)    '```python\ndef is_prime(n):\n  """\n  Determines if a number is prime.\n\n  Args:\n    n: The number to be tested.\n\n  Returns:\n    True if the number is prime, False otherwise.\n  """\n\n  # Check if the number is 1.\n  if n == 1:\n    return False\n\n  # Check if the number is 2.\n  if n == 2:\n    return True\n\n'Using models deployed on Vertex Model Garden‚ÄãVertex Model Garden exposes open-sourced models that can be deployed and served on Vertex AI. If you have successfully deployed a model from Vertex Model Garden, you can find a corresponding Vertex AI endpoint in the console or via API.from langchain.llms import VertexAIModelGardenAPI Reference:VertexAIModelGardenllm = VertexAIModelGarden(    project="YOUR PROJECT",    endpoint_id="YOUR ENDPOINT_ID")llm("What is the meaning of life?")Like all LLMs, we can then compose it with other components:from langchain.prompts import PromptTemplateprompt = PromptTemplate.from_template("What is the meaning of {thing}?")API Reference:PromptTemplatellm_oss_chain = prompt | llmllm_oss_chain.invoke({"thing": "life"})PreviousForefrontAINextGooseAISetting upQuestion-answering exampleCode generation exampleUsing models deployed on Vertex Model Garden IntegrationsCallbacksChat modelsChat loadersDocument loadersDocument transformersLLMsAI21Aleph AlphaAmazon API GatewayAnyscaleAzure MLAzure OpenAIBananaBasetenBeamBedrockBittensorCerebriumAIChatGLMClarifaiCohereC TransformersCTranslate2DatabricksDeepInfraDeepSparseEden AIFireworksForefrontAIGoogle Vertex AI PaLMGooseAIGPT4AllHugging Face HubHugging Face Local PipelinesHuggingface TextGen InferenceJSONFormerKoboldAI APILlama.cppLLM Caching integrationsManifestMinimaxModalMosaicMLNLP CloudOctoAIOllamaOpaquePromptsOpenAIOpenLLMOpenLMPetalsPipelineAIPredibasePrediction GuardPromptLayer OpenAIRELLMReplicateRunhouseSageMakerEndpointStochasticAINebula (Symbl.ai)TextGenTitan TakeoffTongyi QwenvLLMWriterXorbits Inference (Xinference)MemoryRetrieversText embedding modelsAgents & ToolkitsToolsVector storesGrouped by provider IntegrationsCallbacksChat modelsChat loadersDocument loadersDocument transformersLLMsAI21Aleph AlphaAmazon API GatewayAnyscaleAzure MLAzure OpenAIBananaBasetenBeamBedrockBittensorCerebriumAIChatGLMClarifaiCohereC TransformersCTranslate2DatabricksDeepInfraDeepSparseEden AIFireworksForefrontAIGoogle Vertex AI PaLMGooseAIGPT4AllHugging Face HubHugging Face Local PipelinesHuggingface TextGen InferenceJSONFormerKoboldAI APILlama.cppLLM Caching integrationsManifestMinimaxModalMosaicMLNLP CloudOctoAIOllamaOpaquePromptsOpenAIOpenLLMOpenLMPetalsPipelineAIPredibasePrediction GuardPromptLayer OpenAIRELLMReplicateRunhouseSageMakerEndpointStochasticAINebula (Symbl.ai)TextGenTitan TakeoffTongyi QwenvLLMWriterXorbits Inference (Xinference)MemoryRetrieversText embedding modelsAgents & ToolkitsToolsVector storesGrouped by provider Integrations Callbacks Chat models Chat loaders Document loaders Document transformers LLMs Memory Retrievers Text embedding models Agents & Toolkits Tools Vector stores Grouped by provider IntegrationsLLMsGoogle Vertex AI PaLMOn this pageGoogle Vertex AI PaLMNote: This is seperate from the Google PaLM integration, it exposes Vertex AI PaLM API on Google Cloud. Setting up‚ÄãBy default, Google Cloud does not use customer data to train its foundation models as part of Google Cloud's AI/ML Privacy Commitment. More details about how Google processes data can also be found in Google's Customer Data Processing Addendum (CDPA).To use Vertex AI PaLM you must have the google-cloud-aiplatform Python package installed and either:Have credentials configured for your environment (gcloud, workload identity, etc...)Store the path to a service account JSON file as the GOOGLE_APPLICATION_CREDENTIALS environment variableThis codebase uses the google.auth library which first looks for the application credentials variable mentioned above, and then looks for system-level auth.For more information, see: https://cloud.google.com/docs/authentication/application-default-credentials#GAChttps://googleapis.dev/python/google-auth/latest/reference/google.auth.html#module-google.auth#!pip install google-cloud-aiplatformfrom langchain.llms import VertexAIAPI Reference:VertexAIQuestion-answering example‚Äãfrom langchain import PromptTemplate, LLMChaintemplate = """Question: {question}Answer: Let's think step by step."""prompt = PromptTemplate(template=template, input_variables=["question"])llm = VertexAI()llm_chain = LLMChain(prompt=prompt, llm=llm)question = "What NFL team won the Super Bowl in the year Justin Beiber was born?"llm_chain.run(question)    'Justin Bieber was born on March 1, 1994. The Super Bowl in 1994 was won by the San Francisco 49ers.\nThe final answer: San Francisco 49ers.'Code generation example‚ÄãYou can now leverage the Codey API for code generation within Vertex AI. The model names are:code-bison: for code suggestioncode-gecko: for code completionllm = VertexAI(model_name="code-bison")llm_chain = LLMChain(prompt=prompt, llm=llm)question = "Write a python function that identifies if the number is a prime number?"llm_chain.run(question)    '```python\ndef is_prime(n):\n  """\n  Determines if a number is prime.\n\n  Args:\n    n: The number to be tested.\n\n  Returns:\n    True if the number is prime, False otherwise.\n  """\n\n  # Check if the number is 1.\n  if n == 1:\n    return False\n\n  # Check if the number is 2.\n  if n == 2:\n    return True\n\n'Using models deployed on Vertex Model Garden‚ÄãVertex Model Garden exposes open-sourced models that can be deployed and served on Vertex AI. If you have successfully deployed a model from Vertex Model Garden, you can find a corresponding Vertex AI endpoint in the console or via API.from langchain.llms import VertexAIModelGardenAPI Reference:VertexAIModelGardenllm = VertexAIModelGarden(    project="YOUR PROJECT",    endpoint_id="YOUR ENDPOINT_ID")llm("What is the meaning of life?")Like all LLMs, we can then compose it with other components:from langchain.prompts import PromptTemplateprompt = PromptTemplate.from_template("What is the meaning of {thing}?")API Reference:PromptTemplatellm_oss_chain = prompt | llmllm_oss_chain.invoke({"thing": "life"})PreviousForefrontAINextGooseAISetting upQuestion-answering exampleCode generation exampleUsing models deployed on Vertex Model Garden IntegrationsLLMsGoogle Vertex AI PaLMOn this pageGoogle Vertex AI PaLMNote: This is seperate from the Google PaLM integration, it exposes Vertex AI PaLM API on Google Cloud. Setting up‚ÄãBy default, Google Cloud does not use customer data to train its foundation models as part of Google Cloud's AI/ML Privacy Commitment. More details about how Google processes data can also be found in Google's Customer Data Processing Addendum (CDPA).To use Vertex AI PaLM you must have the google-cloud-aiplatform Python package installed and either:Have credentials configured for your environment (gcloud, workload identity, etc...)Store the path to a service account JSON file as the GOOGLE_APPLICATION_CREDENTIALS environment variableThis codebase uses the google.auth library which first looks for the application credentials variable mentioned above, and then looks for system-level auth.For more information, see: https://cloud.google.com/docs/authentication/application-default-credentials#GAChttps://googleapis.dev/python/google-auth/latest/reference/google.auth.html#module-google.auth#!pip install google-cloud-aiplatformfrom langchain.llms import VertexAIAPI Reference:VertexAIQuestion-answering example‚Äãfrom langchain import PromptTemplate, LLMChaintemplate = """Question: {question}Answer: Let's think step by step."""prompt = PromptTemplate(template=template, input_variables=["question"])llm = VertexAI()llm_chain = LLMChain(prompt=prompt, llm=llm)question = "What NFL team won the Super Bowl in the year Justin Beiber was born?"llm_chain.run(question)    'Justin Bieber was born on March 1, 1994. The Super Bowl in 1994 was won by the San Francisco 49ers.\nThe final answer: San Francisco 49ers.'Code generation example‚ÄãYou can now leverage the Codey API for code generation within Vertex AI. The model names are:code-bison: for code suggestioncode-gecko: for code completionllm = VertexAI(model_name="code-bison")llm_chain = LLMChain(prompt=prompt, llm=llm)question = "Write a python function that identifies if the number is a prime number?"llm_chain.run(question)    '```python\ndef is_prime(n):\n  """\n  Determines if a number is prime.\n\n  Args:\n    n: The number to be tested.\n\n  Returns:\n    True if the number is prime, False otherwise.\n  """\n\n  # Check if the number is 1.\n  if n == 1:\n    return False\n\n  # Check if the number is 2.\n  if n == 2:\n    return True\n\n'Using models deployed on Vertex Model Garden‚ÄãVertex Model Garden exposes open-sourced models that can be deployed and served on Vertex AI. If you have successfully deployed a model from Vertex Model Garden, you can find a corresponding Vertex AI endpoint in the console or via API.from langchain.llms import VertexAIModelGardenAPI Reference:VertexAIModelGardenllm = VertexAIModelGarden(    project="YOUR PROJECT",    endpoint_id="YOUR ENDPOINT_ID")llm("What is the meaning of life?")Like all LLMs, we can then compose it with other components:from langchain.prompts import PromptTemplateprompt = PromptTemplate.from_template("What is the meaning of {thing}?")API Reference:PromptTemplatellm_oss_chain = prompt | llmllm_oss_chain.invoke({"thing": "life"})PreviousForefrontAINextGooseAISetting upQuestion-answering exampleCode generation exampleUsing models deployed on Vertex Model Garden IntegrationsLLMsGoogle Vertex AI PaLMOn this pageGoogle Vertex AI PaLMNote: This is seperate from the Google PaLM integration, it exposes Vertex AI PaLM API on Google Cloud. Setting up‚ÄãBy default, Google Cloud does not use customer data to train its foundation models as part of Google Cloud's AI/ML Privacy Commitment. More details about how Google processes data can also be found in Google's Customer Data Processing Addendum (CDPA).To use Vertex AI PaLM you must have the google-cloud-aiplatform Python package installed and either:Have credentials configured for your environment (gcloud, workload identity, etc...)Store the path to a service account JSON file as the GOOGLE_APPLICATION_CREDENTIALS environment variableThis codebase uses the google.auth library which first looks for the application credentials variable mentioned above, and then looks for system-level auth.For more information, see: https://cloud.google.com/docs/authentication/application-default-credentials#GAChttps://googleapis.dev/python/google-auth/latest/reference/google.auth.html#module-google.auth#!pip install google-cloud-aiplatformfrom langchain.llms import VertexAIAPI Reference:VertexAIQuestion-answering example‚Äãfrom langchain import PromptTemplate, LLMChaintemplate = """Question: {question}Answer: Let's think step by step."""prompt = PromptTemplate(template=template, input_variables=["question"])llm = VertexAI()llm_chain = LLMChain(prompt=prompt, llm=llm)question = "What NFL team won the Super Bowl in the year Justin Beiber was born?"llm_chain.run(question)    'Justin Bieber was born on March 1, 1994. The Super Bowl in 1994 was won by the San Francisco 49ers.\nThe final answer: San Francisco 49ers.'Code generation example‚ÄãYou can now leverage the Codey API for code generation within Vertex AI. The model names are:code-bison: for code suggestioncode-gecko: for code completionllm = VertexAI(model_name="code-bison")llm_chain = LLMChain(prompt=prompt, llm=llm)question = "Write a python function that identifies if the number is a prime number?"llm_chain.run(question)    '```python\ndef is_prime(n):\n  """\n  Determines if a number is prime.\n\n  Args:\n    n: The number to be tested.\n\n  Returns:\n    True if the number is prime, False otherwise.\n  """\n\n  # Check if the number is 1.\n  if n == 1:\n    return False\n\n  # Check if the number is 2.\n  if n == 2:\n    return True\n\n'Using models deployed on Vertex Model Garden‚ÄãVertex Model Garden exposes open-sourced models that can be deployed and served on Vertex AI. If you have successfully deployed a model from Vertex Model Garden, you can find a corresponding Vertex AI endpoint in the console or via API.from langchain.llms import VertexAIModelGardenAPI Reference:VertexAIModelGardenllm = VertexAIModelGarden(    project="YOUR PROJECT",    endpoint_id="YOUR ENDPOINT_ID")llm("What is the meaning of life?")Like all LLMs, we can then compose it with other components:from langchain.prompts import PromptTemplateprompt = PromptTemplate.from_template("What is the meaning of {thing}?")API Reference:PromptTemplatellm_oss_chain = prompt | llmllm_oss_chain.invoke({"thing": "life"})PreviousForefrontAINextGooseAI IntegrationsLLMsGoogle Vertex AI PaLMOn this pageGoogle Vertex AI PaLMNote: This is seperate from the Google PaLM integration, it exposes Vertex AI PaLM API on Google Cloud. Setting up‚ÄãBy default, Google Cloud does not use customer data to train its foundation models as part of Google Cloud's AI/ML Privacy Commitment. More details about how Google processes data can also be found in Google's Customer Data Processing Addendum (CDPA).To use Vertex AI PaLM you must have the google-cloud-aiplatform Python package installed and either:Have credentials configured for your environment (gcloud, workload identity, etc...)Store the path to a service account JSON file as the GOOGLE_APPLICATION_CREDENTIALS environment variableThis codebase uses the google.auth library which first looks for the application credentials variable mentioned above, and then looks for system-level auth.For more information, see: https://cloud.google.com/docs/authentication/application-default-credentials#GAChttps://googleapis.dev/python/google-auth/latest/reference/google.auth.html#module-google.auth#!pip install google-cloud-aiplatformfrom langchain.llms import VertexAIAPI Reference:VertexAIQuestion-answering example‚Äãfrom langchain import PromptTemplate, LLMChaintemplate = """Question: {question}Answer: Let's think step by step."""prompt = PromptTemplate(template=template, input_variables=["question"])llm = VertexAI()llm_chain = LLMChain(prompt=prompt, llm=llm)question = "What NFL team won the Super Bowl in the year Justin Beiber was born?"llm_chain.run(question)    'Justin Bieber was born on March 1, 1994. The Super Bowl in 1994 was won by the San Francisco 49ers.\nThe final answer: San Francisco 49ers.'Code generation example‚ÄãYou can now leverage the Codey API for code generation within Vertex AI. The model names are:code-bison: for code suggestioncode-gecko: for code completionllm = VertexAI(model_name="code-bison")llm_chain = LLMChain(prompt=prompt, llm=llm)question = "Write a python function that identifies if the number is a prime number?"llm_chain.run(question)    '```python\ndef is_prime(n):\n  """\n  Determines if a number is prime.\n\n  Args:\n    n: The number to be tested.\n\n  Returns:\n    True if the number is prime, False otherwise.\n  """\n\n  # Check if the number is 1.\n  if n == 1:\n    return False\n\n  # Check if the number is 2.\n  if n == 2:\n    return True\n\n'Using models deployed on Vertex Model Garden‚ÄãVertex Model Garden exposes open-sourced models that can be deployed and served on Vertex AI. If you have successfully deployed a model from Vertex Model Garden, you can find a corresponding Vertex AI endpoint in the console or via API.from langchain.llms import VertexAIModelGardenAPI Reference:VertexAIModelGardenllm = VertexAIModelGarden(    project="YOUR PROJECT",    endpoint_id="YOUR ENDPOINT_ID")llm("What is the meaning of life?")Like all LLMs, we can then compose it with other components:from langchain.prompts import PromptTemplateprompt = PromptTemplate.from_template("What is the meaning of {thing}?")API Reference:PromptTemplatellm_oss_chain = prompt | llmllm_oss_chain.invoke({"thing": "life"})PreviousForefrontAINextGooseAI On this page Google Vertex AI PaLMNote: This is seperate from the Google PaLM integration, it exposes Vertex AI PaLM API on Google Cloud. Setting up‚ÄãBy default, Google Cloud does not use customer data to train its foundation models as part of Google Cloud's AI/ML Privacy Commitment. More details about how Google processes data can also be found in Google's Customer Data Processing Addendum (CDPA).To use Vertex AI PaLM you must have the google-cloud-aiplatform Python package installed and either:Have credentials configured for your environment (gcloud, workload identity, etc...)Store the path to a service account JSON file as the GOOGLE_APPLICATION_CREDENTIALS environment variableThis codebase uses the google.auth library which first looks for the application credentials variable mentioned above, and then looks for system-level auth.For more information, see: https://cloud.google.com/docs/authentication/application-default-credentials#GAChttps://googleapis.dev/python/google-auth/latest/reference/google.auth.html#module-google.auth#!pip install google-cloud-aiplatformfrom langchain.llms import VertexAIAPI Reference:VertexAIQuestion-answering example‚Äãfrom langchain import PromptTemplate, LLMChaintemplate = """Question: {question}Answer: Let's think step by step."""prompt = PromptTemplate(template=template, input_variables=["question"])llm = VertexAI()llm_chain = LLMChain(prompt=prompt, llm=llm)question = "What NFL team won the Super Bowl in the year Justin Beiber was born?"llm_chain.run(question)    'Justin Bieber was born on March 1, 1994. The Super Bowl in 1994 was won by the San Francisco 49ers.\nThe final answer: San Francisco 49ers.'Code generation example‚ÄãYou can now leverage the Codey API for code generation within Vertex AI. The model names are:code-bison: for code suggestioncode-gecko: for code completionllm = VertexAI(model_name="code-bison")llm_chain = LLMChain(prompt=prompt, llm=llm)question = "Write a python function that identifies if the number is a prime number?"llm_chain.run(question)    '```python\ndef is_prime(n):\n  """\n  Determines if a number is prime.\n\n  Args:\n    n: The number to be tested.\n\n  Returns:\n    True if the number is prime, False otherwise.\n  """\n\n  # Check if the number is 1.\n  if n == 1:\n    return False\n\n  # Check if the number is 2.\n  if n == 2:\n    return True\n\n'Using models deployed on Vertex Model Garden‚ÄãVertex Model Garden exposes open-sourced models that can be deployed and served on Vertex AI. If you have successfully deployed a model from Vertex Model Garden, you can find a corresponding Vertex AI endpoint in the console or via API.from langchain.llms import VertexAIModelGardenAPI Reference:VertexAIModelGardenllm = VertexAIModelGarden(    project="YOUR PROJECT",    endpoint_id="YOUR ENDPOINT_ID")llm("What is the meaning of life?")Like all LLMs, we can then compose it with other components:from langchain.prompts import PromptTemplateprompt = PromptTemplate.from_template("What is the meaning of {thing}?")API Reference:PromptTemplatellm_oss_chain = prompt | llmllm_oss_chain.invoke({"thing": "life"}) #!pip install google-cloud-aiplatform #!pip install google-cloud-aiplatform  from langchain.llms import VertexAI from langchain.llms import VertexAI  API Reference:VertexAI from langchain import PromptTemplate, LLMChain from langchain import PromptTemplate, LLMChain  template = """Question: {question}Answer: Let's think step by step."""prompt = PromptTemplate(template=template, input_variables=["question"]) template = """Question: {question}Answer: Let's think step by step."""prompt = PromptTemplate(template=template, input_variables=["question"])  llm = VertexAI() llm = VertexAI()  llm_chain = LLMChain(prompt=prompt, llm=llm) llm_chain = LLMChain(prompt=prompt, llm=llm)  question = "What NFL team won the Super Bowl in the year Justin Beiber was born?"llm_chain.run(question) question = "What NFL team won the Super Bowl in the year Justin Beiber was born?"llm_chain.run(question)      'Justin Bieber was born on March 1, 1994. The Super Bowl in 1994 was won by the San Francisco 49ers.\nThe final answer: San Francisco 49ers.'     'Justin Bieber was born on March 1, 1994. The Super Bowl in 1994 was won by the San Francisco 49ers.\nThe final answer: San Francisco 49ers.'     'Justin Bieber was born on March 1, 1994. The Super Bowl in 1994 was won by the San Francisco 49ers.\nThe final answer: San Francisco 49ers.'  llm = VertexAI(model_name="code-bison") llm = VertexAI(model_name="code-bison")  llm_chain = LLMChain(prompt=prompt, llm=llm) llm_chain = LLMChain(prompt=prompt, llm=llm)  question = "Write a python function that identifies if the number is a prime number?"llm_chain.run(question) question = "Write a python function that identifies if the number is a prime number?"llm_chain.run(question)      '```python\ndef is_prime(n):\n  """\n  Determines if a number is prime.\n\n  Args:\n    n: The number to be tested.\n\n  Returns:\n    True if the number is prime, False otherwise.\n  """\n\n  # Check if the number is 1.\n  if n == 1:\n    return False\n\n  # Check if the number is 2.\n  if n == 2:\n    return True\n\n'     '```python\ndef is_prime(n):\n  """\n  Determines if a number is prime.\n\n  Args:\n    n: The number to be tested.\n\n  Returns:\n    True if the number is prime, False otherwise.\n  """\n\n  # Check if the number is 1.\n  if n == 1:\n    return False\n\n  # Check if the number is 2.\n  if n == 2:\n    return True\n\n'     '```python\ndef is_prime(n):\n  """\n  Determines if a number is prime.\n\n  Args:\n    n: The number to be tested.\n\n  Returns:\n    True if the number is prime, False otherwise.\n  """\n\n  # Check if the number is 1.\n  if n == 1:\n    return False\n\n  # Check if the number is 2.\n  if n == 2:\n    return True\n\n'  from langchain.llms import VertexAIModelGarden from langchain.llms import VertexAIModelGarden  API Reference:VertexAIModelGarden llm = VertexAIModelGarden(    project="YOUR PROJECT",    endpoint_id="YOUR ENDPOINT_ID") llm = VertexAIModelGarden(    project="YOUR PROJECT",    endpoint_id="YOUR ENDPOINT_ID")  llm("What is the meaning of life?") llm("What is the meaning of life?")  from langchain.prompts import PromptTemplateprompt = PromptTemplate.from_template("What is the meaning of {thing}?") from langchain.prompts import PromptTemplateprompt = PromptTemplate.from_template("What is the meaning of {thing}?")  API Reference:PromptTemplate llm_oss_chain = prompt | llmllm_oss_chain.invoke({"thing": "life"}) llm_oss_chain = prompt | llmllm_oss_chain.invoke({"thing": "life"})  Previous ForefrontAI Next GooseAI Setting upQuestion-answering exampleCode generation exampleUsing models deployed on Vertex Model Garden Setting upQuestion-answering exampleCode generation exampleUsing models deployed on Vertex Model Garden CommunityDiscordTwitterGitHubPythonJS/TSMoreHomepageBlogCopyright ¬© 2023 LangChain, Inc. CommunityDiscordTwitterGitHubPythonJS/TSMoreHomepageBlog CommunityDiscordTwitter Community GitHubPythonJS/TS GitHub MoreHomepageBlog More Copyright ¬© 2023 LangChain, Inc. Copyright ¬© 2023 LangChain, Inc. Skip to main content (#docusaurus_skipToContent_fallback) ü¶úÔ∏èüîó LangChain (/) Docs (/docs/get_started/introduction) Use cases (/docs/use_cases) Integrations (/docs/integrations) API (https://api.python.langchain.com) LangSmith (https://smith.langchain.com) JS/TS Docs (https://js.langchain.com/docs)  (https://github.com/hwchase17/langchain) Integrations (/docs/integrations) Callbacks (/docs/integrations/callbacks/) Chat models (/docs/integrations/chat/) Chat loaders (/docs/integrations/chat_loaders/) Document loaders (/docs/integrations/document_loaders/) Document transformers (/docs/integrations/document_transformers/) LLMs (/docs/integrations/llms/) AI21 (/docs/integrations/llms/ai21) Aleph Alpha (/docs/integrations/llms/aleph_alpha) Amazon API Gateway (/docs/integrations/llms/amazon_api_gateway) Anyscale (/docs/integrations/llms/anyscale) Azure ML (/docs/integrations/llms/azure_ml) Azure OpenAI (/docs/integrations/llms/azure_openai) Banana (/docs/integrations/llms/banana) Baseten (/docs/integrations/llms/baseten) Beam (/docs/integrations/llms/beam) Bedrock (/docs/integrations/llms/bedrock) Bittensor (/docs/integrations/llms/bittensor) CerebriumAI (/docs/integrations/llms/cerebriumai) ChatGLM (/docs/integrations/llms/chatglm) Clarifai (/docs/integrations/llms/clarifai) Cohere (/docs/integrations/llms/cohere) C Transformers (/docs/integrations/llms/ctransformers) CTranslate2 (/docs/integrations/llms/ctranslate2) Databricks (/docs/integrations/llms/databricks) DeepInfra (/docs/integrations/llms/deepinfra) DeepSparse (/docs/integrations/llms/deepsparse) Eden AI (/docs/integrations/llms/edenai) Fireworks (/docs/integrations/llms/fireworks) ForefrontAI (/docs/integrations/llms/forefrontai) Google Vertex AI PaLM (/docs/integrations/llms/google_vertex_ai_palm) GooseAI (/docs/integrations/llms/gooseai) GPT4All (/docs/integrations/llms/gpt4all) Hugging Face Hub (/docs/integrations/llms/huggingface_hub) Hugging Face Local Pipelines (/docs/integrations/llms/huggingface_pipelines) Huggingface TextGen Inference (/docs/integrations/llms/huggingface_textgen_inference) JSONFormer (/docs/integrations/llms/jsonformer_experimental) KoboldAI API (/docs/integrations/llms/koboldai) Llama.cpp (/docs/integrations/llms/llamacpp) LLM Caching integrations (/docs/integrations/llms/llm_caching) Manifest (/docs/integrations/llms/manifest) Minimax (/docs/integrations/llms/minimax) Modal (/docs/integrations/llms/modal) MosaicML (/docs/integrations/llms/mosaicml) NLP Cloud (/docs/integrations/llms/nlpcloud) OctoAI (/docs/integrations/llms/octoai) Ollama (/docs/integrations/llms/ollama) OpaquePrompts (/docs/integrations/llms/opaqueprompts) OpenAI (/docs/integrations/llms/openai) OpenLLM (/docs/integrations/llms/openllm) OpenLM (/docs/integrations/llms/openlm) Petals (/docs/integrations/llms/petals) PipelineAI (/docs/integrations/llms/pipelineai) Predibase (/docs/integrations/llms/predibase) Prediction Guard (/docs/integrations/llms/predictionguard) PromptLayer OpenAI (/docs/integrations/llms/promptlayer_openai) RELLM (/docs/integrations/llms/rellm_experimental) Replicate (/docs/integrations/llms/replicate) Runhouse (/docs/integrations/llms/runhouse) SageMakerEndpoint (/docs/integrations/llms/sagemaker) StochasticAI (/docs/integrations/llms/stochasticai) Nebula (Symbl.ai) (/docs/integrations/llms/symblai_nebula) TextGen (/docs/integrations/llms/textgen) Titan Takeoff (/docs/integrations/llms/titan_takeoff) Tongyi Qwen (/docs/integrations/llms/tongyi) vLLM (/docs/integrations/llms/vllm) Writer (/docs/integrations/llms/writer) Xorbits Inference (Xinference) (/docs/integrations/llms/xinference) Memory (/docs/integrations/memory/) Retrievers (/docs/integrations/retrievers/) Text embedding models (/docs/integrations/text_embedding/) Agents & Toolkits (/docs/integrations/toolkits/) Tools (/docs/integrations/tools/) Vector stores (/docs/integrations/vectorstores/) Grouped by provider (/docs/integrations/providers/)  (/) Integrations (/docs/integrations) LLMs (/docs/integrations/llms/) Vertex AI PaLM API (https://cloud.google.com/vertex-ai/docs/generative-ai/learn/overview) ‚Äã (#setting-up) does not use (https://cloud.google.com/vertex-ai/docs/generative-ai/data-governance#foundation_model_development) Google's Customer Data Processing Addendum (CDPA) (https://cloud.google.com/terms/data-processing-addendum) https://cloud.google.com/docs/authentication/application-default-credentials#GAC (https://cloud.google.com/docs/authentication/application-default-credentials#GAC) https://googleapis.dev/python/google-auth/latest/reference/google.auth.html#module-google.auth (https://googleapis.dev/python/google-auth/latest/reference/google.auth.html#module-google.auth) VertexAI (https://api.python.langchain.com/en/latest/llms/langchain.llms.vertexai.VertexAI.html) ‚Äã (#question-answering-example) ‚Äã (#code-generation-example) ‚Äã (#using-models-deployed-on-vertex-model-garden) exposes (https://cloud.google.com/vertex-ai/docs/start/explore-models) endpoint (https://cloud.google.com/vertex-ai/docs/general/deployment#what_happens_when_you_deploy_a_model) VertexAIModelGarden (https://api.python.langchain.com/en/latest/llms/langchain.llms.vertexai.VertexAIModelGarden.html) PromptTemplate (https://api.python.langchain.com/en/latest/prompts/langchain.prompts.prompt.PromptTemplate.html) PreviousForefrontAI (/docs/integrations/llms/forefrontai) NextGooseAI (/docs/integrations/llms/gooseai) Setting up (#setting-up) Question-answering example (#question-answering-example) Code generation example (#code-generation-example) Using models deployed on Vertex Model Garden (#using-models-deployed-on-vertex-model-garden) Discord (https://discord.gg/cU2adEyC7w) Twitter (https://twitter.com/LangChainAI) Python (https://github.com/hwchase17/langchain) JS/TS (https://github.com/hwchase17/langchainjs) Homepage (https://langchain.com) Blog (https://blog.langchain.dev)