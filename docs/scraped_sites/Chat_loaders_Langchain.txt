Like document loaders, chat loaders are utilities designed to help load conversations from popular communication platforms such as Facebook, Slack, Discord, etc. These are loaded into memory as LangChain chat message objects. Such utilities facilitate tasks such as fine-tuning a language model to match your personal style or voice.  This brief guide will illustrate the process using OpenAI's fine-tuning API comprised of six steps: This guide is not wholly comprehensive but is designed to take you through the fundamentals of going from raw data to fine-tuned model. We will demonstrate the procedure through an example of fine-tuning a gpt-3.5-turbo model on Facebook Messenger data.  To export your Facebook messenger data, you can follow the instructions here.  You must select "JSON format" (instead of HTML) when exporting your data to be compatible with the current loader. OpenAI requires at least 10 examples to fine-tune your model, but they recommend between 50-100 for more optimal results. You can use the example data stored at this google drive link to test the process. Once you've obtained your chat data, you can load it into memory as LangChain chat message objects. Here‚Äôs an example of loading data using the Python code: In this snippet, we point the loader to a directory of Facebook chat dumps which are then loaded as multiple "sessions" of messages, one session per conversation file. Once you've loaded the messages, you should decide which person you want to fine-tune the model to (usually yourself). You can also decide to merge consecutive messages from the same sender into a single chat message. For both of these tasks, you can use the chat_loaders utilities to do so: Convert the chat messages to dictionaries using the convert_messages_for_finetuning function. Then, group the data into chunks for better context modeling and overlap management. At this point, the data is ready for upload to OpenAI. You can choose to split up conversations into smaller chunks for training if you do not have enough conversations to train on. Feel free to play around with different chunk sizes or with adding system messages to the fine-tuning data. Ensure you have set your OpenAI API key by following these instructions, then upload the training file. An audit is performed to ensure data compliance, so you may have to wait a few minutes for the dataset to become ready for use. Once this is done, you can proceed to the model training! Start the fine-tuning job with your chosen base model. This might take a while. Check the status with openai.FineTuningJob.retrieve(job.id).status and wait for it to report succeeded. You're almost there! Use the fine-tuned model in LangChain. And that's it! You've successfully fine-tuned a model and used it in LangChain. LangChain currently supports the following chat loaders. Feel free to contribute more! This notebook shows how to create your own chat loader that works on copy-pasted messages (from dms) to a list of LangChain messages. This notebook shows how to load data from Facebook in a format you can finetune on. The overall steps are: This loader goes over how to load data from GMail. There are many ways you could want to load data from GMail. This loader is currently fairly opionated in how to do so. The way it does it is it first looks for all messages that you have sent. It then looks for messages where you are responding to a previous email. It then fetches that previous email, and creates a training example of that email, followed by your email. This notebook shows how to use the iMessage chat loader. This class helps convert iMessage conversations to LangChain chat messages. This notebook shows how to use the Slack chat loader. This class helps map exported slack conversations to LangChain chat messages. This notebook shows how to use the Telegram chat loader. This class helps map exported Telegram conversations to LangChain chat messages. This notebook shows how to load chat messages from Twitter to finetune on. We do this by utilizing Apify. This notebook shows how to use the WhatsApp chat loader. This class helps map exported Telegram conversations to LangChain chat messages. IntegrationsCallbacksChat modelsChat loadersDiscordFacebook MessengerGMailiMessageSlackTelegramTwitter (via Apify)WhatsAppDocument loadersDocument transformersLLMsMemoryRetrieversText embedding modelsAgents & ToolkitsToolsVector storesGrouped by provider Callbacks Chat models Chat loadersDiscordFacebook MessengerGMailiMessageSlackTelegramTwitter (via Apify)WhatsApp Discord Facebook Messenger GMail iMessage Slack Telegram Twitter (via Apify) WhatsApp Document loaders Document transformers LLMs Memory Retrievers Text embedding models Agents & Toolkits Tools Vector stores Grouped by provider  Integrations Chat loaders Export your Facebook Messenger chat data in a compatible format for your intended chat loader. Load the chat data into memory as LangChain chat message objects. (this is what is covered in each integration notebook in this section of the documentation).Assign a person to the "AI" role and optionally filter, group, and merge messages. Assign a person to the "AI" role and optionally filter, group, and merge messages. Export these acquired messages in a format expected by the fine-tuning API. Upload this data to OpenAI. Fine-tune your model. Implement the fine-tuned model in LangChain. FolderFacebookMessengerChatLoader convert_messages_for_finetuning ChatPromptTemplate StrOutputParser 1. Export your chat data 2. Load the chat 3. Export messages to OpenAI format 4. Upload the data to OpenAI 5. Fine-tune the model 6. Use the model in LangChain Supported Chat Loaders Discord Twitter Python JS/TS Homepage Blog Skip to main contentü¶úÔ∏èüîó LangChainDocsUse casesIntegrationsAPILangSmithJS/TS DocsCTRLKIntegrationsCallbacksChat modelsChat loadersDiscordFacebook MessengerGMailiMessageSlackTelegramTwitter (via Apify)WhatsAppDocument loadersDocument transformersLLMsMemoryRetrieversText embedding modelsAgents & ToolkitsToolsVector storesGrouped by providerIntegrationsChat loadersOn this pageChat loadersLike document loaders, chat loaders are utilities designed to help load conversations from popular communication platforms such as Facebook, Slack, Discord, etc. These are loaded into memory as LangChain chat message objects. Such utilities facilitate tasks such as fine-tuning a language model to match your personal style or voice. This brief guide will illustrate the process using OpenAI's fine-tuning API comprised of six steps:Export your Facebook Messenger chat data in a compatible format for your intended chat loader.Load the chat data into memory as LangChain chat message objects. (this is what is covered in each integration notebook in this section of the documentation).Assign a person to the "AI" role and optionally filter, group, and merge messages.Export these acquired messages in a format expected by the fine-tuning API.Upload this data to OpenAI.Fine-tune your model.Implement the fine-tuned model in LangChain.This guide is not wholly comprehensive but is designed to take you through the fundamentals of going from raw data to fine-tuned model.We will demonstrate the procedure through an example of fine-tuning a gpt-3.5-turbo model on Facebook Messenger data. 1. Export your chat data‚ÄãTo export your Facebook messenger data, you can follow the instructions here. JSON formatYou must select "JSON format" (instead of HTML) when exporting your data to be compatible with the current loader.OpenAI requires at least 10 examples to fine-tune your model, but they recommend between 50-100 for more optimal results. You can use the example data stored at this google drive link to test the process.2. Load the chat‚ÄãOnce you've obtained your chat data, you can load it into memory as LangChain chat message objects. Here‚Äôs an example of loading data using the Python code:from langchain.chat_loaders.facebook_messenger import FolderFacebookMessengerChatLoaderloader = FolderFacebookMessengerChatLoader(    path="./facebook_messenger_chats",)chat_sessions = loader.load()API Reference:FolderFacebookMessengerChatLoaderIn this snippet, we point the loader to a directory of Facebook chat dumps which are then loaded as multiple "sessions" of messages, one session per conversation file.Once you've loaded the messages, you should decide which person you want to fine-tune the model to (usually yourself). You can also decide to merge consecutive messages from the same sender into a single chat message. For both of these tasks, you can use the chat_loaders utilities to do so:from langchain.chat_loaders.utils import (    merge_chat_runs,    map_ai_messages,)merged_sessions = merge_chat_runs(chat_sessions)alternating_sessions = list(map_ai_messages(merged_sessions, "My Name"))3. Export messages to OpenAI format‚ÄãConvert the chat messages to dictionaries using the convert_messages_for_finetuning function. Then, group the data into chunks for better context modeling and overlap management.from langchain.adapters.openai import convert_messages_for_finetuningopenai_messages = convert_messages_for_finetuning(chat_sessions)API Reference:convert_messages_for_finetuningAt this point, the data is ready for upload to OpenAI. You can choose to split up conversations into smaller chunks for training if you do not have enough conversations to train on. Feel free to play around with different chunk sizes or with adding system messages to the fine-tuning data.chunk_size = 8overlap = 2message_groups = [    conversation_messages[i: i + chunk_size]     for conversation_messages in openai_messages    for i in range(        0, len(conversation_messages) - chunk_size + 1,         chunk_size - overlap)]len(message_groups)# 94. Upload the data to OpenAI‚ÄãEnsure you have set your OpenAI API key by following these instructions, then upload the training file. An audit is performed to ensure data compliance, so you may have to wait a few minutes for the dataset to become ready for use.import timeimport jsonimport ioimport openaimy_file = io.BytesIO()for group in message_groups:    my_file.write((json.dumps({"messages": group}) + "\n").encode('utf-8'))my_file.seek(0)training_file = openai.File.create(  file=my_file,  purpose='fine-tune')# Wait while the file is processedstatus = openai.File.retrieve(training_file.id).statusstart_time = time.time()while status != "processed":    print(f"Status=[{status}]... {time.time() - start_time:.2f}s", end="\r", flush=True)    time.sleep(5)    status = openai.File.retrieve(training_file.id).statusprint(f"File {training_file.id} ready after {time.time() - start_time:.2f} seconds.")Once this is done, you can proceed to the model training!5. Fine-tune the model‚ÄãStart the fine-tuning job with your chosen base model.job = openai.FineTuningJob.create(    training_file=training_file.id,    model="gpt-3.5-turbo",)This might take a while. Check the status with openai.FineTuningJob.retrieve(job.id).status and wait for it to report succeeded.# It may take 10-20+ minutes to complete training.status = openai.FineTuningJob.retrieve(job.id).statusstart_time = time.time()while status != "succeeded":    print(f"Status=[{status}]... {time.time() - start_time:.2f}s", end="\r", flush=True)    time.sleep(5)    job = openai.FineTuningJob.retrieve(job.id)    status = job.status6. Use the model in LangChain‚ÄãYou're almost there! Use the fine-tuned model in LangChain.from langchain import chat_modelsmodel_name = job.fine_tuned_model# Example: ft:gpt-3.5-turbo-0613:personal::5mty86jblapsedmodel = chat_models.ChatOpenAI(model=model_name)from langchain.prompts import ChatPromptTemplatefrom langchain.schema.output_parser import StrOutputParser prompt = ChatPromptTemplate.from_messages(    [        ("human", "{input}"),    ])chain = prompt | model | StrOutputParser()for tok in chain.stream({"input": "What classes are you taking?"}):    print(tok, end="", flush=True)# The usual - Potions, Transfiguration, Defense Against the Dark Arts. What about you?API Reference:ChatPromptTemplateStrOutputParserAnd that's it! You've successfully fine-tuned a model and used it in LangChain.Supported Chat Loaders‚ÄãLangChain currently supports the following chat loaders. Feel free to contribute more!üìÑÔ∏è DiscordThis notebook shows how to create your own chat loader that works on copy-pasted messages (from dms) to a list of LangChain messages.üìÑÔ∏è Facebook MessengerThis notebook shows how to load data from Facebook in a format you can finetune on. The overall steps are:üìÑÔ∏è GMailThis loader goes over how to load data from GMail. There are many ways you could want to load data from GMail. This loader is currently fairly opionated in how to do so. The way it does it is it first looks for all messages that you have sent. It then looks for messages where you are responding to a previous email. It then fetches that previous email, and creates a training example of that email, followed by your email.üìÑÔ∏è iMessageThis notebook shows how to use the iMessage chat loader. This class helps convert iMessage conversations to LangChain chat messages.üìÑÔ∏è SlackThis notebook shows how to use the Slack chat loader. This class helps map exported slack conversations to LangChain chat messages.üìÑÔ∏è TelegramThis notebook shows how to use the Telegram chat loader. This class helps map exported Telegram conversations to LangChain chat messages.üìÑÔ∏è Twitter (via Apify)This notebook shows how to load chat messages from Twitter to finetune on. We do this by utilizing Apify.üìÑÔ∏è WhatsAppThis notebook shows how to use the WhatsApp chat loader. This class helps map exported Telegram conversations to LangChain chat messages.PreviousPromptLayer ChatOpenAINextDiscord1. Export your chat data2. Load the chat3. Export messages to OpenAI format4. Upload the data to OpenAI5. Fine-tune the model6. Use the model in LangChainSupported Chat LoadersCommunityDiscordTwitterGitHubPythonJS/TSMoreHomepageBlogCopyright ¬© 2023 LangChain, Inc. Skip to main content ü¶úÔ∏èüîó LangChainDocsUse casesIntegrationsAPILangSmithJS/TS DocsCTRLK ü¶úÔ∏èüîó LangChainDocsUse casesIntegrationsAPI LangSmithJS/TS DocsCTRLK  CTRLK CTRLK  CTRLK   IntegrationsCallbacksChat modelsChat loadersDiscordFacebook MessengerGMailiMessageSlackTelegramTwitter (via Apify)WhatsAppDocument loadersDocument transformersLLMsMemoryRetrieversText embedding modelsAgents & ToolkitsToolsVector storesGrouped by providerIntegrationsChat loadersOn this pageChat loadersLike document loaders, chat loaders are utilities designed to help load conversations from popular communication platforms such as Facebook, Slack, Discord, etc. These are loaded into memory as LangChain chat message objects. Such utilities facilitate tasks such as fine-tuning a language model to match your personal style or voice. This brief guide will illustrate the process using OpenAI's fine-tuning API comprised of six steps:Export your Facebook Messenger chat data in a compatible format for your intended chat loader.Load the chat data into memory as LangChain chat message objects. (this is what is covered in each integration notebook in this section of the documentation).Assign a person to the "AI" role and optionally filter, group, and merge messages.Export these acquired messages in a format expected by the fine-tuning API.Upload this data to OpenAI.Fine-tune your model.Implement the fine-tuned model in LangChain.This guide is not wholly comprehensive but is designed to take you through the fundamentals of going from raw data to fine-tuned model.We will demonstrate the procedure through an example of fine-tuning a gpt-3.5-turbo model on Facebook Messenger data. 1. Export your chat data‚ÄãTo export your Facebook messenger data, you can follow the instructions here. JSON formatYou must select "JSON format" (instead of HTML) when exporting your data to be compatible with the current loader.OpenAI requires at least 10 examples to fine-tune your model, but they recommend between 50-100 for more optimal results. An audit is performed to ensure data compliance, so you may have to wait a few minutes for the dataset to become ready for use.import timeimport jsonimport ioimport openaimy_file = io.BytesIO()for group in message_groups:    my_file.write((json.dumps({"messages": group}) + "\n").encode('utf-8'))my_file.seek(0)training_file = openai.File.create(  file=my_file,  purpose='fine-tune')# Wait while the file is processedstatus = openai.File.retrieve(training_file.id).statusstart_time = time.time()while status != "processed":    print(f"Status=[{status}]... {time.time() - start_time:.2f}s", end="\r", flush=True)    time.sleep(5)    status = openai.File.retrieve(training_file.id).statusprint(f"File {training_file.id} ready after {time.time() - start_time:.2f} seconds.")Once this is done, you can proceed to the model training!5. Fine-tune the model‚ÄãStart the fine-tuning job with your chosen base model.job = openai.FineTuningJob.create(    training_file=training_file.id,    model="gpt-3.5-turbo",)This might take a while. Check the status with openai.FineTuningJob.retrieve(job.id).status and wait for it to report succeeded.# It may take 10-20+ minutes to complete training.status = openai.FineTuningJob.retrieve(job.id).statusstart_time = time.time()while status != "succeeded":    print(f"Status=[{status}]... {time.time() - start_time:.2f}s", end="\r", flush=True)    time.sleep(5)    job = openai.FineTuningJob.retrieve(job.id)    status = job.status6. Use the model in LangChain‚ÄãYou're almost there! Use the fine-tuned model in LangChain.from langchain import chat_modelsmodel_name = job.fine_tuned_model# Example: ft:gpt-3.5-turbo-0613:personal::5mty86jblapsedmodel = chat_models.ChatOpenAI(model=model_name)from langchain.prompts import ChatPromptTemplatefrom langchain.schema.output_parser import StrOutputParser prompt = ChatPromptTemplate.from_messages(    [        ("human", "{input}"),    ])chain = prompt | model | StrOutputParser()for tok in chain.stream({"input": "What classes are you taking?"}):    print(tok, end="", flush=True)# The usual - Potions, Transfiguration, Defense Against the Dark Arts. What about you?API Reference:ChatPromptTemplateStrOutputParserAnd that's it! You've successfully fine-tuned a model and used it in LangChain.Supported Chat Loaders‚ÄãLangChain currently supports the following chat loaders. Feel free to contribute more!üìÑÔ∏è DiscordThis notebook shows how to create your own chat loader that works on copy-pasted messages (from dms) to a list of LangChain messages.üìÑÔ∏è Facebook MessengerThis notebook shows how to load data from Facebook in a format you can finetune on. The overall steps are:üìÑÔ∏è GMailThis loader goes over how to load data from GMail. There are many ways you could want to load data from GMail. This loader is currently fairly opionated in how to do so. The way it does it is it first looks for all messages that you have sent. It then looks for messages where you are responding to a previous email. It then fetches that previous email, and creates a training example of that email, followed by your email.üìÑÔ∏è iMessageThis notebook shows how to use the iMessage chat loader. This class helps convert iMessage conversations to LangChain chat messages.üìÑÔ∏è SlackThis notebook shows how to use the Slack chat loader. This class helps map exported slack conversations to LangChain chat messages.üìÑÔ∏è TelegramThis notebook shows how to use the Telegram chat loader. This class helps map exported Telegram conversations to LangChain chat messages.üìÑÔ∏è Twitter (via Apify)This notebook shows how to load chat messages from Twitter to finetune on. We do this by utilizing Apify.üìÑÔ∏è WhatsAppThis notebook shows how to use the WhatsApp chat loader. This class helps map exported Telegram conversations to LangChain chat messages.PreviousPromptLayer ChatOpenAINextDiscord1. Export your chat data2. Load the chat3. Export messages to OpenAI format4. Upload the data to OpenAI5. Fine-tune the model6. Use the model in LangChainSupported Chat Loaders IntegrationsCallbacksChat modelsChat loadersDiscordFacebook MessengerGMailiMessageSlackTelegramTwitter (via Apify)WhatsAppDocument loadersDocument transformersLLMsMemoryRetrieversText embedding modelsAgents & ToolkitsToolsVector storesGrouped by providerIntegrationsChat loadersOn this pageChat loadersLike document loaders, chat loaders are utilities designed to help load conversations from popular communication platforms such as Facebook, Slack, Discord, etc. These are loaded into memory as LangChain chat message objects. Such utilities facilitate tasks such as fine-tuning a language model to match your personal style or voice. This brief guide will illustrate the process using OpenAI's fine-tuning API comprised of six steps:Export your Facebook Messenger chat data in a compatible format for your intended chat loader.Load the chat data into memory as LangChain chat message objects. (this is what is covered in each integration notebook in this section of the documentation).Assign a person to the "AI" role and optionally filter, group, and merge messages.Export these acquired messages in a format expected by the fine-tuning API.Upload this data to OpenAI.Fine-tune your model.Implement the fine-tuned model in LangChain.This guide is not wholly comprehensive but is designed to take you through the fundamentals of going from raw data to fine-tuned model.We will demonstrate the procedure through an example of fine-tuning a gpt-3.5-turbo model on Facebook Messenger data. 1. Export your chat data‚ÄãTo export your Facebook messenger data, you can follow the instructions here. JSON formatYou must select "JSON format" (instead of HTML) when exporting your data to be compatible with the current loader.OpenAI requires at least 10 examples to fine-tune your model, but they recommend between 50-100 for more optimal results. An audit is performed to ensure data compliance, so you may have to wait a few minutes for the dataset to become ready for use.import timeimport jsonimport ioimport openaimy_file = io.BytesIO()for group in message_groups:    my_file.write((json.dumps({"messages": group}) + "\n").encode('utf-8'))my_file.seek(0)training_file = openai.File.create(  file=my_file,  purpose='fine-tune')# Wait while the file is processedstatus = openai.File.retrieve(training_file.id).statusstart_time = time.time()while status != "processed":    print(f"Status=[{status}]... {time.time() - start_time:.2f}s", end="\r", flush=True)    time.sleep(5)    status = openai.File.retrieve(training_file.id).statusprint(f"File {training_file.id} ready after {time.time() - start_time:.2f} seconds.")Once this is done, you can proceed to the model training!5. Fine-tune the model‚ÄãStart the fine-tuning job with your chosen base model.job = openai.FineTuningJob.create(    training_file=training_file.id,    model="gpt-3.5-turbo",)This might take a while. Check the status with openai.FineTuningJob.retrieve(job.id).status and wait for it to report succeeded.# It may take 10-20+ minutes to complete training.status = openai.FineTuningJob.retrieve(job.id).statusstart_time = time.time()while status != "succeeded":    print(f"Status=[{status}]... {time.time() - start_time:.2f}s", end="\r", flush=True)    time.sleep(5)    job = openai.FineTuningJob.retrieve(job.id)    status = job.status6. Use the model in LangChain‚ÄãYou're almost there! Use the fine-tuned model in LangChain.from langchain import chat_modelsmodel_name = job.fine_tuned_model# Example: ft:gpt-3.5-turbo-0613:personal::5mty86jblapsedmodel = chat_models.ChatOpenAI(model=model_name)from langchain.prompts import ChatPromptTemplatefrom langchain.schema.output_parser import StrOutputParser prompt = ChatPromptTemplate.from_messages(    [        ("human", "{input}"),    ])chain = prompt | model | StrOutputParser()for tok in chain.stream({"input": "What classes are you taking?"}):    print(tok, end="", flush=True)# The usual - Potions, Transfiguration, Defense Against the Dark Arts. What about you?API Reference:ChatPromptTemplateStrOutputParserAnd that's it! You've successfully fine-tuned a model and used it in LangChain.Supported Chat Loaders‚ÄãLangChain currently supports the following chat loaders. Feel free to contribute more!üìÑÔ∏è DiscordThis notebook shows how to create your own chat loader that works on copy-pasted messages (from dms) to a list of LangChain messages.üìÑÔ∏è Facebook MessengerThis notebook shows how to load data from Facebook in a format you can finetune on. The overall steps are:üìÑÔ∏è GMailThis loader goes over how to load data from GMail. There are many ways you could want to load data from GMail. This loader is currently fairly opionated in how to do so. The way it does it is it first looks for all messages that you have sent. It then looks for messages where you are responding to a previous email. It then fetches that previous email, and creates a training example of that email, followed by your email.üìÑÔ∏è iMessageThis notebook shows how to use the iMessage chat loader. This class helps convert iMessage conversations to LangChain chat messages.üìÑÔ∏è SlackThis notebook shows how to use the Slack chat loader. This class helps map exported slack conversations to LangChain chat messages.üìÑÔ∏è TelegramThis notebook shows how to use the Telegram chat loader. This class helps map exported Telegram conversations to LangChain chat messages.üìÑÔ∏è Twitter (via Apify)This notebook shows how to load chat messages from Twitter to finetune on. We do this by utilizing Apify.üìÑÔ∏è WhatsAppThis notebook shows how to use the WhatsApp chat loader. This class helps map exported Telegram conversations to LangChain chat messages.PreviousPromptLayer ChatOpenAINextDiscord1. Export your chat data2. Load the chat3. Export messages to OpenAI format4. Upload the data to OpenAI5. Fine-tune the model6. Use the model in LangChainSupported Chat Loaders IntegrationsCallbacksChat modelsChat loadersDiscordFacebook MessengerGMailiMessageSlackTelegramTwitter (via Apify)WhatsAppDocument loadersDocument transformersLLMsMemoryRetrieversText embedding modelsAgents & ToolkitsToolsVector storesGrouped by provider IntegrationsCallbacksChat modelsChat loadersDiscordFacebook MessengerGMailiMessageSlackTelegramTwitter (via Apify)WhatsAppDocument loadersDocument transformersLLMsMemoryRetrieversText embedding modelsAgents & ToolkitsToolsVector storesGrouped by provider Integrations Callbacks Chat models Chat loaders Document loaders Document transformers LLMs Memory Retrievers Text embedding models Agents & Toolkits Tools Vector stores Grouped by provider IntegrationsChat loadersOn this pageChat loadersLike document loaders, chat loaders are utilities designed to help load conversations from popular communication platforms such as Facebook, Slack, Discord, etc. These are loaded into memory as LangChain chat message objects. Such utilities facilitate tasks such as fine-tuning a language model to match your personal style or voice. This brief guide will illustrate the process using OpenAI's fine-tuning API comprised of six steps:Export your Facebook Messenger chat data in a compatible format for your intended chat loader.Load the chat data into memory as LangChain chat message objects. (this is what is covered in each integration notebook in this section of the documentation).Assign a person to the "AI" role and optionally filter, group, and merge messages.Export these acquired messages in a format expected by the fine-tuning API.Upload this data to OpenAI.Fine-tune your model.Implement the fine-tuned model in LangChain.This guide is not wholly comprehensive but is designed to take you through the fundamentals of going from raw data to fine-tuned model.We will demonstrate the procedure through an example of fine-tuning a gpt-3.5-turbo model on Facebook Messenger data. 1. Export your chat data‚ÄãTo export your Facebook messenger data, you can follow the instructions here. JSON formatYou must select "JSON format" (instead of HTML) when exporting your data to be compatible with the current loader.OpenAI requires at least 10 examples to fine-tune your model, but they recommend between 50-100 for more optimal results. An audit is performed to ensure data compliance, so you may have to wait a few minutes for the dataset to become ready for use.import timeimport jsonimport ioimport openaimy_file = io.BytesIO()for group in message_groups:    my_file.write((json.dumps({"messages": group}) + "\n").encode('utf-8'))my_file.seek(0)training_file = openai.File.create(  file=my_file,  purpose='fine-tune')# Wait while the file is processedstatus = openai.File.retrieve(training_file.id).statusstart_time = time.time()while status != "processed":    print(f"Status=[{status}]... {time.time() - start_time:.2f}s", end="\r", flush=True)    time.sleep(5)    status = openai.File.retrieve(training_file.id).statusprint(f"File {training_file.id} ready after {time.time() - start_time:.2f} seconds.")Once this is done, you can proceed to the model training!5. Fine-tune the model‚ÄãStart the fine-tuning job with your chosen base model.job = openai.FineTuningJob.create(    training_file=training_file.id,    model="gpt-3.5-turbo",)This might take a while. Check the status with openai.FineTuningJob.retrieve(job.id).status and wait for it to report succeeded.# It may take 10-20+ minutes to complete training.status = openai.FineTuningJob.retrieve(job.id).statusstart_time = time.time()while status != "succeeded":    print(f"Status=[{status}]... {time.time() - start_time:.2f}s", end="\r", flush=True)    time.sleep(5)    job = openai.FineTuningJob.retrieve(job.id)    status = job.status6. Use the model in LangChain‚ÄãYou're almost there! Use the fine-tuned model in LangChain.from langchain import chat_modelsmodel_name = job.fine_tuned_model# Example: ft:gpt-3.5-turbo-0613:personal::5mty86jblapsedmodel = chat_models.ChatOpenAI(model=model_name)from langchain.prompts import ChatPromptTemplatefrom langchain.schema.output_parser import StrOutputParser prompt = ChatPromptTemplate.from_messages(    [        ("human", "{input}"),    ])chain = prompt | model | StrOutputParser()for tok in chain.stream({"input": "What classes are you taking?"}):    print(tok, end="", flush=True)# The usual - Potions, Transfiguration, Defense Against the Dark Arts. What about you?API Reference:ChatPromptTemplateStrOutputParserAnd that's it! You've successfully fine-tuned a model and used it in LangChain.Supported Chat Loaders‚ÄãLangChain currently supports the following chat loaders. Feel free to contribute more!üìÑÔ∏è DiscordThis notebook shows how to create your own chat loader that works on copy-pasted messages (from dms) to a list of LangChain messages.üìÑÔ∏è Facebook MessengerThis notebook shows how to load data from Facebook in a format you can finetune on. The overall steps are:üìÑÔ∏è GMailThis loader goes over how to load data from GMail. There are many ways you could want to load data from GMail. This loader is currently fairly opionated in how to do so. The way it does it is it first looks for all messages that you have sent. It then looks for messages where you are responding to a previous email. It then fetches that previous email, and creates a training example of that email, followed by your email.üìÑÔ∏è iMessageThis notebook shows how to use the iMessage chat loader. This class helps convert iMessage conversations to LangChain chat messages.üìÑÔ∏è SlackThis notebook shows how to use the Slack chat loader. This class helps map exported slack conversations to LangChain chat messages.üìÑÔ∏è TelegramThis notebook shows how to use the Telegram chat loader. This class helps map exported Telegram conversations to LangChain chat messages.üìÑÔ∏è Twitter (via Apify)This notebook shows how to load chat messages from Twitter to finetune on. We do this by utilizing Apify.üìÑÔ∏è WhatsAppThis notebook shows how to use the WhatsApp chat loader. This class helps map exported Telegram conversations to LangChain chat messages.PreviousPromptLayer ChatOpenAINextDiscord1. Export your chat data2. Load the chat3. Export messages to OpenAI format4. Upload the data to OpenAI5. Fine-tune the model6. Use the model in LangChainSupported Chat Loaders IntegrationsChat loadersOn this pageChat loadersLike document loaders, chat loaders are utilities designed to help load conversations from popular communication platforms such as Facebook, Slack, Discord, etc. These are loaded into memory as LangChain chat message objects. Such utilities facilitate tasks such as fine-tuning a language model to match your personal style or voice. This brief guide will illustrate the process using OpenAI's fine-tuning API comprised of six steps:Export your Facebook Messenger chat data in a compatible format for your intended chat loader.Load the chat data into memory as LangChain chat message objects. (this is what is covered in each integration notebook in this section of the documentation).Assign a person to the "AI" role and optionally filter, group, and merge messages.Export these acquired messages in a format expected by the fine-tuning API.Upload this data to OpenAI.Fine-tune your model.Implement the fine-tuned model in LangChain.This guide is not wholly comprehensive but is designed to take you through the fundamentals of going from raw data to fine-tuned model.We will demonstrate the procedure through an example of fine-tuning a gpt-3.5-turbo model on Facebook Messenger data. 1. Export your chat data‚ÄãTo export your Facebook messenger data, you can follow the instructions here. JSON formatYou must select "JSON format" (instead of HTML) when exporting your data to be compatible with the current loader.OpenAI requires at least 10 examples to fine-tune your model, but they recommend between 50-100 for more optimal results. An audit is performed to ensure data compliance, so you may have to wait a few minutes for the dataset to become ready for use.import timeimport jsonimport ioimport openaimy_file = io.BytesIO()for group in message_groups:    my_file.write((json.dumps({"messages": group}) + "\n").encode('utf-8'))my_file.seek(0)training_file = openai.File.create(  file=my_file,  purpose='fine-tune')# Wait while the file is processedstatus = openai.File.retrieve(training_file.id).statusstart_time = time.time()while status != "processed":    print(f"Status=[{status}]... {time.time() - start_time:.2f}s", end="\r", flush=True)    time.sleep(5)    status = openai.File.retrieve(training_file.id).statusprint(f"File {training_file.id} ready after {time.time() - start_time:.2f} seconds.")Once this is done, you can proceed to the model training!5. Fine-tune the model‚ÄãStart the fine-tuning job with your chosen base model.job = openai.FineTuningJob.create(    training_file=training_file.id,    model="gpt-3.5-turbo",)This might take a while. Check the status with openai.FineTuningJob.retrieve(job.id).status and wait for it to report succeeded.# It may take 10-20+ minutes to complete training.status = openai.FineTuningJob.retrieve(job.id).statusstart_time = time.time()while status != "succeeded":    print(f"Status=[{status}]... {time.time() - start_time:.2f}s", end="\r", flush=True)    time.sleep(5)    job = openai.FineTuningJob.retrieve(job.id)    status = job.status6. Use the model in LangChain‚ÄãYou're almost there! Use the fine-tuned model in LangChain.from langchain import chat_modelsmodel_name = job.fine_tuned_model# Example: ft:gpt-3.5-turbo-0613:personal::5mty86jblapsedmodel = chat_models.ChatOpenAI(model=model_name)from langchain.prompts import ChatPromptTemplatefrom langchain.schema.output_parser import StrOutputParser prompt = ChatPromptTemplate.from_messages(    [        ("human", "{input}"),    ])chain = prompt | model | StrOutputParser()for tok in chain.stream({"input": "What classes are you taking?"}):    print(tok, end="", flush=True)# The usual - Potions, Transfiguration, Defense Against the Dark Arts. What about you?API Reference:ChatPromptTemplateStrOutputParserAnd that's it! You've successfully fine-tuned a model and used it in LangChain.Supported Chat Loaders‚ÄãLangChain currently supports the following chat loaders. Feel free to contribute more!üìÑÔ∏è DiscordThis notebook shows how to create your own chat loader that works on copy-pasted messages (from dms) to a list of LangChain messages.üìÑÔ∏è Facebook MessengerThis notebook shows how to load data from Facebook in a format you can finetune on. The overall steps are:üìÑÔ∏è GMailThis loader goes over how to load data from GMail. There are many ways you could want to load data from GMail. This loader is currently fairly opionated in how to do so. The way it does it is it first looks for all messages that you have sent. It then looks for messages where you are responding to a previous email. It then fetches that previous email, and creates a training example of that email, followed by your email.üìÑÔ∏è iMessageThis notebook shows how to use the iMessage chat loader. This class helps convert iMessage conversations to LangChain chat messages.üìÑÔ∏è SlackThis notebook shows how to use the Slack chat loader. This class helps map exported slack conversations to LangChain chat messages.üìÑÔ∏è TelegramThis notebook shows how to use the Telegram chat loader. This class helps map exported Telegram conversations to LangChain chat messages.üìÑÔ∏è Twitter (via Apify)This notebook shows how to load chat messages from Twitter to finetune on. We do this by utilizing Apify.üìÑÔ∏è WhatsAppThis notebook shows how to use the WhatsApp chat loader. This class helps map exported Telegram conversations to LangChain chat messages.PreviousPromptLayer ChatOpenAINextDiscord IntegrationsChat loadersOn this pageChat loadersLike document loaders, chat loaders are utilities designed to help load conversations from popular communication platforms such as Facebook, Slack, Discord, etc. These are loaded into memory as LangChain chat message objects. Such utilities facilitate tasks such as fine-tuning a language model to match your personal style or voice. This brief guide will illustrate the process using OpenAI's fine-tuning API comprised of six steps:Export your Facebook Messenger chat data in a compatible format for your intended chat loader.Load the chat data into memory as LangChain chat message objects. (this is what is covered in each integration notebook in this section of the documentation).Assign a person to the "AI" role and optionally filter, group, and merge messages.Export these acquired messages in a format expected by the fine-tuning API.Upload this data to OpenAI.Fine-tune your model.Implement the fine-tuned model in LangChain.This guide is not wholly comprehensive but is designed to take you through the fundamentals of going from raw data to fine-tuned model.We will demonstrate the procedure through an example of fine-tuning a gpt-3.5-turbo model on Facebook Messenger data. 1. Export your chat data‚ÄãTo export your Facebook messenger data, you can follow the instructions here. JSON formatYou must select "JSON format" (instead of HTML) when exporting your data to be compatible with the current loader.OpenAI requires at least 10 examples to fine-tune your model, but they recommend between 50-100 for more optimal results. An audit is performed to ensure data compliance, so you may have to wait a few minutes for the dataset to become ready for use.import timeimport jsonimport ioimport openaimy_file = io.BytesIO()for group in message_groups:    my_file.write((json.dumps({"messages": group}) + "\n").encode('utf-8'))my_file.seek(0)training_file = openai.File.create(  file=my_file,  purpose='fine-tune')# Wait while the file is processedstatus = openai.File.retrieve(training_file.id).statusstart_time = time.time()while status != "processed":    print(f"Status=[{status}]... {time.time() - start_time:.2f}s", end="\r", flush=True)    time.sleep(5)    status = openai.File.retrieve(training_file.id).statusprint(f"File {training_file.id} ready after {time.time() - start_time:.2f} seconds.")Once this is done, you can proceed to the model training!5. Fine-tune the model‚ÄãStart the fine-tuning job with your chosen base model.job = openai.FineTuningJob.create(    training_file=training_file.id,    model="gpt-3.5-turbo",)This might take a while. Check the status with openai.FineTuningJob.retrieve(job.id).status and wait for it to report succeeded.# It may take 10-20+ minutes to complete training.status = openai.FineTuningJob.retrieve(job.id).statusstart_time = time.time()while status != "succeeded":    print(f"Status=[{status}]... {time.time() - start_time:.2f}s", end="\r", flush=True)    time.sleep(5)    job = openai.FineTuningJob.retrieve(job.id)    status = job.status6. Use the model in LangChain‚ÄãYou're almost there! Use the fine-tuned model in LangChain.from langchain import chat_modelsmodel_name = job.fine_tuned_model# Example: ft:gpt-3.5-turbo-0613:personal::5mty86jblapsedmodel = chat_models.ChatOpenAI(model=model_name)from langchain.prompts import ChatPromptTemplatefrom langchain.schema.output_parser import StrOutputParser prompt = ChatPromptTemplate.from_messages(    [        ("human", "{input}"),    ])chain = prompt | model | StrOutputParser()for tok in chain.stream({"input": "What classes are you taking?"}):    print(tok, end="", flush=True)# The usual - Potions, Transfiguration, Defense Against the Dark Arts. What about you?API Reference:ChatPromptTemplateStrOutputParserAnd that's it! You've successfully fine-tuned a model and used it in LangChain.Supported Chat Loaders‚ÄãLangChain currently supports the following chat loaders. Feel free to contribute more!üìÑÔ∏è DiscordThis notebook shows how to create your own chat loader that works on copy-pasted messages (from dms) to a list of LangChain messages.üìÑÔ∏è Facebook MessengerThis notebook shows how to load data from Facebook in a format you can finetune on. The overall steps are:üìÑÔ∏è GMailThis loader goes over how to load data from GMail. There are many ways you could want to load data from GMail. This loader is currently fairly opionated in how to do so. The way it does it is it first looks for all messages that you have sent. It then looks for messages where you are responding to a previous email. It then fetches that previous email, and creates a training example of that email, followed by your email.üìÑÔ∏è iMessageThis notebook shows how to use the iMessage chat loader. This class helps convert iMessage conversations to LangChain chat messages.üìÑÔ∏è SlackThis notebook shows how to use the Slack chat loader. This class helps map exported slack conversations to LangChain chat messages.üìÑÔ∏è TelegramThis notebook shows how to use the Telegram chat loader. This class helps map exported Telegram conversations to LangChain chat messages.üìÑÔ∏è Twitter (via Apify)This notebook shows how to load chat messages from Twitter to finetune on. We do this by utilizing Apify.üìÑÔ∏è WhatsAppThis notebook shows how to use the WhatsApp chat loader. This class helps map exported Telegram conversations to LangChain chat messages.PreviousPromptLayer ChatOpenAINextDiscord On this page Chat loadersLike document loaders, chat loaders are utilities designed to help load conversations from popular communication platforms such as Facebook, Slack, Discord, etc. These are loaded into memory as LangChain chat message objects. Such utilities facilitate tasks such as fine-tuning a language model to match your personal style or voice. This brief guide will illustrate the process using OpenAI's fine-tuning API comprised of six steps:Export your Facebook Messenger chat data in a compatible format for your intended chat loader.Load the chat data into memory as LangChain chat message objects. (this is what is covered in each integration notebook in this section of the documentation).Assign a person to the "AI" role and optionally filter, group, and merge messages.Export these acquired messages in a format expected by the fine-tuning API.Upload this data to OpenAI.Fine-tune your model.Implement the fine-tuned model in LangChain.This guide is not wholly comprehensive but is designed to take you through the fundamentals of going from raw data to fine-tuned model.We will demonstrate the procedure through an example of fine-tuning a gpt-3.5-turbo model on Facebook Messenger data. 1. Export your chat data‚ÄãTo export your Facebook messenger data, you can follow the instructions here. JSON formatYou must select "JSON format" (instead of HTML) when exporting your data to be compatible with the current loader.OpenAI requires at least 10 examples to fine-tune your model, but they recommend between 50-100 for more optimal results. An audit is performed to ensure data compliance, so you may have to wait a few minutes for the dataset to become ready for use.import timeimport jsonimport ioimport openaimy_file = io.BytesIO()for group in message_groups:    my_file.write((json.dumps({"messages": group}) + "\n").encode('utf-8'))my_file.seek(0)training_file = openai.File.create(  file=my_file,  purpose='fine-tune')# Wait while the file is processedstatus = openai.File.retrieve(training_file.id).statusstart_time = time.time()while status != "processed":    print(f"Status=[{status}]... {time.time() - start_time:.2f}s", end="\r", flush=True)    time.sleep(5)    status = openai.File.retrieve(training_file.id).statusprint(f"File {training_file.id} ready after {time.time() - start_time:.2f} seconds.")Once this is done, you can proceed to the model training!5. Fine-tune the model‚ÄãStart the fine-tuning job with your chosen base model.job = openai.FineTuningJob.create(    training_file=training_file.id,    model="gpt-3.5-turbo",)This might take a while. Check the status with openai.FineTuningJob.retrieve(job.id).status and wait for it to report succeeded.# It may take 10-20+ minutes to complete training.status = openai.FineTuningJob.retrieve(job.id).statusstart_time = time.time()while status != "succeeded":    print(f"Status=[{status}]... {time.time() - start_time:.2f}s", end="\r", flush=True)    time.sleep(5)    job = openai.FineTuningJob.retrieve(job.id)    status = job.status6. Use the model in LangChain‚ÄãYou're almost there! Use the fine-tuned model in LangChain.from langchain import chat_modelsmodel_name = job.fine_tuned_model# Example: ft:gpt-3.5-turbo-0613:personal::5mty86jblapsedmodel = chat_models.ChatOpenAI(model=model_name)from langchain.prompts import ChatPromptTemplatefrom langchain.schema.output_parser import StrOutputParser prompt = ChatPromptTemplate.from_messages(    [        ("human", "{input}"),    ])chain = prompt | model | StrOutputParser()for tok in chain.stream({"input": "What classes are you taking?"}):    print(tok, end="", flush=True)# The usual - Potions, Transfiguration, Defense Against the Dark Arts. What about you?API Reference:ChatPromptTemplateStrOutputParserAnd that's it! You've successfully fine-tuned a model and used it in LangChain.Supported Chat Loaders‚ÄãLangChain currently supports the following chat loaders. Feel free to contribute more!üìÑÔ∏è DiscordThis notebook shows how to create your own chat loader that works on copy-pasted messages (from dms) to a list of LangChain messages.üìÑÔ∏è Facebook MessengerThis notebook shows how to load data from Facebook in a format you can finetune on. The overall steps are:üìÑÔ∏è GMailThis loader goes over how to load data from GMail. There are many ways you could want to load data from GMail. This loader is currently fairly opionated in how to do so. The way it does it is it first looks for all messages that you have sent. It then looks for messages where you are responding to a previous email. It then fetches that previous email, and creates a training example of that email, followed by your email.üìÑÔ∏è iMessageThis notebook shows how to use the iMessage chat loader. This class helps convert iMessage conversations to LangChain chat messages.üìÑÔ∏è SlackThis notebook shows how to use the Slack chat loader. This class helps map exported slack conversations to LangChain chat messages.üìÑÔ∏è TelegramThis notebook shows how to use the Telegram chat loader. This class helps map exported Telegram conversations to LangChain chat messages.üìÑÔ∏è Twitter (via Apify)This notebook shows how to load chat messages from Twitter to finetune on. We do this by utilizing Apify.üìÑÔ∏è WhatsAppThis notebook shows how to use the WhatsApp chat loader. This class helps map exported Telegram conversations to LangChain chat messages. JSON formatYou must select "JSON format" (instead of HTML) when exporting your data to be compatible with the current loader. JSON format You must select "JSON format" (instead of HTML) when exporting your data to be compatible with the current loader. from langchain.chat_loaders.facebook_messenger import FolderFacebookMessengerChatLoaderloader = FolderFacebookMessengerChatLoader(    path="./facebook_messenger_chats",)chat_sessions = loader.load() from langchain.chat_loaders.facebook_messenger import FolderFacebookMessengerChatLoaderloader = FolderFacebookMessengerChatLoader(    path="./facebook_messenger_chats",)chat_sessions = loader.load()  API Reference:FolderFacebookMessengerChatLoader from langchain.chat_loaders.utils import (    merge_chat_runs,    map_ai_messages,)merged_sessions = merge_chat_runs(chat_sessions)alternating_sessions = list(map_ai_messages(merged_sessions, "My Name")) from langchain.chat_loaders.utils import (    merge_chat_runs,    map_ai_messages,)merged_sessions = merge_chat_runs(chat_sessions)alternating_sessions = list(map_ai_messages(merged_sessions, "My Name"))  from langchain.adapters.openai import convert_messages_for_finetuningopenai_messages = convert_messages_for_finetuning(chat_sessions) from langchain.adapters.openai import convert_messages_for_finetuningopenai_messages = convert_messages_for_finetuning(chat_sessions)  API Reference:convert_messages_for_finetuning chunk_size = 8overlap = 2message_groups = [    conversation_messages[i: i + chunk_size]     for conversation_messages in openai_messages    for i in range(        0, len(conversation_messages) - chunk_size + 1,         chunk_size - overlap)]len(message_groups)# 9 chunk_size = 8overlap = 2message_groups = [    conversation_messages[i: i + chunk_size]     for conversation_messages in openai_messages    for i in range(        0, len(conversation_messages) - chunk_size + 1,         chunk_size - overlap)]len(message_groups)# 9  import timeimport jsonimport ioimport openaimy_file = io.BytesIO()for group in message_groups:    my_file.write((json.dumps({"messages": group}) + "\n").encode('utf-8'))my_file.seek(0)training_file = openai.File.create(  file=my_file,  purpose='fine-tune')# Wait while the file is processedstatus = openai.File.retrieve(training_file.id).statusstart_time = time.time()while status != "processed":    print(f"Status=[{status}]... {time.time() - start_time:.2f}s", end="\r", flush=True)    time.sleep(5)    status = openai.File.retrieve(training_file.id).statusprint(f"File {training_file.id} ready after {time.time() - start_time:.2f} seconds.") import timeimport jsonimport ioimport openaimy_file = io.BytesIO()for group in message_groups:    my_file.write((json.dumps({"messages": group}) + "\n").encode('utf-8'))my_file.seek(0)training_file = openai.File.create(  file=my_file,  purpose='fine-tune')# Wait while the file is processedstatus = openai.File.retrieve(training_file.id).statusstart_time = time.time()while status != "processed":    print(f"Status=[{status}]... {time.time() - start_time:.2f}s", end="\r", flush=True)    time.sleep(5)    status = openai.File.retrieve(training_file.id).statusprint(f"File {training_file.id} ready after {time.time() - start_time:.2f} seconds.")  job = openai.FineTuningJob.create(    training_file=training_file.id,    model="gpt-3.5-turbo",) job = openai.FineTuningJob.create(    training_file=training_file.id,    model="gpt-3.5-turbo",)  # It may take 10-20+ minutes to complete training.status = openai.FineTuningJob.retrieve(job.id).statusstart_time = time.time()while status != "succeeded":    print(f"Status=[{status}]... {time.time() - start_time:.2f}s", end="\r", flush=True)    time.sleep(5)    job = openai.FineTuningJob.retrieve(job.id)    status = job.status # It may take 10-20+ minutes to complete training.status = openai.FineTuningJob.retrieve(job.id).statusstart_time = time.time()while status != "succeeded":    print(f"Status=[{status}]... {time.time() - start_time:.2f}s", end="\r", flush=True)    time.sleep(5)    job = openai.FineTuningJob.retrieve(job.id)    status = job.status  from langchain import chat_modelsmodel_name = job.fine_tuned_model# Example: ft:gpt-3.5-turbo-0613:personal::5mty86jblapsedmodel = chat_models.ChatOpenAI(model=model_name) from langchain import chat_modelsmodel_name = job.fine_tuned_model# Example: ft:gpt-3.5-turbo-0613:personal::5mty86jblapsedmodel = chat_models.ChatOpenAI(model=model_name)  from langchain.prompts import ChatPromptTemplatefrom langchain.schema.output_parser import StrOutputParser prompt = ChatPromptTemplate.from_messages(    [        ("human", "{input}"),    ])chain = prompt | model | StrOutputParser()for tok in chain.stream({"input": "What classes are you taking?"}):    print(tok, end="", flush=True)# The usual - Potions, Transfiguration, Defense Against the Dark Arts. What about you? from langchain.prompts import ChatPromptTemplatefrom langchain.schema.output_parser import StrOutputParser prompt = ChatPromptTemplate.from_messages(    [        ("human", "{input}"),    ])chain = prompt | model | StrOutputParser()for tok in chain.stream({"input": "What classes are you taking?"}):    print(tok, end="", flush=True)# The usual - Potions, Transfiguration, Defense Against the Dark Arts. What about you?  API Reference:ChatPromptTemplateStrOutputParser Previous PromptLayer ChatOpenAI Next Discord 1. Export your chat data2. Load the chat3. Export messages to OpenAI format4. Upload the data to OpenAI5. Fine-tune the model6. Use the model in LangChainSupported Chat Loaders 1. Export your chat data2. Load the chat3. Export messages to OpenAI format4. Upload the data to OpenAI5. Fine-tune the model6. Use the model in LangChainSupported Chat Loaders CommunityDiscordTwitterGitHubPythonJS/TSMoreHomepageBlogCopyright ¬© 2023 LangChain, Inc. CommunityDiscordTwitterGitHubPythonJS/TSMoreHomepageBlog CommunityDiscordTwitter Community GitHubPythonJS/TS GitHub MoreHomepageBlog More Copyright ¬© 2023 LangChain, Inc. Copyright ¬© 2023 LangChain, Inc. Skip to main content (#docusaurus_skipToContent_fallback) ü¶úÔ∏èüîó LangChain (/) Docs (/docs/get_started/introduction) Use cases (/docs/use_cases) Integrations (/docs/integrations) API (https://api.python.langchain.com) LangSmith (https://smith.langchain.com) JS/TS Docs (https://js.langchain.com/docs)  (https://github.com/hwchase17/langchain) Integrations (/docs/integrations) Callbacks (/docs/integrations/callbacks/) Chat models (/docs/integrations/chat/) Chat loaders (/docs/integrations/chat_loaders/) Discord (/docs/integrations/chat_loaders/discord) Facebook Messenger (/docs/integrations/chat_loaders/facebook) GMail (/docs/integrations/chat_loaders/gmail) iMessage (/docs/integrations/chat_loaders/imessage) Slack (/docs/integrations/chat_loaders/slack) Telegram (/docs/integrations/chat_loaders/telegram) Twitter (via Apify) (/docs/integrations/chat_loaders/twitter) WhatsApp (/docs/integrations/chat_loaders/whatsapp) Document loaders (/docs/integrations/document_loaders/) Document transformers (/docs/integrations/document_transformers/) LLMs (/docs/integrations/llms/) Memory (/docs/integrations/memory/) Retrievers (/docs/integrations/retrievers/) Text embedding models (/docs/integrations/text_embedding/) Agents & Toolkits (/docs/integrations/toolkits/) Tools (/docs/integrations/tools/) Vector stores (/docs/integrations/vectorstores/) Grouped by provider (/docs/integrations/providers/)  (/) Integrations (/docs/integrations) OpenAI's fine-tuning API (https://platform.openai.com/docs/guides/fine-tuning) ‚Äã (#1-export-your-chat-data) instructions here (https://www.zapptales.com/en/download-facebook-messenger-chat-history-how-to/) this google drive link (https://drive.google.com/file/d/1rh1s1o2i7B-Sk1v9o8KNgivLVGwJ-osV/view?usp=sharing) ‚Äã (#2-load-the-chat) FolderFacebookMessengerChatLoader (https://api.python.langchain.com/en/latest/chat_loaders/langchain.chat_loaders.facebook_messenger.FolderFacebookMessengerChatLoader.html) ‚Äã (#3-export-messages-to-openai-format) convert_messages_for_finetuning (https://api.python.langchain.com/en/latest/adapters/langchain.adapters.openai.convert_messages_for_finetuning.html) ‚Äã (#4-upload-the-data-to-openai) instructions (https://platform.openai.com/account/api-keys) ‚Äã (#5-fine-tune-the-model) ‚Äã (#6-use-the-model-in-langchain) ChatPromptTemplate (https://api.python.langchain.com/en/latest/prompts/langchain.prompts.chat.ChatPromptTemplate.html) StrOutputParser (https://api.python.langchain.com/en/latest/schema/langchain.schema.output_parser.StrOutputParser.html) ‚Äã (#supported-chat-loaders) üìÑÔ∏è DiscordThis notebook shows how to create your own chat loader that works on copy-pasted messages (from dms) to a list of LangChain messages. (/docs/integrations/chat_loaders/discord) üìÑÔ∏è Facebook MessengerThis notebook shows how to load data from Facebook in a format you can finetune on. The overall steps are: (/docs/integrations/chat_loaders/facebook) üìÑÔ∏è GMailThis loader goes over how to load data from GMail. There are many ways you could want to load data from GMail. This loader is currently fairly opionated in how to do so. The way it does it is it first looks for all messages that you have sent. It then looks for messages where you are responding to a previous email. It then fetches that previous email, and creates a training example of that email, followed by your email. (/docs/integrations/chat_loaders/gmail) üìÑÔ∏è iMessageThis notebook shows how to use the iMessage chat loader. This class helps convert iMessage conversations to LangChain chat messages. (/docs/integrations/chat_loaders/imessage) üìÑÔ∏è SlackThis notebook shows how to use the Slack chat loader. This class helps map exported slack conversations to LangChain chat messages. (/docs/integrations/chat_loaders/slack) üìÑÔ∏è TelegramThis notebook shows how to use the Telegram chat loader. This class helps map exported Telegram conversations to LangChain chat messages. (/docs/integrations/chat_loaders/telegram) üìÑÔ∏è Twitter (via Apify)This notebook shows how to load chat messages from Twitter to finetune on. We do this by utilizing Apify. (/docs/integrations/chat_loaders/twitter) üìÑÔ∏è WhatsAppThis notebook shows how to use the WhatsApp chat loader. This class helps map exported Telegram conversations to LangChain chat messages. (/docs/integrations/chat_loaders/whatsapp) PreviousPromptLayer ChatOpenAI (/docs/integrations/chat/promptlayer_chatopenai) NextDiscord (/docs/integrations/chat_loaders/discord) 1. Export your chat data (#1-export-your-chat-data) 2. Load the chat (#2-load-the-chat) 3. Export messages to OpenAI format (#3-export-messages-to-openai-format) 4. Upload the data to OpenAI (#4-upload-the-data-to-openai) 5. Fine-tune the model (#5-fine-tune-the-model) 6. Use the model in LangChain (#6-use-the-model-in-langchain) Supported Chat Loaders (#supported-chat-loaders) Discord (https://discord.gg/cU2adEyC7w) Twitter (https://twitter.com/LangChainAI) Python (https://github.com/hwchase17/langchain) JS/TS (https://github.com/hwchase17/langchainjs) Homepage (https://langchain.com) Blog (https://blog.langchain.dev)