Note: This is seperate from the Google PaLM integration. Google has chosen to offer an enterprise version of PaLM through GCP, and this supports the models made available through there.  By default, Google Cloud does not use Customer Data to train its foundation models as part of Google Cloud`s AI/ML Privacy Commitment. More details about how Google processes data can also be found in Google's Customer Data Processing Addendum (CDPA). To use Vertex AI PaLM you must have the google-cloud-aiplatform Python package installed and either: This codebase uses the google.auth library which first looks for the application credentials variable mentioned above, and then looks for system-level auth. For more information, see:  You can make use of templating by using a MessagePromptTemplate. You can build a ChatPromptTemplate from one or more MessagePromptTemplates. You can use ChatPromptTemplate's format_prompt -- this returns a PromptValue, which you can convert to a string or Message object, depending on whether you want to use the formatted value as input to an llm or chat model. For convenience, there is a from_template method exposed on the template. If you were to use this template, this is what it would look like: You can now leverage the Codey API for code chat within Vertex AI. The model name is: IntegrationsCallbacksChat modelsAnthropicAnthropic FunctionsAnyscaleAzureAzureML Chat Online EndpointBedrock ChatERNIE-Bot ChatGoogle Cloud Platform Vertex AI PaLMJinaChatKonkoüöÖ LiteLLMLlama APIOllamaOpenAIPromptLayer ChatOpenAIChat loadersDocument loadersDocument transformersLLMsMemoryRetrieversText embedding modelsAgents & ToolkitsToolsVector storesGrouped by provider Callbacks Chat modelsAnthropicAnthropic FunctionsAnyscaleAzureAzureML Chat Online EndpointBedrock ChatERNIE-Bot ChatGoogle Cloud Platform Vertex AI PaLMJinaChatKonkoüöÖ LiteLLMLlama APIOllamaOpenAIPromptLayer ChatOpenAI Anthropic Anthropic Functions Anyscale Azure AzureML Chat Online Endpoint Bedrock Chat ERNIE-Bot Chat Google Cloud Platform Vertex AI PaLM JinaChat Konko üöÖ LiteLLM Llama API Ollama OpenAI PromptLayer ChatOpenAI Chat loaders Document loaders Document transformers LLMs Memory Retrievers Text embedding models Agents & Toolkits Tools Vector stores Grouped by provider  Integrations Chat models Google Cloud Platform Vertex AI PaLM Have credentials configured for your environment (gcloud, workload identity, etc...) Store the path to a service account JSON file as the GOOGLE_APPLICATION_CREDENTIALS environment variable https://cloud.google.com/docs/authentication/application-default-credentials#GAC https://googleapis.dev/python/google-auth/latest/reference/google.auth.html#module-google.auth ChatVertexAI ChatPromptTemplate SystemMessagePromptTemplate HumanMessagePromptTemplate HumanMessage SystemMessage codechat-bison: for code assistance Discord Twitter Python JS/TS Homepage Blog Skip to main contentü¶úÔ∏èüîó LangChainDocsUse casesIntegrationsAPILangSmithJS/TS DocsCTRLKIntegrationsCallbacksChat modelsAnthropicAnthropic FunctionsAnyscaleAzureAzureML Chat Online EndpointBedrock ChatERNIE-Bot ChatGoogle Cloud Platform Vertex AI PaLMJinaChatKonkoüöÖ LiteLLMLlama APIOllamaOpenAIPromptLayer ChatOpenAIChat loadersDocument loadersDocument transformersLLMsMemoryRetrieversText embedding modelsAgents & ToolkitsToolsVector storesGrouped by providerIntegrationsChat modelsGoogle Cloud Platform Vertex AI PaLMGoogle Cloud Platform Vertex AI PaLMNote: This is seperate from the Google PaLM integration. Google has chosen to offer an enterprise version of PaLM through GCP, and this supports the models made available through there. By default, Google Cloud does not use Customer Data to train its foundation models as part of Google Cloud`s AI/ML Privacy Commitment. More details about how Google processes data can also be found in Google's Customer Data Processing Addendum (CDPA).To use Vertex AI PaLM you must have the google-cloud-aiplatform Python package installed and either:Have credentials configured for your environment (gcloud, workload identity, etc...)Store the path to a service account JSON file as the GOOGLE_APPLICATION_CREDENTIALS environment variableThis codebase uses the google.auth library which first looks for the application credentials variable mentioned above, and then looks for system-level auth.For more information, see: https://cloud.google.com/docs/authentication/application-default-credentials#GAChttps://googleapis.dev/python/google-auth/latest/reference/google.auth.html#module-google.auth#!pip install google-cloud-aiplatformfrom langchain.chat_models import ChatVertexAIfrom langchain.prompts.chat import (    ChatPromptTemplate,    SystemMessagePromptTemplate,    HumanMessagePromptTemplate,)from langchain.schema import HumanMessage, SystemMessageAPI Reference:ChatVertexAIChatPromptTemplateSystemMessagePromptTemplateHumanMessagePromptTemplateHumanMessageSystemMessagechat = ChatVertexAI()messages = [    SystemMessage(        content="You are a helpful assistant that translates English to French."    ),    HumanMessage(        content="Translate this sentence from English to French. I love programming."    ),]chat(messages)    AIMessage(content='Sure, here is the translation of the sentence "I love programming" from English to French:\n\nJ\'aime programmer.', additional_kwargs={}, example=False)You can make use of templating by using a MessagePromptTemplate. You can build a ChatPromptTemplate from one or more MessagePromptTemplates. You can use ChatPromptTemplate's format_prompt -- this returns a PromptValue, which you can convert to a string or Message object, depending on whether you want to use the formatted value as input to an llm or chat model.For convenience, there is a from_template method exposed on the template. If you were to use this template, this is what it would look like:template = (    "You are a helpful assistant that translates {input_language} to {output_language}.")system_message_prompt = SystemMessagePromptTemplate.from_template(template)human_template = "{text}"human_message_prompt = HumanMessagePromptTemplate.from_template(human_template)chat_prompt = ChatPromptTemplate.from_messages(    [system_message_prompt, human_message_prompt])# get a chat completion from the formatted messageschat(    chat_prompt.format_prompt(        input_language="English", output_language="French", text="I love programming."    ).to_messages())    AIMessage(content='Sure, here is the translation of "I love programming" in French:\n\nJ\'aime programmer.', additional_kwargs={}, example=False)You can now leverage the Codey API for code chat within Vertex AI. The model name is:codechat-bison: for code assistancechat = ChatVertexAI(model_name="codechat-bison")messages = [    HumanMessage(        content="How do I create a python function to identify all prime numbers?"    )]chat(messages)    AIMessage(content='The following Python function can be used to identify all prime numbers up to a given integer:\n\n```\ndef is_prime(n):\n  """\n  Determines whether the given integer is prime.\n\n  Args:\n    n: The integer to be tested for primality.\n\n  Returns:\n    True if n is prime, False otherwise.\n  """\n\n  # Check if n is divisible by 2.\n  if n % 2 == 0:\n    return False\n\n  # Check if n is divisible by any integer from 3 to the square root', additional_kwargs={}, example=False)PreviousERNIE-Bot ChatNextJinaChatCommunityDiscordTwitterGitHubPythonJS/TSMoreHomepageBlogCopyright ¬© 2023 LangChain, Inc. Skip to main content ü¶úÔ∏èüîó LangChainDocsUse casesIntegrationsAPILangSmithJS/TS DocsCTRLK ü¶úÔ∏èüîó LangChainDocsUse casesIntegrationsAPI LangSmithJS/TS DocsCTRLK  CTRLK CTRLK  CTRLK   IntegrationsCallbacksChat modelsAnthropicAnthropic FunctionsAnyscaleAzureAzureML Chat Online EndpointBedrock ChatERNIE-Bot ChatGoogle Cloud Platform Vertex AI PaLMJinaChatKonkoüöÖ LiteLLMLlama APIOllamaOpenAIPromptLayer ChatOpenAIChat loadersDocument loadersDocument transformersLLMsMemoryRetrieversText embedding modelsAgents & ToolkitsToolsVector storesGrouped by providerIntegrationsChat modelsGoogle Cloud Platform Vertex AI PaLMGoogle Cloud Platform Vertex AI PaLMNote: This is seperate from the Google PaLM integration. Google has chosen to offer an enterprise version of PaLM through GCP, and this supports the models made available through there. By default, Google Cloud does not use Customer Data to train its foundation models as part of Google Cloud`s AI/ML Privacy Commitment. More details about how Google processes data can also be found in Google's Customer Data Processing Addendum (CDPA).To use Vertex AI PaLM you must have the google-cloud-aiplatform Python package installed and either:Have credentials configured for your environment (gcloud, workload identity, etc...)Store the path to a service account JSON file as the GOOGLE_APPLICATION_CREDENTIALS environment variableThis codebase uses the google.auth library which first looks for the application credentials variable mentioned above, and then looks for system-level auth.For more information, see: https://cloud.google.com/docs/authentication/application-default-credentials#GAChttps://googleapis.dev/python/google-auth/latest/reference/google.auth.html#module-google.auth#!pip install google-cloud-aiplatformfrom langchain.chat_models import ChatVertexAIfrom langchain.prompts.chat import (    ChatPromptTemplate,    SystemMessagePromptTemplate,    HumanMessagePromptTemplate,)from langchain.schema import HumanMessage, SystemMessageAPI Reference:ChatVertexAIChatPromptTemplateSystemMessagePromptTemplateHumanMessagePromptTemplateHumanMessageSystemMessagechat = ChatVertexAI()messages = [    SystemMessage(        content="You are a helpful assistant that translates English to French."    ),    HumanMessage(        content="Translate this sentence from English to French. I love programming."    ),]chat(messages)    AIMessage(content='Sure, here is the translation of the sentence "I love programming" from English to French:\n\nJ\'aime programmer.', additional_kwargs={}, example=False)You can make use of templating by using a MessagePromptTemplate. You can build a ChatPromptTemplate from one or more MessagePromptTemplates. You can use ChatPromptTemplate's format_prompt -- this returns a PromptValue, which you can convert to a string or Message object, depending on whether you want to use the formatted value as input to an llm or chat model.For convenience, there is a from_template method exposed on the template. If you were to use this template, this is what it would look like:template = (    "You are a helpful assistant that translates {input_language} to {output_language}.")system_message_prompt = SystemMessagePromptTemplate.from_template(template)human_template = "{text}"human_message_prompt = HumanMessagePromptTemplate.from_template(human_template)chat_prompt = ChatPromptTemplate.from_messages(    [system_message_prompt, human_message_prompt])# get a chat completion from the formatted messageschat(    chat_prompt.format_prompt(        input_language="English", output_language="French", text="I love programming."    ).to_messages())    AIMessage(content='Sure, here is the translation of "I love programming" in French:\n\nJ\'aime programmer.', additional_kwargs={}, example=False)You can now leverage the Codey API for code chat within Vertex AI. The model name is:codechat-bison: for code assistancechat = ChatVertexAI(model_name="codechat-bison")messages = [    HumanMessage(        content="How do I create a python function to identify all prime numbers?"    )]chat(messages)    AIMessage(content='The following Python function can be used to identify all prime numbers up to a given integer:\n\n```\ndef is_prime(n):\n  """\n  Determines whether the given integer is prime.\n\n  Args:\n    n: The integer to be tested for primality.\n\n  Returns:\n    True if n is prime, False otherwise.\n  """\n\n  # Check if n is divisible by 2.\n  if n % 2 == 0:\n    return False\n\n  # Check if n is divisible by any integer from 3 to the square root', additional_kwargs={}, example=False)PreviousERNIE-Bot ChatNextJinaChat IntegrationsCallbacksChat modelsAnthropicAnthropic FunctionsAnyscaleAzureAzureML Chat Online EndpointBedrock ChatERNIE-Bot ChatGoogle Cloud Platform Vertex AI PaLMJinaChatKonkoüöÖ LiteLLMLlama APIOllamaOpenAIPromptLayer ChatOpenAIChat loadersDocument loadersDocument transformersLLMsMemoryRetrieversText embedding modelsAgents & ToolkitsToolsVector storesGrouped by providerIntegrationsChat modelsGoogle Cloud Platform Vertex AI PaLMGoogle Cloud Platform Vertex AI PaLMNote: This is seperate from the Google PaLM integration. Google has chosen to offer an enterprise version of PaLM through GCP, and this supports the models made available through there. By default, Google Cloud does not use Customer Data to train its foundation models as part of Google Cloud`s AI/ML Privacy Commitment. More details about how Google processes data can also be found in Google's Customer Data Processing Addendum (CDPA).To use Vertex AI PaLM you must have the google-cloud-aiplatform Python package installed and either:Have credentials configured for your environment (gcloud, workload identity, etc...)Store the path to a service account JSON file as the GOOGLE_APPLICATION_CREDENTIALS environment variableThis codebase uses the google.auth library which first looks for the application credentials variable mentioned above, and then looks for system-level auth.For more information, see: https://cloud.google.com/docs/authentication/application-default-credentials#GAChttps://googleapis.dev/python/google-auth/latest/reference/google.auth.html#module-google.auth#!pip install google-cloud-aiplatformfrom langchain.chat_models import ChatVertexAIfrom langchain.prompts.chat import (    ChatPromptTemplate,    SystemMessagePromptTemplate,    HumanMessagePromptTemplate,)from langchain.schema import HumanMessage, SystemMessageAPI Reference:ChatVertexAIChatPromptTemplateSystemMessagePromptTemplateHumanMessagePromptTemplateHumanMessageSystemMessagechat = ChatVertexAI()messages = [    SystemMessage(        content="You are a helpful assistant that translates English to French."    ),    HumanMessage(        content="Translate this sentence from English to French. I love programming."    ),]chat(messages)    AIMessage(content='Sure, here is the translation of the sentence "I love programming" from English to French:\n\nJ\'aime programmer.', additional_kwargs={}, example=False)You can make use of templating by using a MessagePromptTemplate. You can build a ChatPromptTemplate from one or more MessagePromptTemplates. You can use ChatPromptTemplate's format_prompt -- this returns a PromptValue, which you can convert to a string or Message object, depending on whether you want to use the formatted value as input to an llm or chat model.For convenience, there is a from_template method exposed on the template. If you were to use this template, this is what it would look like:template = (    "You are a helpful assistant that translates {input_language} to {output_language}.")system_message_prompt = SystemMessagePromptTemplate.from_template(template)human_template = "{text}"human_message_prompt = HumanMessagePromptTemplate.from_template(human_template)chat_prompt = ChatPromptTemplate.from_messages(    [system_message_prompt, human_message_prompt])# get a chat completion from the formatted messageschat(    chat_prompt.format_prompt(        input_language="English", output_language="French", text="I love programming."    ).to_messages())    AIMessage(content='Sure, here is the translation of "I love programming" in French:\n\nJ\'aime programmer.', additional_kwargs={}, example=False)You can now leverage the Codey API for code chat within Vertex AI. The model name is:codechat-bison: for code assistancechat = ChatVertexAI(model_name="codechat-bison")messages = [    HumanMessage(        content="How do I create a python function to identify all prime numbers?"    )]chat(messages)    AIMessage(content='The following Python function can be used to identify all prime numbers up to a given integer:\n\n```\ndef is_prime(n):\n  """\n  Determines whether the given integer is prime.\n\n  Args:\n    n: The integer to be tested for primality.\n\n  Returns:\n    True if n is prime, False otherwise.\n  """\n\n  # Check if n is divisible by 2.\n  if n % 2 == 0:\n    return False\n\n  # Check if n is divisible by any integer from 3 to the square root', additional_kwargs={}, example=False)PreviousERNIE-Bot ChatNextJinaChat IntegrationsCallbacksChat modelsAnthropicAnthropic FunctionsAnyscaleAzureAzureML Chat Online EndpointBedrock ChatERNIE-Bot ChatGoogle Cloud Platform Vertex AI PaLMJinaChatKonkoüöÖ LiteLLMLlama APIOllamaOpenAIPromptLayer ChatOpenAIChat loadersDocument loadersDocument transformersLLMsMemoryRetrieversText embedding modelsAgents & ToolkitsToolsVector storesGrouped by provider IntegrationsCallbacksChat modelsAnthropicAnthropic FunctionsAnyscaleAzureAzureML Chat Online EndpointBedrock ChatERNIE-Bot ChatGoogle Cloud Platform Vertex AI PaLMJinaChatKonkoüöÖ LiteLLMLlama APIOllamaOpenAIPromptLayer ChatOpenAIChat loadersDocument loadersDocument transformersLLMsMemoryRetrieversText embedding modelsAgents & ToolkitsToolsVector storesGrouped by provider Integrations Callbacks Chat models Chat loaders Document loaders Document transformers LLMs Memory Retrievers Text embedding models Agents & Toolkits Tools Vector stores Grouped by provider IntegrationsChat modelsGoogle Cloud Platform Vertex AI PaLMGoogle Cloud Platform Vertex AI PaLMNote: This is seperate from the Google PaLM integration. Google has chosen to offer an enterprise version of PaLM through GCP, and this supports the models made available through there. By default, Google Cloud does not use Customer Data to train its foundation models as part of Google Cloud`s AI/ML Privacy Commitment. More details about how Google processes data can also be found in Google's Customer Data Processing Addendum (CDPA).To use Vertex AI PaLM you must have the google-cloud-aiplatform Python package installed and either:Have credentials configured for your environment (gcloud, workload identity, etc...)Store the path to a service account JSON file as the GOOGLE_APPLICATION_CREDENTIALS environment variableThis codebase uses the google.auth library which first looks for the application credentials variable mentioned above, and then looks for system-level auth.For more information, see: https://cloud.google.com/docs/authentication/application-default-credentials#GAChttps://googleapis.dev/python/google-auth/latest/reference/google.auth.html#module-google.auth#!pip install google-cloud-aiplatformfrom langchain.chat_models import ChatVertexAIfrom langchain.prompts.chat import (    ChatPromptTemplate,    SystemMessagePromptTemplate,    HumanMessagePromptTemplate,)from langchain.schema import HumanMessage, SystemMessageAPI Reference:ChatVertexAIChatPromptTemplateSystemMessagePromptTemplateHumanMessagePromptTemplateHumanMessageSystemMessagechat = ChatVertexAI()messages = [    SystemMessage(        content="You are a helpful assistant that translates English to French."    ),    HumanMessage(        content="Translate this sentence from English to French. I love programming."    ),]chat(messages)    AIMessage(content='Sure, here is the translation of the sentence "I love programming" from English to French:\n\nJ\'aime programmer.', additional_kwargs={}, example=False)You can make use of templating by using a MessagePromptTemplate. You can build a ChatPromptTemplate from one or more MessagePromptTemplates. You can use ChatPromptTemplate's format_prompt -- this returns a PromptValue, which you can convert to a string or Message object, depending on whether you want to use the formatted value as input to an llm or chat model.For convenience, there is a from_template method exposed on the template. If you were to use this template, this is what it would look like:template = (    "You are a helpful assistant that translates {input_language} to {output_language}.")system_message_prompt = SystemMessagePromptTemplate.from_template(template)human_template = "{text}"human_message_prompt = HumanMessagePromptTemplate.from_template(human_template)chat_prompt = ChatPromptTemplate.from_messages(    [system_message_prompt, human_message_prompt])# get a chat completion from the formatted messageschat(    chat_prompt.format_prompt(        input_language="English", output_language="French", text="I love programming."    ).to_messages())    AIMessage(content='Sure, here is the translation of "I love programming" in French:\n\nJ\'aime programmer.', additional_kwargs={}, example=False)You can now leverage the Codey API for code chat within Vertex AI. The model name is:codechat-bison: for code assistancechat = ChatVertexAI(model_name="codechat-bison")messages = [    HumanMessage(        content="How do I create a python function to identify all prime numbers?"    )]chat(messages)    AIMessage(content='The following Python function can be used to identify all prime numbers up to a given integer:\n\n```\ndef is_prime(n):\n  """\n  Determines whether the given integer is prime.\n\n  Args:\n    n: The integer to be tested for primality.\n\n  Returns:\n    True if n is prime, False otherwise.\n  """\n\n  # Check if n is divisible by 2.\n  if n % 2 == 0:\n    return False\n\n  # Check if n is divisible by any integer from 3 to the square root', additional_kwargs={}, example=False)PreviousERNIE-Bot ChatNextJinaChat IntegrationsChat modelsGoogle Cloud Platform Vertex AI PaLMGoogle Cloud Platform Vertex AI PaLMNote: This is seperate from the Google PaLM integration. Google has chosen to offer an enterprise version of PaLM through GCP, and this supports the models made available through there. By default, Google Cloud does not use Customer Data to train its foundation models as part of Google Cloud`s AI/ML Privacy Commitment. More details about how Google processes data can also be found in Google's Customer Data Processing Addendum (CDPA).To use Vertex AI PaLM you must have the google-cloud-aiplatform Python package installed and either:Have credentials configured for your environment (gcloud, workload identity, etc...)Store the path to a service account JSON file as the GOOGLE_APPLICATION_CREDENTIALS environment variableThis codebase uses the google.auth library which first looks for the application credentials variable mentioned above, and then looks for system-level auth.For more information, see: https://cloud.google.com/docs/authentication/application-default-credentials#GAChttps://googleapis.dev/python/google-auth/latest/reference/google.auth.html#module-google.auth#!pip install google-cloud-aiplatformfrom langchain.chat_models import ChatVertexAIfrom langchain.prompts.chat import (    ChatPromptTemplate,    SystemMessagePromptTemplate,    HumanMessagePromptTemplate,)from langchain.schema import HumanMessage, SystemMessageAPI Reference:ChatVertexAIChatPromptTemplateSystemMessagePromptTemplateHumanMessagePromptTemplateHumanMessageSystemMessagechat = ChatVertexAI()messages = [    SystemMessage(        content="You are a helpful assistant that translates English to French."    ),    HumanMessage(        content="Translate this sentence from English to French. I love programming."    ),]chat(messages)    AIMessage(content='Sure, here is the translation of the sentence "I love programming" from English to French:\n\nJ\'aime programmer.', additional_kwargs={}, example=False)You can make use of templating by using a MessagePromptTemplate. You can build a ChatPromptTemplate from one or more MessagePromptTemplates. You can use ChatPromptTemplate's format_prompt -- this returns a PromptValue, which you can convert to a string or Message object, depending on whether you want to use the formatted value as input to an llm or chat model.For convenience, there is a from_template method exposed on the template. If you were to use this template, this is what it would look like:template = (    "You are a helpful assistant that translates {input_language} to {output_language}.")system_message_prompt = SystemMessagePromptTemplate.from_template(template)human_template = "{text}"human_message_prompt = HumanMessagePromptTemplate.from_template(human_template)chat_prompt = ChatPromptTemplate.from_messages(    [system_message_prompt, human_message_prompt])# get a chat completion from the formatted messageschat(    chat_prompt.format_prompt(        input_language="English", output_language="French", text="I love programming."    ).to_messages())    AIMessage(content='Sure, here is the translation of "I love programming" in French:\n\nJ\'aime programmer.', additional_kwargs={}, example=False)You can now leverage the Codey API for code chat within Vertex AI. The model name is:codechat-bison: for code assistancechat = ChatVertexAI(model_name="codechat-bison")messages = [    HumanMessage(        content="How do I create a python function to identify all prime numbers?"    )]chat(messages)    AIMessage(content='The following Python function can be used to identify all prime numbers up to a given integer:\n\n```\ndef is_prime(n):\n  """\n  Determines whether the given integer is prime.\n\n  Args:\n    n: The integer to be tested for primality.\n\n  Returns:\n    True if n is prime, False otherwise.\n  """\n\n  # Check if n is divisible by 2.\n  if n % 2 == 0:\n    return False\n\n  # Check if n is divisible by any integer from 3 to the square root', additional_kwargs={}, example=False)PreviousERNIE-Bot ChatNextJinaChat IntegrationsChat modelsGoogle Cloud Platform Vertex AI PaLMGoogle Cloud Platform Vertex AI PaLMNote: This is seperate from the Google PaLM integration. Google has chosen to offer an enterprise version of PaLM through GCP, and this supports the models made available through there. By default, Google Cloud does not use Customer Data to train its foundation models as part of Google Cloud`s AI/ML Privacy Commitment. More details about how Google processes data can also be found in Google's Customer Data Processing Addendum (CDPA).To use Vertex AI PaLM you must have the google-cloud-aiplatform Python package installed and either:Have credentials configured for your environment (gcloud, workload identity, etc...)Store the path to a service account JSON file as the GOOGLE_APPLICATION_CREDENTIALS environment variableThis codebase uses the google.auth library which first looks for the application credentials variable mentioned above, and then looks for system-level auth.For more information, see: https://cloud.google.com/docs/authentication/application-default-credentials#GAChttps://googleapis.dev/python/google-auth/latest/reference/google.auth.html#module-google.auth#!pip install google-cloud-aiplatformfrom langchain.chat_models import ChatVertexAIfrom langchain.prompts.chat import (    ChatPromptTemplate,    SystemMessagePromptTemplate,    HumanMessagePromptTemplate,)from langchain.schema import HumanMessage, SystemMessageAPI Reference:ChatVertexAIChatPromptTemplateSystemMessagePromptTemplateHumanMessagePromptTemplateHumanMessageSystemMessagechat = ChatVertexAI()messages = [    SystemMessage(        content="You are a helpful assistant that translates English to French."    ),    HumanMessage(        content="Translate this sentence from English to French. I love programming."    ),]chat(messages)    AIMessage(content='Sure, here is the translation of the sentence "I love programming" from English to French:\n\nJ\'aime programmer.', additional_kwargs={}, example=False)You can make use of templating by using a MessagePromptTemplate. You can build a ChatPromptTemplate from one or more MessagePromptTemplates. You can use ChatPromptTemplate's format_prompt -- this returns a PromptValue, which you can convert to a string or Message object, depending on whether you want to use the formatted value as input to an llm or chat model.For convenience, there is a from_template method exposed on the template. If you were to use this template, this is what it would look like:template = (    "You are a helpful assistant that translates {input_language} to {output_language}.")system_message_prompt = SystemMessagePromptTemplate.from_template(template)human_template = "{text}"human_message_prompt = HumanMessagePromptTemplate.from_template(human_template)chat_prompt = ChatPromptTemplate.from_messages(    [system_message_prompt, human_message_prompt])# get a chat completion from the formatted messageschat(    chat_prompt.format_prompt(        input_language="English", output_language="French", text="I love programming."    ).to_messages())    AIMessage(content='Sure, here is the translation of "I love programming" in French:\n\nJ\'aime programmer.', additional_kwargs={}, example=False)You can now leverage the Codey API for code chat within Vertex AI. The model name is:codechat-bison: for code assistancechat = ChatVertexAI(model_name="codechat-bison")messages = [    HumanMessage(        content="How do I create a python function to identify all prime numbers?"    )]chat(messages)    AIMessage(content='The following Python function can be used to identify all prime numbers up to a given integer:\n\n```\ndef is_prime(n):\n  """\n  Determines whether the given integer is prime.\n\n  Args:\n    n: The integer to be tested for primality.\n\n  Returns:\n    True if n is prime, False otherwise.\n  """\n\n  # Check if n is divisible by 2.\n  if n % 2 == 0:\n    return False\n\n  # Check if n is divisible by any integer from 3 to the square root', additional_kwargs={}, example=False)PreviousERNIE-Bot ChatNextJinaChat IntegrationsChat modelsGoogle Cloud Platform Vertex AI PaLMGoogle Cloud Platform Vertex AI PaLMNote: This is seperate from the Google PaLM integration. Google has chosen to offer an enterprise version of PaLM through GCP, and this supports the models made available through there. By default, Google Cloud does not use Customer Data to train its foundation models as part of Google Cloud`s AI/ML Privacy Commitment. More details about how Google processes data can also be found in Google's Customer Data Processing Addendum (CDPA).To use Vertex AI PaLM you must have the google-cloud-aiplatform Python package installed and either:Have credentials configured for your environment (gcloud, workload identity, etc...)Store the path to a service account JSON file as the GOOGLE_APPLICATION_CREDENTIALS environment variableThis codebase uses the google.auth library which first looks for the application credentials variable mentioned above, and then looks for system-level auth.For more information, see: https://cloud.google.com/docs/authentication/application-default-credentials#GAChttps://googleapis.dev/python/google-auth/latest/reference/google.auth.html#module-google.auth#!pip install google-cloud-aiplatformfrom langchain.chat_models import ChatVertexAIfrom langchain.prompts.chat import (    ChatPromptTemplate,    SystemMessagePromptTemplate,    HumanMessagePromptTemplate,)from langchain.schema import HumanMessage, SystemMessageAPI Reference:ChatVertexAIChatPromptTemplateSystemMessagePromptTemplateHumanMessagePromptTemplateHumanMessageSystemMessagechat = ChatVertexAI()messages = [    SystemMessage(        content="You are a helpful assistant that translates English to French."    ),    HumanMessage(        content="Translate this sentence from English to French. I love programming."    ),]chat(messages)    AIMessage(content='Sure, here is the translation of the sentence "I love programming" from English to French:\n\nJ\'aime programmer.', additional_kwargs={}, example=False)You can make use of templating by using a MessagePromptTemplate. You can build a ChatPromptTemplate from one or more MessagePromptTemplates. You can use ChatPromptTemplate's format_prompt -- this returns a PromptValue, which you can convert to a string or Message object, depending on whether you want to use the formatted value as input to an llm or chat model.For convenience, there is a from_template method exposed on the template. If you were to use this template, this is what it would look like:template = (    "You are a helpful assistant that translates {input_language} to {output_language}.")system_message_prompt = SystemMessagePromptTemplate.from_template(template)human_template = "{text}"human_message_prompt = HumanMessagePromptTemplate.from_template(human_template)chat_prompt = ChatPromptTemplate.from_messages(    [system_message_prompt, human_message_prompt])# get a chat completion from the formatted messageschat(    chat_prompt.format_prompt(        input_language="English", output_language="French", text="I love programming."    ).to_messages())    AIMessage(content='Sure, here is the translation of "I love programming" in French:\n\nJ\'aime programmer.', additional_kwargs={}, example=False)You can now leverage the Codey API for code chat within Vertex AI. The model name is:codechat-bison: for code assistancechat = ChatVertexAI(model_name="codechat-bison")messages = [    HumanMessage(        content="How do I create a python function to identify all prime numbers?"    )]chat(messages)    AIMessage(content='The following Python function can be used to identify all prime numbers up to a given integer:\n\n```\ndef is_prime(n):\n  """\n  Determines whether the given integer is prime.\n\n  Args:\n    n: The integer to be tested for primality.\n\n  Returns:\n    True if n is prime, False otherwise.\n  """\n\n  # Check if n is divisible by 2.\n  if n % 2 == 0:\n    return False\n\n  # Check if n is divisible by any integer from 3 to the square root', additional_kwargs={}, example=False)PreviousERNIE-Bot ChatNextJinaChat Google Cloud Platform Vertex AI PaLMNote: This is seperate from the Google PaLM integration. Google has chosen to offer an enterprise version of PaLM through GCP, and this supports the models made available through there. By default, Google Cloud does not use Customer Data to train its foundation models as part of Google Cloud`s AI/ML Privacy Commitment. More details about how Google processes data can also be found in Google's Customer Data Processing Addendum (CDPA).To use Vertex AI PaLM you must have the google-cloud-aiplatform Python package installed and either:Have credentials configured for your environment (gcloud, workload identity, etc...)Store the path to a service account JSON file as the GOOGLE_APPLICATION_CREDENTIALS environment variableThis codebase uses the google.auth library which first looks for the application credentials variable mentioned above, and then looks for system-level auth.For more information, see: https://cloud.google.com/docs/authentication/application-default-credentials#GAChttps://googleapis.dev/python/google-auth/latest/reference/google.auth.html#module-google.auth#!pip install google-cloud-aiplatformfrom langchain.chat_models import ChatVertexAIfrom langchain.prompts.chat import (    ChatPromptTemplate,    SystemMessagePromptTemplate,    HumanMessagePromptTemplate,)from langchain.schema import HumanMessage, SystemMessageAPI Reference:ChatVertexAIChatPromptTemplateSystemMessagePromptTemplateHumanMessagePromptTemplateHumanMessageSystemMessagechat = ChatVertexAI()messages = [    SystemMessage(        content="You are a helpful assistant that translates English to French."    ),    HumanMessage(        content="Translate this sentence from English to French. I love programming."    ),]chat(messages)    AIMessage(content='Sure, here is the translation of the sentence "I love programming" from English to French:\n\nJ\'aime programmer.', additional_kwargs={}, example=False)You can make use of templating by using a MessagePromptTemplate. You can build a ChatPromptTemplate from one or more MessagePromptTemplates. You can use ChatPromptTemplate's format_prompt -- this returns a PromptValue, which you can convert to a string or Message object, depending on whether you want to use the formatted value as input to an llm or chat model.For convenience, there is a from_template method exposed on the template. If you were to use this template, this is what it would look like:template = (    "You are a helpful assistant that translates {input_language} to {output_language}.")system_message_prompt = SystemMessagePromptTemplate.from_template(template)human_template = "{text}"human_message_prompt = HumanMessagePromptTemplate.from_template(human_template)chat_prompt = ChatPromptTemplate.from_messages(    [system_message_prompt, human_message_prompt])# get a chat completion from the formatted messageschat(    chat_prompt.format_prompt(        input_language="English", output_language="French", text="I love programming."    ).to_messages())    AIMessage(content='Sure, here is the translation of "I love programming" in French:\n\nJ\'aime programmer.', additional_kwargs={}, example=False)You can now leverage the Codey API for code chat within Vertex AI. The model name is:codechat-bison: for code assistancechat = ChatVertexAI(model_name="codechat-bison")messages = [    HumanMessage(        content="How do I create a python function to identify all prime numbers?"    )]chat(messages)    AIMessage(content='The following Python function can be used to identify all prime numbers up to a given integer:\n\n```\ndef is_prime(n):\n  """\n  Determines whether the given integer is prime.\n\n  Args:\n    n: The integer to be tested for primality.\n\n  Returns:\n    True if n is prime, False otherwise.\n  """\n\n  # Check if n is divisible by 2.\n  if n % 2 == 0:\n    return False\n\n  # Check if n is divisible by any integer from 3 to the square root', additional_kwargs={}, example=False) #!pip install google-cloud-aiplatform #!pip install google-cloud-aiplatform  from langchain.chat_models import ChatVertexAIfrom langchain.prompts.chat import (    ChatPromptTemplate,    SystemMessagePromptTemplate,    HumanMessagePromptTemplate,)from langchain.schema import HumanMessage, SystemMessage from langchain.chat_models import ChatVertexAIfrom langchain.prompts.chat import (    ChatPromptTemplate,    SystemMessagePromptTemplate,    HumanMessagePromptTemplate,)from langchain.schema import HumanMessage, SystemMessage  API Reference:ChatVertexAIChatPromptTemplateSystemMessagePromptTemplateHumanMessagePromptTemplateHumanMessageSystemMessage chat = ChatVertexAI() chat = ChatVertexAI()  messages = [    SystemMessage(        content="You are a helpful assistant that translates English to French."    ),    HumanMessage(        content="Translate this sentence from English to French. I love programming."    ),]chat(messages) messages = [    SystemMessage(        content="You are a helpful assistant that translates English to French."    ),    HumanMessage(        content="Translate this sentence from English to French. I love programming."    ),]chat(messages)      AIMessage(content='Sure, here is the translation of the sentence "I love programming" from English to French:\n\nJ\'aime programmer.', additional_kwargs={}, example=False)     AIMessage(content='Sure, here is the translation of the sentence "I love programming" from English to French:\n\nJ\'aime programmer.', additional_kwargs={}, example=False)     AIMessage(content='Sure, here is the translation of the sentence "I love programming" from English to French:\n\nJ\'aime programmer.', additional_kwargs={}, example=False)  template = (    "You are a helpful assistant that translates {input_language} to {output_language}.")system_message_prompt = SystemMessagePromptTemplate.from_template(template)human_template = "{text}"human_message_prompt = HumanMessagePromptTemplate.from_template(human_template) template = (    "You are a helpful assistant that translates {input_language} to {output_language}.")system_message_prompt = SystemMessagePromptTemplate.from_template(template)human_template = "{text}"human_message_prompt = HumanMessagePromptTemplate.from_template(human_template)  chat_prompt = ChatPromptTemplate.from_messages(    [system_message_prompt, human_message_prompt])# get a chat completion from the formatted messageschat(    chat_prompt.format_prompt(        input_language="English", output_language="French", text="I love programming."    ).to_messages()) chat_prompt = ChatPromptTemplate.from_messages(    [system_message_prompt, human_message_prompt])# get a chat completion from the formatted messageschat(    chat_prompt.format_prompt(        input_language="English", output_language="French", text="I love programming."    ).to_messages())      AIMessage(content='Sure, here is the translation of "I love programming" in French:\n\nJ\'aime programmer.', additional_kwargs={}, example=False)     AIMessage(content='Sure, here is the translation of "I love programming" in French:\n\nJ\'aime programmer.', additional_kwargs={}, example=False)     AIMessage(content='Sure, here is the translation of "I love programming" in French:\n\nJ\'aime programmer.', additional_kwargs={}, example=False)  chat = ChatVertexAI(model_name="codechat-bison") chat = ChatVertexAI(model_name="codechat-bison")  messages = [    HumanMessage(        content="How do I create a python function to identify all prime numbers?"    )]chat(messages) messages = [    HumanMessage(        content="How do I create a python function to identify all prime numbers?"    )]chat(messages)      AIMessage(content='The following Python function can be used to identify all prime numbers up to a given integer:\n\n```\ndef is_prime(n):\n  """\n  Determines whether the given integer is prime.\n\n  Args:\n    n: The integer to be tested for primality.\n\n  Returns:\n    True if n is prime, False otherwise.\n  """\n\n  # Check if n is divisible by 2.\n  if n % 2 == 0:\n    return False\n\n  # Check if n is divisible by any integer from 3 to the square root', additional_kwargs={}, example=False)     AIMessage(content='The following Python function can be used to identify all prime numbers up to a given integer:\n\n```\ndef is_prime(n):\n  """\n  Determines whether the given integer is prime.\n\n  Args:\n    n: The integer to be tested for primality.\n\n  Returns:\n    True if n is prime, False otherwise.\n  """\n\n  # Check if n is divisible by 2.\n  if n % 2 == 0:\n    return False\n\n  # Check if n is divisible by any integer from 3 to the square root', additional_kwargs={}, example=False)     AIMessage(content='The following Python function can be used to identify all prime numbers up to a given integer:\n\n```\ndef is_prime(n):\n  """\n  Determines whether the given integer is prime.\n\n  Args:\n    n: The integer to be tested for primality.\n\n  Returns:\n    True if n is prime, False otherwise.\n  """\n\n  # Check if n is divisible by 2.\n  if n % 2 == 0:\n    return False\n\n  # Check if n is divisible by any integer from 3 to the square root', additional_kwargs={}, example=False)  Previous ERNIE-Bot Chat Next JinaChat CommunityDiscordTwitterGitHubPythonJS/TSMoreHomepageBlogCopyright ¬© 2023 LangChain, Inc. CommunityDiscordTwitterGitHubPythonJS/TSMoreHomepageBlog CommunityDiscordTwitter Community GitHubPythonJS/TS GitHub MoreHomepageBlog More Copyright ¬© 2023 LangChain, Inc. Copyright ¬© 2023 LangChain, Inc. Skip to main content (#docusaurus_skipToContent_fallback) ü¶úÔ∏èüîó LangChain (/) Docs (/docs/get_started/introduction) Use cases (/docs/use_cases) Integrations (/docs/integrations) API (https://api.python.langchain.com) LangSmith (https://smith.langchain.com) JS/TS Docs (https://js.langchain.com/docs)  (https://github.com/hwchase17/langchain) Integrations (/docs/integrations) Callbacks (/docs/integrations/callbacks/) Chat models (/docs/integrations/chat/) Anthropic (/docs/integrations/chat/anthropic) Anthropic Functions (/docs/integrations/chat/anthropic_functions) Anyscale (/docs/integrations/chat/anyscale) Azure (/docs/integrations/chat/azure_chat_openai) AzureML Chat Online Endpoint (/docs/integrations/chat/azureml_chat_endpoint) Bedrock Chat (/docs/integrations/chat/bedrock) ERNIE-Bot Chat (/docs/integrations/chat/ernie) Google Cloud Platform Vertex AI PaLM (/docs/integrations/chat/google_vertex_ai_palm) JinaChat (/docs/integrations/chat/jinachat) Konko (/docs/integrations/chat/konko) üöÖ LiteLLM (/docs/integrations/chat/litellm) Llama API (/docs/integrations/chat/llama_api) Ollama (/docs/integrations/chat/ollama) OpenAI (/docs/integrations/chat/openai) PromptLayer ChatOpenAI (/docs/integrations/chat/promptlayer_chatopenai) Chat loaders (/docs/integrations/chat_loaders/) Document loaders (/docs/integrations/document_loaders/) Document transformers (/docs/integrations/document_transformers/) LLMs (/docs/integrations/llms/) Memory (/docs/integrations/memory/) Retrievers (/docs/integrations/retrievers/) Text embedding models (/docs/integrations/text_embedding/) Agents & Toolkits (/docs/integrations/toolkits/) Tools (/docs/integrations/tools/) Vector stores (/docs/integrations/vectorstores/) Grouped by provider (/docs/integrations/providers/)  (/) Integrations (/docs/integrations) Chat models (/docs/integrations/chat/) does not use (https://cloud.google.com/vertex-ai/docs/generative-ai/data-governance#foundation_model_development) Google's Customer Data Processing Addendum (CDPA) (https://cloud.google.com/terms/data-processing-addendum) https://cloud.google.com/docs/authentication/application-default-credentials#GAC (https://cloud.google.com/docs/authentication/application-default-credentials#GAC) https://googleapis.dev/python/google-auth/latest/reference/google.auth.html#module-google.auth (https://googleapis.dev/python/google-auth/latest/reference/google.auth.html#module-google.auth) ChatVertexAI (https://api.python.langchain.com/en/latest/chat_models/langchain.chat_models.vertexai.ChatVertexAI.html) ChatPromptTemplate (https://api.python.langchain.com/en/latest/prompts/langchain.prompts.chat.ChatPromptTemplate.html) SystemMessagePromptTemplate (https://api.python.langchain.com/en/latest/prompts/langchain.prompts.chat.SystemMessagePromptTemplate.html) HumanMessagePromptTemplate (https://api.python.langchain.com/en/latest/prompts/langchain.prompts.chat.HumanMessagePromptTemplate.html) HumanMessage (https://api.python.langchain.com/en/latest/schema/langchain.schema.messages.HumanMessage.html) SystemMessage (https://api.python.langchain.com/en/latest/schema/langchain.schema.messages.SystemMessage.html) PreviousERNIE-Bot Chat (/docs/integrations/chat/ernie) NextJinaChat (/docs/integrations/chat/jinachat) Discord (https://discord.gg/cU2adEyC7w) Twitter (https://twitter.com/LangChainAI) Python (https://github.com/hwchase17/langchain) JS/TS (https://github.com/hwchase17/langchainjs) Homepage (https://langchain.com) Blog (https://blog.langchain.dev)