Baseten provides all the infrastructure you need to deploy and serve ML models performantly, scalably, and cost-efficiently. This example demonstrates using Langchain with models deployed on Baseten. To run this notebook, you'll need a Baseten account and an API key. You'll also need to install the Baseten Python package: First, you'll need to deploy a model to Baseten. You can deploy foundation models like WizardLM and Alpaca with one click from the Baseten model library or if you have your own model, deploy it with this tutorial. In this example, we'll work with WizardLM. Deploy WizardLM here and follow along with the deployed model's version ID. We can chain together multiple calls to one or multiple models, which is the whole point of Langchain! This example uses WizardLM to plan a meal with an entree, three sides, and an alcoholic and non-alcoholic beverage pairing. IntegrationsCallbacksChat modelsChat loadersDocument loadersDocument transformersLLMsAI21Aleph AlphaAmazon API GatewayAnyscaleAzure MLAzure OpenAIBananaBasetenBeamBedrockBittensorCerebriumAIChatGLMClarifaiCohereC TransformersCTranslate2DatabricksDeepInfraDeepSparseEden AIFireworksForefrontAIGoogle Vertex AI PaLMGooseAIGPT4AllHugging Face HubHugging Face Local PipelinesHuggingface TextGen InferenceJSONFormerKoboldAI APILlama.cppLLM Caching integrationsManifestMinimaxModalMosaicMLNLP CloudOctoAIOllamaOpaquePromptsOpenAIOpenLLMOpenLMPetalsPipelineAIPredibasePrediction GuardPromptLayer OpenAIRELLMReplicateRunhouseSageMakerEndpointStochasticAINebula (Symbl.ai)TextGenTitan TakeoffTongyi QwenvLLMWriterXorbits Inference (Xinference)MemoryRetrieversText embedding modelsAgents & ToolkitsToolsVector storesGrouped by provider Callbacks Chat models Chat loaders Document loaders Document transformers LLMsAI21Aleph AlphaAmazon API GatewayAnyscaleAzure MLAzure OpenAIBananaBasetenBeamBedrockBittensorCerebriumAIChatGLMClarifaiCohereC TransformersCTranslate2DatabricksDeepInfraDeepSparseEden AIFireworksForefrontAIGoogle Vertex AI PaLMGooseAIGPT4AllHugging Face HubHugging Face Local PipelinesHuggingface TextGen InferenceJSONFormerKoboldAI APILlama.cppLLM Caching integrationsManifestMinimaxModalMosaicMLNLP CloudOctoAIOllamaOpaquePromptsOpenAIOpenLLMOpenLMPetalsPipelineAIPredibasePrediction GuardPromptLayer OpenAIRELLMReplicateRunhouseSageMakerEndpointStochasticAINebula (Symbl.ai)TextGenTitan TakeoffTongyi QwenvLLMWriterXorbits Inference (Xinference) AI21 Aleph Alpha Amazon API Gateway Anyscale Azure ML Azure OpenAI Banana Baseten Beam Bedrock Bittensor CerebriumAI ChatGLM Clarifai Cohere C Transformers CTranslate2 Databricks DeepInfra DeepSparse Eden AI Fireworks ForefrontAI Google Vertex AI PaLM GooseAI GPT4All Hugging Face Hub Hugging Face Local Pipelines Huggingface TextGen Inference JSONFormer KoboldAI API Llama.cpp LLM Caching integrations Manifest Minimax Modal MosaicML NLP Cloud OctoAI Ollama OpaquePrompts OpenAI OpenLLM OpenLM Petals PipelineAI Predibase Prediction Guard PromptLayer OpenAI RELLM Replicate Runhouse SageMakerEndpoint StochasticAI Nebula (Symbl.ai) TextGen Titan Takeoff Tongyi Qwen vLLM Writer Xorbits Inference (Xinference) Memory Retrievers Text embedding models Agents & Toolkits Tools Vector stores Grouped by provider  Integrations LLMs Baseten Baseten SimpleSequentialChain Discord Twitter Python JS/TS Homepage Blog Skip to main contentü¶úÔ∏èüîó LangChainDocsUse casesIntegrationsAPILangSmithJS/TS DocsCTRLKIntegrationsCallbacksChat modelsChat loadersDocument loadersDocument transformersLLMsAI21Aleph AlphaAmazon API GatewayAnyscaleAzure MLAzure OpenAIBananaBasetenBeamBedrockBittensorCerebriumAIChatGLMClarifaiCohereC TransformersCTranslate2DatabricksDeepInfraDeepSparseEden AIFireworksForefrontAIGoogle Vertex AI PaLMGooseAIGPT4AllHugging Face HubHugging Face Local PipelinesHuggingface TextGen InferenceJSONFormerKoboldAI APILlama.cppLLM Caching integrationsManifestMinimaxModalMosaicMLNLP CloudOctoAIOllamaOpaquePromptsOpenAIOpenLLMOpenLMPetalsPipelineAIPredibasePrediction GuardPromptLayer OpenAIRELLMReplicateRunhouseSageMakerEndpointStochasticAINebula (Symbl.ai)TextGenTitan TakeoffTongyi QwenvLLMWriterXorbits Inference (Xinference)MemoryRetrieversText embedding modelsAgents & ToolkitsToolsVector storesGrouped by providerIntegrationsLLMsBasetenBasetenBaseten provides all the infrastructure you need to deploy and serve ML models performantly, scalably, and cost-efficiently.This example demonstrates using Langchain with models deployed on Baseten.SetupTo run this notebook, you'll need a Baseten account and an API key.You'll also need to install the Baseten Python package:pip install basetenimport basetenbaseten.login("YOUR_API_KEY")Single model callFirst, you'll need to deploy a model to Baseten.You can deploy foundation models like WizardLM and Alpaca with one click from the Baseten model library or if you have your own model, deploy it with this tutorial.In this example, we'll work with WizardLM. Deploy WizardLM here and follow along with the deployed model's version ID.from langchain.llms import BasetenAPI Reference:Baseten# Load the modelwizardlm = Baseten(model="MODEL_VERSION_ID", verbose=True)# Prompt the modelwizardlm("What is the difference between a Wizard and a Sorcerer?")Chained model callsWe can chain together multiple calls to one or multiple models, which is the whole point of Langchain!This example uses WizardLM to plan a meal with an entree, three sides, and an alcoholic and non-alcoholic beverage pairing.from langchain.chains import SimpleSequentialChainfrom langchain import PromptTemplate, LLMChainAPI Reference:SimpleSequentialChain# Build the first link in the chainprompt = PromptTemplate(    input_variables=["cuisine"],    template="Name a complex entree for a {cuisine} dinner. Respond with just the name of a single dish.",)link_one = LLMChain(llm=wizardlm, prompt=prompt)# Build the second link in the chainprompt = PromptTemplate(    input_variables=["entree"],    template="What are three sides that would go with {entree}. Respond with only a list of the sides.",)link_two = LLMChain(llm=wizardlm, prompt=prompt)# Build the third link in the chainprompt = PromptTemplate(    input_variables=["sides"],    template="What is one alcoholic and one non-alcoholic beverage that would go well with this list of sides: {sides}. Respond with only the names of the beverages.",)link_three = LLMChain(llm=wizardlm, prompt=prompt)# Run the full chain!menu_maker = SimpleSequentialChain(    chains=[link_one, link_two, link_three], verbose=True)menu_maker.run("South Indian")PreviousBananaNextBeamCommunityDiscordTwitterGitHubPythonJS/TSMoreHomepageBlogCopyright ¬© 2023 LangChain, Inc. Skip to main content ü¶úÔ∏èüîó LangChainDocsUse casesIntegrationsAPILangSmithJS/TS DocsCTRLK ü¶úÔ∏èüîó LangChainDocsUse casesIntegrationsAPI LangSmithJS/TS DocsCTRLK  CTRLK CTRLK  CTRLK   IntegrationsCallbacksChat modelsChat loadersDocument loadersDocument transformersLLMsAI21Aleph AlphaAmazon API GatewayAnyscaleAzure MLAzure OpenAIBananaBasetenBeamBedrockBittensorCerebriumAIChatGLMClarifaiCohereC TransformersCTranslate2DatabricksDeepInfraDeepSparseEden AIFireworksForefrontAIGoogle Vertex AI PaLMGooseAIGPT4AllHugging Face HubHugging Face Local PipelinesHuggingface TextGen InferenceJSONFormerKoboldAI APILlama.cppLLM Caching integrationsManifestMinimaxModalMosaicMLNLP CloudOctoAIOllamaOpaquePromptsOpenAIOpenLLMOpenLMPetalsPipelineAIPredibasePrediction GuardPromptLayer OpenAIRELLMReplicateRunhouseSageMakerEndpointStochasticAINebula (Symbl.ai)TextGenTitan TakeoffTongyi QwenvLLMWriterXorbits Inference (Xinference)MemoryRetrieversText embedding modelsAgents & ToolkitsToolsVector storesGrouped by providerIntegrationsLLMsBasetenBasetenBaseten provides all the infrastructure you need to deploy and serve ML models performantly, scalably, and cost-efficiently.This example demonstrates using Langchain with models deployed on Baseten.SetupTo run this notebook, you'll need a Baseten account and an API key.You'll also need to install the Baseten Python package:pip install basetenimport basetenbaseten.login("YOUR_API_KEY")Single model callFirst, you'll need to deploy a model to Baseten.You can deploy foundation models like WizardLM and Alpaca with one click from the Baseten model library or if you have your own model, deploy it with this tutorial.In this example, we'll work with WizardLM. Deploy WizardLM here and follow along with the deployed model's version ID.from langchain.llms import BasetenAPI Reference:Baseten# Load the modelwizardlm = Baseten(model="MODEL_VERSION_ID", verbose=True)# Prompt the modelwizardlm("What is the difference between a Wizard and a Sorcerer?")Chained model callsWe can chain together multiple calls to one or multiple models, which is the whole point of Langchain!This example uses WizardLM to plan a meal with an entree, three sides, and an alcoholic and non-alcoholic beverage pairing.from langchain.chains import SimpleSequentialChainfrom langchain import PromptTemplate, LLMChainAPI Reference:SimpleSequentialChain# Build the first link in the chainprompt = PromptTemplate(    input_variables=["cuisine"],    template="Name a complex entree for a {cuisine} dinner. Respond with just the name of a single dish.",)link_one = LLMChain(llm=wizardlm, prompt=prompt)# Build the second link in the chainprompt = PromptTemplate(    input_variables=["entree"],    template="What are three sides that would go with {entree}. Respond with only a list of the sides.",)link_two = LLMChain(llm=wizardlm, prompt=prompt)# Build the third link in the chainprompt = PromptTemplate(    input_variables=["sides"],    template="What is one alcoholic and one non-alcoholic beverage that would go well with this list of sides: {sides}. Respond with only the names of the beverages.",)link_three = LLMChain(llm=wizardlm, prompt=prompt)# Run the full chain!menu_maker = SimpleSequentialChain(    chains=[link_one, link_two, link_three], verbose=True)menu_maker.run("South Indian")PreviousBananaNextBeam IntegrationsCallbacksChat modelsChat loadersDocument loadersDocument transformersLLMsAI21Aleph AlphaAmazon API GatewayAnyscaleAzure MLAzure OpenAIBananaBasetenBeamBedrockBittensorCerebriumAIChatGLMClarifaiCohereC TransformersCTranslate2DatabricksDeepInfraDeepSparseEden AIFireworksForefrontAIGoogle Vertex AI PaLMGooseAIGPT4AllHugging Face HubHugging Face Local PipelinesHuggingface TextGen InferenceJSONFormerKoboldAI APILlama.cppLLM Caching integrationsManifestMinimaxModalMosaicMLNLP CloudOctoAIOllamaOpaquePromptsOpenAIOpenLLMOpenLMPetalsPipelineAIPredibasePrediction GuardPromptLayer OpenAIRELLMReplicateRunhouseSageMakerEndpointStochasticAINebula (Symbl.ai)TextGenTitan TakeoffTongyi QwenvLLMWriterXorbits Inference (Xinference)MemoryRetrieversText embedding modelsAgents & ToolkitsToolsVector storesGrouped by providerIntegrationsLLMsBasetenBasetenBaseten provides all the infrastructure you need to deploy and serve ML models performantly, scalably, and cost-efficiently.This example demonstrates using Langchain with models deployed on Baseten.SetupTo run this notebook, you'll need a Baseten account and an API key.You'll also need to install the Baseten Python package:pip install basetenimport basetenbaseten.login("YOUR_API_KEY")Single model callFirst, you'll need to deploy a model to Baseten.You can deploy foundation models like WizardLM and Alpaca with one click from the Baseten model library or if you have your own model, deploy it with this tutorial.In this example, we'll work with WizardLM. Deploy WizardLM here and follow along with the deployed model's version ID.from langchain.llms import BasetenAPI Reference:Baseten# Load the modelwizardlm = Baseten(model="MODEL_VERSION_ID", verbose=True)# Prompt the modelwizardlm("What is the difference between a Wizard and a Sorcerer?")Chained model callsWe can chain together multiple calls to one or multiple models, which is the whole point of Langchain!This example uses WizardLM to plan a meal with an entree, three sides, and an alcoholic and non-alcoholic beverage pairing.from langchain.chains import SimpleSequentialChainfrom langchain import PromptTemplate, LLMChainAPI Reference:SimpleSequentialChain# Build the first link in the chainprompt = PromptTemplate(    input_variables=["cuisine"],    template="Name a complex entree for a {cuisine} dinner. Respond with just the name of a single dish.",)link_one = LLMChain(llm=wizardlm, prompt=prompt)# Build the second link in the chainprompt = PromptTemplate(    input_variables=["entree"],    template="What are three sides that would go with {entree}. Respond with only a list of the sides.",)link_two = LLMChain(llm=wizardlm, prompt=prompt)# Build the third link in the chainprompt = PromptTemplate(    input_variables=["sides"],    template="What is one alcoholic and one non-alcoholic beverage that would go well with this list of sides: {sides}. Respond with only the names of the beverages.",)link_three = LLMChain(llm=wizardlm, prompt=prompt)# Run the full chain!menu_maker = SimpleSequentialChain(    chains=[link_one, link_two, link_three], verbose=True)menu_maker.run("South Indian")PreviousBananaNextBeam IntegrationsCallbacksChat modelsChat loadersDocument loadersDocument transformersLLMsAI21Aleph AlphaAmazon API GatewayAnyscaleAzure MLAzure OpenAIBananaBasetenBeamBedrockBittensorCerebriumAIChatGLMClarifaiCohereC TransformersCTranslate2DatabricksDeepInfraDeepSparseEden AIFireworksForefrontAIGoogle Vertex AI PaLMGooseAIGPT4AllHugging Face HubHugging Face Local PipelinesHuggingface TextGen InferenceJSONFormerKoboldAI APILlama.cppLLM Caching integrationsManifestMinimaxModalMosaicMLNLP CloudOctoAIOllamaOpaquePromptsOpenAIOpenLLMOpenLMPetalsPipelineAIPredibasePrediction GuardPromptLayer OpenAIRELLMReplicateRunhouseSageMakerEndpointStochasticAINebula (Symbl.ai)TextGenTitan TakeoffTongyi QwenvLLMWriterXorbits Inference (Xinference)MemoryRetrieversText embedding modelsAgents & ToolkitsToolsVector storesGrouped by provider IntegrationsCallbacksChat modelsChat loadersDocument loadersDocument transformersLLMsAI21Aleph AlphaAmazon API GatewayAnyscaleAzure MLAzure OpenAIBananaBasetenBeamBedrockBittensorCerebriumAIChatGLMClarifaiCohereC TransformersCTranslate2DatabricksDeepInfraDeepSparseEden AIFireworksForefrontAIGoogle Vertex AI PaLMGooseAIGPT4AllHugging Face HubHugging Face Local PipelinesHuggingface TextGen InferenceJSONFormerKoboldAI APILlama.cppLLM Caching integrationsManifestMinimaxModalMosaicMLNLP CloudOctoAIOllamaOpaquePromptsOpenAIOpenLLMOpenLMPetalsPipelineAIPredibasePrediction GuardPromptLayer OpenAIRELLMReplicateRunhouseSageMakerEndpointStochasticAINebula (Symbl.ai)TextGenTitan TakeoffTongyi QwenvLLMWriterXorbits Inference (Xinference)MemoryRetrieversText embedding modelsAgents & ToolkitsToolsVector storesGrouped by provider Integrations Callbacks Chat models Chat loaders Document loaders Document transformers LLMs Memory Retrievers Text embedding models Agents & Toolkits Tools Vector stores Grouped by provider IntegrationsLLMsBasetenBasetenBaseten provides all the infrastructure you need to deploy and serve ML models performantly, scalably, and cost-efficiently.This example demonstrates using Langchain with models deployed on Baseten.SetupTo run this notebook, you'll need a Baseten account and an API key.You'll also need to install the Baseten Python package:pip install basetenimport basetenbaseten.login("YOUR_API_KEY")Single model callFirst, you'll need to deploy a model to Baseten.You can deploy foundation models like WizardLM and Alpaca with one click from the Baseten model library or if you have your own model, deploy it with this tutorial.In this example, we'll work with WizardLM. Deploy WizardLM here and follow along with the deployed model's version ID.from langchain.llms import BasetenAPI Reference:Baseten# Load the modelwizardlm = Baseten(model="MODEL_VERSION_ID", verbose=True)# Prompt the modelwizardlm("What is the difference between a Wizard and a Sorcerer?")Chained model callsWe can chain together multiple calls to one or multiple models, which is the whole point of Langchain!This example uses WizardLM to plan a meal with an entree, three sides, and an alcoholic and non-alcoholic beverage pairing.from langchain.chains import SimpleSequentialChainfrom langchain import PromptTemplate, LLMChainAPI Reference:SimpleSequentialChain# Build the first link in the chainprompt = PromptTemplate(    input_variables=["cuisine"],    template="Name a complex entree for a {cuisine} dinner. Respond with just the name of a single dish.",)link_one = LLMChain(llm=wizardlm, prompt=prompt)# Build the second link in the chainprompt = PromptTemplate(    input_variables=["entree"],    template="What are three sides that would go with {entree}. Respond with only a list of the sides.",)link_two = LLMChain(llm=wizardlm, prompt=prompt)# Build the third link in the chainprompt = PromptTemplate(    input_variables=["sides"],    template="What is one alcoholic and one non-alcoholic beverage that would go well with this list of sides: {sides}. Respond with only the names of the beverages.",)link_three = LLMChain(llm=wizardlm, prompt=prompt)# Run the full chain!menu_maker = SimpleSequentialChain(    chains=[link_one, link_two, link_three], verbose=True)menu_maker.run("South Indian")PreviousBananaNextBeam IntegrationsLLMsBasetenBasetenBaseten provides all the infrastructure you need to deploy and serve ML models performantly, scalably, and cost-efficiently.This example demonstrates using Langchain with models deployed on Baseten.SetupTo run this notebook, you'll need a Baseten account and an API key.You'll also need to install the Baseten Python package:pip install basetenimport basetenbaseten.login("YOUR_API_KEY")Single model callFirst, you'll need to deploy a model to Baseten.You can deploy foundation models like WizardLM and Alpaca with one click from the Baseten model library or if you have your own model, deploy it with this tutorial.In this example, we'll work with WizardLM. Deploy WizardLM here and follow along with the deployed model's version ID.from langchain.llms import BasetenAPI Reference:Baseten# Load the modelwizardlm = Baseten(model="MODEL_VERSION_ID", verbose=True)# Prompt the modelwizardlm("What is the difference between a Wizard and a Sorcerer?")Chained model callsWe can chain together multiple calls to one or multiple models, which is the whole point of Langchain!This example uses WizardLM to plan a meal with an entree, three sides, and an alcoholic and non-alcoholic beverage pairing.from langchain.chains import SimpleSequentialChainfrom langchain import PromptTemplate, LLMChainAPI Reference:SimpleSequentialChain# Build the first link in the chainprompt = PromptTemplate(    input_variables=["cuisine"],    template="Name a complex entree for a {cuisine} dinner. Respond with just the name of a single dish.",)link_one = LLMChain(llm=wizardlm, prompt=prompt)# Build the second link in the chainprompt = PromptTemplate(    input_variables=["entree"],    template="What are three sides that would go with {entree}. Respond with only a list of the sides.",)link_two = LLMChain(llm=wizardlm, prompt=prompt)# Build the third link in the chainprompt = PromptTemplate(    input_variables=["sides"],    template="What is one alcoholic and one non-alcoholic beverage that would go well with this list of sides: {sides}. Respond with only the names of the beverages.",)link_three = LLMChain(llm=wizardlm, prompt=prompt)# Run the full chain!menu_maker = SimpleSequentialChain(    chains=[link_one, link_two, link_three], verbose=True)menu_maker.run("South Indian")PreviousBananaNextBeam IntegrationsLLMsBasetenBasetenBaseten provides all the infrastructure you need to deploy and serve ML models performantly, scalably, and cost-efficiently.This example demonstrates using Langchain with models deployed on Baseten.SetupTo run this notebook, you'll need a Baseten account and an API key.You'll also need to install the Baseten Python package:pip install basetenimport basetenbaseten.login("YOUR_API_KEY")Single model callFirst, you'll need to deploy a model to Baseten.You can deploy foundation models like WizardLM and Alpaca with one click from the Baseten model library or if you have your own model, deploy it with this tutorial.In this example, we'll work with WizardLM. Deploy WizardLM here and follow along with the deployed model's version ID.from langchain.llms import BasetenAPI Reference:Baseten# Load the modelwizardlm = Baseten(model="MODEL_VERSION_ID", verbose=True)# Prompt the modelwizardlm("What is the difference between a Wizard and a Sorcerer?")Chained model callsWe can chain together multiple calls to one or multiple models, which is the whole point of Langchain!This example uses WizardLM to plan a meal with an entree, three sides, and an alcoholic and non-alcoholic beverage pairing.from langchain.chains import SimpleSequentialChainfrom langchain import PromptTemplate, LLMChainAPI Reference:SimpleSequentialChain# Build the first link in the chainprompt = PromptTemplate(    input_variables=["cuisine"],    template="Name a complex entree for a {cuisine} dinner. Respond with just the name of a single dish.",)link_one = LLMChain(llm=wizardlm, prompt=prompt)# Build the second link in the chainprompt = PromptTemplate(    input_variables=["entree"],    template="What are three sides that would go with {entree}. Respond with only a list of the sides.",)link_two = LLMChain(llm=wizardlm, prompt=prompt)# Build the third link in the chainprompt = PromptTemplate(    input_variables=["sides"],    template="What is one alcoholic and one non-alcoholic beverage that would go well with this list of sides: {sides}. Respond with only the names of the beverages.",)link_three = LLMChain(llm=wizardlm, prompt=prompt)# Run the full chain!menu_maker = SimpleSequentialChain(    chains=[link_one, link_two, link_three], verbose=True)menu_maker.run("South Indian")PreviousBananaNextBeam IntegrationsLLMsBasetenBasetenBaseten provides all the infrastructure you need to deploy and serve ML models performantly, scalably, and cost-efficiently.This example demonstrates using Langchain with models deployed on Baseten.SetupTo run this notebook, you'll need a Baseten account and an API key.You'll also need to install the Baseten Python package:pip install basetenimport basetenbaseten.login("YOUR_API_KEY")Single model callFirst, you'll need to deploy a model to Baseten.You can deploy foundation models like WizardLM and Alpaca with one click from the Baseten model library or if you have your own model, deploy it with this tutorial.In this example, we'll work with WizardLM. Deploy WizardLM here and follow along with the deployed model's version ID.from langchain.llms import BasetenAPI Reference:Baseten# Load the modelwizardlm = Baseten(model="MODEL_VERSION_ID", verbose=True)# Prompt the modelwizardlm("What is the difference between a Wizard and a Sorcerer?")Chained model callsWe can chain together multiple calls to one or multiple models, which is the whole point of Langchain!This example uses WizardLM to plan a meal with an entree, three sides, and an alcoholic and non-alcoholic beverage pairing.from langchain.chains import SimpleSequentialChainfrom langchain import PromptTemplate, LLMChainAPI Reference:SimpleSequentialChain# Build the first link in the chainprompt = PromptTemplate(    input_variables=["cuisine"],    template="Name a complex entree for a {cuisine} dinner. Respond with just the name of a single dish.",)link_one = LLMChain(llm=wizardlm, prompt=prompt)# Build the second link in the chainprompt = PromptTemplate(    input_variables=["entree"],    template="What are three sides that would go with {entree}. Respond with only a list of the sides.",)link_two = LLMChain(llm=wizardlm, prompt=prompt)# Build the third link in the chainprompt = PromptTemplate(    input_variables=["sides"],    template="What is one alcoholic and one non-alcoholic beverage that would go well with this list of sides: {sides}. Respond with only the names of the beverages.",)link_three = LLMChain(llm=wizardlm, prompt=prompt)# Run the full chain!menu_maker = SimpleSequentialChain(    chains=[link_one, link_two, link_three], verbose=True)menu_maker.run("South Indian")PreviousBananaNextBeam BasetenBaseten provides all the infrastructure you need to deploy and serve ML models performantly, scalably, and cost-efficiently.This example demonstrates using Langchain with models deployed on Baseten.SetupTo run this notebook, you'll need a Baseten account and an API key.You'll also need to install the Baseten Python package:pip install basetenimport basetenbaseten.login("YOUR_API_KEY")Single model callFirst, you'll need to deploy a model to Baseten.You can deploy foundation models like WizardLM and Alpaca with one click from the Baseten model library or if you have your own model, deploy it with this tutorial.In this example, we'll work with WizardLM. Deploy WizardLM here and follow along with the deployed model's version ID.from langchain.llms import BasetenAPI Reference:Baseten# Load the modelwizardlm = Baseten(model="MODEL_VERSION_ID", verbose=True)# Prompt the modelwizardlm("What is the difference between a Wizard and a Sorcerer?")Chained model callsWe can chain together multiple calls to one or multiple models, which is the whole point of Langchain!This example uses WizardLM to plan a meal with an entree, three sides, and an alcoholic and non-alcoholic beverage pairing.from langchain.chains import SimpleSequentialChainfrom langchain import PromptTemplate, LLMChainAPI Reference:SimpleSequentialChain# Build the first link in the chainprompt = PromptTemplate(    input_variables=["cuisine"],    template="Name a complex entree for a {cuisine} dinner. Respond with just the name of a single dish.",)link_one = LLMChain(llm=wizardlm, prompt=prompt)# Build the second link in the chainprompt = PromptTemplate(    input_variables=["entree"],    template="What are three sides that would go with {entree}. Respond with only a list of the sides.",)link_two = LLMChain(llm=wizardlm, prompt=prompt)# Build the third link in the chainprompt = PromptTemplate(    input_variables=["sides"],    template="What is one alcoholic and one non-alcoholic beverage that would go well with this list of sides: {sides}. Respond with only the names of the beverages.",)link_three = LLMChain(llm=wizardlm, prompt=prompt)# Run the full chain!menu_maker = SimpleSequentialChain(    chains=[link_one, link_two, link_three], verbose=True)menu_maker.run("South Indian") pip install baseten pip install baseten  import basetenbaseten.login("YOUR_API_KEY") import basetenbaseten.login("YOUR_API_KEY")  from langchain.llms import Baseten from langchain.llms import Baseten  API Reference:Baseten # Load the modelwizardlm = Baseten(model="MODEL_VERSION_ID", verbose=True) # Load the modelwizardlm = Baseten(model="MODEL_VERSION_ID", verbose=True)  # Prompt the modelwizardlm("What is the difference between a Wizard and a Sorcerer?") # Prompt the modelwizardlm("What is the difference between a Wizard and a Sorcerer?")  from langchain.chains import SimpleSequentialChainfrom langchain import PromptTemplate, LLMChain from langchain.chains import SimpleSequentialChainfrom langchain import PromptTemplate, LLMChain  API Reference:SimpleSequentialChain # Build the first link in the chainprompt = PromptTemplate(    input_variables=["cuisine"],    template="Name a complex entree for a {cuisine} dinner. Respond with just the name of a single dish.",)link_one = LLMChain(llm=wizardlm, prompt=prompt) # Build the first link in the chainprompt = PromptTemplate(    input_variables=["cuisine"],    template="Name a complex entree for a {cuisine} dinner. Respond with just the name of a single dish.",)link_one = LLMChain(llm=wizardlm, prompt=prompt)  # Build the second link in the chainprompt = PromptTemplate(    input_variables=["entree"],    template="What are three sides that would go with {entree}. Respond with only a list of the sides.",)link_two = LLMChain(llm=wizardlm, prompt=prompt) # Build the second link in the chainprompt = PromptTemplate(    input_variables=["entree"],    template="What are three sides that would go with {entree}. Respond with only a list of the sides.",)link_two = LLMChain(llm=wizardlm, prompt=prompt)  # Build the third link in the chainprompt = PromptTemplate(    input_variables=["sides"],    template="What is one alcoholic and one non-alcoholic beverage that would go well with this list of sides: {sides}. Respond with only the names of the beverages.",)link_three = LLMChain(llm=wizardlm, prompt=prompt) # Build the third link in the chainprompt = PromptTemplate(    input_variables=["sides"],    template="What is one alcoholic and one non-alcoholic beverage that would go well with this list of sides: {sides}. Respond with only the names of the beverages.",)link_three = LLMChain(llm=wizardlm, prompt=prompt)  # Run the full chain!menu_maker = SimpleSequentialChain(    chains=[link_one, link_two, link_three], verbose=True)menu_maker.run("South Indian") # Run the full chain!menu_maker = SimpleSequentialChain(    chains=[link_one, link_two, link_three], verbose=True)menu_maker.run("South Indian")  Previous Banana Next Beam CommunityDiscordTwitterGitHubPythonJS/TSMoreHomepageBlogCopyright ¬© 2023 LangChain, Inc. CommunityDiscordTwitterGitHubPythonJS/TSMoreHomepageBlog CommunityDiscordTwitter Community GitHubPythonJS/TS GitHub MoreHomepageBlog More Copyright ¬© 2023 LangChain, Inc. Copyright ¬© 2023 LangChain, Inc. Skip to main content (#docusaurus_skipToContent_fallback) ü¶úÔ∏èüîó LangChain (/) Docs (/docs/get_started/introduction) Use cases (/docs/use_cases) Integrations (/docs/integrations) API (https://api.python.langchain.com) LangSmith (https://smith.langchain.com) JS/TS Docs (https://js.langchain.com/docs)  (https://github.com/hwchase17/langchain) Integrations (/docs/integrations) Callbacks (/docs/integrations/callbacks/) Chat models (/docs/integrations/chat/) Chat loaders (/docs/integrations/chat_loaders/) Document loaders (/docs/integrations/document_loaders/) Document transformers (/docs/integrations/document_transformers/) LLMs (/docs/integrations/llms/) AI21 (/docs/integrations/llms/ai21) Aleph Alpha (/docs/integrations/llms/aleph_alpha) Amazon API Gateway (/docs/integrations/llms/amazon_api_gateway) Anyscale (/docs/integrations/llms/anyscale) Azure ML (/docs/integrations/llms/azure_ml) Azure OpenAI (/docs/integrations/llms/azure_openai) Banana (/docs/integrations/llms/banana) Baseten (/docs/integrations/llms/baseten) Beam (/docs/integrations/llms/beam) Bedrock (/docs/integrations/llms/bedrock) Bittensor (/docs/integrations/llms/bittensor) CerebriumAI (/docs/integrations/llms/cerebriumai) ChatGLM (/docs/integrations/llms/chatglm) Clarifai (/docs/integrations/llms/clarifai) Cohere (/docs/integrations/llms/cohere) C Transformers (/docs/integrations/llms/ctransformers) CTranslate2 (/docs/integrations/llms/ctranslate2) Databricks (/docs/integrations/llms/databricks) DeepInfra (/docs/integrations/llms/deepinfra) DeepSparse (/docs/integrations/llms/deepsparse) Eden AI (/docs/integrations/llms/edenai) Fireworks (/docs/integrations/llms/fireworks) ForefrontAI (/docs/integrations/llms/forefrontai) Google Vertex AI PaLM (/docs/integrations/llms/google_vertex_ai_palm) GooseAI (/docs/integrations/llms/gooseai) GPT4All (/docs/integrations/llms/gpt4all) Hugging Face Hub (/docs/integrations/llms/huggingface_hub) Hugging Face Local Pipelines (/docs/integrations/llms/huggingface_pipelines) Huggingface TextGen Inference (/docs/integrations/llms/huggingface_textgen_inference) JSONFormer (/docs/integrations/llms/jsonformer_experimental) KoboldAI API (/docs/integrations/llms/koboldai) Llama.cpp (/docs/integrations/llms/llamacpp) LLM Caching integrations (/docs/integrations/llms/llm_caching) Manifest (/docs/integrations/llms/manifest) Minimax (/docs/integrations/llms/minimax) Modal (/docs/integrations/llms/modal) MosaicML (/docs/integrations/llms/mosaicml) NLP Cloud (/docs/integrations/llms/nlpcloud) OctoAI (/docs/integrations/llms/octoai) Ollama (/docs/integrations/llms/ollama) OpaquePrompts (/docs/integrations/llms/opaqueprompts) OpenAI (/docs/integrations/llms/openai) OpenLLM (/docs/integrations/llms/openllm) OpenLM (/docs/integrations/llms/openlm) Petals (/docs/integrations/llms/petals) PipelineAI (/docs/integrations/llms/pipelineai) Predibase (/docs/integrations/llms/predibase) Prediction Guard (/docs/integrations/llms/predictionguard) PromptLayer OpenAI (/docs/integrations/llms/promptlayer_openai) RELLM (/docs/integrations/llms/rellm_experimental) Replicate (/docs/integrations/llms/replicate) Runhouse (/docs/integrations/llms/runhouse) SageMakerEndpoint (/docs/integrations/llms/sagemaker) StochasticAI (/docs/integrations/llms/stochasticai) Nebula (Symbl.ai) (/docs/integrations/llms/symblai_nebula) TextGen (/docs/integrations/llms/textgen) Titan Takeoff (/docs/integrations/llms/titan_takeoff) Tongyi Qwen (/docs/integrations/llms/tongyi) vLLM (/docs/integrations/llms/vllm) Writer (/docs/integrations/llms/writer) Xorbits Inference (Xinference) (/docs/integrations/llms/xinference) Memory (/docs/integrations/memory/) Retrievers (/docs/integrations/retrievers/) Text embedding models (/docs/integrations/text_embedding/) Agents & Toolkits (/docs/integrations/toolkits/) Tools (/docs/integrations/tools/) Vector stores (/docs/integrations/vectorstores/) Grouped by provider (/docs/integrations/providers/)  (/) Integrations (/docs/integrations) LLMs (/docs/integrations/llms/) Baseten (https://baseten.co) Baseten account (https://baseten.co) API key (https://docs.baseten.co/settings/api-keys) Baseten model library (https://app.baseten.co/explore/) deploy it with this tutorial (https://docs.baseten.co/deploying-models/deploy) Deploy WizardLM here (https://app.baseten.co/explore/llama) model's version ID (https://docs.baseten.co/managing-models/manage) Baseten (https://api.python.langchain.com/en/latest/llms/langchain.llms.baseten.Baseten.html) SimpleSequentialChain (https://api.python.langchain.com/en/latest/chains/langchain.chains.sequential.SimpleSequentialChain.html) PreviousBanana (/docs/integrations/llms/banana) NextBeam (/docs/integrations/llms/beam) Discord (https://discord.gg/cU2adEyC7w) Twitter (https://twitter.com/LangChainAI) Python (https://github.com/hwchase17/langchain) JS/TS (https://github.com/hwchase17/langchainjs) Homepage (https://langchain.com) Blog (https://blog.langchain.dev)