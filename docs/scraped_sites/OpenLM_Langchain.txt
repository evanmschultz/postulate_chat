OpenLM is a zero-dependency OpenAI-compatible LLM provider that can call different inference endpoints directly via HTTP.  It implements the OpenAI Completion class so that it can be used as a drop-in replacement for the OpenAI API. This changeset utilizes BaseOpenAI for minimal added code. This examples goes over how to use LangChain to interact with both OpenAI and HuggingFace. You'll need API keys from both. Install dependencies and set API keys. Here we're going to call two models in an LLMChain, text-davinci-003 from OpenAI and gpt2 on HuggingFace. IntegrationsCallbacksChat modelsChat loadersDocument loadersDocument transformersLLMsAI21Aleph AlphaAmazon API GatewayAnyscaleAzure MLAzure OpenAIBananaBasetenBeamBedrockBittensorCerebriumAIChatGLMClarifaiCohereC TransformersCTranslate2DatabricksDeepInfraDeepSparseEden AIFireworksForefrontAIGoogle Vertex AI PaLMGooseAIGPT4AllHugging Face HubHugging Face Local PipelinesHuggingface TextGen InferenceJSONFormerKoboldAI APILlama.cppLLM Caching integrationsManifestMinimaxModalMosaicMLNLP CloudOctoAIOllamaOpaquePromptsOpenAIOpenLLMOpenLMPetalsPipelineAIPredibasePrediction GuardPromptLayer OpenAIRELLMReplicateRunhouseSageMakerEndpointStochasticAINebula (Symbl.ai)TextGenTitan TakeoffTongyi QwenvLLMWriterXorbits Inference (Xinference)MemoryRetrieversText embedding modelsAgents & ToolkitsToolsVector storesGrouped by provider Callbacks Chat models Chat loaders Document loaders Document transformers LLMsAI21Aleph AlphaAmazon API GatewayAnyscaleAzure MLAzure OpenAIBananaBasetenBeamBedrockBittensorCerebriumAIChatGLMClarifaiCohereC TransformersCTranslate2DatabricksDeepInfraDeepSparseEden AIFireworksForefrontAIGoogle Vertex AI PaLMGooseAIGPT4AllHugging Face HubHugging Face Local PipelinesHuggingface TextGen InferenceJSONFormerKoboldAI APILlama.cppLLM Caching integrationsManifestMinimaxModalMosaicMLNLP CloudOctoAIOllamaOpaquePromptsOpenAIOpenLLMOpenLMPetalsPipelineAIPredibasePrediction GuardPromptLayer OpenAIRELLMReplicateRunhouseSageMakerEndpointStochasticAINebula (Symbl.ai)TextGenTitan TakeoffTongyi QwenvLLMWriterXorbits Inference (Xinference) AI21 Aleph Alpha Amazon API Gateway Anyscale Azure ML Azure OpenAI Banana Baseten Beam Bedrock Bittensor CerebriumAI ChatGLM Clarifai Cohere C Transformers CTranslate2 Databricks DeepInfra DeepSparse Eden AI Fireworks ForefrontAI Google Vertex AI PaLM GooseAI GPT4All Hugging Face Hub Hugging Face Local Pipelines Huggingface TextGen Inference JSONFormer KoboldAI API Llama.cpp LLM Caching integrations Manifest Minimax Modal MosaicML NLP Cloud OctoAI Ollama OpaquePrompts OpenAI OpenLLM OpenLM Petals PipelineAI Predibase Prediction Guard PromptLayer OpenAI RELLM Replicate Runhouse SageMakerEndpoint StochasticAI Nebula (Symbl.ai) TextGen Titan Takeoff Tongyi Qwen vLLM Writer Xorbits Inference (Xinference) Memory Retrievers Text embedding models Agents & Toolkits Tools Vector stores Grouped by provider  Integrations LLMs OpenLM OpenLM Setup Using LangChain with OpenLM Discord Twitter Python JS/TS Homepage Blog Skip to main contentü¶úÔ∏èüîó LangChainDocsUse casesIntegrationsAPILangSmithJS/TS DocsCTRLKIntegrationsCallbacksChat modelsChat loadersDocument loadersDocument transformersLLMsAI21Aleph AlphaAmazon API GatewayAnyscaleAzure MLAzure OpenAIBananaBasetenBeamBedrockBittensorCerebriumAIChatGLMClarifaiCohereC TransformersCTranslate2DatabricksDeepInfraDeepSparseEden AIFireworksForefrontAIGoogle Vertex AI PaLMGooseAIGPT4AllHugging Face HubHugging Face Local PipelinesHuggingface TextGen InferenceJSONFormerKoboldAI APILlama.cppLLM Caching integrationsManifestMinimaxModalMosaicMLNLP CloudOctoAIOllamaOpaquePromptsOpenAIOpenLLMOpenLMPetalsPipelineAIPredibasePrediction GuardPromptLayer OpenAIRELLMReplicateRunhouseSageMakerEndpointStochasticAINebula (Symbl.ai)TextGenTitan TakeoffTongyi QwenvLLMWriterXorbits Inference (Xinference)MemoryRetrieversText embedding modelsAgents & ToolkitsToolsVector storesGrouped by providerIntegrationsLLMsOpenLMOn this pageOpenLMOpenLM is a zero-dependency OpenAI-compatible LLM provider that can call different inference endpoints directly via HTTP. It implements the OpenAI Completion class so that it can be used as a drop-in replacement for the OpenAI API. This changeset utilizes BaseOpenAI for minimal added code.This examples goes over how to use LangChain to interact with both OpenAI and HuggingFace. You'll need API keys from both.Setup‚ÄãInstall dependencies and set API keys.# Uncomment to install openlm and openai if you haven't already# !pip install openlm# !pip install openaifrom getpass import getpassimport osimport subprocess# Check if OPENAI_API_KEY environment variable is setif "OPENAI_API_KEY" not in os.environ:    print("Enter your OpenAI API key:")    os.environ["OPENAI_API_KEY"] = getpass()# Check if HF_API_TOKEN environment variable is setif "HF_API_TOKEN" not in os.environ:    print("Enter your HuggingFace Hub API key:")    os.environ["HF_API_TOKEN"] = getpass()Using LangChain with OpenLM‚ÄãHere we're going to call two models in an LLMChain, text-davinci-003 from OpenAI and gpt2 on HuggingFace.from langchain.llms import OpenLMfrom langchain import PromptTemplate, LLMChainAPI Reference:OpenLMquestion = "What is the capital of France?"template = """Question: {question}Answer: Let's think step by step."""prompt = PromptTemplate(template=template, input_variables=["question"])for model in ["text-davinci-003", "huggingface.co/gpt2"]:    llm = OpenLM(model=model)    llm_chain = LLMChain(prompt=prompt, llm=llm)    result = llm_chain.run(question)    print(        """Model: {}Result: {}""".format(            model, result        )    )    Model: text-davinci-003    Result:  France is a country in Europe. The capital of France is Paris.    Model: huggingface.co/gpt2    Result: Question: What is the capital of France?        Answer: Let's think step by step. I am not going to lie, this is a complicated issue, and I don't see any solutions to all this, but it is still far morePreviousOpenLLMNextPetalsSetupUsing LangChain with OpenLMCommunityDiscordTwitterGitHubPythonJS/TSMoreHomepageBlogCopyright ¬© 2023 LangChain, Inc. Skip to main content ü¶úÔ∏èüîó LangChainDocsUse casesIntegrationsAPILangSmithJS/TS DocsCTRLK ü¶úÔ∏èüîó LangChainDocsUse casesIntegrationsAPI LangSmithJS/TS DocsCTRLK  CTRLK CTRLK  CTRLK   IntegrationsCallbacksChat modelsChat loadersDocument loadersDocument transformersLLMsAI21Aleph AlphaAmazon API GatewayAnyscaleAzure MLAzure OpenAIBananaBasetenBeamBedrockBittensorCerebriumAIChatGLMClarifaiCohereC TransformersCTranslate2DatabricksDeepInfraDeepSparseEden AIFireworksForefrontAIGoogle Vertex AI PaLMGooseAIGPT4AllHugging Face HubHugging Face Local PipelinesHuggingface TextGen InferenceJSONFormerKoboldAI APILlama.cppLLM Caching integrationsManifestMinimaxModalMosaicMLNLP CloudOctoAIOllamaOpaquePromptsOpenAIOpenLLMOpenLMPetalsPipelineAIPredibasePrediction GuardPromptLayer OpenAIRELLMReplicateRunhouseSageMakerEndpointStochasticAINebula (Symbl.ai)TextGenTitan TakeoffTongyi QwenvLLMWriterXorbits Inference (Xinference)MemoryRetrieversText embedding modelsAgents & ToolkitsToolsVector storesGrouped by providerIntegrationsLLMsOpenLMOn this pageOpenLMOpenLM is a zero-dependency OpenAI-compatible LLM provider that can call different inference endpoints directly via HTTP. It implements the OpenAI Completion class so that it can be used as a drop-in replacement for the OpenAI API. This changeset utilizes BaseOpenAI for minimal added code.This examples goes over how to use LangChain to interact with both OpenAI and HuggingFace. You'll need API keys from both.Setup‚ÄãInstall dependencies and set API keys.# Uncomment to install openlm and openai if you haven't already# !pip install openlm# !pip install openaifrom getpass import getpassimport osimport subprocess# Check if OPENAI_API_KEY environment variable is setif "OPENAI_API_KEY" not in os.environ:    print("Enter your OpenAI API key:")    os.environ["OPENAI_API_KEY"] = getpass()# Check if HF_API_TOKEN environment variable is setif "HF_API_TOKEN" not in os.environ:    print("Enter your HuggingFace Hub API key:")    os.environ["HF_API_TOKEN"] = getpass()Using LangChain with OpenLM‚ÄãHere we're going to call two models in an LLMChain, text-davinci-003 from OpenAI and gpt2 on HuggingFace.from langchain.llms import OpenLMfrom langchain import PromptTemplate, LLMChainAPI Reference:OpenLMquestion = "What is the capital of France?"template = """Question: {question}Answer: Let's think step by step."""prompt = PromptTemplate(template=template, input_variables=["question"])for model in ["text-davinci-003", "huggingface.co/gpt2"]:    llm = OpenLM(model=model)    llm_chain = LLMChain(prompt=prompt, llm=llm)    result = llm_chain.run(question)    print(        """Model: {}Result: {}""".format(            model, result        )    )    Model: text-davinci-003    Result:  France is a country in Europe. The capital of France is Paris.    Model: huggingface.co/gpt2    Result: Question: What is the capital of France?        Answer: Let's think step by step. I am not going to lie, this is a complicated issue, and I don't see any solutions to all this, but it is still far morePreviousOpenLLMNextPetalsSetupUsing LangChain with OpenLM IntegrationsCallbacksChat modelsChat loadersDocument loadersDocument transformersLLMsAI21Aleph AlphaAmazon API GatewayAnyscaleAzure MLAzure OpenAIBananaBasetenBeamBedrockBittensorCerebriumAIChatGLMClarifaiCohereC TransformersCTranslate2DatabricksDeepInfraDeepSparseEden AIFireworksForefrontAIGoogle Vertex AI PaLMGooseAIGPT4AllHugging Face HubHugging Face Local PipelinesHuggingface TextGen InferenceJSONFormerKoboldAI APILlama.cppLLM Caching integrationsManifestMinimaxModalMosaicMLNLP CloudOctoAIOllamaOpaquePromptsOpenAIOpenLLMOpenLMPetalsPipelineAIPredibasePrediction GuardPromptLayer OpenAIRELLMReplicateRunhouseSageMakerEndpointStochasticAINebula (Symbl.ai)TextGenTitan TakeoffTongyi QwenvLLMWriterXorbits Inference (Xinference)MemoryRetrieversText embedding modelsAgents & ToolkitsToolsVector storesGrouped by providerIntegrationsLLMsOpenLMOn this pageOpenLMOpenLM is a zero-dependency OpenAI-compatible LLM provider that can call different inference endpoints directly via HTTP. It implements the OpenAI Completion class so that it can be used as a drop-in replacement for the OpenAI API. This changeset utilizes BaseOpenAI for minimal added code.This examples goes over how to use LangChain to interact with both OpenAI and HuggingFace. You'll need API keys from both.Setup‚ÄãInstall dependencies and set API keys.# Uncomment to install openlm and openai if you haven't already# !pip install openlm# !pip install openaifrom getpass import getpassimport osimport subprocess# Check if OPENAI_API_KEY environment variable is setif "OPENAI_API_KEY" not in os.environ:    print("Enter your OpenAI API key:")    os.environ["OPENAI_API_KEY"] = getpass()# Check if HF_API_TOKEN environment variable is setif "HF_API_TOKEN" not in os.environ:    print("Enter your HuggingFace Hub API key:")    os.environ["HF_API_TOKEN"] = getpass()Using LangChain with OpenLM‚ÄãHere we're going to call two models in an LLMChain, text-davinci-003 from OpenAI and gpt2 on HuggingFace.from langchain.llms import OpenLMfrom langchain import PromptTemplate, LLMChainAPI Reference:OpenLMquestion = "What is the capital of France?"template = """Question: {question}Answer: Let's think step by step."""prompt = PromptTemplate(template=template, input_variables=["question"])for model in ["text-davinci-003", "huggingface.co/gpt2"]:    llm = OpenLM(model=model)    llm_chain = LLMChain(prompt=prompt, llm=llm)    result = llm_chain.run(question)    print(        """Model: {}Result: {}""".format(            model, result        )    )    Model: text-davinci-003    Result:  France is a country in Europe. The capital of France is Paris.    Model: huggingface.co/gpt2    Result: Question: What is the capital of France?        Answer: Let's think step by step. I am not going to lie, this is a complicated issue, and I don't see any solutions to all this, but it is still far morePreviousOpenLLMNextPetalsSetupUsing LangChain with OpenLM IntegrationsCallbacksChat modelsChat loadersDocument loadersDocument transformersLLMsAI21Aleph AlphaAmazon API GatewayAnyscaleAzure MLAzure OpenAIBananaBasetenBeamBedrockBittensorCerebriumAIChatGLMClarifaiCohereC TransformersCTranslate2DatabricksDeepInfraDeepSparseEden AIFireworksForefrontAIGoogle Vertex AI PaLMGooseAIGPT4AllHugging Face HubHugging Face Local PipelinesHuggingface TextGen InferenceJSONFormerKoboldAI APILlama.cppLLM Caching integrationsManifestMinimaxModalMosaicMLNLP CloudOctoAIOllamaOpaquePromptsOpenAIOpenLLMOpenLMPetalsPipelineAIPredibasePrediction GuardPromptLayer OpenAIRELLMReplicateRunhouseSageMakerEndpointStochasticAINebula (Symbl.ai)TextGenTitan TakeoffTongyi QwenvLLMWriterXorbits Inference (Xinference)MemoryRetrieversText embedding modelsAgents & ToolkitsToolsVector storesGrouped by provider IntegrationsCallbacksChat modelsChat loadersDocument loadersDocument transformersLLMsAI21Aleph AlphaAmazon API GatewayAnyscaleAzure MLAzure OpenAIBananaBasetenBeamBedrockBittensorCerebriumAIChatGLMClarifaiCohereC TransformersCTranslate2DatabricksDeepInfraDeepSparseEden AIFireworksForefrontAIGoogle Vertex AI PaLMGooseAIGPT4AllHugging Face HubHugging Face Local PipelinesHuggingface TextGen InferenceJSONFormerKoboldAI APILlama.cppLLM Caching integrationsManifestMinimaxModalMosaicMLNLP CloudOctoAIOllamaOpaquePromptsOpenAIOpenLLMOpenLMPetalsPipelineAIPredibasePrediction GuardPromptLayer OpenAIRELLMReplicateRunhouseSageMakerEndpointStochasticAINebula (Symbl.ai)TextGenTitan TakeoffTongyi QwenvLLMWriterXorbits Inference (Xinference)MemoryRetrieversText embedding modelsAgents & ToolkitsToolsVector storesGrouped by provider Integrations Callbacks Chat models Chat loaders Document loaders Document transformers LLMs Memory Retrievers Text embedding models Agents & Toolkits Tools Vector stores Grouped by provider IntegrationsLLMsOpenLMOn this pageOpenLMOpenLM is a zero-dependency OpenAI-compatible LLM provider that can call different inference endpoints directly via HTTP. It implements the OpenAI Completion class so that it can be used as a drop-in replacement for the OpenAI API. This changeset utilizes BaseOpenAI for minimal added code.This examples goes over how to use LangChain to interact with both OpenAI and HuggingFace. You'll need API keys from both.Setup‚ÄãInstall dependencies and set API keys.# Uncomment to install openlm and openai if you haven't already# !pip install openlm# !pip install openaifrom getpass import getpassimport osimport subprocess# Check if OPENAI_API_KEY environment variable is setif "OPENAI_API_KEY" not in os.environ:    print("Enter your OpenAI API key:")    os.environ["OPENAI_API_KEY"] = getpass()# Check if HF_API_TOKEN environment variable is setif "HF_API_TOKEN" not in os.environ:    print("Enter your HuggingFace Hub API key:")    os.environ["HF_API_TOKEN"] = getpass()Using LangChain with OpenLM‚ÄãHere we're going to call two models in an LLMChain, text-davinci-003 from OpenAI and gpt2 on HuggingFace.from langchain.llms import OpenLMfrom langchain import PromptTemplate, LLMChainAPI Reference:OpenLMquestion = "What is the capital of France?"template = """Question: {question}Answer: Let's think step by step."""prompt = PromptTemplate(template=template, input_variables=["question"])for model in ["text-davinci-003", "huggingface.co/gpt2"]:    llm = OpenLM(model=model)    llm_chain = LLMChain(prompt=prompt, llm=llm)    result = llm_chain.run(question)    print(        """Model: {}Result: {}""".format(            model, result        )    )    Model: text-davinci-003    Result:  France is a country in Europe. The capital of France is Paris.    Model: huggingface.co/gpt2    Result: Question: What is the capital of France?        Answer: Let's think step by step. I am not going to lie, this is a complicated issue, and I don't see any solutions to all this, but it is still far morePreviousOpenLLMNextPetalsSetupUsing LangChain with OpenLM IntegrationsLLMsOpenLMOn this pageOpenLMOpenLM is a zero-dependency OpenAI-compatible LLM provider that can call different inference endpoints directly via HTTP. It implements the OpenAI Completion class so that it can be used as a drop-in replacement for the OpenAI API. This changeset utilizes BaseOpenAI for minimal added code.This examples goes over how to use LangChain to interact with both OpenAI and HuggingFace. You'll need API keys from both.Setup‚ÄãInstall dependencies and set API keys.# Uncomment to install openlm and openai if you haven't already# !pip install openlm# !pip install openaifrom getpass import getpassimport osimport subprocess# Check if OPENAI_API_KEY environment variable is setif "OPENAI_API_KEY" not in os.environ:    print("Enter your OpenAI API key:")    os.environ["OPENAI_API_KEY"] = getpass()# Check if HF_API_TOKEN environment variable is setif "HF_API_TOKEN" not in os.environ:    print("Enter your HuggingFace Hub API key:")    os.environ["HF_API_TOKEN"] = getpass()Using LangChain with OpenLM‚ÄãHere we're going to call two models in an LLMChain, text-davinci-003 from OpenAI and gpt2 on HuggingFace.from langchain.llms import OpenLMfrom langchain import PromptTemplate, LLMChainAPI Reference:OpenLMquestion = "What is the capital of France?"template = """Question: {question}Answer: Let's think step by step."""prompt = PromptTemplate(template=template, input_variables=["question"])for model in ["text-davinci-003", "huggingface.co/gpt2"]:    llm = OpenLM(model=model)    llm_chain = LLMChain(prompt=prompt, llm=llm)    result = llm_chain.run(question)    print(        """Model: {}Result: {}""".format(            model, result        )    )    Model: text-davinci-003    Result:  France is a country in Europe. The capital of France is Paris.    Model: huggingface.co/gpt2    Result: Question: What is the capital of France?        Answer: Let's think step by step. I am not going to lie, this is a complicated issue, and I don't see any solutions to all this, but it is still far morePreviousOpenLLMNextPetalsSetupUsing LangChain with OpenLM IntegrationsLLMsOpenLMOn this pageOpenLMOpenLM is a zero-dependency OpenAI-compatible LLM provider that can call different inference endpoints directly via HTTP. It implements the OpenAI Completion class so that it can be used as a drop-in replacement for the OpenAI API. This changeset utilizes BaseOpenAI for minimal added code.This examples goes over how to use LangChain to interact with both OpenAI and HuggingFace. You'll need API keys from both.Setup‚ÄãInstall dependencies and set API keys.# Uncomment to install openlm and openai if you haven't already# !pip install openlm# !pip install openaifrom getpass import getpassimport osimport subprocess# Check if OPENAI_API_KEY environment variable is setif "OPENAI_API_KEY" not in os.environ:    print("Enter your OpenAI API key:")    os.environ["OPENAI_API_KEY"] = getpass()# Check if HF_API_TOKEN environment variable is setif "HF_API_TOKEN" not in os.environ:    print("Enter your HuggingFace Hub API key:")    os.environ["HF_API_TOKEN"] = getpass()Using LangChain with OpenLM‚ÄãHere we're going to call two models in an LLMChain, text-davinci-003 from OpenAI and gpt2 on HuggingFace.from langchain.llms import OpenLMfrom langchain import PromptTemplate, LLMChainAPI Reference:OpenLMquestion = "What is the capital of France?"template = """Question: {question}Answer: Let's think step by step."""prompt = PromptTemplate(template=template, input_variables=["question"])for model in ["text-davinci-003", "huggingface.co/gpt2"]:    llm = OpenLM(model=model)    llm_chain = LLMChain(prompt=prompt, llm=llm)    result = llm_chain.run(question)    print(        """Model: {}Result: {}""".format(            model, result        )    )    Model: text-davinci-003    Result:  France is a country in Europe. The capital of France is Paris.    Model: huggingface.co/gpt2    Result: Question: What is the capital of France?        Answer: Let's think step by step. I am not going to lie, this is a complicated issue, and I don't see any solutions to all this, but it is still far morePreviousOpenLLMNextPetals IntegrationsLLMsOpenLMOn this pageOpenLMOpenLM is a zero-dependency OpenAI-compatible LLM provider that can call different inference endpoints directly via HTTP. It implements the OpenAI Completion class so that it can be used as a drop-in replacement for the OpenAI API. This changeset utilizes BaseOpenAI for minimal added code.This examples goes over how to use LangChain to interact with both OpenAI and HuggingFace. You'll need API keys from both.Setup‚ÄãInstall dependencies and set API keys.# Uncomment to install openlm and openai if you haven't already# !pip install openlm# !pip install openaifrom getpass import getpassimport osimport subprocess# Check if OPENAI_API_KEY environment variable is setif "OPENAI_API_KEY" not in os.environ:    print("Enter your OpenAI API key:")    os.environ["OPENAI_API_KEY"] = getpass()# Check if HF_API_TOKEN environment variable is setif "HF_API_TOKEN" not in os.environ:    print("Enter your HuggingFace Hub API key:")    os.environ["HF_API_TOKEN"] = getpass()Using LangChain with OpenLM‚ÄãHere we're going to call two models in an LLMChain, text-davinci-003 from OpenAI and gpt2 on HuggingFace.from langchain.llms import OpenLMfrom langchain import PromptTemplate, LLMChainAPI Reference:OpenLMquestion = "What is the capital of France?"template = """Question: {question}Answer: Let's think step by step."""prompt = PromptTemplate(template=template, input_variables=["question"])for model in ["text-davinci-003", "huggingface.co/gpt2"]:    llm = OpenLM(model=model)    llm_chain = LLMChain(prompt=prompt, llm=llm)    result = llm_chain.run(question)    print(        """Model: {}Result: {}""".format(            model, result        )    )    Model: text-davinci-003    Result:  France is a country in Europe. The capital of France is Paris.    Model: huggingface.co/gpt2    Result: Question: What is the capital of France?        Answer: Let's think step by step. I am not going to lie, this is a complicated issue, and I don't see any solutions to all this, but it is still far morePreviousOpenLLMNextPetals On this page OpenLMOpenLM is a zero-dependency OpenAI-compatible LLM provider that can call different inference endpoints directly via HTTP. It implements the OpenAI Completion class so that it can be used as a drop-in replacement for the OpenAI API. This changeset utilizes BaseOpenAI for minimal added code.This examples goes over how to use LangChain to interact with both OpenAI and HuggingFace. You'll need API keys from both.Setup‚ÄãInstall dependencies and set API keys.# Uncomment to install openlm and openai if you haven't already# !pip install openlm# !pip install openaifrom getpass import getpassimport osimport subprocess# Check if OPENAI_API_KEY environment variable is setif "OPENAI_API_KEY" not in os.environ:    print("Enter your OpenAI API key:")    os.environ["OPENAI_API_KEY"] = getpass()# Check if HF_API_TOKEN environment variable is setif "HF_API_TOKEN" not in os.environ:    print("Enter your HuggingFace Hub API key:")    os.environ["HF_API_TOKEN"] = getpass()Using LangChain with OpenLM‚ÄãHere we're going to call two models in an LLMChain, text-davinci-003 from OpenAI and gpt2 on HuggingFace.from langchain.llms import OpenLMfrom langchain import PromptTemplate, LLMChainAPI Reference:OpenLMquestion = "What is the capital of France?"template = """Question: {question}Answer: Let's think step by step."""prompt = PromptTemplate(template=template, input_variables=["question"])for model in ["text-davinci-003", "huggingface.co/gpt2"]:    llm = OpenLM(model=model)    llm_chain = LLMChain(prompt=prompt, llm=llm)    result = llm_chain.run(question)    print(        """Model: {}Result: {}""".format(            model, result        )    )    Model: text-davinci-003    Result:  France is a country in Europe. The capital of France is Paris.    Model: huggingface.co/gpt2    Result: Question: What is the capital of France?        Answer: Let's think step by step. I am not going to lie, this is a complicated issue, and I don't see any solutions to all this, but it is still far more # Uncomment to install openlm and openai if you haven't already# !pip install openlm# !pip install openai # Uncomment to install openlm and openai if you haven't already# !pip install openlm# !pip install openai  from getpass import getpassimport osimport subprocess# Check if OPENAI_API_KEY environment variable is setif "OPENAI_API_KEY" not in os.environ:    print("Enter your OpenAI API key:")    os.environ["OPENAI_API_KEY"] = getpass()# Check if HF_API_TOKEN environment variable is setif "HF_API_TOKEN" not in os.environ:    print("Enter your HuggingFace Hub API key:")    os.environ["HF_API_TOKEN"] = getpass() from getpass import getpassimport osimport subprocess# Check if OPENAI_API_KEY environment variable is setif "OPENAI_API_KEY" not in os.environ:    print("Enter your OpenAI API key:")    os.environ["OPENAI_API_KEY"] = getpass()# Check if HF_API_TOKEN environment variable is setif "HF_API_TOKEN" not in os.environ:    print("Enter your HuggingFace Hub API key:")    os.environ["HF_API_TOKEN"] = getpass()  from langchain.llms import OpenLMfrom langchain import PromptTemplate, LLMChain from langchain.llms import OpenLMfrom langchain import PromptTemplate, LLMChain  API Reference:OpenLM question = "What is the capital of France?"template = """Question: {question}Answer: Let's think step by step."""prompt = PromptTemplate(template=template, input_variables=["question"])for model in ["text-davinci-003", "huggingface.co/gpt2"]:    llm = OpenLM(model=model)    llm_chain = LLMChain(prompt=prompt, llm=llm)    result = llm_chain.run(question)    print(        """Model: {}Result: {}""".format(            model, result        )    ) question = "What is the capital of France?"template = """Question: {question}Answer: Let's think step by step."""prompt = PromptTemplate(template=template, input_variables=["question"])for model in ["text-davinci-003", "huggingface.co/gpt2"]:    llm = OpenLM(model=model)    llm_chain = LLMChain(prompt=prompt, llm=llm)    result = llm_chain.run(question)    print(        """Model: {}Result: {}""".format(            model, result        )    )      Model: text-davinci-003    Result:  France is a country in Europe. The capital of France is Paris.    Model: huggingface.co/gpt2    Result: Question: What is the capital of France?        Answer: Let's think step by step. I am not going to lie, this is a complicated issue, and I don't see any solutions to all this, but it is still far more     Model: text-davinci-003    Result:  France is a country in Europe. The capital of France is Paris.    Model: huggingface.co/gpt2    Result: Question: What is the capital of France?        Answer: Let's think step by step. I am not going to lie, this is a complicated issue, and I don't see any solutions to all this, but it is still far more     Model: text-davinci-003    Result:  France is a country in Europe. The capital of France is Paris.    Model: huggingface.co/gpt2    Result: Question: What is the capital of France?        Answer: Let's think step by step. I am not going to lie, this is a complicated issue, and I don't see any solutions to all this, but it is still far more  Previous OpenLLM Next Petals SetupUsing LangChain with OpenLM SetupUsing LangChain with OpenLM CommunityDiscordTwitterGitHubPythonJS/TSMoreHomepageBlogCopyright ¬© 2023 LangChain, Inc. CommunityDiscordTwitterGitHubPythonJS/TSMoreHomepageBlog CommunityDiscordTwitter Community GitHubPythonJS/TS GitHub MoreHomepageBlog More Copyright ¬© 2023 LangChain, Inc. Copyright ¬© 2023 LangChain, Inc. Skip to main content (#docusaurus_skipToContent_fallback) ü¶úÔ∏èüîó LangChain (/) Docs (/docs/get_started/introduction) Use cases (/docs/use_cases) Integrations (/docs/integrations) API (https://api.python.langchain.com) LangSmith (https://smith.langchain.com) JS/TS Docs (https://js.langchain.com/docs)  (https://github.com/hwchase17/langchain) Integrations (/docs/integrations) Callbacks (/docs/integrations/callbacks/) Chat models (/docs/integrations/chat/) Chat loaders (/docs/integrations/chat_loaders/) Document loaders (/docs/integrations/document_loaders/) Document transformers (/docs/integrations/document_transformers/) LLMs (/docs/integrations/llms/) AI21 (/docs/integrations/llms/ai21) Aleph Alpha (/docs/integrations/llms/aleph_alpha) Amazon API Gateway (/docs/integrations/llms/amazon_api_gateway) Anyscale (/docs/integrations/llms/anyscale) Azure ML (/docs/integrations/llms/azure_ml) Azure OpenAI (/docs/integrations/llms/azure_openai) Banana (/docs/integrations/llms/banana) Baseten (/docs/integrations/llms/baseten) Beam (/docs/integrations/llms/beam) Bedrock (/docs/integrations/llms/bedrock) Bittensor (/docs/integrations/llms/bittensor) CerebriumAI (/docs/integrations/llms/cerebriumai) ChatGLM (/docs/integrations/llms/chatglm) Clarifai (/docs/integrations/llms/clarifai) Cohere (/docs/integrations/llms/cohere) C Transformers (/docs/integrations/llms/ctransformers) CTranslate2 (/docs/integrations/llms/ctranslate2) Databricks (/docs/integrations/llms/databricks) DeepInfra (/docs/integrations/llms/deepinfra) DeepSparse (/docs/integrations/llms/deepsparse) Eden AI (/docs/integrations/llms/edenai) Fireworks (/docs/integrations/llms/fireworks) ForefrontAI (/docs/integrations/llms/forefrontai) Google Vertex AI PaLM (/docs/integrations/llms/google_vertex_ai_palm) GooseAI (/docs/integrations/llms/gooseai) GPT4All (/docs/integrations/llms/gpt4all) Hugging Face Hub (/docs/integrations/llms/huggingface_hub) Hugging Face Local Pipelines (/docs/integrations/llms/huggingface_pipelines) Huggingface TextGen Inference (/docs/integrations/llms/huggingface_textgen_inference) JSONFormer (/docs/integrations/llms/jsonformer_experimental) KoboldAI API (/docs/integrations/llms/koboldai) Llama.cpp (/docs/integrations/llms/llamacpp) LLM Caching integrations (/docs/integrations/llms/llm_caching) Manifest (/docs/integrations/llms/manifest) Minimax (/docs/integrations/llms/minimax) Modal (/docs/integrations/llms/modal) MosaicML (/docs/integrations/llms/mosaicml) NLP Cloud (/docs/integrations/llms/nlpcloud) OctoAI (/docs/integrations/llms/octoai) Ollama (/docs/integrations/llms/ollama) OpaquePrompts (/docs/integrations/llms/opaqueprompts) OpenAI (/docs/integrations/llms/openai) OpenLLM (/docs/integrations/llms/openllm) OpenLM (/docs/integrations/llms/openlm) Petals (/docs/integrations/llms/petals) PipelineAI (/docs/integrations/llms/pipelineai) Predibase (/docs/integrations/llms/predibase) Prediction Guard (/docs/integrations/llms/predictionguard) PromptLayer OpenAI (/docs/integrations/llms/promptlayer_openai) RELLM (/docs/integrations/llms/rellm_experimental) Replicate (/docs/integrations/llms/replicate) Runhouse (/docs/integrations/llms/runhouse) SageMakerEndpoint (/docs/integrations/llms/sagemaker) StochasticAI (/docs/integrations/llms/stochasticai) Nebula (Symbl.ai) (/docs/integrations/llms/symblai_nebula) TextGen (/docs/integrations/llms/textgen) Titan Takeoff (/docs/integrations/llms/titan_takeoff) Tongyi Qwen (/docs/integrations/llms/tongyi) vLLM (/docs/integrations/llms/vllm) Writer (/docs/integrations/llms/writer) Xorbits Inference (Xinference) (/docs/integrations/llms/xinference) Memory (/docs/integrations/memory/) Retrievers (/docs/integrations/retrievers/) Text embedding models (/docs/integrations/text_embedding/) Agents & Toolkits (/docs/integrations/toolkits/) Tools (/docs/integrations/tools/) Vector stores (/docs/integrations/vectorstores/) Grouped by provider (/docs/integrations/providers/)  (/) Integrations (/docs/integrations) LLMs (/docs/integrations/llms/) OpenLM (https://github.com/r2d4/openlm) ‚Äã (#setup) ‚Äã (#using-langchain-with-openlm) OpenLM (https://api.python.langchain.com/en/latest/llms/langchain.llms.openlm.OpenLM.html) PreviousOpenLLM (/docs/integrations/llms/openllm) NextPetals (/docs/integrations/llms/petals) Setup (#setup) Using LangChain with OpenLM (#using-langchain-with-openlm) Discord (https://discord.gg/cU2adEyC7w) Twitter (https://twitter.com/LangChainAI) Python (https://github.com/hwchase17/langchain) JS/TS (https://github.com/hwchase17/langchainjs) Homepage (https://langchain.com) Blog (https://blog.langchain.dev)