Lord of the Retrievers, also known as MergerRetriever, takes a list of retrievers as input and merges the results of their get_relevant_documents() methods into a single list. The merged results will be a list of documents that are relevant to the query and that have been ranked by the different retrievers. The MergerRetriever class can be used to improve the accuracy of document retrieval in a number of ways. First, it can combine the results of multiple retrievers, which can help to reduce the risk of bias in the results. Second, it can rank the results of the different retrievers, which can help to ensure that the most relevant documents are returned first. No matter the architecture of your model, there is a sustancial performance degradation when you include 10+ retrieved documents. In brief: When models must access relevant information  in the middle of long contexts, then tend to ignore the provided documents. See: https://arxiv.org/abs//2307.03172 IntegrationsCallbacksChat modelsChat loadersDocument loadersDocument transformersLLMsMemoryRetrieversAmazon KendraArxivAzure Cognitive SearchBM25ChaindeskChatGPT PluginCohere RerankerDocArray RetrieverElasticSearch BM25Google Cloud Enterprise SearchGoogle Drive RetrieverkNNLOTR (Merger Retriever)MetalPinecone Hybrid SearchPubMedRePhraseQueryRetrieverSVMTF-IDFVespaWeaviate Hybrid SearchWikipediaZepText embedding modelsAgents & ToolkitsToolsVector storesGrouped by provider Callbacks Chat models Chat loaders Document loaders Document transformers LLMs Memory RetrieversAmazon KendraArxivAzure Cognitive SearchBM25ChaindeskChatGPT PluginCohere RerankerDocArray RetrieverElasticSearch BM25Google Cloud Enterprise SearchGoogle Drive RetrieverkNNLOTR (Merger Retriever)MetalPinecone Hybrid SearchPubMedRePhraseQueryRetrieverSVMTF-IDFVespaWeaviate Hybrid SearchWikipediaZep Amazon Kendra Arxiv Azure Cognitive Search BM25 Chaindesk ChatGPT Plugin Cohere Reranker DocArray Retriever ElasticSearch BM25 Google Cloud Enterprise Search Google Drive Retriever kNN LOTR (Merger Retriever) Metal Pinecone Hybrid Search PubMed RePhraseQueryRetriever SVM TF-IDF Vespa Weaviate Hybrid Search Wikipedia Zep Text embedding models Agents & Toolkits Tools Vector stores Grouped by provider  Integrations Retrievers LOTR (Merger Retriever) MergerRetriever Chroma HuggingFaceEmbeddings OpenAIEmbeddings EmbeddingsRedundantFilter EmbeddingsClusteringFilter DocumentCompressorPipeline ContextualCompressionRetriever LongContextReorder Remove redundant results from the merged retrievers. Pick a representative sample of documents from the merged retrievers. Re-order results to avoid performance degradation. Discord Twitter Python JS/TS Homepage Blog Skip to main contentü¶úÔ∏èüîó LangChainDocsUse casesIntegrationsAPILangSmithJS/TS DocsCTRLKIntegrationsCallbacksChat modelsChat loadersDocument loadersDocument transformersLLMsMemoryRetrieversAmazon KendraArxivAzure Cognitive SearchBM25ChaindeskChatGPT PluginCohere RerankerDocArray RetrieverElasticSearch BM25Google Cloud Enterprise SearchGoogle Drive RetrieverkNNLOTR (Merger Retriever)MetalPinecone Hybrid SearchPubMedRePhraseQueryRetrieverSVMTF-IDFVespaWeaviate Hybrid SearchWikipediaZepText embedding modelsAgents & ToolkitsToolsVector storesGrouped by providerIntegrationsRetrieversLOTR (Merger Retriever)On this pageLOTR (Merger Retriever)Lord of the Retrievers, also known as MergerRetriever, takes a list of retrievers as input and merges the results of their get_relevant_documents() methods into a single list. The merged results will be a list of documents that are relevant to the query and that have been ranked by the different retrievers.The MergerRetriever class can be used to improve the accuracy of document retrieval in a number of ways. First, it can combine the results of multiple retrievers, which can help to reduce the risk of bias in the results. Second, it can rank the results of the different retrievers, which can help to ensure that the most relevant documents are returned first.import osimport chromadbfrom langchain.retrievers.merger_retriever import MergerRetrieverfrom langchain.vectorstores import Chromafrom langchain.embeddings import HuggingFaceEmbeddingsfrom langchain.embeddings import OpenAIEmbeddingsfrom langchain.document_transformers import (    EmbeddingsRedundantFilter,    EmbeddingsClusteringFilter,)from langchain.retrievers.document_compressors import DocumentCompressorPipelinefrom langchain.retrievers import ContextualCompressionRetriever# Get 3 diff embeddings.all_mini = HuggingFaceEmbeddings(model_name="all-MiniLM-L6-v2")multi_qa_mini = HuggingFaceEmbeddings(model_name="multi-qa-MiniLM-L6-dot-v1")filter_embeddings = OpenAIEmbeddings()ABS_PATH = os.path.dirname(os.path.abspath(__file__))DB_DIR = os.path.join(ABS_PATH, "db")# Instantiate 2 diff cromadb indexs, each one with a diff embedding.client_settings = chromadb.config.Settings(    is_persistent=True,    persist_directory=DB_DIR,    anonymized_telemetry=False,)db_all = Chroma(    collection_name="project_store_all",    persist_directory=DB_DIR,    client_settings=client_settings,    embedding_function=all_mini,)db_multi_qa = Chroma(    collection_name="project_store_multi",    persist_directory=DB_DIR,    client_settings=client_settings,    embedding_function=multi_qa_mini,)# Define 2 diff retrievers with 2 diff embeddings and diff search type.retriever_all = db_all.as_retriever(    search_type="similarity", search_kwargs={"k": 5, "include_metadata": True})retriever_multi_qa = db_multi_qa.as_retriever(    search_type="mmr", search_kwargs={"k": 5, "include_metadata": True})# The Lord of the Retrievers will hold the ouput of boths retrievers and can be used as any other# retriever on different types of chains.lotr = MergerRetriever(retrievers=[retriever_all, retriever_multi_qa])API Reference:MergerRetrieverChromaHuggingFaceEmbeddingsOpenAIEmbeddingsEmbeddingsRedundantFilterEmbeddingsClusteringFilterDocumentCompressorPipelineContextualCompressionRetrieverRemove redundant results from the merged retrievers.‚Äã# We can remove redundant results from both retrievers using yet another embedding.# Using multiples embeddings in diff steps could help reduce biases.filter = EmbeddingsRedundantFilter(embeddings=filter_embeddings)pipeline = DocumentCompressorPipeline(transformers=[filter])compression_retriever = ContextualCompressionRetriever(    base_compressor=pipeline, base_retriever=lotr)Pick a representative sample of documents from the merged retrievers.‚Äã# This filter will divide the documents vectors into clusters or "centers" of meaning.# Then it will pick the closest document to that center for the final results.# By default the result document will be ordered/grouped by clusters.filter_ordered_cluster = EmbeddingsClusteringFilter(    embeddings=filter_embeddings,    num_clusters=10,    num_closest=1,)# If you want the final document to be ordered by the original retriever scores# you need to add the "sorted" parameter.filter_ordered_by_retriever = EmbeddingsClusteringFilter(    embeddings=filter_embeddings,    num_clusters=10,    num_closest=1,    sorted=True,)pipeline = DocumentCompressorPipeline(transformers=[filter_ordered_by_retriever])compression_retriever = ContextualCompressionRetriever(    base_compressor=pipeline, base_retriever=lotr)Re-order results to avoid performance degradation.‚ÄãNo matter the architecture of your model, there is a sustancial performance degradation when you include 10+ retrieved documents. See: https://arxiv.org/abs//2307.03172# You can use an additional document transformer to reorder documents after removing redudance.from langchain.document_transformers import LongContextReorderfilter = EmbeddingsRedundantFilter(embeddings=filter_embeddings)reordering = LongContextReorder()pipeline = DocumentCompressorPipeline(transformers=[filter, reordering])compression_retriever_reordered = ContextualCompressionRetriever(    base_compressor=pipeline, base_retriever=lotr)API Reference:LongContextReorderPreviouskNNNextMetalRemove redundant results from the merged retrievers.Pick a representative sample of documents from the merged retrievers.Re-order results to avoid performance degradation.CommunityDiscordTwitterGitHubPythonJS/TSMoreHomepageBlogCopyright ¬© 2023 LangChain, Inc. Skip to main content ü¶úÔ∏èüîó LangChainDocsUse casesIntegrationsAPILangSmithJS/TS DocsCTRLK ü¶úÔ∏èüîó LangChainDocsUse casesIntegrationsAPI LangSmithJS/TS DocsCTRLK  CTRLK CTRLK  CTRLK   IntegrationsCallbacksChat modelsChat loadersDocument loadersDocument transformersLLMsMemoryRetrieversAmazon KendraArxivAzure Cognitive SearchBM25ChaindeskChatGPT PluginCohere RerankerDocArray RetrieverElasticSearch BM25Google Cloud Enterprise SearchGoogle Drive RetrieverkNNLOTR (Merger Retriever)MetalPinecone Hybrid SearchPubMedRePhraseQueryRetrieverSVMTF-IDFVespaWeaviate Hybrid SearchWikipediaZepText embedding modelsAgents & ToolkitsToolsVector storesGrouped by providerIntegrationsRetrieversLOTR (Merger Retriever)On this pageLOTR (Merger Retriever)Lord of the Retrievers, also known as MergerRetriever, takes a list of retrievers as input and merges the results of their get_relevant_documents() methods into a single list. The merged results will be a list of documents that are relevant to the query and that have been ranked by the different retrievers.The MergerRetriever class can be used to improve the accuracy of document retrieval in a number of ways. First, it can combine the results of multiple retrievers, which can help to reduce the risk of bias in the results. Second, it can rank the results of the different retrievers, which can help to ensure that the most relevant documents are returned first.import osimport chromadbfrom langchain.retrievers.merger_retriever import MergerRetrieverfrom langchain.vectorstores import Chromafrom langchain.embeddings import HuggingFaceEmbeddingsfrom langchain.embeddings import OpenAIEmbeddingsfrom langchain.document_transformers import (    EmbeddingsRedundantFilter,    EmbeddingsClusteringFilter,)from langchain.retrievers.document_compressors import DocumentCompressorPipelinefrom langchain.retrievers import ContextualCompressionRetriever# Get 3 diff embeddings.all_mini = HuggingFaceEmbeddings(model_name="all-MiniLM-L6-v2")multi_qa_mini = HuggingFaceEmbeddings(model_name="multi-qa-MiniLM-L6-dot-v1")filter_embeddings = OpenAIEmbeddings()ABS_PATH = os.path.dirname(os.path.abspath(__file__))DB_DIR = os.path.join(ABS_PATH, "db")# Instantiate 2 diff cromadb indexs, each one with a diff embedding.client_settings = chromadb.config.Settings(    is_persistent=True,    persist_directory=DB_DIR,    anonymized_telemetry=False,)db_all = Chroma(    collection_name="project_store_all",    persist_directory=DB_DIR,    client_settings=client_settings,    embedding_function=all_mini,)db_multi_qa = Chroma(    collection_name="project_store_multi",    persist_directory=DB_DIR,    client_settings=client_settings,    embedding_function=multi_qa_mini,)# Define 2 diff retrievers with 2 diff embeddings and diff search type.retriever_all = db_all.as_retriever(    search_type="similarity", search_kwargs={"k": 5, "include_metadata": True})retriever_multi_qa = db_multi_qa.as_retriever(    search_type="mmr", search_kwargs={"k": 5, "include_metadata": True})# The Lord of the Retrievers will hold the ouput of boths retrievers and can be used as any other# retriever on different types of chains.lotr = MergerRetriever(retrievers=[retriever_all, retriever_multi_qa])API Reference:MergerRetrieverChromaHuggingFaceEmbeddingsOpenAIEmbeddingsEmbeddingsRedundantFilterEmbeddingsClusteringFilterDocumentCompressorPipelineContextualCompressionRetrieverRemove redundant results from the merged retrievers.‚Äã# We can remove redundant results from both retrievers using yet another embedding.# Using multiples embeddings in diff steps could help reduce biases.filter = EmbeddingsRedundantFilter(embeddings=filter_embeddings)pipeline = DocumentCompressorPipeline(transformers=[filter])compression_retriever = ContextualCompressionRetriever(    base_compressor=pipeline, base_retriever=lotr)Pick a representative sample of documents from the merged retrievers.‚Äã# This filter will divide the documents vectors into clusters or "centers" of meaning.# Then it will pick the closest document to that center for the final results.# By default the result document will be ordered/grouped by clusters.filter_ordered_cluster = EmbeddingsClusteringFilter(    embeddings=filter_embeddings,    num_clusters=10,    num_closest=1,)# If you want the final document to be ordered by the original retriever scores# you need to add the "sorted" parameter.filter_ordered_by_retriever = EmbeddingsClusteringFilter(    embeddings=filter_embeddings,    num_clusters=10,    num_closest=1,    sorted=True,)pipeline = DocumentCompressorPipeline(transformers=[filter_ordered_by_retriever])compression_retriever = ContextualCompressionRetriever(    base_compressor=pipeline, base_retriever=lotr)Re-order results to avoid performance degradation.‚ÄãNo matter the architecture of your model, there is a sustancial performance degradation when you include 10+ retrieved documents. See: https://arxiv.org/abs//2307.03172# You can use an additional document transformer to reorder documents after removing redudance.from langchain.document_transformers import LongContextReorderfilter = EmbeddingsRedundantFilter(embeddings=filter_embeddings)reordering = LongContextReorder()pipeline = DocumentCompressorPipeline(transformers=[filter, reordering])compression_retriever_reordered = ContextualCompressionRetriever(    base_compressor=pipeline, base_retriever=lotr)API Reference:LongContextReorderPreviouskNNNextMetalRemove redundant results from the merged retrievers.Pick a representative sample of documents from the merged retrievers.Re-order results to avoid performance degradation. IntegrationsCallbacksChat modelsChat loadersDocument loadersDocument transformersLLMsMemoryRetrieversAmazon KendraArxivAzure Cognitive SearchBM25ChaindeskChatGPT PluginCohere RerankerDocArray RetrieverElasticSearch BM25Google Cloud Enterprise SearchGoogle Drive RetrieverkNNLOTR (Merger Retriever)MetalPinecone Hybrid SearchPubMedRePhraseQueryRetrieverSVMTF-IDFVespaWeaviate Hybrid SearchWikipediaZepText embedding modelsAgents & ToolkitsToolsVector storesGrouped by providerIntegrationsRetrieversLOTR (Merger Retriever)On this pageLOTR (Merger Retriever)Lord of the Retrievers, also known as MergerRetriever, takes a list of retrievers as input and merges the results of their get_relevant_documents() methods into a single list. The merged results will be a list of documents that are relevant to the query and that have been ranked by the different retrievers.The MergerRetriever class can be used to improve the accuracy of document retrieval in a number of ways. First, it can combine the results of multiple retrievers, which can help to reduce the risk of bias in the results. Second, it can rank the results of the different retrievers, which can help to ensure that the most relevant documents are returned first.import osimport chromadbfrom langchain.retrievers.merger_retriever import MergerRetrieverfrom langchain.vectorstores import Chromafrom langchain.embeddings import HuggingFaceEmbeddingsfrom langchain.embeddings import OpenAIEmbeddingsfrom langchain.document_transformers import (    EmbeddingsRedundantFilter,    EmbeddingsClusteringFilter,)from langchain.retrievers.document_compressors import DocumentCompressorPipelinefrom langchain.retrievers import ContextualCompressionRetriever# Get 3 diff embeddings.all_mini = HuggingFaceEmbeddings(model_name="all-MiniLM-L6-v2")multi_qa_mini = HuggingFaceEmbeddings(model_name="multi-qa-MiniLM-L6-dot-v1")filter_embeddings = OpenAIEmbeddings()ABS_PATH = os.path.dirname(os.path.abspath(__file__))DB_DIR = os.path.join(ABS_PATH, "db")# Instantiate 2 diff cromadb indexs, each one with a diff embedding.client_settings = chromadb.config.Settings(    is_persistent=True,    persist_directory=DB_DIR,    anonymized_telemetry=False,)db_all = Chroma(    collection_name="project_store_all",    persist_directory=DB_DIR,    client_settings=client_settings,    embedding_function=all_mini,)db_multi_qa = Chroma(    collection_name="project_store_multi",    persist_directory=DB_DIR,    client_settings=client_settings,    embedding_function=multi_qa_mini,)# Define 2 diff retrievers with 2 diff embeddings and diff search type.retriever_all = db_all.as_retriever(    search_type="similarity", search_kwargs={"k": 5, "include_metadata": True})retriever_multi_qa = db_multi_qa.as_retriever(    search_type="mmr", search_kwargs={"k": 5, "include_metadata": True})# The Lord of the Retrievers will hold the ouput of boths retrievers and can be used as any other# retriever on different types of chains.lotr = MergerRetriever(retrievers=[retriever_all, retriever_multi_qa])API Reference:MergerRetrieverChromaHuggingFaceEmbeddingsOpenAIEmbeddingsEmbeddingsRedundantFilterEmbeddingsClusteringFilterDocumentCompressorPipelineContextualCompressionRetrieverRemove redundant results from the merged retrievers.‚Äã# We can remove redundant results from both retrievers using yet another embedding.# Using multiples embeddings in diff steps could help reduce biases.filter = EmbeddingsRedundantFilter(embeddings=filter_embeddings)pipeline = DocumentCompressorPipeline(transformers=[filter])compression_retriever = ContextualCompressionRetriever(    base_compressor=pipeline, base_retriever=lotr)Pick a representative sample of documents from the merged retrievers.‚Äã# This filter will divide the documents vectors into clusters or "centers" of meaning.# Then it will pick the closest document to that center for the final results.# By default the result document will be ordered/grouped by clusters.filter_ordered_cluster = EmbeddingsClusteringFilter(    embeddings=filter_embeddings,    num_clusters=10,    num_closest=1,)# If you want the final document to be ordered by the original retriever scores# you need to add the "sorted" parameter.filter_ordered_by_retriever = EmbeddingsClusteringFilter(    embeddings=filter_embeddings,    num_clusters=10,    num_closest=1,    sorted=True,)pipeline = DocumentCompressorPipeline(transformers=[filter_ordered_by_retriever])compression_retriever = ContextualCompressionRetriever(    base_compressor=pipeline, base_retriever=lotr)Re-order results to avoid performance degradation.‚ÄãNo matter the architecture of your model, there is a sustancial performance degradation when you include 10+ retrieved documents. See: https://arxiv.org/abs//2307.03172# You can use an additional document transformer to reorder documents after removing redudance.from langchain.document_transformers import LongContextReorderfilter = EmbeddingsRedundantFilter(embeddings=filter_embeddings)reordering = LongContextReorder()pipeline = DocumentCompressorPipeline(transformers=[filter, reordering])compression_retriever_reordered = ContextualCompressionRetriever(    base_compressor=pipeline, base_retriever=lotr)API Reference:LongContextReorderPreviouskNNNextMetalRemove redundant results from the merged retrievers.Pick a representative sample of documents from the merged retrievers.Re-order results to avoid performance degradation. IntegrationsCallbacksChat modelsChat loadersDocument loadersDocument transformersLLMsMemoryRetrieversAmazon KendraArxivAzure Cognitive SearchBM25ChaindeskChatGPT PluginCohere RerankerDocArray RetrieverElasticSearch BM25Google Cloud Enterprise SearchGoogle Drive RetrieverkNNLOTR (Merger Retriever)MetalPinecone Hybrid SearchPubMedRePhraseQueryRetrieverSVMTF-IDFVespaWeaviate Hybrid SearchWikipediaZepText embedding modelsAgents & ToolkitsToolsVector storesGrouped by provider IntegrationsCallbacksChat modelsChat loadersDocument loadersDocument transformersLLMsMemoryRetrieversAmazon KendraArxivAzure Cognitive SearchBM25ChaindeskChatGPT PluginCohere RerankerDocArray RetrieverElasticSearch BM25Google Cloud Enterprise SearchGoogle Drive RetrieverkNNLOTR (Merger Retriever)MetalPinecone Hybrid SearchPubMedRePhraseQueryRetrieverSVMTF-IDFVespaWeaviate Hybrid SearchWikipediaZepText embedding modelsAgents & ToolkitsToolsVector storesGrouped by provider Integrations Callbacks Chat models Chat loaders Document loaders Document transformers LLMs Memory Retrievers Text embedding models Agents & Toolkits Tools Vector stores Grouped by provider IntegrationsRetrieversLOTR (Merger Retriever)On this pageLOTR (Merger Retriever)Lord of the Retrievers, also known as MergerRetriever, takes a list of retrievers as input and merges the results of their get_relevant_documents() methods into a single list. The merged results will be a list of documents that are relevant to the query and that have been ranked by the different retrievers.The MergerRetriever class can be used to improve the accuracy of document retrieval in a number of ways. First, it can combine the results of multiple retrievers, which can help to reduce the risk of bias in the results. Second, it can rank the results of the different retrievers, which can help to ensure that the most relevant documents are returned first.import osimport chromadbfrom langchain.retrievers.merger_retriever import MergerRetrieverfrom langchain.vectorstores import Chromafrom langchain.embeddings import HuggingFaceEmbeddingsfrom langchain.embeddings import OpenAIEmbeddingsfrom langchain.document_transformers import (    EmbeddingsRedundantFilter,    EmbeddingsClusteringFilter,)from langchain.retrievers.document_compressors import DocumentCompressorPipelinefrom langchain.retrievers import ContextualCompressionRetriever# Get 3 diff embeddings.all_mini = HuggingFaceEmbeddings(model_name="all-MiniLM-L6-v2")multi_qa_mini = HuggingFaceEmbeddings(model_name="multi-qa-MiniLM-L6-dot-v1")filter_embeddings = OpenAIEmbeddings()ABS_PATH = os.path.dirname(os.path.abspath(__file__))DB_DIR = os.path.join(ABS_PATH, "db")# Instantiate 2 diff cromadb indexs, each one with a diff embedding.client_settings = chromadb.config.Settings(    is_persistent=True,    persist_directory=DB_DIR,    anonymized_telemetry=False,)db_all = Chroma(    collection_name="project_store_all",    persist_directory=DB_DIR,    client_settings=client_settings,    embedding_function=all_mini,)db_multi_qa = Chroma(    collection_name="project_store_multi",    persist_directory=DB_DIR,    client_settings=client_settings,    embedding_function=multi_qa_mini,)# Define 2 diff retrievers with 2 diff embeddings and diff search type.retriever_all = db_all.as_retriever(    search_type="similarity", search_kwargs={"k": 5, "include_metadata": True})retriever_multi_qa = db_multi_qa.as_retriever(    search_type="mmr", search_kwargs={"k": 5, "include_metadata": True})# The Lord of the Retrievers will hold the ouput of boths retrievers and can be used as any other# retriever on different types of chains.lotr = MergerRetriever(retrievers=[retriever_all, retriever_multi_qa])API Reference:MergerRetrieverChromaHuggingFaceEmbeddingsOpenAIEmbeddingsEmbeddingsRedundantFilterEmbeddingsClusteringFilterDocumentCompressorPipelineContextualCompressionRetrieverRemove redundant results from the merged retrievers.‚Äã# We can remove redundant results from both retrievers using yet another embedding.# Using multiples embeddings in diff steps could help reduce biases.filter = EmbeddingsRedundantFilter(embeddings=filter_embeddings)pipeline = DocumentCompressorPipeline(transformers=[filter])compression_retriever = ContextualCompressionRetriever(    base_compressor=pipeline, base_retriever=lotr)Pick a representative sample of documents from the merged retrievers.‚Äã# This filter will divide the documents vectors into clusters or "centers" of meaning.# Then it will pick the closest document to that center for the final results.# By default the result document will be ordered/grouped by clusters.filter_ordered_cluster = EmbeddingsClusteringFilter(    embeddings=filter_embeddings,    num_clusters=10,    num_closest=1,)# If you want the final document to be ordered by the original retriever scores# you need to add the "sorted" parameter.filter_ordered_by_retriever = EmbeddingsClusteringFilter(    embeddings=filter_embeddings,    num_clusters=10,    num_closest=1,    sorted=True,)pipeline = DocumentCompressorPipeline(transformers=[filter_ordered_by_retriever])compression_retriever = ContextualCompressionRetriever(    base_compressor=pipeline, base_retriever=lotr)Re-order results to avoid performance degradation.‚ÄãNo matter the architecture of your model, there is a sustancial performance degradation when you include 10+ retrieved documents. See: https://arxiv.org/abs//2307.03172# You can use an additional document transformer to reorder documents after removing redudance.from langchain.document_transformers import LongContextReorderfilter = EmbeddingsRedundantFilter(embeddings=filter_embeddings)reordering = LongContextReorder()pipeline = DocumentCompressorPipeline(transformers=[filter, reordering])compression_retriever_reordered = ContextualCompressionRetriever(    base_compressor=pipeline, base_retriever=lotr)API Reference:LongContextReorderPreviouskNNNextMetalRemove redundant results from the merged retrievers.Pick a representative sample of documents from the merged retrievers.Re-order results to avoid performance degradation. IntegrationsRetrieversLOTR (Merger Retriever)On this pageLOTR (Merger Retriever)Lord of the Retrievers, also known as MergerRetriever, takes a list of retrievers as input and merges the results of their get_relevant_documents() methods into a single list. The merged results will be a list of documents that are relevant to the query and that have been ranked by the different retrievers.The MergerRetriever class can be used to improve the accuracy of document retrieval in a number of ways. First, it can combine the results of multiple retrievers, which can help to reduce the risk of bias in the results. Second, it can rank the results of the different retrievers, which can help to ensure that the most relevant documents are returned first.import osimport chromadbfrom langchain.retrievers.merger_retriever import MergerRetrieverfrom langchain.vectorstores import Chromafrom langchain.embeddings import HuggingFaceEmbeddingsfrom langchain.embeddings import OpenAIEmbeddingsfrom langchain.document_transformers import (    EmbeddingsRedundantFilter,    EmbeddingsClusteringFilter,)from langchain.retrievers.document_compressors import DocumentCompressorPipelinefrom langchain.retrievers import ContextualCompressionRetriever# Get 3 diff embeddings.all_mini = HuggingFaceEmbeddings(model_name="all-MiniLM-L6-v2")multi_qa_mini = HuggingFaceEmbeddings(model_name="multi-qa-MiniLM-L6-dot-v1")filter_embeddings = OpenAIEmbeddings()ABS_PATH = os.path.dirname(os.path.abspath(__file__))DB_DIR = os.path.join(ABS_PATH, "db")# Instantiate 2 diff cromadb indexs, each one with a diff embedding.client_settings = chromadb.config.Settings(    is_persistent=True,    persist_directory=DB_DIR,    anonymized_telemetry=False,)db_all = Chroma(    collection_name="project_store_all",    persist_directory=DB_DIR,    client_settings=client_settings,    embedding_function=all_mini,)db_multi_qa = Chroma(    collection_name="project_store_multi",    persist_directory=DB_DIR,    client_settings=client_settings,    embedding_function=multi_qa_mini,)# Define 2 diff retrievers with 2 diff embeddings and diff search type.retriever_all = db_all.as_retriever(    search_type="similarity", search_kwargs={"k": 5, "include_metadata": True})retriever_multi_qa = db_multi_qa.as_retriever(    search_type="mmr", search_kwargs={"k": 5, "include_metadata": True})# The Lord of the Retrievers will hold the ouput of boths retrievers and can be used as any other# retriever on different types of chains.lotr = MergerRetriever(retrievers=[retriever_all, retriever_multi_qa])API Reference:MergerRetrieverChromaHuggingFaceEmbeddingsOpenAIEmbeddingsEmbeddingsRedundantFilterEmbeddingsClusteringFilterDocumentCompressorPipelineContextualCompressionRetrieverRemove redundant results from the merged retrievers.‚Äã# We can remove redundant results from both retrievers using yet another embedding.# Using multiples embeddings in diff steps could help reduce biases.filter = EmbeddingsRedundantFilter(embeddings=filter_embeddings)pipeline = DocumentCompressorPipeline(transformers=[filter])compression_retriever = ContextualCompressionRetriever(    base_compressor=pipeline, base_retriever=lotr)Pick a representative sample of documents from the merged retrievers.‚Äã# This filter will divide the documents vectors into clusters or "centers" of meaning.# Then it will pick the closest document to that center for the final results.# By default the result document will be ordered/grouped by clusters.filter_ordered_cluster = EmbeddingsClusteringFilter(    embeddings=filter_embeddings,    num_clusters=10,    num_closest=1,)# If you want the final document to be ordered by the original retriever scores# you need to add the "sorted" parameter.filter_ordered_by_retriever = EmbeddingsClusteringFilter(    embeddings=filter_embeddings,    num_clusters=10,    num_closest=1,    sorted=True,)pipeline = DocumentCompressorPipeline(transformers=[filter_ordered_by_retriever])compression_retriever = ContextualCompressionRetriever(    base_compressor=pipeline, base_retriever=lotr)Re-order results to avoid performance degradation.‚ÄãNo matter the architecture of your model, there is a sustancial performance degradation when you include 10+ retrieved documents. See: https://arxiv.org/abs//2307.03172# You can use an additional document transformer to reorder documents after removing redudance.from langchain.document_transformers import LongContextReorderfilter = EmbeddingsRedundantFilter(embeddings=filter_embeddings)reordering = LongContextReorder()pipeline = DocumentCompressorPipeline(transformers=[filter, reordering])compression_retriever_reordered = ContextualCompressionRetriever(    base_compressor=pipeline, base_retriever=lotr)API Reference:LongContextReorderPreviouskNNNextMetal IntegrationsRetrieversLOTR (Merger Retriever)On this pageLOTR (Merger Retriever)Lord of the Retrievers, also known as MergerRetriever, takes a list of retrievers as input and merges the results of their get_relevant_documents() methods into a single list. The merged results will be a list of documents that are relevant to the query and that have been ranked by the different retrievers.The MergerRetriever class can be used to improve the accuracy of document retrieval in a number of ways. First, it can combine the results of multiple retrievers, which can help to reduce the risk of bias in the results. Second, it can rank the results of the different retrievers, which can help to ensure that the most relevant documents are returned first.import osimport chromadbfrom langchain.retrievers.merger_retriever import MergerRetrieverfrom langchain.vectorstores import Chromafrom langchain.embeddings import HuggingFaceEmbeddingsfrom langchain.embeddings import OpenAIEmbeddingsfrom langchain.document_transformers import (    EmbeddingsRedundantFilter,    EmbeddingsClusteringFilter,)from langchain.retrievers.document_compressors import DocumentCompressorPipelinefrom langchain.retrievers import ContextualCompressionRetriever# Get 3 diff embeddings.all_mini = HuggingFaceEmbeddings(model_name="all-MiniLM-L6-v2")multi_qa_mini = HuggingFaceEmbeddings(model_name="multi-qa-MiniLM-L6-dot-v1")filter_embeddings = OpenAIEmbeddings()ABS_PATH = os.path.dirname(os.path.abspath(__file__))DB_DIR = os.path.join(ABS_PATH, "db")# Instantiate 2 diff cromadb indexs, each one with a diff embedding.client_settings = chromadb.config.Settings(    is_persistent=True,    persist_directory=DB_DIR,    anonymized_telemetry=False,)db_all = Chroma(    collection_name="project_store_all",    persist_directory=DB_DIR,    client_settings=client_settings,    embedding_function=all_mini,)db_multi_qa = Chroma(    collection_name="project_store_multi",    persist_directory=DB_DIR,    client_settings=client_settings,    embedding_function=multi_qa_mini,)# Define 2 diff retrievers with 2 diff embeddings and diff search type.retriever_all = db_all.as_retriever(    search_type="similarity", search_kwargs={"k": 5, "include_metadata": True})retriever_multi_qa = db_multi_qa.as_retriever(    search_type="mmr", search_kwargs={"k": 5, "include_metadata": True})# The Lord of the Retrievers will hold the ouput of boths retrievers and can be used as any other# retriever on different types of chains.lotr = MergerRetriever(retrievers=[retriever_all, retriever_multi_qa])API Reference:MergerRetrieverChromaHuggingFaceEmbeddingsOpenAIEmbeddingsEmbeddingsRedundantFilterEmbeddingsClusteringFilterDocumentCompressorPipelineContextualCompressionRetrieverRemove redundant results from the merged retrievers.‚Äã# We can remove redundant results from both retrievers using yet another embedding.# Using multiples embeddings in diff steps could help reduce biases.filter = EmbeddingsRedundantFilter(embeddings=filter_embeddings)pipeline = DocumentCompressorPipeline(transformers=[filter])compression_retriever = ContextualCompressionRetriever(    base_compressor=pipeline, base_retriever=lotr)Pick a representative sample of documents from the merged retrievers.‚Äã# This filter will divide the documents vectors into clusters or "centers" of meaning.# Then it will pick the closest document to that center for the final results.# By default the result document will be ordered/grouped by clusters.filter_ordered_cluster = EmbeddingsClusteringFilter(    embeddings=filter_embeddings,    num_clusters=10,    num_closest=1,)# If you want the final document to be ordered by the original retriever scores# you need to add the "sorted" parameter.filter_ordered_by_retriever = EmbeddingsClusteringFilter(    embeddings=filter_embeddings,    num_clusters=10,    num_closest=1,    sorted=True,)pipeline = DocumentCompressorPipeline(transformers=[filter_ordered_by_retriever])compression_retriever = ContextualCompressionRetriever(    base_compressor=pipeline, base_retriever=lotr)Re-order results to avoid performance degradation.‚ÄãNo matter the architecture of your model, there is a sustancial performance degradation when you include 10+ retrieved documents. See: https://arxiv.org/abs//2307.03172# You can use an additional document transformer to reorder documents after removing redudance.from langchain.document_transformers import LongContextReorderfilter = EmbeddingsRedundantFilter(embeddings=filter_embeddings)reordering = LongContextReorder()pipeline = DocumentCompressorPipeline(transformers=[filter, reordering])compression_retriever_reordered = ContextualCompressionRetriever(    base_compressor=pipeline, base_retriever=lotr)API Reference:LongContextReorderPreviouskNNNextMetal On this page LOTR (Merger Retriever)Lord of the Retrievers, also known as MergerRetriever, takes a list of retrievers as input and merges the results of their get_relevant_documents() methods into a single list. The merged results will be a list of documents that are relevant to the query and that have been ranked by the different retrievers.The MergerRetriever class can be used to improve the accuracy of document retrieval in a number of ways. First, it can combine the results of multiple retrievers, which can help to reduce the risk of bias in the results. Second, it can rank the results of the different retrievers, which can help to ensure that the most relevant documents are returned first.import osimport chromadbfrom langchain.retrievers.merger_retriever import MergerRetrieverfrom langchain.vectorstores import Chromafrom langchain.embeddings import HuggingFaceEmbeddingsfrom langchain.embeddings import OpenAIEmbeddingsfrom langchain.document_transformers import (    EmbeddingsRedundantFilter,    EmbeddingsClusteringFilter,)from langchain.retrievers.document_compressors import DocumentCompressorPipelinefrom langchain.retrievers import ContextualCompressionRetriever# Get 3 diff embeddings.all_mini = HuggingFaceEmbeddings(model_name="all-MiniLM-L6-v2")multi_qa_mini = HuggingFaceEmbeddings(model_name="multi-qa-MiniLM-L6-dot-v1")filter_embeddings = OpenAIEmbeddings()ABS_PATH = os.path.dirname(os.path.abspath(__file__))DB_DIR = os.path.join(ABS_PATH, "db")# Instantiate 2 diff cromadb indexs, each one with a diff embedding.client_settings = chromadb.config.Settings(    is_persistent=True,    persist_directory=DB_DIR,    anonymized_telemetry=False,)db_all = Chroma(    collection_name="project_store_all",    persist_directory=DB_DIR,    client_settings=client_settings,    embedding_function=all_mini,)db_multi_qa = Chroma(    collection_name="project_store_multi",    persist_directory=DB_DIR,    client_settings=client_settings,    embedding_function=multi_qa_mini,)# Define 2 diff retrievers with 2 diff embeddings and diff search type.retriever_all = db_all.as_retriever(    search_type="similarity", search_kwargs={"k": 5, "include_metadata": True})retriever_multi_qa = db_multi_qa.as_retriever(    search_type="mmr", search_kwargs={"k": 5, "include_metadata": True})# The Lord of the Retrievers will hold the ouput of boths retrievers and can be used as any other# retriever on different types of chains.lotr = MergerRetriever(retrievers=[retriever_all, retriever_multi_qa])API Reference:MergerRetrieverChromaHuggingFaceEmbeddingsOpenAIEmbeddingsEmbeddingsRedundantFilterEmbeddingsClusteringFilterDocumentCompressorPipelineContextualCompressionRetrieverRemove redundant results from the merged retrievers.‚Äã# We can remove redundant results from both retrievers using yet another embedding.# Using multiples embeddings in diff steps could help reduce biases.filter = EmbeddingsRedundantFilter(embeddings=filter_embeddings)pipeline = DocumentCompressorPipeline(transformers=[filter])compression_retriever = ContextualCompressionRetriever(    base_compressor=pipeline, base_retriever=lotr)Pick a representative sample of documents from the merged retrievers.‚Äã# This filter will divide the documents vectors into clusters or "centers" of meaning.# Then it will pick the closest document to that center for the final results.# By default the result document will be ordered/grouped by clusters.filter_ordered_cluster = EmbeddingsClusteringFilter(    embeddings=filter_embeddings,    num_clusters=10,    num_closest=1,)# If you want the final document to be ordered by the original retriever scores# you need to add the "sorted" parameter.filter_ordered_by_retriever = EmbeddingsClusteringFilter(    embeddings=filter_embeddings,    num_clusters=10,    num_closest=1,    sorted=True,)pipeline = DocumentCompressorPipeline(transformers=[filter_ordered_by_retriever])compression_retriever = ContextualCompressionRetriever(    base_compressor=pipeline, base_retriever=lotr)Re-order results to avoid performance degradation.‚ÄãNo matter the architecture of your model, there is a sustancial performance degradation when you include 10+ retrieved documents. See: https://arxiv.org/abs//2307.03172# You can use an additional document transformer to reorder documents after removing redudance.from langchain.document_transformers import LongContextReorderfilter = EmbeddingsRedundantFilter(embeddings=filter_embeddings)reordering = LongContextReorder()pipeline = DocumentCompressorPipeline(transformers=[filter, reordering])compression_retriever_reordered = ContextualCompressionRetriever(    base_compressor=pipeline, base_retriever=lotr)API Reference:LongContextReorder import osimport chromadbfrom langchain.retrievers.merger_retriever import MergerRetrieverfrom langchain.vectorstores import Chromafrom langchain.embeddings import HuggingFaceEmbeddingsfrom langchain.embeddings import OpenAIEmbeddingsfrom langchain.document_transformers import (    EmbeddingsRedundantFilter,    EmbeddingsClusteringFilter,)from langchain.retrievers.document_compressors import DocumentCompressorPipelinefrom langchain.retrievers import ContextualCompressionRetriever# Get 3 diff embeddings.all_mini = HuggingFaceEmbeddings(model_name="all-MiniLM-L6-v2")multi_qa_mini = HuggingFaceEmbeddings(model_name="multi-qa-MiniLM-L6-dot-v1")filter_embeddings = OpenAIEmbeddings()ABS_PATH = os.path.dirname(os.path.abspath(__file__))DB_DIR = os.path.join(ABS_PATH, "db")# Instantiate 2 diff cromadb indexs, each one with a diff embedding.client_settings = chromadb.config.Settings(    is_persistent=True,    persist_directory=DB_DIR,    anonymized_telemetry=False,)db_all = Chroma(    collection_name="project_store_all",    persist_directory=DB_DIR,    client_settings=client_settings,    embedding_function=all_mini,)db_multi_qa = Chroma(    collection_name="project_store_multi",    persist_directory=DB_DIR,    client_settings=client_settings,    embedding_function=multi_qa_mini,)# Define 2 diff retrievers with 2 diff embeddings and diff search type.retriever_all = db_all.as_retriever(    search_type="similarity", search_kwargs={"k": 5, "include_metadata": True})retriever_multi_qa = db_multi_qa.as_retriever(    search_type="mmr", search_kwargs={"k": 5, "include_metadata": True})# The Lord of the Retrievers will hold the ouput of boths retrievers and can be used as any other# retriever on different types of chains.lotr = MergerRetriever(retrievers=[retriever_all, retriever_multi_qa]) import osimport chromadbfrom langchain.retrievers.merger_retriever import MergerRetrieverfrom langchain.vectorstores import Chromafrom langchain.embeddings import HuggingFaceEmbeddingsfrom langchain.embeddings import OpenAIEmbeddingsfrom langchain.document_transformers import (    EmbeddingsRedundantFilter,    EmbeddingsClusteringFilter,)from langchain.retrievers.document_compressors import DocumentCompressorPipelinefrom langchain.retrievers import ContextualCompressionRetriever# Get 3 diff embeddings.all_mini = HuggingFaceEmbeddings(model_name="all-MiniLM-L6-v2")multi_qa_mini = HuggingFaceEmbeddings(model_name="multi-qa-MiniLM-L6-dot-v1")filter_embeddings = OpenAIEmbeddings()ABS_PATH = os.path.dirname(os.path.abspath(__file__))DB_DIR = os.path.join(ABS_PATH, "db")# Instantiate 2 diff cromadb indexs, each one with a diff embedding.client_settings = chromadb.config.Settings(    is_persistent=True,    persist_directory=DB_DIR,    anonymized_telemetry=False,)db_all = Chroma(    collection_name="project_store_all",    persist_directory=DB_DIR,    client_settings=client_settings,    embedding_function=all_mini,)db_multi_qa = Chroma(    collection_name="project_store_multi",    persist_directory=DB_DIR,    client_settings=client_settings,    embedding_function=multi_qa_mini,)# Define 2 diff retrievers with 2 diff embeddings and diff search type.retriever_all = db_all.as_retriever(    search_type="similarity", search_kwargs={"k": 5, "include_metadata": True})retriever_multi_qa = db_multi_qa.as_retriever(    search_type="mmr", search_kwargs={"k": 5, "include_metadata": True})# The Lord of the Retrievers will hold the ouput of boths retrievers and can be used as any other# retriever on different types of chains.lotr = MergerRetriever(retrievers=[retriever_all, retriever_multi_qa])  API Reference:MergerRetrieverChromaHuggingFaceEmbeddingsOpenAIEmbeddingsEmbeddingsRedundantFilterEmbeddingsClusteringFilterDocumentCompressorPipelineContextualCompressionRetriever # We can remove redundant results from both retrievers using yet another embedding.# Using multiples embeddings in diff steps could help reduce biases.filter = EmbeddingsRedundantFilter(embeddings=filter_embeddings)pipeline = DocumentCompressorPipeline(transformers=[filter])compression_retriever = ContextualCompressionRetriever(    base_compressor=pipeline, base_retriever=lotr) # We can remove redundant results from both retrievers using yet another embedding.# Using multiples embeddings in diff steps could help reduce biases.filter = EmbeddingsRedundantFilter(embeddings=filter_embeddings)pipeline = DocumentCompressorPipeline(transformers=[filter])compression_retriever = ContextualCompressionRetriever(    base_compressor=pipeline, base_retriever=lotr)  # This filter will divide the documents vectors into clusters or "centers" of meaning.# Then it will pick the closest document to that center for the final results.# By default the result document will be ordered/grouped by clusters.filter_ordered_cluster = EmbeddingsClusteringFilter(    embeddings=filter_embeddings,    num_clusters=10,    num_closest=1,)# If you want the final document to be ordered by the original retriever scores# you need to add the "sorted" parameter.filter_ordered_by_retriever = EmbeddingsClusteringFilter(    embeddings=filter_embeddings,    num_clusters=10,    num_closest=1,    sorted=True,)pipeline = DocumentCompressorPipeline(transformers=[filter_ordered_by_retriever])compression_retriever = ContextualCompressionRetriever(    base_compressor=pipeline, base_retriever=lotr) # This filter will divide the documents vectors into clusters or "centers" of meaning.# Then it will pick the closest document to that center for the final results.# By default the result document will be ordered/grouped by clusters.filter_ordered_cluster = EmbeddingsClusteringFilter(    embeddings=filter_embeddings,    num_clusters=10,    num_closest=1,)# If you want the final document to be ordered by the original retriever scores# you need to add the "sorted" parameter.filter_ordered_by_retriever = EmbeddingsClusteringFilter(    embeddings=filter_embeddings,    num_clusters=10,    num_closest=1,    sorted=True,)pipeline = DocumentCompressorPipeline(transformers=[filter_ordered_by_retriever])compression_retriever = ContextualCompressionRetriever(    base_compressor=pipeline, base_retriever=lotr)  # You can use an additional document transformer to reorder documents after removing redudance.from langchain.document_transformers import LongContextReorderfilter = EmbeddingsRedundantFilter(embeddings=filter_embeddings)reordering = LongContextReorder()pipeline = DocumentCompressorPipeline(transformers=[filter, reordering])compression_retriever_reordered = ContextualCompressionRetriever(    base_compressor=pipeline, base_retriever=lotr) # You can use an additional document transformer to reorder documents after removing redudance.from langchain.document_transformers import LongContextReorderfilter = EmbeddingsRedundantFilter(embeddings=filter_embeddings)reordering = LongContextReorder()pipeline = DocumentCompressorPipeline(transformers=[filter, reordering])compression_retriever_reordered = ContextualCompressionRetriever(    base_compressor=pipeline, base_retriever=lotr)  API Reference:LongContextReorder Previous kNN Next Metal Remove redundant results from the merged retrievers.Pick a representative sample of documents from the merged retrievers.Re-order results to avoid performance degradation. Remove redundant results from the merged retrievers.Pick a representative sample of documents from the merged retrievers.Re-order results to avoid performance degradation. CommunityDiscordTwitterGitHubPythonJS/TSMoreHomepageBlogCopyright ¬© 2023 LangChain, Inc. CommunityDiscordTwitterGitHubPythonJS/TSMoreHomepageBlog CommunityDiscordTwitter Community GitHubPythonJS/TS GitHub MoreHomepageBlog More Copyright ¬© 2023 LangChain, Inc. Copyright ¬© 2023 LangChain, Inc. Skip to main content (#docusaurus_skipToContent_fallback) ü¶úÔ∏èüîó LangChain (/) Docs (/docs/get_started/introduction) Use cases (/docs/use_cases) Integrations (/docs/integrations) API (https://api.python.langchain.com) LangSmith (https://smith.langchain.com) JS/TS Docs (https://js.langchain.com/docs)  (https://github.com/hwchase17/langchain) Integrations (/docs/integrations) Callbacks (/docs/integrations/callbacks/) Chat models (/docs/integrations/chat/) Chat loaders (/docs/integrations/chat_loaders/) Document loaders (/docs/integrations/document_loaders/) Document transformers (/docs/integrations/document_transformers/) LLMs (/docs/integrations/llms/) Memory (/docs/integrations/memory/) Retrievers (/docs/integrations/retrievers/) Amazon Kendra (/docs/integrations/retrievers/amazon_kendra_retriever) Arxiv (/docs/integrations/retrievers/arxiv) Azure Cognitive Search (/docs/integrations/retrievers/azure_cognitive_search) BM25 (/docs/integrations/retrievers/bm25) Chaindesk (/docs/integrations/retrievers/chaindesk) ChatGPT Plugin (/docs/integrations/retrievers/chatgpt-plugin) Cohere Reranker (/docs/integrations/retrievers/cohere-reranker) DocArray Retriever (/docs/integrations/retrievers/docarray_retriever) ElasticSearch BM25 (/docs/integrations/retrievers/elastic_search_bm25) Google Cloud Enterprise Search (/docs/integrations/retrievers/google_cloud_enterprise_search) Google Drive Retriever (/docs/integrations/retrievers/google_drive) kNN (/docs/integrations/retrievers/knn) LOTR (Merger Retriever) (/docs/integrations/retrievers/merger_retriever) Metal (/docs/integrations/retrievers/metal) Pinecone Hybrid Search (/docs/integrations/retrievers/pinecone_hybrid_search) PubMed (/docs/integrations/retrievers/pubmed) RePhraseQueryRetriever (/docs/integrations/retrievers/re_phrase) SVM (/docs/integrations/retrievers/svm) TF-IDF (/docs/integrations/retrievers/tf_idf) Vespa (/docs/integrations/retrievers/vespa) Weaviate Hybrid Search (/docs/integrations/retrievers/weaviate-hybrid) Wikipedia (/docs/integrations/retrievers/wikipedia) Zep (/docs/integrations/retrievers/zep_memorystore) Text embedding models (/docs/integrations/text_embedding/) Agents & Toolkits (/docs/integrations/toolkits/) Tools (/docs/integrations/tools/) Vector stores (/docs/integrations/vectorstores/) Grouped by provider (/docs/integrations/providers/)  (/) Integrations (/docs/integrations) Retrievers (/docs/integrations/retrievers/) MergerRetriever (https://api.python.langchain.com/en/latest/retrievers/langchain.retrievers.merger_retriever.MergerRetriever.html) Chroma (https://api.python.langchain.com/en/latest/vectorstores/langchain.vectorstores.chroma.Chroma.html) HuggingFaceEmbeddings (https://api.python.langchain.com/en/latest/embeddings/langchain.embeddings.huggingface.HuggingFaceEmbeddings.html) OpenAIEmbeddings (https://api.python.langchain.com/en/latest/embeddings/langchain.embeddings.openai.OpenAIEmbeddings.html) EmbeddingsRedundantFilter (https://api.python.langchain.com/en/latest/document_transformers/langchain.document_transformers.embeddings_redundant_filter.EmbeddingsRedundantFilter.html) EmbeddingsClusteringFilter (https://api.python.langchain.com/en/latest/document_transformers/langchain.document_transformers.embeddings_redundant_filter.EmbeddingsClusteringFilter.html) DocumentCompressorPipeline (https://api.python.langchain.com/en/latest/retrievers/langchain.retrievers.document_compressors.base.DocumentCompressorPipeline.html) ContextualCompressionRetriever (https://api.python.langchain.com/en/latest/retrievers/langchain.retrievers.contextual_compression.ContextualCompressionRetriever.html) ‚Äã (#remove-redundant-results-from-the-merged-retrievers) ‚Äã (#pick-a-representative-sample-of-documents-from-the-merged-retrievers) ‚Äã (#re-order-results-to-avoid-performance-degradation) https://arxiv.org/abs//2307.03172 (https://arxiv.org/abs//2307.03172) LongContextReorder (https://api.python.langchain.com/en/latest/document_transformers/langchain.document_transformers.long_context_reorder.LongContextReorder.html) PreviouskNN (/docs/integrations/retrievers/knn) NextMetal (/docs/integrations/retrievers/metal) Remove redundant results from the merged retrievers. (#remove-redundant-results-from-the-merged-retrievers) Pick a representative sample of documents from the merged retrievers. (#pick-a-representative-sample-of-documents-from-the-merged-retrievers) Re-order results to avoid performance degradation. (#re-order-results-to-avoid-performance-degradation) Discord (https://discord.gg/cU2adEyC7w) Twitter (https://twitter.com/LangChainAI) Python (https://github.com/hwchase17/langchain) JS/TS (https://github.com/hwchase17/langchainjs) Homepage (https://langchain.com) Blog (https://blog.langchain.dev)