DeepEval package for unit testing LLMs. Using Confident, everyone can build robust language models through faster iterations using both unit testing and integration testing. We provide support for each step in the iteration from synthetic data creation to testing. In this guide we will demonstrate how to test and measure LLMs in performance. We show how you can use our callback to measure performance and how you can define your own metric and log them into our dashboard. DeepEval also offers: To get the DeepEval API credentials, follow the next steps: When you log in, you will also be asked to set the implementation name. The implementation name is required to describe the type of implementation. (Think of what you want to call your project. We recommend making it descriptive.) You can, by default, use the DeepEvalCallbackHandler to set up the metrics you want to track. However, this has limited support for metrics at the moment (more to be added soon). It currently supports: To use the DeepEvalCallbackHandler, we need the implementation_name.  You can then feed it into your LLM with OpenAI. You can then check the metric if it was successful by calling the is_successful() method. Once you have ran that, you should be able to see our dashboard below.   To track an LLM in a chain without callbacks, you can plug into it at the end. We can start by defining a simple chain as shown below. After defining a chain, you can then manually check for answer similarity. You can create your own custom metrics here.  DeepEval also offers other features such as being able to automatically create unit tests, tests for hallucination. If you are interested, check out our Github repository here https://github.com/confident-ai/deepeval. We welcome any PRs and discussions on how to improve LLM performance. IntegrationsCallbacksArgillaConfidentContextInfinoLabel StudioLLMonitorPromptLayerStreamlitChat modelsChat loadersDocument loadersDocument transformersLLMsMemoryRetrieversText embedding modelsAgents & ToolkitsToolsVector storesGrouped by provider CallbacksArgillaConfidentContextInfinoLabel StudioLLMonitorPromptLayerStreamlit Argilla Confident Context Infino Label Studio LLMonitor PromptLayer Streamlit Chat models Chat loaders Document loaders Document transformers LLMs Memory Retrievers Text embedding models Agents & Toolkits Tools Vector stores Grouped by provider  Integrations Callbacks Confident How to generate synthetic data How to measure performance A dashboard to monitor and review results over time Go to https://app.confident-ai.com Click on "Organization" Copy the API Key. Answer Relevancy Bias Toxicness DeepEvalCallbackHandler OpenAI RetrievalQA TextLoader OpenAIEmbeddings OpenAI CharacterTextSplitter Chroma Installation and SetupGetting API CredentialsSetup DeepEval Getting API Credentials Setup DeepEval Get StartedScenario 1: Feeding into LLMScenario 2: Tracking an LLM in a chain without callbacksWhat's next? Scenario 1: Feeding into LLM Scenario 2: Tracking an LLM in a chain without callbacks What's next? Discord Twitter Python JS/TS Homepage Blog Skip to main contentü¶úÔ∏èüîó LangChainDocsUse casesIntegrationsAPILangSmithJS/TS DocsCTRLKIntegrationsCallbacksArgillaConfidentContextInfinoLabel StudioLLMonitorPromptLayerStreamlitChat modelsChat loadersDocument loadersDocument transformersLLMsMemoryRetrieversText embedding modelsAgents & ToolkitsToolsVector storesGrouped by providerIntegrationsCallbacksConfidentOn this pageConfidentDeepEval package for unit testing LLMs. from synthetic data creation to testing.In this guide we will demonstrate how to test and measure LLMs in performance. We show how you can use our callback to measure performance and how you can define your own metric and log them into our dashboard.DeepEval also offers:How to generate synthetic dataHow to measure performanceA dashboard to monitor and review results over timeInstallation and Setup‚Äãpip install deepeval --upgradeGetting API Credentials‚ÄãTo get the DeepEval API credentials, follow the next steps:Go to https://app.confident-ai.comClick on "Organization"Copy the API Key.When you log in, you will also be asked to set the implementation name. The implementation name is required to describe the type of implementation. (Think of what you want to call your project. We recommend making it descriptive.)deepeval loginSetup DeepEval‚ÄãYou can, by default, use the DeepEvalCallbackHandler to set up the metrics you want to track. However, this has limited support for metrics at the moment (more to be added soon). It currently supports:Answer RelevancyBiasToxicnessfrom deepeval.metrics.answer_relevancy import AnswerRelevancy# Here we want to make sure the answer is minimally relevantanswer_relevancy_metric = AnswerRelevancy(minimum_score=0.5)Get Started‚ÄãTo use the DeepEvalCallbackHandler, we need the implementation_name. import osfrom langchain.callbacks.confident_callback import DeepEvalCallbackHandlerdeepeval_callback = DeepEvalCallbackHandler(    implementation_name="langchainQuickstart",    metrics=[answer_relevancy_metric])API Reference:DeepEvalCallbackHandlerScenario 1: Feeding into LLM‚ÄãYou can then feed it into your LLM with OpenAI.from langchain.llms import OpenAIllm = OpenAI(    temperature=0,    callbacks=[deepeval_callback],    verbose=True,    openai_api_key="<YOUR_API_KEY>",)output = llm.generate(    [        "What is the best evaluation tool out there? (no bias at all)",    ])API Reference:OpenAI    LLMResult(generations=[[Generation(text='\n\nQ: What did the fish say when he hit the wall? \nA: Dam.', generation_info={'finish_reason': 'stop', 'logprobs': None})], [Generation(text='\n\nThe Moon \n\nThe moon is high in the midnight sky,\nSparkling like a star above.\nThe night so peaceful, so serene,\nFilling up the air with love.\n\nEver changing and renewing,\nA never-ending light of grace.\nThe moon remains a constant view,\nA reminder of life‚Äôs gentle pace.\n\nThrough time and space it guides us on,\nA never-fading beacon of hope.\nThe moon shines down on us all,\nAs it continues to rise and elope.', generation_info={'finish_reason': 'stop', 'logprobs': None})], [Generation(text='\n\nQ. What did one magnet say to the other magnet?\nA. "I find you very attractive!"', generation_info={'finish_reason': 'stop', 'logprobs': None})], [Generation(text="\n\nThe world is charged with the grandeur of God.\nIt will flame out, like shining from shook foil;\nIt gathers to a greatness, like the ooze of oil\nCrushed. Why do men then now not reck his rod?\n\nGenerations have trod, have trod, have trod;\nAnd all is seared with trade; bleared, smeared with toil;\nAnd wears man's smudge and shares man's smell: the soil\nIs bare now, nor can foot feel, being shod.\n\nAnd for all this, nature is never spent;\nThere lives the dearest freshness deep down things;\nAnd though the last lights off the black West went\nOh, morning, at the brown brink eastward, springs ‚Äî\n\nBecause the Holy Ghost over the bent\nWorld broods with warm breast and with ah! bright wings.\n\n~Gerard Manley Hopkins", generation_info={'finish_reason': 'stop', 'logprobs': None})], [Generation(text='\n\nQ: What did one ocean say to the other ocean?\nA: Nothing, they just waved.', generation_info={'finish_reason': 'stop', 'logprobs': None})], [Generation(text="\n\nA poem for you\n\nOn a field of green\n\nThe sky so blue\n\nA gentle breeze, the sun above\n\nA beautiful world, for us to love\n\nLife is a journey, full of surprise\n\nFull of joy and full of surprise\n\nBe brave and take small steps\n\nThe future will be revealed with depth\n\nIn the morning, when dawn arrives\n\nA fresh start, no reason to hide\n\nSomewhere down the road, there's a heart that beats\n\nBelieve in yourself, you'll always succeed.", generation_info={'finish_reason': 'stop', 'logprobs': None})]], llm_output={'token_usage': {'completion_tokens': 504, 'total_tokens': 528, 'prompt_tokens': 24}, 'model_name': 'text-davinci-003'})You can then check the metric if it was successful by calling the is_successful() method.answer_relevancy_metric.is_successful()# returns True/FalseOnce you have ran that, you should be able to see our dashboard below. Scenario 2: Tracking an LLM in a chain without callbacks‚ÄãTo track an LLM in a chain without callbacks, you can plug into it at the end.We can start by defining a simple chain as shown below.import requestsfrom langchain.chains import RetrievalQAfrom langchain.document_loaders import TextLoaderfrom langchain.embeddings.openai import OpenAIEmbeddingsfrom langchain.llms import OpenAIfrom langchain.text_splitter import CharacterTextSplitterfrom langchain.vectorstores import Chromatext_file_url = "https://raw.githubusercontent.com/hwchase17/chat-your-data/master/state_of_the_union.txt"openai_api_key = "sk-XXX"with open("state_of_the_union.txt", "w") as f:  response = requests.get(text_file_url)  f.write(response.text)loader = TextLoader("state_of_the_union.txt")documents = loader.load()text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)texts = text_splitter.split_documents(documents)embeddings = OpenAIEmbeddings(openai_api_key=openai_api_key)docsearch = Chroma.from_documents(texts, embeddings)qa = RetrievalQA.from_chain_type(  llm=OpenAI(openai_api_key=openai_api_key), chain_type="stuff",  retriever=docsearch.as_retriever())# Providing a new question-answering pipelinequery = "Who is the president?"result = qa.run(query)API Reference:RetrievalQATextLoaderOpenAIEmbeddingsOpenAICharacterTextSplitterChromaAfter defining a chain, you can then manually check for answer similarity.answer_relevancy_metric.measure(result, query)answer_relevancy_metric.is_successful()What's next?‚ÄãYou can create your own custom metrics here. DeepEval also offers other features such as being able to automatically create unit tests, tests for hallucination.If you are interested, check out our Github repository here https://github.com/confident-ai/deepeval. We welcome any PRs and discussions on how to improve LLM performance.PreviousArgillaNextContextInstallation and SetupGetting API CredentialsSetup DeepEvalGet StartedScenario 1: Feeding into LLMScenario 2: Tracking an LLM in a chain without callbacksWhat's next?CommunityDiscordTwitterGitHubPythonJS/TSMoreHomepageBlogCopyright ¬© 2023 LangChain, Inc. Skip to main content ü¶úÔ∏èüîó LangChainDocsUse casesIntegrationsAPILangSmithJS/TS DocsCTRLK ü¶úÔ∏èüîó LangChainDocsUse casesIntegrationsAPI LangSmithJS/TS DocsCTRLK  CTRLK CTRLK  CTRLK   IntegrationsCallbacksArgillaConfidentContextInfinoLabel StudioLLMonitorPromptLayerStreamlitChat modelsChat loadersDocument loadersDocument transformersLLMsMemoryRetrieversText embedding modelsAgents & ToolkitsToolsVector storesGrouped by providerIntegrationsCallbacksConfidentOn this pageConfidentDeepEval package for unit testing LLMs. from synthetic data creation to testing.In this guide we will demonstrate how to test and measure LLMs in performance. We show how you can use our callback to measure performance and how you can define your own metric and log them into our dashboard.DeepEval also offers:How to generate synthetic dataHow to measure performanceA dashboard to monitor and review results over timeInstallation and Setup‚Äãpip install deepeval --upgradeGetting API Credentials‚ÄãTo get the DeepEval API credentials, follow the next steps:Go to https://app.confident-ai.comClick on "Organization"Copy the API Key.When you log in, you will also be asked to set the implementation name. The implementation name is required to describe the type of implementation. (Think of what you want to call your project. We recommend making it descriptive.)deepeval loginSetup DeepEval‚ÄãYou can, by default, use the DeepEvalCallbackHandler to set up the metrics you want to track. However, this has limited support for metrics at the moment (more to be added soon). It currently supports:Answer RelevancyBiasToxicnessfrom deepeval.metrics.answer_relevancy import AnswerRelevancy# Here we want to make sure the answer is minimally relevantanswer_relevancy_metric = AnswerRelevancy(minimum_score=0.5)Get Started‚ÄãTo use the DeepEvalCallbackHandler, we need the implementation_name. import osfrom langchain.callbacks.confident_callback import DeepEvalCallbackHandlerdeepeval_callback = DeepEvalCallbackHandler(    implementation_name="langchainQuickstart",    metrics=[answer_relevancy_metric])API Reference:DeepEvalCallbackHandlerScenario 1: Feeding into LLM‚ÄãYou can then feed it into your LLM with OpenAI.from langchain.llms import OpenAIllm = OpenAI(    temperature=0,    callbacks=[deepeval_callback],    verbose=True,    openai_api_key="<YOUR_API_KEY>",)output = llm.generate(    [        "What is the best evaluation tool out there? (no bias at all)",    ])API Reference:OpenAI    LLMResult(generations=[[Generation(text='\n\nQ: What did the fish say when he hit the wall? \nA: Dam.', generation_info={'finish_reason': 'stop', 'logprobs': None})], [Generation(text='\n\nThe Moon \n\nThe moon is high in the midnight sky,\nSparkling like a star above.\nThe night so peaceful, so serene,\nFilling up the air with love.\n\nEver changing and renewing,\nA never-ending light of grace.\nThe moon remains a constant view,\nA reminder of life‚Äôs gentle pace.\n\nThrough time and space it guides us on,\nA never-fading beacon of hope.\nThe moon shines down on us all,\nAs it continues to rise and elope.', generation_info={'finish_reason': 'stop', 'logprobs': None})], [Generation(text='\n\nQ. What did one magnet say to the other magnet?\nA. "I find you very attractive!"', generation_info={'finish_reason': 'stop', 'logprobs': None})], [Generation(text="\n\nThe world is charged with the grandeur of God.\nIt will flame out, like shining from shook foil;\nIt gathers to a greatness, like the ooze of oil\nCrushed. Why do men then now not reck his rod?\n\nGenerations have trod, have trod, have trod;\nAnd all is seared with trade; bleared, smeared with toil;\nAnd wears man's smudge and shares man's smell: the soil\nIs bare now, nor can foot feel, being shod.\n\nAnd for all this, nature is never spent;\nThere lives the dearest freshness deep down things;\nAnd though the last lights off the black West went\nOh, morning, at the brown brink eastward, springs ‚Äî\n\nBecause the Holy Ghost over the bent\nWorld broods with warm breast and with ah! bright wings.\n\n~Gerard Manley Hopkins", generation_info={'finish_reason': 'stop', 'logprobs': None})], [Generation(text='\n\nQ: What did one ocean say to the other ocean?\nA: Nothing, they just waved.', generation_info={'finish_reason': 'stop', 'logprobs': None})], [Generation(text="\n\nA poem for you\n\nOn a field of green\n\nThe sky so blue\n\nA gentle breeze, the sun above\n\nA beautiful world, for us to love\n\nLife is a journey, full of surprise\n\nFull of joy and full of surprise\n\nBe brave and take small steps\n\nThe future will be revealed with depth\n\nIn the morning, when dawn arrives\n\nA fresh start, no reason to hide\n\nSomewhere down the road, there's a heart that beats\n\nBelieve in yourself, you'll always succeed.", generation_info={'finish_reason': 'stop', 'logprobs': None})]], llm_output={'token_usage': {'completion_tokens': 504, 'total_tokens': 528, 'prompt_tokens': 24}, 'model_name': 'text-davinci-003'})You can then check the metric if it was successful by calling the is_successful() method.answer_relevancy_metric.is_successful()# returns True/FalseOnce you have ran that, you should be able to see our dashboard below. Scenario 2: Tracking an LLM in a chain without callbacks‚ÄãTo track an LLM in a chain without callbacks, you can plug into it at the end.We can start by defining a simple chain as shown below.import requestsfrom langchain.chains import RetrievalQAfrom langchain.document_loaders import TextLoaderfrom langchain.embeddings.openai import OpenAIEmbeddingsfrom langchain.llms import OpenAIfrom langchain.text_splitter import CharacterTextSplitterfrom langchain.vectorstores import Chromatext_file_url = "https://raw.githubusercontent.com/hwchase17/chat-your-data/master/state_of_the_union.txt"openai_api_key = "sk-XXX"with open("state_of_the_union.txt", "w") as f:  response = requests.get(text_file_url)  f.write(response.text)loader = TextLoader("state_of_the_union.txt")documents = loader.load()text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)texts = text_splitter.split_documents(documents)embeddings = OpenAIEmbeddings(openai_api_key=openai_api_key)docsearch = Chroma.from_documents(texts, embeddings)qa = RetrievalQA.from_chain_type(  llm=OpenAI(openai_api_key=openai_api_key), chain_type="stuff",  retriever=docsearch.as_retriever())# Providing a new question-answering pipelinequery = "Who is the president?"result = qa.run(query)API Reference:RetrievalQATextLoaderOpenAIEmbeddingsOpenAICharacterTextSplitterChromaAfter defining a chain, you can then manually check for answer similarity.answer_relevancy_metric.measure(result, query)answer_relevancy_metric.is_successful()What's next?‚ÄãYou can create your own custom metrics here. DeepEval also offers other features such as being able to automatically create unit tests, tests for hallucination.If you are interested, check out our Github repository here https://github.com/confident-ai/deepeval. We welcome any PRs and discussions on how to improve LLM performance.PreviousArgillaNextContextInstallation and SetupGetting API CredentialsSetup DeepEvalGet StartedScenario 1: Feeding into LLMScenario 2: Tracking an LLM in a chain without callbacksWhat's next? IntegrationsCallbacksArgillaConfidentContextInfinoLabel StudioLLMonitorPromptLayerStreamlitChat modelsChat loadersDocument loadersDocument transformersLLMsMemoryRetrieversText embedding modelsAgents & ToolkitsToolsVector storesGrouped by providerIntegrationsCallbacksConfidentOn this pageConfidentDeepEval package for unit testing LLMs. from synthetic data creation to testing.In this guide we will demonstrate how to test and measure LLMs in performance. We show how you can use our callback to measure performance and how you can define your own metric and log them into our dashboard.DeepEval also offers:How to generate synthetic dataHow to measure performanceA dashboard to monitor and review results over timeInstallation and Setup‚Äãpip install deepeval --upgradeGetting API Credentials‚ÄãTo get the DeepEval API credentials, follow the next steps:Go to https://app.confident-ai.comClick on "Organization"Copy the API Key.When you log in, you will also be asked to set the implementation name. The implementation name is required to describe the type of implementation. (Think of what you want to call your project. We recommend making it descriptive.)deepeval loginSetup DeepEval‚ÄãYou can, by default, use the DeepEvalCallbackHandler to set up the metrics you want to track. However, this has limited support for metrics at the moment (more to be added soon). It currently supports:Answer RelevancyBiasToxicnessfrom deepeval.metrics.answer_relevancy import AnswerRelevancy# Here we want to make sure the answer is minimally relevantanswer_relevancy_metric = AnswerRelevancy(minimum_score=0.5)Get Started‚ÄãTo use the DeepEvalCallbackHandler, we need the implementation_name. import osfrom langchain.callbacks.confident_callback import DeepEvalCallbackHandlerdeepeval_callback = DeepEvalCallbackHandler(    implementation_name="langchainQuickstart",    metrics=[answer_relevancy_metric])API Reference:DeepEvalCallbackHandlerScenario 1: Feeding into LLM‚ÄãYou can then feed it into your LLM with OpenAI.from langchain.llms import OpenAIllm = OpenAI(    temperature=0,    callbacks=[deepeval_callback],    verbose=True,    openai_api_key="<YOUR_API_KEY>",)output = llm.generate(    [        "What is the best evaluation tool out there? (no bias at all)",    ])API Reference:OpenAI    LLMResult(generations=[[Generation(text='\n\nQ: What did the fish say when he hit the wall? \nA: Dam.', generation_info={'finish_reason': 'stop', 'logprobs': None})], [Generation(text='\n\nThe Moon \n\nThe moon is high in the midnight sky,\nSparkling like a star above.\nThe night so peaceful, so serene,\nFilling up the air with love.\n\nEver changing and renewing,\nA never-ending light of grace.\nThe moon remains a constant view,\nA reminder of life‚Äôs gentle pace.\n\nThrough time and space it guides us on,\nA never-fading beacon of hope.\nThe moon shines down on us all,\nAs it continues to rise and elope.', generation_info={'finish_reason': 'stop', 'logprobs': None})], [Generation(text='\n\nQ. What did one magnet say to the other magnet?\nA. "I find you very attractive!"', generation_info={'finish_reason': 'stop', 'logprobs': None})], [Generation(text="\n\nThe world is charged with the grandeur of God.\nIt will flame out, like shining from shook foil;\nIt gathers to a greatness, like the ooze of oil\nCrushed. Why do men then now not reck his rod?\n\nGenerations have trod, have trod, have trod;\nAnd all is seared with trade; bleared, smeared with toil;\nAnd wears man's smudge and shares man's smell: the soil\nIs bare now, nor can foot feel, being shod.\n\nAnd for all this, nature is never spent;\nThere lives the dearest freshness deep down things;\nAnd though the last lights off the black West went\nOh, morning, at the brown brink eastward, springs ‚Äî\n\nBecause the Holy Ghost over the bent\nWorld broods with warm breast and with ah! bright wings.\n\n~Gerard Manley Hopkins", generation_info={'finish_reason': 'stop', 'logprobs': None})], [Generation(text='\n\nQ: What did one ocean say to the other ocean?\nA: Nothing, they just waved.', generation_info={'finish_reason': 'stop', 'logprobs': None})], [Generation(text="\n\nA poem for you\n\nOn a field of green\n\nThe sky so blue\n\nA gentle breeze, the sun above\n\nA beautiful world, for us to love\n\nLife is a journey, full of surprise\n\nFull of joy and full of surprise\n\nBe brave and take small steps\n\nThe future will be revealed with depth\n\nIn the morning, when dawn arrives\n\nA fresh start, no reason to hide\n\nSomewhere down the road, there's a heart that beats\n\nBelieve in yourself, you'll always succeed.", generation_info={'finish_reason': 'stop', 'logprobs': None})]], llm_output={'token_usage': {'completion_tokens': 504, 'total_tokens': 528, 'prompt_tokens': 24}, 'model_name': 'text-davinci-003'})You can then check the metric if it was successful by calling the is_successful() method.answer_relevancy_metric.is_successful()# returns True/FalseOnce you have ran that, you should be able to see our dashboard below. Scenario 2: Tracking an LLM in a chain without callbacks‚ÄãTo track an LLM in a chain without callbacks, you can plug into it at the end.We can start by defining a simple chain as shown below.import requestsfrom langchain.chains import RetrievalQAfrom langchain.document_loaders import TextLoaderfrom langchain.embeddings.openai import OpenAIEmbeddingsfrom langchain.llms import OpenAIfrom langchain.text_splitter import CharacterTextSplitterfrom langchain.vectorstores import Chromatext_file_url = "https://raw.githubusercontent.com/hwchase17/chat-your-data/master/state_of_the_union.txt"openai_api_key = "sk-XXX"with open("state_of_the_union.txt", "w") as f:  response = requests.get(text_file_url)  f.write(response.text)loader = TextLoader("state_of_the_union.txt")documents = loader.load()text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)texts = text_splitter.split_documents(documents)embeddings = OpenAIEmbeddings(openai_api_key=openai_api_key)docsearch = Chroma.from_documents(texts, embeddings)qa = RetrievalQA.from_chain_type(  llm=OpenAI(openai_api_key=openai_api_key), chain_type="stuff",  retriever=docsearch.as_retriever())# Providing a new question-answering pipelinequery = "Who is the president?"result = qa.run(query)API Reference:RetrievalQATextLoaderOpenAIEmbeddingsOpenAICharacterTextSplitterChromaAfter defining a chain, you can then manually check for answer similarity.answer_relevancy_metric.measure(result, query)answer_relevancy_metric.is_successful()What's next?‚ÄãYou can create your own custom metrics here. DeepEval also offers other features such as being able to automatically create unit tests, tests for hallucination.If you are interested, check out our Github repository here https://github.com/confident-ai/deepeval. We welcome any PRs and discussions on how to improve LLM performance.PreviousArgillaNextContextInstallation and SetupGetting API CredentialsSetup DeepEvalGet StartedScenario 1: Feeding into LLMScenario 2: Tracking an LLM in a chain without callbacksWhat's next? IntegrationsCallbacksArgillaConfidentContextInfinoLabel StudioLLMonitorPromptLayerStreamlitChat modelsChat loadersDocument loadersDocument transformersLLMsMemoryRetrieversText embedding modelsAgents & ToolkitsToolsVector storesGrouped by provider IntegrationsCallbacksArgillaConfidentContextInfinoLabel StudioLLMonitorPromptLayerStreamlitChat modelsChat loadersDocument loadersDocument transformersLLMsMemoryRetrieversText embedding modelsAgents & ToolkitsToolsVector storesGrouped by provider Integrations Callbacks Chat models Chat loaders Document loaders Document transformers LLMs Memory Retrievers Text embedding models Agents & Toolkits Tools Vector stores Grouped by provider IntegrationsCallbacksConfidentOn this pageConfidentDeepEval package for unit testing LLMs. from synthetic data creation to testing.In this guide we will demonstrate how to test and measure LLMs in performance. We show how you can use our callback to measure performance and how you can define your own metric and log them into our dashboard.DeepEval also offers:How to generate synthetic dataHow to measure performanceA dashboard to monitor and review results over timeInstallation and Setup‚Äãpip install deepeval --upgradeGetting API Credentials‚ÄãTo get the DeepEval API credentials, follow the next steps:Go to https://app.confident-ai.comClick on "Organization"Copy the API Key.When you log in, you will also be asked to set the implementation name. The implementation name is required to describe the type of implementation. (Think of what you want to call your project. We recommend making it descriptive.)deepeval loginSetup DeepEval‚ÄãYou can, by default, use the DeepEvalCallbackHandler to set up the metrics you want to track. However, this has limited support for metrics at the moment (more to be added soon). It currently supports:Answer RelevancyBiasToxicnessfrom deepeval.metrics.answer_relevancy import AnswerRelevancy# Here we want to make sure the answer is minimally relevantanswer_relevancy_metric = AnswerRelevancy(minimum_score=0.5)Get Started‚ÄãTo use the DeepEvalCallbackHandler, we need the implementation_name. import osfrom langchain.callbacks.confident_callback import DeepEvalCallbackHandlerdeepeval_callback = DeepEvalCallbackHandler(    implementation_name="langchainQuickstart",    metrics=[answer_relevancy_metric])API Reference:DeepEvalCallbackHandlerScenario 1: Feeding into LLM‚ÄãYou can then feed it into your LLM with OpenAI.from langchain.llms import OpenAIllm = OpenAI(    temperature=0,    callbacks=[deepeval_callback],    verbose=True,    openai_api_key="<YOUR_API_KEY>",)output = llm.generate(    [        "What is the best evaluation tool out there? (no bias at all)",    ])API Reference:OpenAI    LLMResult(generations=[[Generation(text='\n\nQ: What did the fish say when he hit the wall? \nA: Dam.', generation_info={'finish_reason': 'stop', 'logprobs': None})], [Generation(text='\n\nThe Moon \n\nThe moon is high in the midnight sky,\nSparkling like a star above.\nThe night so peaceful, so serene,\nFilling up the air with love.\n\nEver changing and renewing,\nA never-ending light of grace.\nThe moon remains a constant view,\nA reminder of life‚Äôs gentle pace.\n\nThrough time and space it guides us on,\nA never-fading beacon of hope.\nThe moon shines down on us all,\nAs it continues to rise and elope.', generation_info={'finish_reason': 'stop', 'logprobs': None})], [Generation(text='\n\nQ. What did one magnet say to the other magnet?\nA. "I find you very attractive!"', generation_info={'finish_reason': 'stop', 'logprobs': None})], [Generation(text="\n\nThe world is charged with the grandeur of God.\nIt will flame out, like shining from shook foil;\nIt gathers to a greatness, like the ooze of oil\nCrushed. Why do men then now not reck his rod?\n\nGenerations have trod, have trod, have trod;\nAnd all is seared with trade; bleared, smeared with toil;\nAnd wears man's smudge and shares man's smell: the soil\nIs bare now, nor can foot feel, being shod.\n\nAnd for all this, nature is never spent;\nThere lives the dearest freshness deep down things;\nAnd though the last lights off the black West went\nOh, morning, at the brown brink eastward, springs ‚Äî\n\nBecause the Holy Ghost over the bent\nWorld broods with warm breast and with ah! bright wings.\n\n~Gerard Manley Hopkins", generation_info={'finish_reason': 'stop', 'logprobs': None})], [Generation(text='\n\nQ: What did one ocean say to the other ocean?\nA: Nothing, they just waved.', generation_info={'finish_reason': 'stop', 'logprobs': None})], [Generation(text="\n\nA poem for you\n\nOn a field of green\n\nThe sky so blue\n\nA gentle breeze, the sun above\n\nA beautiful world, for us to love\n\nLife is a journey, full of surprise\n\nFull of joy and full of surprise\n\nBe brave and take small steps\n\nThe future will be revealed with depth\n\nIn the morning, when dawn arrives\n\nA fresh start, no reason to hide\n\nSomewhere down the road, there's a heart that beats\n\nBelieve in yourself, you'll always succeed.", generation_info={'finish_reason': 'stop', 'logprobs': None})]], llm_output={'token_usage': {'completion_tokens': 504, 'total_tokens': 528, 'prompt_tokens': 24}, 'model_name': 'text-davinci-003'})You can then check the metric if it was successful by calling the is_successful() method.answer_relevancy_metric.is_successful()# returns True/FalseOnce you have ran that, you should be able to see our dashboard below. Scenario 2: Tracking an LLM in a chain without callbacks‚ÄãTo track an LLM in a chain without callbacks, you can plug into it at the end.We can start by defining a simple chain as shown below.import requestsfrom langchain.chains import RetrievalQAfrom langchain.document_loaders import TextLoaderfrom langchain.embeddings.openai import OpenAIEmbeddingsfrom langchain.llms import OpenAIfrom langchain.text_splitter import CharacterTextSplitterfrom langchain.vectorstores import Chromatext_file_url = "https://raw.githubusercontent.com/hwchase17/chat-your-data/master/state_of_the_union.txt"openai_api_key = "sk-XXX"with open("state_of_the_union.txt", "w") as f:  response = requests.get(text_file_url)  f.write(response.text)loader = TextLoader("state_of_the_union.txt")documents = loader.load()text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)texts = text_splitter.split_documents(documents)embeddings = OpenAIEmbeddings(openai_api_key=openai_api_key)docsearch = Chroma.from_documents(texts, embeddings)qa = RetrievalQA.from_chain_type(  llm=OpenAI(openai_api_key=openai_api_key), chain_type="stuff",  retriever=docsearch.as_retriever())# Providing a new question-answering pipelinequery = "Who is the president?"result = qa.run(query)API Reference:RetrievalQATextLoaderOpenAIEmbeddingsOpenAICharacterTextSplitterChromaAfter defining a chain, you can then manually check for answer similarity.answer_relevancy_metric.measure(result, query)answer_relevancy_metric.is_successful()What's next?‚ÄãYou can create your own custom metrics here. DeepEval also offers other features such as being able to automatically create unit tests, tests for hallucination.If you are interested, check out our Github repository here https://github.com/confident-ai/deepeval. We welcome any PRs and discussions on how to improve LLM performance.PreviousArgillaNextContextInstallation and SetupGetting API CredentialsSetup DeepEvalGet StartedScenario 1: Feeding into LLMScenario 2: Tracking an LLM in a chain without callbacksWhat's next? IntegrationsCallbacksConfidentOn this pageConfidentDeepEval package for unit testing LLMs. from synthetic data creation to testing.In this guide we will demonstrate how to test and measure LLMs in performance. We show how you can use our callback to measure performance and how you can define your own metric and log them into our dashboard.DeepEval also offers:How to generate synthetic dataHow to measure performanceA dashboard to monitor and review results over timeInstallation and Setup‚Äãpip install deepeval --upgradeGetting API Credentials‚ÄãTo get the DeepEval API credentials, follow the next steps:Go to https://app.confident-ai.comClick on "Organization"Copy the API Key.When you log in, you will also be asked to set the implementation name. The implementation name is required to describe the type of implementation. (Think of what you want to call your project. We recommend making it descriptive.)deepeval loginSetup DeepEval‚ÄãYou can, by default, use the DeepEvalCallbackHandler to set up the metrics you want to track. However, this has limited support for metrics at the moment (more to be added soon). It currently supports:Answer RelevancyBiasToxicnessfrom deepeval.metrics.answer_relevancy import AnswerRelevancy# Here we want to make sure the answer is minimally relevantanswer_relevancy_metric = AnswerRelevancy(minimum_score=0.5)Get Started‚ÄãTo use the DeepEvalCallbackHandler, we need the implementation_name. import osfrom langchain.callbacks.confident_callback import DeepEvalCallbackHandlerdeepeval_callback = DeepEvalCallbackHandler(    implementation_name="langchainQuickstart",    metrics=[answer_relevancy_metric])API Reference:DeepEvalCallbackHandlerScenario 1: Feeding into LLM‚ÄãYou can then feed it into your LLM with OpenAI.from langchain.llms import OpenAIllm = OpenAI(    temperature=0,    callbacks=[deepeval_callback],    verbose=True,    openai_api_key="<YOUR_API_KEY>",)output = llm.generate(    [        "What is the best evaluation tool out there? (no bias at all)",    ])API Reference:OpenAI    LLMResult(generations=[[Generation(text='\n\nQ: What did the fish say when he hit the wall? \nA: Dam.', generation_info={'finish_reason': 'stop', 'logprobs': None})], [Generation(text='\n\nThe Moon \n\nThe moon is high in the midnight sky,\nSparkling like a star above.\nThe night so peaceful, so serene,\nFilling up the air with love.\n\nEver changing and renewing,\nA never-ending light of grace.\nThe moon remains a constant view,\nA reminder of life‚Äôs gentle pace.\n\nThrough time and space it guides us on,\nA never-fading beacon of hope.\nThe moon shines down on us all,\nAs it continues to rise and elope.', generation_info={'finish_reason': 'stop', 'logprobs': None})], [Generation(text='\n\nQ. What did one magnet say to the other magnet?\nA. "I find you very attractive!"', generation_info={'finish_reason': 'stop', 'logprobs': None})], [Generation(text="\n\nThe world is charged with the grandeur of God.\nIt will flame out, like shining from shook foil;\nIt gathers to a greatness, like the ooze of oil\nCrushed. Why do men then now not reck his rod?\n\nGenerations have trod, have trod, have trod;\nAnd all is seared with trade; bleared, smeared with toil;\nAnd wears man's smudge and shares man's smell: the soil\nIs bare now, nor can foot feel, being shod.\n\nAnd for all this, nature is never spent;\nThere lives the dearest freshness deep down things;\nAnd though the last lights off the black West went\nOh, morning, at the brown brink eastward, springs ‚Äî\n\nBecause the Holy Ghost over the bent\nWorld broods with warm breast and with ah! bright wings.\n\n~Gerard Manley Hopkins", generation_info={'finish_reason': 'stop', 'logprobs': None})], [Generation(text='\n\nQ: What did one ocean say to the other ocean?\nA: Nothing, they just waved.', generation_info={'finish_reason': 'stop', 'logprobs': None})], [Generation(text="\n\nA poem for you\n\nOn a field of green\n\nThe sky so blue\n\nA gentle breeze, the sun above\n\nA beautiful world, for us to love\n\nLife is a journey, full of surprise\n\nFull of joy and full of surprise\n\nBe brave and take small steps\n\nThe future will be revealed with depth\n\nIn the morning, when dawn arrives\n\nA fresh start, no reason to hide\n\nSomewhere down the road, there's a heart that beats\n\nBelieve in yourself, you'll always succeed.", generation_info={'finish_reason': 'stop', 'logprobs': None})]], llm_output={'token_usage': {'completion_tokens': 504, 'total_tokens': 528, 'prompt_tokens': 24}, 'model_name': 'text-davinci-003'})You can then check the metric if it was successful by calling the is_successful() method.answer_relevancy_metric.is_successful()# returns True/FalseOnce you have ran that, you should be able to see our dashboard below. Scenario 2: Tracking an LLM in a chain without callbacks‚ÄãTo track an LLM in a chain without callbacks, you can plug into it at the end.We can start by defining a simple chain as shown below.import requestsfrom langchain.chains import RetrievalQAfrom langchain.document_loaders import TextLoaderfrom langchain.embeddings.openai import OpenAIEmbeddingsfrom langchain.llms import OpenAIfrom langchain.text_splitter import CharacterTextSplitterfrom langchain.vectorstores import Chromatext_file_url = "https://raw.githubusercontent.com/hwchase17/chat-your-data/master/state_of_the_union.txt"openai_api_key = "sk-XXX"with open("state_of_the_union.txt", "w") as f:  response = requests.get(text_file_url)  f.write(response.text)loader = TextLoader("state_of_the_union.txt")documents = loader.load()text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)texts = text_splitter.split_documents(documents)embeddings = OpenAIEmbeddings(openai_api_key=openai_api_key)docsearch = Chroma.from_documents(texts, embeddings)qa = RetrievalQA.from_chain_type(  llm=OpenAI(openai_api_key=openai_api_key), chain_type="stuff",  retriever=docsearch.as_retriever())# Providing a new question-answering pipelinequery = "Who is the president?"result = qa.run(query)API Reference:RetrievalQATextLoaderOpenAIEmbeddingsOpenAICharacterTextSplitterChromaAfter defining a chain, you can then manually check for answer similarity.answer_relevancy_metric.measure(result, query)answer_relevancy_metric.is_successful()What's next?‚ÄãYou can create your own custom metrics here. DeepEval also offers other features such as being able to automatically create unit tests, tests for hallucination.If you are interested, check out our Github repository here https://github.com/confident-ai/deepeval. We welcome any PRs and discussions on how to improve LLM performance.PreviousArgillaNextContext IntegrationsCallbacksConfidentOn this pageConfidentDeepEval package for unit testing LLMs. from synthetic data creation to testing.In this guide we will demonstrate how to test and measure LLMs in performance. We show how you can use our callback to measure performance and how you can define your own metric and log them into our dashboard.DeepEval also offers:How to generate synthetic dataHow to measure performanceA dashboard to monitor and review results over timeInstallation and Setup‚Äãpip install deepeval --upgradeGetting API Credentials‚ÄãTo get the DeepEval API credentials, follow the next steps:Go to https://app.confident-ai.comClick on "Organization"Copy the API Key.When you log in, you will also be asked to set the implementation name. The implementation name is required to describe the type of implementation. (Think of what you want to call your project. We recommend making it descriptive.)deepeval loginSetup DeepEval‚ÄãYou can, by default, use the DeepEvalCallbackHandler to set up the metrics you want to track. However, this has limited support for metrics at the moment (more to be added soon). It currently supports:Answer RelevancyBiasToxicnessfrom deepeval.metrics.answer_relevancy import AnswerRelevancy# Here we want to make sure the answer is minimally relevantanswer_relevancy_metric = AnswerRelevancy(minimum_score=0.5)Get Started‚ÄãTo use the DeepEvalCallbackHandler, we need the implementation_name. import osfrom langchain.callbacks.confident_callback import DeepEvalCallbackHandlerdeepeval_callback = DeepEvalCallbackHandler(    implementation_name="langchainQuickstart",    metrics=[answer_relevancy_metric])API Reference:DeepEvalCallbackHandlerScenario 1: Feeding into LLM‚ÄãYou can then feed it into your LLM with OpenAI.from langchain.llms import OpenAIllm = OpenAI(    temperature=0,    callbacks=[deepeval_callback],    verbose=True,    openai_api_key="<YOUR_API_KEY>",)output = llm.generate(    [        "What is the best evaluation tool out there? (no bias at all)",    ])API Reference:OpenAI    LLMResult(generations=[[Generation(text='\n\nQ: What did the fish say when he hit the wall? \nA: Dam.', generation_info={'finish_reason': 'stop', 'logprobs': None})], [Generation(text='\n\nThe Moon \n\nThe moon is high in the midnight sky,\nSparkling like a star above.\nThe night so peaceful, so serene,\nFilling up the air with love.\n\nEver changing and renewing,\nA never-ending light of grace.\nThe moon remains a constant view,\nA reminder of life‚Äôs gentle pace.\n\nThrough time and space it guides us on,\nA never-fading beacon of hope.\nThe moon shines down on us all,\nAs it continues to rise and elope.', generation_info={'finish_reason': 'stop', 'logprobs': None})], [Generation(text='\n\nQ. What did one magnet say to the other magnet?\nA. "I find you very attractive!"', generation_info={'finish_reason': 'stop', 'logprobs': None})], [Generation(text="\n\nThe world is charged with the grandeur of God.\nIt will flame out, like shining from shook foil;\nIt gathers to a greatness, like the ooze of oil\nCrushed. Why do men then now not reck his rod?\n\nGenerations have trod, have trod, have trod;\nAnd all is seared with trade; bleared, smeared with toil;\nAnd wears man's smudge and shares man's smell: the soil\nIs bare now, nor can foot feel, being shod.\n\nAnd for all this, nature is never spent;\nThere lives the dearest freshness deep down things;\nAnd though the last lights off the black West went\nOh, morning, at the brown brink eastward, springs ‚Äî\n\nBecause the Holy Ghost over the bent\nWorld broods with warm breast and with ah! bright wings.\n\n~Gerard Manley Hopkins", generation_info={'finish_reason': 'stop', 'logprobs': None})], [Generation(text='\n\nQ: What did one ocean say to the other ocean?\nA: Nothing, they just waved.', generation_info={'finish_reason': 'stop', 'logprobs': None})], [Generation(text="\n\nA poem for you\n\nOn a field of green\n\nThe sky so blue\n\nA gentle breeze, the sun above\n\nA beautiful world, for us to love\n\nLife is a journey, full of surprise\n\nFull of joy and full of surprise\n\nBe brave and take small steps\n\nThe future will be revealed with depth\n\nIn the morning, when dawn arrives\n\nA fresh start, no reason to hide\n\nSomewhere down the road, there's a heart that beats\n\nBelieve in yourself, you'll always succeed.", generation_info={'finish_reason': 'stop', 'logprobs': None})]], llm_output={'token_usage': {'completion_tokens': 504, 'total_tokens': 528, 'prompt_tokens': 24}, 'model_name': 'text-davinci-003'})You can then check the metric if it was successful by calling the is_successful() method.answer_relevancy_metric.is_successful()# returns True/FalseOnce you have ran that, you should be able to see our dashboard below. Scenario 2: Tracking an LLM in a chain without callbacks‚ÄãTo track an LLM in a chain without callbacks, you can plug into it at the end.We can start by defining a simple chain as shown below.import requestsfrom langchain.chains import RetrievalQAfrom langchain.document_loaders import TextLoaderfrom langchain.embeddings.openai import OpenAIEmbeddingsfrom langchain.llms import OpenAIfrom langchain.text_splitter import CharacterTextSplitterfrom langchain.vectorstores import Chromatext_file_url = "https://raw.githubusercontent.com/hwchase17/chat-your-data/master/state_of_the_union.txt"openai_api_key = "sk-XXX"with open("state_of_the_union.txt", "w") as f:  response = requests.get(text_file_url)  f.write(response.text)loader = TextLoader("state_of_the_union.txt")documents = loader.load()text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)texts = text_splitter.split_documents(documents)embeddings = OpenAIEmbeddings(openai_api_key=openai_api_key)docsearch = Chroma.from_documents(texts, embeddings)qa = RetrievalQA.from_chain_type(  llm=OpenAI(openai_api_key=openai_api_key), chain_type="stuff",  retriever=docsearch.as_retriever())# Providing a new question-answering pipelinequery = "Who is the president?"result = qa.run(query)API Reference:RetrievalQATextLoaderOpenAIEmbeddingsOpenAICharacterTextSplitterChromaAfter defining a chain, you can then manually check for answer similarity.answer_relevancy_metric.measure(result, query)answer_relevancy_metric.is_successful()What's next?‚ÄãYou can create your own custom metrics here. DeepEval also offers other features such as being able to automatically create unit tests, tests for hallucination.If you are interested, check out our Github repository here https://github.com/confident-ai/deepeval. We welcome any PRs and discussions on how to improve LLM performance.PreviousArgillaNextContext On this page ConfidentDeepEval package for unit testing LLMs. from synthetic data creation to testing.In this guide we will demonstrate how to test and measure LLMs in performance. We show how you can use our callback to measure performance and how you can define your own metric and log them into our dashboard.DeepEval also offers:How to generate synthetic dataHow to measure performanceA dashboard to monitor and review results over timeInstallation and Setup‚Äãpip install deepeval --upgradeGetting API Credentials‚ÄãTo get the DeepEval API credentials, follow the next steps:Go to https://app.confident-ai.comClick on "Organization"Copy the API Key.When you log in, you will also be asked to set the implementation name. The implementation name is required to describe the type of implementation. (Think of what you want to call your project. We recommend making it descriptive.)deepeval loginSetup DeepEval‚ÄãYou can, by default, use the DeepEvalCallbackHandler to set up the metrics you want to track. However, this has limited support for metrics at the moment (more to be added soon). It currently supports:Answer RelevancyBiasToxicnessfrom deepeval.metrics.answer_relevancy import AnswerRelevancy# Here we want to make sure the answer is minimally relevantanswer_relevancy_metric = AnswerRelevancy(minimum_score=0.5)Get Started‚ÄãTo use the DeepEvalCallbackHandler, we need the implementation_name. import osfrom langchain.callbacks.confident_callback import DeepEvalCallbackHandlerdeepeval_callback = DeepEvalCallbackHandler(    implementation_name="langchainQuickstart",    metrics=[answer_relevancy_metric])API Reference:DeepEvalCallbackHandlerScenario 1: Feeding into LLM‚ÄãYou can then feed it into your LLM with OpenAI.from langchain.llms import OpenAIllm = OpenAI(    temperature=0,    callbacks=[deepeval_callback],    verbose=True,    openai_api_key="<YOUR_API_KEY>",)output = llm.generate(    [        "What is the best evaluation tool out there? (no bias at all)",    ])API Reference:OpenAI    LLMResult(generations=[[Generation(text='\n\nQ: What did the fish say when he hit the wall? \nA: Dam.', generation_info={'finish_reason': 'stop', 'logprobs': None})], [Generation(text='\n\nThe Moon \n\nThe moon is high in the midnight sky,\nSparkling like a star above.\nThe night so peaceful, so serene,\nFilling up the air with love.\n\nEver changing and renewing,\nA never-ending light of grace.\nThe moon remains a constant view,\nA reminder of life‚Äôs gentle pace.\n\nThrough time and space it guides us on,\nA never-fading beacon of hope.\nThe moon shines down on us all,\nAs it continues to rise and elope.', generation_info={'finish_reason': 'stop', 'logprobs': None})], [Generation(text='\n\nQ. What did one magnet say to the other magnet?\nA. "I find you very attractive!"', generation_info={'finish_reason': 'stop', 'logprobs': None})], [Generation(text="\n\nThe world is charged with the grandeur of God.\nIt will flame out, like shining from shook foil;\nIt gathers to a greatness, like the ooze of oil\nCrushed. Why do men then now not reck his rod?\n\nGenerations have trod, have trod, have trod;\nAnd all is seared with trade; bleared, smeared with toil;\nAnd wears man's smudge and shares man's smell: the soil\nIs bare now, nor can foot feel, being shod.\n\nAnd for all this, nature is never spent;\nThere lives the dearest freshness deep down things;\nAnd though the last lights off the black West went\nOh, morning, at the brown brink eastward, springs ‚Äî\n\nBecause the Holy Ghost over the bent\nWorld broods with warm breast and with ah! bright wings.\n\n~Gerard Manley Hopkins", generation_info={'finish_reason': 'stop', 'logprobs': None})], [Generation(text='\n\nQ: What did one ocean say to the other ocean?\nA: Nothing, they just waved.', generation_info={'finish_reason': 'stop', 'logprobs': None})], [Generation(text="\n\nA poem for you\n\nOn a field of green\n\nThe sky so blue\n\nA gentle breeze, the sun above\n\nA beautiful world, for us to love\n\nLife is a journey, full of surprise\n\nFull of joy and full of surprise\n\nBe brave and take small steps\n\nThe future will be revealed with depth\n\nIn the morning, when dawn arrives\n\nA fresh start, no reason to hide\n\nSomewhere down the road, there's a heart that beats\n\nBelieve in yourself, you'll always succeed.", generation_info={'finish_reason': 'stop', 'logprobs': None})]], llm_output={'token_usage': {'completion_tokens': 504, 'total_tokens': 528, 'prompt_tokens': 24}, 'model_name': 'text-davinci-003'})You can then check the metric if it was successful by calling the is_successful() method.answer_relevancy_metric.is_successful()# returns True/FalseOnce you have ran that, you should be able to see our dashboard below. Scenario 2: Tracking an LLM in a chain without callbacks‚ÄãTo track an LLM in a chain without callbacks, you can plug into it at the end.We can start by defining a simple chain as shown below.import requestsfrom langchain.chains import RetrievalQAfrom langchain.document_loaders import TextLoaderfrom langchain.embeddings.openai import OpenAIEmbeddingsfrom langchain.llms import OpenAIfrom langchain.text_splitter import CharacterTextSplitterfrom langchain.vectorstores import Chromatext_file_url = "https://raw.githubusercontent.com/hwchase17/chat-your-data/master/state_of_the_union.txt"openai_api_key = "sk-XXX"with open("state_of_the_union.txt", "w") as f:  response = requests.get(text_file_url)  f.write(response.text)loader = TextLoader("state_of_the_union.txt")documents = loader.load()text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)texts = text_splitter.split_documents(documents)embeddings = OpenAIEmbeddings(openai_api_key=openai_api_key)docsearch = Chroma.from_documents(texts, embeddings)qa = RetrievalQA.from_chain_type(  llm=OpenAI(openai_api_key=openai_api_key), chain_type="stuff",  retriever=docsearch.as_retriever())# Providing a new question-answering pipelinequery = "Who is the president?"result = qa.run(query)API Reference:RetrievalQATextLoaderOpenAIEmbeddingsOpenAICharacterTextSplitterChromaAfter defining a chain, you can then manually check for answer similarity.answer_relevancy_metric.measure(result, query)answer_relevancy_metric.is_successful()What's next?‚ÄãYou can create your own custom metrics here. DeepEval also offers other features such as being able to automatically create unit tests, tests for hallucination.If you are interested, check out our Github repository here https://github.com/confident-ai/deepeval. We welcome any PRs and discussions on how to improve LLM performance. pip install deepeval --upgrade pip install deepeval --upgrade  deepeval login deepeval login  from deepeval.metrics.answer_relevancy import AnswerRelevancy# Here we want to make sure the answer is minimally relevantanswer_relevancy_metric = AnswerRelevancy(minimum_score=0.5) from deepeval.metrics.answer_relevancy import AnswerRelevancy# Here we want to make sure the answer is minimally relevantanswer_relevancy_metric = AnswerRelevancy(minimum_score=0.5)  import osfrom langchain.callbacks.confident_callback import DeepEvalCallbackHandlerdeepeval_callback = DeepEvalCallbackHandler(    implementation_name="langchainQuickstart",    metrics=[answer_relevancy_metric]) import osfrom langchain.callbacks.confident_callback import DeepEvalCallbackHandlerdeepeval_callback = DeepEvalCallbackHandler(    implementation_name="langchainQuickstart",    metrics=[answer_relevancy_metric])  API Reference:DeepEvalCallbackHandler from langchain.llms import OpenAIllm = OpenAI(    temperature=0,    callbacks=[deepeval_callback],    verbose=True,    openai_api_key="<YOUR_API_KEY>",)output = llm.generate(    [        "What is the best evaluation tool out there? (no bias at all)",    ]) from langchain.llms import OpenAIllm = OpenAI(    temperature=0,    callbacks=[deepeval_callback],    verbose=True,    openai_api_key="<YOUR_API_KEY>",)output = llm.generate(    [        "What is the best evaluation tool out there? (no bias at all)",    ])  API Reference:OpenAI     LLMResult(generations=[[Generation(text='\n\nQ: What did the fish say when he hit the wall? \nA: Dam.', generation_info={'finish_reason': 'stop', 'logprobs': None})], [Generation(text='\n\nThe Moon \n\nThe moon is high in the midnight sky,\nSparkling like a star above.\nThe night so peaceful, so serene,\nFilling up the air with love.\n\nEver changing and renewing,\nA never-ending light of grace.\nThe moon remains a constant view,\nA reminder of life‚Äôs gentle pace.\n\nThrough time and space it guides us on,\nA never-fading beacon of hope.\nThe moon shines down on us all,\nAs it continues to rise and elope.', generation_info={'finish_reason': 'stop', 'logprobs': None})], [Generation(text='\n\nQ. What did one magnet say to the other magnet?\nA. "I find you very attractive!"', generation_info={'finish_reason': 'stop', 'logprobs': None})], [Generation(text="\n\nThe world is charged with the grandeur of God.\nIt will flame out, like shining from shook foil;\nIt gathers to a greatness, like the ooze of oil\nCrushed. Why do men then now not reck his rod?\n\nGenerations have trod, have trod, have trod;\nAnd all is seared with trade; bleared, smeared with toil;\nAnd wears man's smudge and shares man's smell: the soil\nIs bare now, nor can foot feel, being shod.\n\nAnd for all this, nature is never spent;\nThere lives the dearest freshness deep down things;\nAnd though the last lights off the black West went\nOh, morning, at the brown brink eastward, springs ‚Äî\n\nBecause the Holy Ghost over the bent\nWorld broods with warm breast and with ah! bright wings.\n\n~Gerard Manley Hopkins", generation_info={'finish_reason': 'stop', 'logprobs': None})], [Generation(text='\n\nQ: What did one ocean say to the other ocean?\nA: Nothing, they just waved.', generation_info={'finish_reason': 'stop', 'logprobs': None})], [Generation(text="\n\nA poem for you\n\nOn a field of green\n\nThe sky so blue\n\nA gentle breeze, the sun above\n\nA beautiful world, for us to love\n\nLife is a journey, full of surprise\n\nFull of joy and full of surprise\n\nBe brave and take small steps\n\nThe future will be revealed with depth\n\nIn the morning, when dawn arrives\n\nA fresh start, no reason to hide\n\nSomewhere down the road, there's a heart that beats\n\nBelieve in yourself, you'll always succeed.", generation_info={'finish_reason': 'stop', 'logprobs': None})]], llm_output={'token_usage': {'completion_tokens': 504, 'total_tokens': 528, 'prompt_tokens': 24}, 'model_name': 'text-davinci-003'})     LLMResult(generations=[[Generation(text='\n\nQ: What did the fish say when he hit the wall? \nA: Dam.', generation_info={'finish_reason': 'stop', 'logprobs': None})], [Generation(text='\n\nThe Moon \n\nThe moon is high in the midnight sky,\nSparkling like a star above.\nThe night so peaceful, so serene,\nFilling up the air with love.\n\nEver changing and renewing,\nA never-ending light of grace.\nThe moon remains a constant view,\nA reminder of life‚Äôs gentle pace.\n\nThrough time and space it guides us on,\nA never-fading beacon of hope.\nThe moon shines down on us all,\nAs it continues to rise and elope.', generation_info={'finish_reason': 'stop', 'logprobs': None})], [Generation(text='\n\nQ. What did one magnet say to the other magnet?\nA. "I find you very attractive!"', generation_info={'finish_reason': 'stop', 'logprobs': None})], [Generation(text="\n\nThe world is charged with the grandeur of God.\nIt will flame out, like shining from shook foil;\nIt gathers to a greatness, like the ooze of oil\nCrushed. Why do men then now not reck his rod?\n\nGenerations have trod, have trod, have trod;\nAnd all is seared with trade; bleared, smeared with toil;\nAnd wears man's smudge and shares man's smell: the soil\nIs bare now, nor can foot feel, being shod.\n\nAnd for all this, nature is never spent;\nThere lives the dearest freshness deep down things;\nAnd though the last lights off the black West went\nOh, morning, at the brown brink eastward, springs ‚Äî\n\nBecause the Holy Ghost over the bent\nWorld broods with warm breast and with ah! bright wings.\n\n~Gerard Manley Hopkins", generation_info={'finish_reason': 'stop', 'logprobs': None})], [Generation(text='\n\nQ: What did one ocean say to the other ocean?\nA: Nothing, they just waved.', generation_info={'finish_reason': 'stop', 'logprobs': None})], [Generation(text="\n\nA poem for you\n\nOn a field of green\n\nThe sky so blue\n\nA gentle breeze, the sun above\n\nA beautiful world, for us to love\n\nLife is a journey, full of surprise\n\nFull of joy and full of surprise\n\nBe brave and take small steps\n\nThe future will be revealed with depth\n\nIn the morning, when dawn arrives\n\nA fresh start, no reason to hide\n\nSomewhere down the road, there's a heart that beats\n\nBelieve in yourself, you'll always succeed.", generation_info={'finish_reason': 'stop', 'logprobs': None})]], llm_output={'token_usage': {'completion_tokens': 504, 'total_tokens': 528, 'prompt_tokens': 24}, 'model_name': 'text-davinci-003'})     LLMResult(generations=[[Generation(text='\n\nQ: What did the fish say when he hit the wall? \nA: Dam.', generation_info={'finish_reason': 'stop', 'logprobs': None})], [Generation(text='\n\nThe Moon \n\nThe moon is high in the midnight sky,\nSparkling like a star above.\nThe night so peaceful, so serene,\nFilling up the air with love.\n\nEver changing and renewing,\nA never-ending light of grace.\nThe moon remains a constant view,\nA reminder of life‚Äôs gentle pace.\n\nThrough time and space it guides us on,\nA never-fading beacon of hope.\nThe moon shines down on us all,\nAs it continues to rise and elope.', generation_info={'finish_reason': 'stop', 'logprobs': None})], [Generation(text='\n\nQ. What did one magnet say to the other magnet?\nA. "I find you very attractive!"', generation_info={'finish_reason': 'stop', 'logprobs': None})], [Generation(text="\n\nThe world is charged with the grandeur of God.\nIt will flame out, like shining from shook foil;\nIt gathers to a greatness, like the ooze of oil\nCrushed. Why do men then now not reck his rod?\n\nGenerations have trod, have trod, have trod;\nAnd all is seared with trade; bleared, smeared with toil;\nAnd wears man's smudge and shares man's smell: the soil\nIs bare now, nor can foot feel, being shod.\n\nAnd for all this, nature is never spent;\nThere lives the dearest freshness deep down things;\nAnd though the last lights off the black West went\nOh, morning, at the brown brink eastward, springs ‚Äî\n\nBecause the Holy Ghost over the bent\nWorld broods with warm breast and with ah! bright wings.\n\n~Gerard Manley Hopkins", generation_info={'finish_reason': 'stop', 'logprobs': None})], [Generation(text='\n\nQ: What did one ocean say to the other ocean?\nA: Nothing, they just waved.', generation_info={'finish_reason': 'stop', 'logprobs': None})], [Generation(text="\n\nA poem for you\n\nOn a field of green\n\nThe sky so blue\n\nA gentle breeze, the sun above\n\nA beautiful world, for us to love\n\nLife is a journey, full of surprise\n\nFull of joy and full of surprise\n\nBe brave and take small steps\n\nThe future will be revealed with depth\n\nIn the morning, when dawn arrives\n\nA fresh start, no reason to hide\n\nSomewhere down the road, there's a heart that beats\n\nBelieve in yourself, you'll always succeed.", generation_info={'finish_reason': 'stop', 'logprobs': None})]], llm_output={'token_usage': {'completion_tokens': 504, 'total_tokens': 528, 'prompt_tokens': 24}, 'model_name': 'text-davinci-003'})  answer_relevancy_metric.is_successful()# returns True/False answer_relevancy_metric.is_successful()# returns True/False  import requestsfrom langchain.chains import RetrievalQAfrom langchain.document_loaders import TextLoaderfrom langchain.embeddings.openai import OpenAIEmbeddingsfrom langchain.llms import OpenAIfrom langchain.text_splitter import CharacterTextSplitterfrom langchain.vectorstores import Chromatext_file_url = "https://raw.githubusercontent.com/hwchase17/chat-your-data/master/state_of_the_union.txt"openai_api_key = "sk-XXX"with open("state_of_the_union.txt", "w") as f:  response = requests.get(text_file_url)  f.write(response.text)loader = TextLoader("state_of_the_union.txt")documents = loader.load()text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)texts = text_splitter.split_documents(documents)embeddings = OpenAIEmbeddings(openai_api_key=openai_api_key)docsearch = Chroma.from_documents(texts, embeddings)qa = RetrievalQA.from_chain_type(  llm=OpenAI(openai_api_key=openai_api_key), chain_type="stuff",  retriever=docsearch.as_retriever())# Providing a new question-answering pipelinequery = "Who is the president?"result = qa.run(query) import requestsfrom langchain.chains import RetrievalQAfrom langchain.document_loaders import TextLoaderfrom langchain.embeddings.openai import OpenAIEmbeddingsfrom langchain.llms import OpenAIfrom langchain.text_splitter import CharacterTextSplitterfrom langchain.vectorstores import Chromatext_file_url = "https://raw.githubusercontent.com/hwchase17/chat-your-data/master/state_of_the_union.txt"openai_api_key = "sk-XXX"with open("state_of_the_union.txt", "w") as f:  response = requests.get(text_file_url)  f.write(response.text)loader = TextLoader("state_of_the_union.txt")documents = loader.load()text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)texts = text_splitter.split_documents(documents)embeddings = OpenAIEmbeddings(openai_api_key=openai_api_key)docsearch = Chroma.from_documents(texts, embeddings)qa = RetrievalQA.from_chain_type(  llm=OpenAI(openai_api_key=openai_api_key), chain_type="stuff",  retriever=docsearch.as_retriever())# Providing a new question-answering pipelinequery = "Who is the president?"result = qa.run(query)  API Reference:RetrievalQATextLoaderOpenAIEmbeddingsOpenAICharacterTextSplitterChroma answer_relevancy_metric.measure(result, query)answer_relevancy_metric.is_successful() answer_relevancy_metric.measure(result, query)answer_relevancy_metric.is_successful()  Previous Argilla Next Context Installation and SetupGetting API CredentialsSetup DeepEvalGet StartedScenario 1: Feeding into LLMScenario 2: Tracking an LLM in a chain without callbacksWhat's next? Installation and SetupGetting API CredentialsSetup DeepEvalGet StartedScenario 1: Feeding into LLMScenario 2: Tracking an LLM in a chain without callbacksWhat's next? CommunityDiscordTwitterGitHubPythonJS/TSMoreHomepageBlogCopyright ¬© 2023 LangChain, Inc. CommunityDiscordTwitterGitHubPythonJS/TSMoreHomepageBlog CommunityDiscordTwitter Community GitHubPythonJS/TS GitHub MoreHomepageBlog More Copyright ¬© 2023 LangChain, Inc. Copyright ¬© 2023 LangChain, Inc. Skip to main content (#docusaurus_skipToContent_fallback) ü¶úÔ∏èüîó LangChain (/) Docs (/docs/get_started/introduction) Use cases (/docs/use_cases) Integrations (/docs/integrations) API (https://api.python.langchain.com) LangSmith (https://smith.langchain.com) JS/TS Docs (https://js.langchain.com/docs)  (https://github.com/hwchase17/langchain) Integrations (/docs/integrations) Callbacks (/docs/integrations/callbacks/) Argilla (/docs/integrations/callbacks/argilla) Confident (/docs/integrations/callbacks/confident) Context (/docs/integrations/callbacks/context) Infino (/docs/integrations/callbacks/infino) Label Studio (/docs/integrations/callbacks/labelstudio) LLMonitor (/docs/integrations/callbacks/llmonitor) PromptLayer (/docs/integrations/callbacks/promptlayer) Streamlit (/docs/integrations/callbacks/streamlit) Chat models (/docs/integrations/chat/) Chat loaders (/docs/integrations/chat_loaders/) Document loaders (/docs/integrations/document_loaders/) Document transformers (/docs/integrations/document_transformers/) LLMs (/docs/integrations/llms/) Memory (/docs/integrations/memory/) Retrievers (/docs/integrations/retrievers/) Text embedding models (/docs/integrations/text_embedding/) Agents & Toolkits (/docs/integrations/toolkits/) Tools (/docs/integrations/tools/) Vector stores (/docs/integrations/vectorstores/) Grouped by provider (/docs/integrations/providers/)  (/) Integrations (/docs/integrations) Callbacks (/docs/integrations/callbacks/) DeepEval (https://confident-ai.com) ‚Äã (#installation-and-setup) ‚Äã (#getting-api-credentials) https://app.confident-ai.com (https://app.confident-ai.com) ‚Äã (#setup-deepeval) Answer Relevancy (https://docs.confident-ai.com/docs/measuring_llm_performance/answer_relevancy) Bias (https://docs.confident-ai.com/docs/measuring_llm_performance/debias) Toxicness (https://docs.confident-ai.com/docs/measuring_llm_performance/non_toxic) ‚Äã (#get-started) DeepEvalCallbackHandler (https://api.python.langchain.com/en/latest/callbacks/langchain.callbacks.confident_callback.DeepEvalCallbackHandler.html) ‚Äã (#scenario-1-feeding-into-llm) OpenAI (https://api.python.langchain.com/en/latest/llms/langchain.llms.openai.OpenAI.html) ‚Äã (#scenario-2-tracking-an-llm-in-a-chain-without-callbacks) RetrievalQA (https://api.python.langchain.com/en/latest/chains/langchain.chains.retrieval_qa.base.RetrievalQA.html) TextLoader (https://api.python.langchain.com/en/latest/document_loaders/langchain.document_loaders.text.TextLoader.html) OpenAIEmbeddings (https://api.python.langchain.com/en/latest/embeddings/langchain.embeddings.openai.OpenAIEmbeddings.html) OpenAI (https://api.python.langchain.com/en/latest/llms/langchain.llms.openai.OpenAI.html) CharacterTextSplitter (https://api.python.langchain.com/en/latest/text_splitter/langchain.text_splitter.CharacterTextSplitter.html) Chroma (https://api.python.langchain.com/en/latest/vectorstores/langchain.vectorstores.chroma.Chroma.html) ‚Äã (#whats-next) here (https://docs.confident-ai.com/docs/quickstart/custom-metrics) automatically create unit tests (https://docs.confident-ai.com/docs/quickstart/synthetic-data-creation) tests for hallucination (https://docs.confident-ai.com/docs/measuring_llm_performance/factual_consistency) https://github.com/confident-ai/deepeval (https://github.com/confident-ai/deepeval) PreviousArgilla (/docs/integrations/callbacks/argilla) NextContext (/docs/integrations/callbacks/context) Installation and Setup (#installation-and-setup) Getting API Credentials (#getting-api-credentials) Setup DeepEval (#setup-deepeval) Get Started (#get-started) Scenario 1: Feeding into LLM (#scenario-1-feeding-into-llm) Scenario 2: Tracking an LLM in a chain without callbacks (#scenario-2-tracking-an-llm-in-a-chain-without-callbacks) What's next? (#whats-next) Discord (https://discord.gg/cU2adEyC7w) Twitter (https://twitter.com/LangChainAI) Python (https://github.com/hwchase17/langchain) JS/TS (https://github.com/hwchase17/langchainjs) Homepage (https://langchain.com) Blog (https://blog.langchain.dev)