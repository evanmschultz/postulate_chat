ChatGLM-6B is an open bilingual language model based on General Language Model (GLM) framework, with 6.2 billion parameters. With the quantization technique, users can deploy locally on consumer-grade graphics cards (only 6GB of GPU memory is required at the INT4 quantization level).  ChatGLM2-6B is the second-generation version of the open-source bilingual (Chinese-English) chat model ChatGLM-6B. It retains the smooth conversation flow and low deployment threshold of the first-generation model, while introducing the new features like better performance, longer context and more efficient inference. This example goes over how to use LangChain to interact with ChatGLM2-6B Inference for text completion. ChatGLM-6B and ChatGLM2-6B has the same api specs, so this example should work with both. IntegrationsCallbacksChat modelsChat loadersDocument loadersDocument transformersLLMsAI21Aleph AlphaAmazon API GatewayAnyscaleAzure MLAzure OpenAIBananaBasetenBeamBedrockBittensorCerebriumAIChatGLMClarifaiCohereC TransformersCTranslate2DatabricksDeepInfraDeepSparseEden AIFireworksForefrontAIGoogle Vertex AI PaLMGooseAIGPT4AllHugging Face HubHugging Face Local PipelinesHuggingface TextGen InferenceJSONFormerKoboldAI APILlama.cppLLM Caching integrationsManifestMinimaxModalMosaicMLNLP CloudOctoAIOllamaOpaquePromptsOpenAIOpenLLMOpenLMPetalsPipelineAIPredibasePrediction GuardPromptLayer OpenAIRELLMReplicateRunhouseSageMakerEndpointStochasticAINebula (Symbl.ai)TextGenTitan TakeoffTongyi QwenvLLMWriterXorbits Inference (Xinference)MemoryRetrieversText embedding modelsAgents & ToolkitsToolsVector storesGrouped by provider Callbacks Chat models Chat loaders Document loaders Document transformers LLMsAI21Aleph AlphaAmazon API GatewayAnyscaleAzure MLAzure OpenAIBananaBasetenBeamBedrockBittensorCerebriumAIChatGLMClarifaiCohereC TransformersCTranslate2DatabricksDeepInfraDeepSparseEden AIFireworksForefrontAIGoogle Vertex AI PaLMGooseAIGPT4AllHugging Face HubHugging Face Local PipelinesHuggingface TextGen InferenceJSONFormerKoboldAI APILlama.cppLLM Caching integrationsManifestMinimaxModalMosaicMLNLP CloudOctoAIOllamaOpaquePromptsOpenAIOpenLLMOpenLMPetalsPipelineAIPredibasePrediction GuardPromptLayer OpenAIRELLMReplicateRunhouseSageMakerEndpointStochasticAINebula (Symbl.ai)TextGenTitan TakeoffTongyi QwenvLLMWriterXorbits Inference (Xinference) AI21 Aleph Alpha Amazon API Gateway Anyscale Azure ML Azure OpenAI Banana Baseten Beam Bedrock Bittensor CerebriumAI ChatGLM Clarifai Cohere C Transformers CTranslate2 Databricks DeepInfra DeepSparse Eden AI Fireworks ForefrontAI Google Vertex AI PaLM GooseAI GPT4All Hugging Face Hub Hugging Face Local Pipelines Huggingface TextGen Inference JSONFormer KoboldAI API Llama.cpp LLM Caching integrations Manifest Minimax Modal MosaicML NLP Cloud OctoAI Ollama OpaquePrompts OpenAI OpenLLM OpenLM Petals PipelineAI Predibase Prediction Guard PromptLayer OpenAI RELLM Replicate Runhouse SageMakerEndpoint StochasticAI Nebula (Symbl.ai) TextGen Titan Takeoff Tongyi Qwen vLLM Writer Xorbits Inference (Xinference) Memory Retrievers Text embedding models Agents & Toolkits Tools Vector stores Grouped by provider  Integrations LLMs ChatGLM ChatGLM Discord Twitter Python JS/TS Homepage Blog Skip to main contentğŸ¦œï¸ğŸ”— LangChainDocsUse casesIntegrationsAPILangSmithJS/TS DocsCTRLKIntegrationsCallbacksChat modelsChat loadersDocument loadersDocument transformersLLMsAI21Aleph AlphaAmazon API GatewayAnyscaleAzure MLAzure OpenAIBananaBasetenBeamBedrockBittensorCerebriumAIChatGLMClarifaiCohereC TransformersCTranslate2DatabricksDeepInfraDeepSparseEden AIFireworksForefrontAIGoogle Vertex AI PaLMGooseAIGPT4AllHugging Face HubHugging Face Local PipelinesHuggingface TextGen InferenceJSONFormerKoboldAI APILlama.cppLLM Caching integrationsManifestMinimaxModalMosaicMLNLP CloudOctoAIOllamaOpaquePromptsOpenAIOpenLLMOpenLMPetalsPipelineAIPredibasePrediction GuardPromptLayer OpenAIRELLMReplicateRunhouseSageMakerEndpointStochasticAINebula (Symbl.ai)TextGenTitan TakeoffTongyi QwenvLLMWriterXorbits Inference (Xinference)MemoryRetrieversText embedding modelsAgents & ToolkitsToolsVector storesGrouped by providerIntegrationsLLMsChatGLMChatGLMChatGLM-6B is an open bilingual language model based on General Language Model (GLM) framework, with 6.2 billion parameters. With the quantization technique, users can deploy locally on consumer-grade graphics cards (only 6GB of GPU memory is required at the INT4 quantization level). ChatGLM2-6B is the second-generation version of the open-source bilingual (Chinese-English) chat model ChatGLM-6B. It retains the smooth conversation flow and low deployment threshold of the first-generation model, while introducing the new features like better performance, longer context and more efficient inference.This example goes over how to use LangChain to interact with ChatGLM2-6B Inference for text completion. ChatGLM-6B and ChatGLM2-6B has the same api specs, so this example should work with both.from langchain.llms import ChatGLMfrom langchain import PromptTemplate, LLMChain# import osAPI Reference:ChatGLMtemplate = """{question}"""prompt = PromptTemplate(template=template, input_variables=["question"])# default endpoint_url for a local deployed ChatGLM api serverendpoint_url = "http://127.0.0.1:8000"# direct access endpoint in a proxied environment# os.environ['NO_PROXY'] = '127.0.0.1'llm = ChatGLM(    endpoint_url=endpoint_url,    max_token=80000,    history=[["æˆ‘å°†ä»ç¾å›½åˆ°ä¸­å›½æ¥æ—…æ¸¸ï¼Œå‡ºè¡Œå‰å¸Œæœ›äº†è§£ä¸­å›½çš„åŸå¸‚", "æ¬¢è¿é—®æˆ‘ä»»ä½•é—®é¢˜ã€‚"]],    top_p=0.9,    model_kwargs={"sample_model_args": False},)# turn on with_history only when you want the LLM object to keep track of the conversation history# and send the accumulated context to the backend model api, which make it stateful. By default it is stateless.# llm.with_history = Truellm_chain = LLMChain(prompt=prompt, llm=llm)question = "åŒ—äº¬å’Œä¸Šæµ·ä¸¤åº§åŸå¸‚æœ‰ä»€ä¹ˆä¸åŒï¼Ÿ"llm_chain.run(question)    ChatGLM payload: {'prompt': 'åŒ—äº¬å’Œä¸Šæµ·ä¸¤åº§åŸå¸‚æœ‰ä»€ä¹ˆä¸åŒï¼Ÿ', 'temperature': 0.1, 'history': [['æˆ‘å°†ä»ç¾å›½åˆ°ä¸­å›½æ¥æ—…æ¸¸ï¼Œå‡ºè¡Œå‰å¸Œæœ›äº†è§£ä¸­å›½çš„åŸå¸‚', 'æ¬¢è¿é—®æˆ‘ä»»ä½•é—®é¢˜ã€‚']], 'max_length': 80000, 'top_p': 0.9, 'sample_model_args': False}    'åŒ—äº¬å’Œä¸Šæµ·æ˜¯ä¸­å›½çš„ä¸¤ä¸ªé¦–éƒ½ï¼Œå®ƒä»¬åœ¨è®¸å¤šæ–¹é¢éƒ½æœ‰æ‰€ä¸åŒã€‚\n\nåŒ—äº¬æ˜¯ä¸­å›½çš„æ”¿æ²»å’Œæ–‡åŒ–ä¸­å¿ƒï¼Œæ‹¥æœ‰æ‚ ä¹…çš„å†å²å’Œç¿çƒ‚çš„æ–‡åŒ–ã€‚å®ƒæ˜¯ä¸­å›½æœ€é‡è¦çš„å¤éƒ½ä¹‹ä¸€ï¼Œä¹Ÿæ˜¯ä¸­å›½å†å²ä¸Šæœ€åä¸€ä¸ªå°å»ºç‹æœçš„éƒ½åŸã€‚åŒ—äº¬æœ‰è®¸å¤šè‘—åçš„å¤è¿¹å’Œæ™¯ç‚¹ï¼Œä¾‹å¦‚ç´«ç¦åŸã€å¤©å®‰é—¨å¹¿åœºå’Œé•¿åŸç­‰ã€‚\n\nä¸Šæµ·æ˜¯ä¸­å›½æœ€ç°ä»£åŒ–çš„åŸå¸‚ä¹‹ä¸€ï¼Œä¹Ÿæ˜¯ä¸­å›½å•†ä¸šå’Œé‡‘èä¸­å¿ƒã€‚ä¸Šæµ·æ‹¥æœ‰è®¸å¤šå›½é™…çŸ¥åçš„ä¼ä¸šå’Œé‡‘èæœºæ„ï¼ŒåŒæ—¶ä¹Ÿæœ‰è®¸å¤šè‘—åçš„æ™¯ç‚¹å’Œç¾é£Ÿã€‚ä¸Šæµ·çš„å¤–æ»©æ˜¯ä¸€ä¸ªå†å²æ‚ ä¹…çš„å•†ä¸šåŒºï¼Œæ‹¥æœ‰è®¸å¤šæ¬§å¼å»ºç­‘å’Œé¤é¦†ã€‚\n\né™¤æ­¤ä¹‹å¤–ï¼ŒåŒ—äº¬å’Œä¸Šæµ·åœ¨äº¤é€šå’Œäººå£æ–¹é¢ä¹Ÿæœ‰å¾ˆå¤§å·®å¼‚ã€‚åŒ—äº¬æ˜¯ä¸­å›½çš„é¦–éƒ½ï¼Œäººå£ä¼—å¤šï¼Œäº¤é€šæ‹¥å µé—®é¢˜è¾ƒä¸ºä¸¥é‡ã€‚è€Œä¸Šæµ·æ˜¯ä¸­å›½çš„å•†ä¸šå’Œé‡‘èä¸­å¿ƒï¼Œäººå£å¯†åº¦è¾ƒä½ï¼Œäº¤é€šç›¸å¯¹è¾ƒä¸ºä¾¿åˆ©ã€‚\n\næ€»çš„æ¥è¯´ï¼ŒåŒ—äº¬å’Œä¸Šæµ·æ˜¯ä¸¤ä¸ªæ‹¥æœ‰ç‹¬ç‰¹é­…åŠ›å’Œç‰¹ç‚¹çš„åŸå¸‚ï¼Œå¯ä»¥æ ¹æ®è‡ªå·±çš„å…´è¶£å’Œæ—¶é—´æ¥é€‰æ‹©å‰å¾€å…¶ä¸­ä¸€åº§åŸå¸‚æ—…æ¸¸ã€‚'PreviousCerebriumAINextClarifaiCommunityDiscordTwitterGitHubPythonJS/TSMoreHomepageBlogCopyright Â© 2023 LangChain, Inc. Skip to main content ğŸ¦œï¸ğŸ”— LangChainDocsUse casesIntegrationsAPILangSmithJS/TS DocsCTRLK ğŸ¦œï¸ğŸ”— LangChainDocsUse casesIntegrationsAPI LangSmithJS/TS DocsCTRLK  CTRLK CTRLK  CTRLK   IntegrationsCallbacksChat modelsChat loadersDocument loadersDocument transformersLLMsAI21Aleph AlphaAmazon API GatewayAnyscaleAzure MLAzure OpenAIBananaBasetenBeamBedrockBittensorCerebriumAIChatGLMClarifaiCohereC TransformersCTranslate2DatabricksDeepInfraDeepSparseEden AIFireworksForefrontAIGoogle Vertex AI PaLMGooseAIGPT4AllHugging Face HubHugging Face Local PipelinesHuggingface TextGen InferenceJSONFormerKoboldAI APILlama.cppLLM Caching integrationsManifestMinimaxModalMosaicMLNLP CloudOctoAIOllamaOpaquePromptsOpenAIOpenLLMOpenLMPetalsPipelineAIPredibasePrediction GuardPromptLayer OpenAIRELLMReplicateRunhouseSageMakerEndpointStochasticAINebula (Symbl.ai)TextGenTitan TakeoffTongyi QwenvLLMWriterXorbits Inference (Xinference)MemoryRetrieversText embedding modelsAgents & ToolkitsToolsVector storesGrouped by providerIntegrationsLLMsChatGLMChatGLMChatGLM-6B is an open bilingual language model based on General Language Model (GLM) framework, with 6.2 billion parameters. With the quantization technique, users can deploy locally on consumer-grade graphics cards (only 6GB of GPU memory is required at the INT4 quantization level). ChatGLM2-6B is the second-generation version of the open-source bilingual (Chinese-English) chat model ChatGLM-6B. It retains the smooth conversation flow and low deployment threshold of the first-generation model, while introducing the new features like better performance, longer context and more efficient inference.This example goes over how to use LangChain to interact with ChatGLM2-6B Inference for text completion. ChatGLM-6B and ChatGLM2-6B has the same api specs, so this example should work with both.from langchain.llms import ChatGLMfrom langchain import PromptTemplate, LLMChain# import osAPI Reference:ChatGLMtemplate = """{question}"""prompt = PromptTemplate(template=template, input_variables=["question"])# default endpoint_url for a local deployed ChatGLM api serverendpoint_url = "http://127.0.0.1:8000"# direct access endpoint in a proxied environment# os.environ['NO_PROXY'] = '127.0.0.1'llm = ChatGLM(    endpoint_url=endpoint_url,    max_token=80000,    history=[["æˆ‘å°†ä»ç¾å›½åˆ°ä¸­å›½æ¥æ—…æ¸¸ï¼Œå‡ºè¡Œå‰å¸Œæœ›äº†è§£ä¸­å›½çš„åŸå¸‚", "æ¬¢è¿é—®æˆ‘ä»»ä½•é—®é¢˜ã€‚"]],    top_p=0.9,    model_kwargs={"sample_model_args": False},)# turn on with_history only when you want the LLM object to keep track of the conversation history# and send the accumulated context to the backend model api, which make it stateful. By default it is stateless.# llm.with_history = Truellm_chain = LLMChain(prompt=prompt, llm=llm)question = "åŒ—äº¬å’Œä¸Šæµ·ä¸¤åº§åŸå¸‚æœ‰ä»€ä¹ˆä¸åŒï¼Ÿ"llm_chain.run(question)    ChatGLM payload: {'prompt': 'åŒ—äº¬å’Œä¸Šæµ·ä¸¤åº§åŸå¸‚æœ‰ä»€ä¹ˆä¸åŒï¼Ÿ', 'temperature': 0.1, 'history': [['æˆ‘å°†ä»ç¾å›½åˆ°ä¸­å›½æ¥æ—…æ¸¸ï¼Œå‡ºè¡Œå‰å¸Œæœ›äº†è§£ä¸­å›½çš„åŸå¸‚', 'æ¬¢è¿é—®æˆ‘ä»»ä½•é—®é¢˜ã€‚']], 'max_length': 80000, 'top_p': 0.9, 'sample_model_args': False}    'åŒ—äº¬å’Œä¸Šæµ·æ˜¯ä¸­å›½çš„ä¸¤ä¸ªé¦–éƒ½ï¼Œå®ƒä»¬åœ¨è®¸å¤šæ–¹é¢éƒ½æœ‰æ‰€ä¸åŒã€‚\n\nåŒ—äº¬æ˜¯ä¸­å›½çš„æ”¿æ²»å’Œæ–‡åŒ–ä¸­å¿ƒï¼Œæ‹¥æœ‰æ‚ ä¹…çš„å†å²å’Œç¿çƒ‚çš„æ–‡åŒ–ã€‚å®ƒæ˜¯ä¸­å›½æœ€é‡è¦çš„å¤éƒ½ä¹‹ä¸€ï¼Œä¹Ÿæ˜¯ä¸­å›½å†å²ä¸Šæœ€åä¸€ä¸ªå°å»ºç‹æœçš„éƒ½åŸã€‚åŒ—äº¬æœ‰è®¸å¤šè‘—åçš„å¤è¿¹å’Œæ™¯ç‚¹ï¼Œä¾‹å¦‚ç´«ç¦åŸã€å¤©å®‰é—¨å¹¿åœºå’Œé•¿åŸç­‰ã€‚\n\nä¸Šæµ·æ˜¯ä¸­å›½æœ€ç°ä»£åŒ–çš„åŸå¸‚ä¹‹ä¸€ï¼Œä¹Ÿæ˜¯ä¸­å›½å•†ä¸šå’Œé‡‘èä¸­å¿ƒã€‚ä¸Šæµ·æ‹¥æœ‰è®¸å¤šå›½é™…çŸ¥åçš„ä¼ä¸šå’Œé‡‘èæœºæ„ï¼ŒåŒæ—¶ä¹Ÿæœ‰è®¸å¤šè‘—åçš„æ™¯ç‚¹å’Œç¾é£Ÿã€‚ä¸Šæµ·çš„å¤–æ»©æ˜¯ä¸€ä¸ªå†å²æ‚ ä¹…çš„å•†ä¸šåŒºï¼Œæ‹¥æœ‰è®¸å¤šæ¬§å¼å»ºç­‘å’Œé¤é¦†ã€‚\n\né™¤æ­¤ä¹‹å¤–ï¼ŒåŒ—äº¬å’Œä¸Šæµ·åœ¨äº¤é€šå’Œäººå£æ–¹é¢ä¹Ÿæœ‰å¾ˆå¤§å·®å¼‚ã€‚åŒ—äº¬æ˜¯ä¸­å›½çš„é¦–éƒ½ï¼Œäººå£ä¼—å¤šï¼Œäº¤é€šæ‹¥å µé—®é¢˜è¾ƒä¸ºä¸¥é‡ã€‚è€Œä¸Šæµ·æ˜¯ä¸­å›½çš„å•†ä¸šå’Œé‡‘èä¸­å¿ƒï¼Œäººå£å¯†åº¦è¾ƒä½ï¼Œäº¤é€šç›¸å¯¹è¾ƒä¸ºä¾¿åˆ©ã€‚\n\næ€»çš„æ¥è¯´ï¼ŒåŒ—äº¬å’Œä¸Šæµ·æ˜¯ä¸¤ä¸ªæ‹¥æœ‰ç‹¬ç‰¹é­…åŠ›å’Œç‰¹ç‚¹çš„åŸå¸‚ï¼Œå¯ä»¥æ ¹æ®è‡ªå·±çš„å…´è¶£å’Œæ—¶é—´æ¥é€‰æ‹©å‰å¾€å…¶ä¸­ä¸€åº§åŸå¸‚æ—…æ¸¸ã€‚'PreviousCerebriumAINextClarifai IntegrationsCallbacksChat modelsChat loadersDocument loadersDocument transformersLLMsAI21Aleph AlphaAmazon API GatewayAnyscaleAzure MLAzure OpenAIBananaBasetenBeamBedrockBittensorCerebriumAIChatGLMClarifaiCohereC TransformersCTranslate2DatabricksDeepInfraDeepSparseEden AIFireworksForefrontAIGoogle Vertex AI PaLMGooseAIGPT4AllHugging Face HubHugging Face Local PipelinesHuggingface TextGen InferenceJSONFormerKoboldAI APILlama.cppLLM Caching integrationsManifestMinimaxModalMosaicMLNLP CloudOctoAIOllamaOpaquePromptsOpenAIOpenLLMOpenLMPetalsPipelineAIPredibasePrediction GuardPromptLayer OpenAIRELLMReplicateRunhouseSageMakerEndpointStochasticAINebula (Symbl.ai)TextGenTitan TakeoffTongyi QwenvLLMWriterXorbits Inference (Xinference)MemoryRetrieversText embedding modelsAgents & ToolkitsToolsVector storesGrouped by providerIntegrationsLLMsChatGLMChatGLMChatGLM-6B is an open bilingual language model based on General Language Model (GLM) framework, with 6.2 billion parameters. With the quantization technique, users can deploy locally on consumer-grade graphics cards (only 6GB of GPU memory is required at the INT4 quantization level). ChatGLM2-6B is the second-generation version of the open-source bilingual (Chinese-English) chat model ChatGLM-6B. It retains the smooth conversation flow and low deployment threshold of the first-generation model, while introducing the new features like better performance, longer context and more efficient inference.This example goes over how to use LangChain to interact with ChatGLM2-6B Inference for text completion. ChatGLM-6B and ChatGLM2-6B has the same api specs, so this example should work with both.from langchain.llms import ChatGLMfrom langchain import PromptTemplate, LLMChain# import osAPI Reference:ChatGLMtemplate = """{question}"""prompt = PromptTemplate(template=template, input_variables=["question"])# default endpoint_url for a local deployed ChatGLM api serverendpoint_url = "http://127.0.0.1:8000"# direct access endpoint in a proxied environment# os.environ['NO_PROXY'] = '127.0.0.1'llm = ChatGLM(    endpoint_url=endpoint_url,    max_token=80000,    history=[["æˆ‘å°†ä»ç¾å›½åˆ°ä¸­å›½æ¥æ—…æ¸¸ï¼Œå‡ºè¡Œå‰å¸Œæœ›äº†è§£ä¸­å›½çš„åŸå¸‚", "æ¬¢è¿é—®æˆ‘ä»»ä½•é—®é¢˜ã€‚"]],    top_p=0.9,    model_kwargs={"sample_model_args": False},)# turn on with_history only when you want the LLM object to keep track of the conversation history# and send the accumulated context to the backend model api, which make it stateful. By default it is stateless.# llm.with_history = Truellm_chain = LLMChain(prompt=prompt, llm=llm)question = "åŒ—äº¬å’Œä¸Šæµ·ä¸¤åº§åŸå¸‚æœ‰ä»€ä¹ˆä¸åŒï¼Ÿ"llm_chain.run(question)    ChatGLM payload: {'prompt': 'åŒ—äº¬å’Œä¸Šæµ·ä¸¤åº§åŸå¸‚æœ‰ä»€ä¹ˆä¸åŒï¼Ÿ', 'temperature': 0.1, 'history': [['æˆ‘å°†ä»ç¾å›½åˆ°ä¸­å›½æ¥æ—…æ¸¸ï¼Œå‡ºè¡Œå‰å¸Œæœ›äº†è§£ä¸­å›½çš„åŸå¸‚', 'æ¬¢è¿é—®æˆ‘ä»»ä½•é—®é¢˜ã€‚']], 'max_length': 80000, 'top_p': 0.9, 'sample_model_args': False}    'åŒ—äº¬å’Œä¸Šæµ·æ˜¯ä¸­å›½çš„ä¸¤ä¸ªé¦–éƒ½ï¼Œå®ƒä»¬åœ¨è®¸å¤šæ–¹é¢éƒ½æœ‰æ‰€ä¸åŒã€‚\n\nåŒ—äº¬æ˜¯ä¸­å›½çš„æ”¿æ²»å’Œæ–‡åŒ–ä¸­å¿ƒï¼Œæ‹¥æœ‰æ‚ ä¹…çš„å†å²å’Œç¿çƒ‚çš„æ–‡åŒ–ã€‚å®ƒæ˜¯ä¸­å›½æœ€é‡è¦çš„å¤éƒ½ä¹‹ä¸€ï¼Œä¹Ÿæ˜¯ä¸­å›½å†å²ä¸Šæœ€åä¸€ä¸ªå°å»ºç‹æœçš„éƒ½åŸã€‚åŒ—äº¬æœ‰è®¸å¤šè‘—åçš„å¤è¿¹å’Œæ™¯ç‚¹ï¼Œä¾‹å¦‚ç´«ç¦åŸã€å¤©å®‰é—¨å¹¿åœºå’Œé•¿åŸç­‰ã€‚\n\nä¸Šæµ·æ˜¯ä¸­å›½æœ€ç°ä»£åŒ–çš„åŸå¸‚ä¹‹ä¸€ï¼Œä¹Ÿæ˜¯ä¸­å›½å•†ä¸šå’Œé‡‘èä¸­å¿ƒã€‚ä¸Šæµ·æ‹¥æœ‰è®¸å¤šå›½é™…çŸ¥åçš„ä¼ä¸šå’Œé‡‘èæœºæ„ï¼ŒåŒæ—¶ä¹Ÿæœ‰è®¸å¤šè‘—åçš„æ™¯ç‚¹å’Œç¾é£Ÿã€‚ä¸Šæµ·çš„å¤–æ»©æ˜¯ä¸€ä¸ªå†å²æ‚ ä¹…çš„å•†ä¸šåŒºï¼Œæ‹¥æœ‰è®¸å¤šæ¬§å¼å»ºç­‘å’Œé¤é¦†ã€‚\n\né™¤æ­¤ä¹‹å¤–ï¼ŒåŒ—äº¬å’Œä¸Šæµ·åœ¨äº¤é€šå’Œäººå£æ–¹é¢ä¹Ÿæœ‰å¾ˆå¤§å·®å¼‚ã€‚åŒ—äº¬æ˜¯ä¸­å›½çš„é¦–éƒ½ï¼Œäººå£ä¼—å¤šï¼Œäº¤é€šæ‹¥å µé—®é¢˜è¾ƒä¸ºä¸¥é‡ã€‚è€Œä¸Šæµ·æ˜¯ä¸­å›½çš„å•†ä¸šå’Œé‡‘èä¸­å¿ƒï¼Œäººå£å¯†åº¦è¾ƒä½ï¼Œäº¤é€šç›¸å¯¹è¾ƒä¸ºä¾¿åˆ©ã€‚\n\næ€»çš„æ¥è¯´ï¼ŒåŒ—äº¬å’Œä¸Šæµ·æ˜¯ä¸¤ä¸ªæ‹¥æœ‰ç‹¬ç‰¹é­…åŠ›å’Œç‰¹ç‚¹çš„åŸå¸‚ï¼Œå¯ä»¥æ ¹æ®è‡ªå·±çš„å…´è¶£å’Œæ—¶é—´æ¥é€‰æ‹©å‰å¾€å…¶ä¸­ä¸€åº§åŸå¸‚æ—…æ¸¸ã€‚'PreviousCerebriumAINextClarifai IntegrationsCallbacksChat modelsChat loadersDocument loadersDocument transformersLLMsAI21Aleph AlphaAmazon API GatewayAnyscaleAzure MLAzure OpenAIBananaBasetenBeamBedrockBittensorCerebriumAIChatGLMClarifaiCohereC TransformersCTranslate2DatabricksDeepInfraDeepSparseEden AIFireworksForefrontAIGoogle Vertex AI PaLMGooseAIGPT4AllHugging Face HubHugging Face Local PipelinesHuggingface TextGen InferenceJSONFormerKoboldAI APILlama.cppLLM Caching integrationsManifestMinimaxModalMosaicMLNLP CloudOctoAIOllamaOpaquePromptsOpenAIOpenLLMOpenLMPetalsPipelineAIPredibasePrediction GuardPromptLayer OpenAIRELLMReplicateRunhouseSageMakerEndpointStochasticAINebula (Symbl.ai)TextGenTitan TakeoffTongyi QwenvLLMWriterXorbits Inference (Xinference)MemoryRetrieversText embedding modelsAgents & ToolkitsToolsVector storesGrouped by provider IntegrationsCallbacksChat modelsChat loadersDocument loadersDocument transformersLLMsAI21Aleph AlphaAmazon API GatewayAnyscaleAzure MLAzure OpenAIBananaBasetenBeamBedrockBittensorCerebriumAIChatGLMClarifaiCohereC TransformersCTranslate2DatabricksDeepInfraDeepSparseEden AIFireworksForefrontAIGoogle Vertex AI PaLMGooseAIGPT4AllHugging Face HubHugging Face Local PipelinesHuggingface TextGen InferenceJSONFormerKoboldAI APILlama.cppLLM Caching integrationsManifestMinimaxModalMosaicMLNLP CloudOctoAIOllamaOpaquePromptsOpenAIOpenLLMOpenLMPetalsPipelineAIPredibasePrediction GuardPromptLayer OpenAIRELLMReplicateRunhouseSageMakerEndpointStochasticAINebula (Symbl.ai)TextGenTitan TakeoffTongyi QwenvLLMWriterXorbits Inference (Xinference)MemoryRetrieversText embedding modelsAgents & ToolkitsToolsVector storesGrouped by provider Integrations Callbacks Chat models Chat loaders Document loaders Document transformers LLMs Memory Retrievers Text embedding models Agents & Toolkits Tools Vector stores Grouped by provider IntegrationsLLMsChatGLMChatGLMChatGLM-6B is an open bilingual language model based on General Language Model (GLM) framework, with 6.2 billion parameters. With the quantization technique, users can deploy locally on consumer-grade graphics cards (only 6GB of GPU memory is required at the INT4 quantization level). ChatGLM2-6B is the second-generation version of the open-source bilingual (Chinese-English) chat model ChatGLM-6B. It retains the smooth conversation flow and low deployment threshold of the first-generation model, while introducing the new features like better performance, longer context and more efficient inference.This example goes over how to use LangChain to interact with ChatGLM2-6B Inference for text completion. ChatGLM-6B and ChatGLM2-6B has the same api specs, so this example should work with both.from langchain.llms import ChatGLMfrom langchain import PromptTemplate, LLMChain# import osAPI Reference:ChatGLMtemplate = """{question}"""prompt = PromptTemplate(template=template, input_variables=["question"])# default endpoint_url for a local deployed ChatGLM api serverendpoint_url = "http://127.0.0.1:8000"# direct access endpoint in a proxied environment# os.environ['NO_PROXY'] = '127.0.0.1'llm = ChatGLM(    endpoint_url=endpoint_url,    max_token=80000,    history=[["æˆ‘å°†ä»ç¾å›½åˆ°ä¸­å›½æ¥æ—…æ¸¸ï¼Œå‡ºè¡Œå‰å¸Œæœ›äº†è§£ä¸­å›½çš„åŸå¸‚", "æ¬¢è¿é—®æˆ‘ä»»ä½•é—®é¢˜ã€‚"]],    top_p=0.9,    model_kwargs={"sample_model_args": False},)# turn on with_history only when you want the LLM object to keep track of the conversation history# and send the accumulated context to the backend model api, which make it stateful. By default it is stateless.# llm.with_history = Truellm_chain = LLMChain(prompt=prompt, llm=llm)question = "åŒ—äº¬å’Œä¸Šæµ·ä¸¤åº§åŸå¸‚æœ‰ä»€ä¹ˆä¸åŒï¼Ÿ"llm_chain.run(question)    ChatGLM payload: {'prompt': 'åŒ—äº¬å’Œä¸Šæµ·ä¸¤åº§åŸå¸‚æœ‰ä»€ä¹ˆä¸åŒï¼Ÿ', 'temperature': 0.1, 'history': [['æˆ‘å°†ä»ç¾å›½åˆ°ä¸­å›½æ¥æ—…æ¸¸ï¼Œå‡ºè¡Œå‰å¸Œæœ›äº†è§£ä¸­å›½çš„åŸå¸‚', 'æ¬¢è¿é—®æˆ‘ä»»ä½•é—®é¢˜ã€‚']], 'max_length': 80000, 'top_p': 0.9, 'sample_model_args': False}    'åŒ—äº¬å’Œä¸Šæµ·æ˜¯ä¸­å›½çš„ä¸¤ä¸ªé¦–éƒ½ï¼Œå®ƒä»¬åœ¨è®¸å¤šæ–¹é¢éƒ½æœ‰æ‰€ä¸åŒã€‚\n\nåŒ—äº¬æ˜¯ä¸­å›½çš„æ”¿æ²»å’Œæ–‡åŒ–ä¸­å¿ƒï¼Œæ‹¥æœ‰æ‚ ä¹…çš„å†å²å’Œç¿çƒ‚çš„æ–‡åŒ–ã€‚å®ƒæ˜¯ä¸­å›½æœ€é‡è¦çš„å¤éƒ½ä¹‹ä¸€ï¼Œä¹Ÿæ˜¯ä¸­å›½å†å²ä¸Šæœ€åä¸€ä¸ªå°å»ºç‹æœçš„éƒ½åŸã€‚åŒ—äº¬æœ‰è®¸å¤šè‘—åçš„å¤è¿¹å’Œæ™¯ç‚¹ï¼Œä¾‹å¦‚ç´«ç¦åŸã€å¤©å®‰é—¨å¹¿åœºå’Œé•¿åŸç­‰ã€‚\n\nä¸Šæµ·æ˜¯ä¸­å›½æœ€ç°ä»£åŒ–çš„åŸå¸‚ä¹‹ä¸€ï¼Œä¹Ÿæ˜¯ä¸­å›½å•†ä¸šå’Œé‡‘èä¸­å¿ƒã€‚ä¸Šæµ·æ‹¥æœ‰è®¸å¤šå›½é™…çŸ¥åçš„ä¼ä¸šå’Œé‡‘èæœºæ„ï¼ŒåŒæ—¶ä¹Ÿæœ‰è®¸å¤šè‘—åçš„æ™¯ç‚¹å’Œç¾é£Ÿã€‚ä¸Šæµ·çš„å¤–æ»©æ˜¯ä¸€ä¸ªå†å²æ‚ ä¹…çš„å•†ä¸šåŒºï¼Œæ‹¥æœ‰è®¸å¤šæ¬§å¼å»ºç­‘å’Œé¤é¦†ã€‚\n\né™¤æ­¤ä¹‹å¤–ï¼ŒåŒ—äº¬å’Œä¸Šæµ·åœ¨äº¤é€šå’Œäººå£æ–¹é¢ä¹Ÿæœ‰å¾ˆå¤§å·®å¼‚ã€‚åŒ—äº¬æ˜¯ä¸­å›½çš„é¦–éƒ½ï¼Œäººå£ä¼—å¤šï¼Œäº¤é€šæ‹¥å µé—®é¢˜è¾ƒä¸ºä¸¥é‡ã€‚è€Œä¸Šæµ·æ˜¯ä¸­å›½çš„å•†ä¸šå’Œé‡‘èä¸­å¿ƒï¼Œäººå£å¯†åº¦è¾ƒä½ï¼Œäº¤é€šç›¸å¯¹è¾ƒä¸ºä¾¿åˆ©ã€‚\n\næ€»çš„æ¥è¯´ï¼ŒåŒ—äº¬å’Œä¸Šæµ·æ˜¯ä¸¤ä¸ªæ‹¥æœ‰ç‹¬ç‰¹é­…åŠ›å’Œç‰¹ç‚¹çš„åŸå¸‚ï¼Œå¯ä»¥æ ¹æ®è‡ªå·±çš„å…´è¶£å’Œæ—¶é—´æ¥é€‰æ‹©å‰å¾€å…¶ä¸­ä¸€åº§åŸå¸‚æ—…æ¸¸ã€‚'PreviousCerebriumAINextClarifai IntegrationsLLMsChatGLMChatGLMChatGLM-6B is an open bilingual language model based on General Language Model (GLM) framework, with 6.2 billion parameters. With the quantization technique, users can deploy locally on consumer-grade graphics cards (only 6GB of GPU memory is required at the INT4 quantization level). ChatGLM2-6B is the second-generation version of the open-source bilingual (Chinese-English) chat model ChatGLM-6B. It retains the smooth conversation flow and low deployment threshold of the first-generation model, while introducing the new features like better performance, longer context and more efficient inference.This example goes over how to use LangChain to interact with ChatGLM2-6B Inference for text completion. ChatGLM-6B and ChatGLM2-6B has the same api specs, so this example should work with both.from langchain.llms import ChatGLMfrom langchain import PromptTemplate, LLMChain# import osAPI Reference:ChatGLMtemplate = """{question}"""prompt = PromptTemplate(template=template, input_variables=["question"])# default endpoint_url for a local deployed ChatGLM api serverendpoint_url = "http://127.0.0.1:8000"# direct access endpoint in a proxied environment# os.environ['NO_PROXY'] = '127.0.0.1'llm = ChatGLM(    endpoint_url=endpoint_url,    max_token=80000,    history=[["æˆ‘å°†ä»ç¾å›½åˆ°ä¸­å›½æ¥æ—…æ¸¸ï¼Œå‡ºè¡Œå‰å¸Œæœ›äº†è§£ä¸­å›½çš„åŸå¸‚", "æ¬¢è¿é—®æˆ‘ä»»ä½•é—®é¢˜ã€‚"]],    top_p=0.9,    model_kwargs={"sample_model_args": False},)# turn on with_history only when you want the LLM object to keep track of the conversation history# and send the accumulated context to the backend model api, which make it stateful. By default it is stateless.# llm.with_history = Truellm_chain = LLMChain(prompt=prompt, llm=llm)question = "åŒ—äº¬å’Œä¸Šæµ·ä¸¤åº§åŸå¸‚æœ‰ä»€ä¹ˆä¸åŒï¼Ÿ"llm_chain.run(question)    ChatGLM payload: {'prompt': 'åŒ—äº¬å’Œä¸Šæµ·ä¸¤åº§åŸå¸‚æœ‰ä»€ä¹ˆä¸åŒï¼Ÿ', 'temperature': 0.1, 'history': [['æˆ‘å°†ä»ç¾å›½åˆ°ä¸­å›½æ¥æ—…æ¸¸ï¼Œå‡ºè¡Œå‰å¸Œæœ›äº†è§£ä¸­å›½çš„åŸå¸‚', 'æ¬¢è¿é—®æˆ‘ä»»ä½•é—®é¢˜ã€‚']], 'max_length': 80000, 'top_p': 0.9, 'sample_model_args': False}    'åŒ—äº¬å’Œä¸Šæµ·æ˜¯ä¸­å›½çš„ä¸¤ä¸ªé¦–éƒ½ï¼Œå®ƒä»¬åœ¨è®¸å¤šæ–¹é¢éƒ½æœ‰æ‰€ä¸åŒã€‚\n\nåŒ—äº¬æ˜¯ä¸­å›½çš„æ”¿æ²»å’Œæ–‡åŒ–ä¸­å¿ƒï¼Œæ‹¥æœ‰æ‚ ä¹…çš„å†å²å’Œç¿çƒ‚çš„æ–‡åŒ–ã€‚å®ƒæ˜¯ä¸­å›½æœ€é‡è¦çš„å¤éƒ½ä¹‹ä¸€ï¼Œä¹Ÿæ˜¯ä¸­å›½å†å²ä¸Šæœ€åä¸€ä¸ªå°å»ºç‹æœçš„éƒ½åŸã€‚åŒ—äº¬æœ‰è®¸å¤šè‘—åçš„å¤è¿¹å’Œæ™¯ç‚¹ï¼Œä¾‹å¦‚ç´«ç¦åŸã€å¤©å®‰é—¨å¹¿åœºå’Œé•¿åŸç­‰ã€‚\n\nä¸Šæµ·æ˜¯ä¸­å›½æœ€ç°ä»£åŒ–çš„åŸå¸‚ä¹‹ä¸€ï¼Œä¹Ÿæ˜¯ä¸­å›½å•†ä¸šå’Œé‡‘èä¸­å¿ƒã€‚ä¸Šæµ·æ‹¥æœ‰è®¸å¤šå›½é™…çŸ¥åçš„ä¼ä¸šå’Œé‡‘èæœºæ„ï¼ŒåŒæ—¶ä¹Ÿæœ‰è®¸å¤šè‘—åçš„æ™¯ç‚¹å’Œç¾é£Ÿã€‚ä¸Šæµ·çš„å¤–æ»©æ˜¯ä¸€ä¸ªå†å²æ‚ ä¹…çš„å•†ä¸šåŒºï¼Œæ‹¥æœ‰è®¸å¤šæ¬§å¼å»ºç­‘å’Œé¤é¦†ã€‚\n\né™¤æ­¤ä¹‹å¤–ï¼ŒåŒ—äº¬å’Œä¸Šæµ·åœ¨äº¤é€šå’Œäººå£æ–¹é¢ä¹Ÿæœ‰å¾ˆå¤§å·®å¼‚ã€‚åŒ—äº¬æ˜¯ä¸­å›½çš„é¦–éƒ½ï¼Œäººå£ä¼—å¤šï¼Œäº¤é€šæ‹¥å µé—®é¢˜è¾ƒä¸ºä¸¥é‡ã€‚è€Œä¸Šæµ·æ˜¯ä¸­å›½çš„å•†ä¸šå’Œé‡‘èä¸­å¿ƒï¼Œäººå£å¯†åº¦è¾ƒä½ï¼Œäº¤é€šç›¸å¯¹è¾ƒä¸ºä¾¿åˆ©ã€‚\n\næ€»çš„æ¥è¯´ï¼ŒåŒ—äº¬å’Œä¸Šæµ·æ˜¯ä¸¤ä¸ªæ‹¥æœ‰ç‹¬ç‰¹é­…åŠ›å’Œç‰¹ç‚¹çš„åŸå¸‚ï¼Œå¯ä»¥æ ¹æ®è‡ªå·±çš„å…´è¶£å’Œæ—¶é—´æ¥é€‰æ‹©å‰å¾€å…¶ä¸­ä¸€åº§åŸå¸‚æ—…æ¸¸ã€‚'PreviousCerebriumAINextClarifai ChatGLMChatGLM-6B is an open bilingual language model based on General Language Model (GLM) framework, with 6.2 billion parameters. With the quantization technique, users can deploy locally on consumer-grade graphics cards (only 6GB of GPU memory is required at the INT4 quantization level). ChatGLM2-6B is the second-generation version of the open-source bilingual (Chinese-English) chat model ChatGLM-6B. It retains the smooth conversation flow and low deployment threshold of the first-generation model, while introducing the new features like better performance, longer context and more efficient inference.This example goes over how to use LangChain to interact with ChatGLM2-6B Inference for text completion. ChatGLM-6B and ChatGLM2-6B has the same api specs, so this example should work with both.from langchain.llms import ChatGLMfrom langchain import PromptTemplate, LLMChain# import osAPI Reference:ChatGLMtemplate = """{question}"""prompt = PromptTemplate(template=template, input_variables=["question"])# default endpoint_url for a local deployed ChatGLM api serverendpoint_url = "http://127.0.0.1:8000"# direct access endpoint in a proxied environment# os.environ['NO_PROXY'] = '127.0.0.1'llm = ChatGLM(    endpoint_url=endpoint_url,    max_token=80000,    history=[["æˆ‘å°†ä»ç¾å›½åˆ°ä¸­å›½æ¥æ—…æ¸¸ï¼Œå‡ºè¡Œå‰å¸Œæœ›äº†è§£ä¸­å›½çš„åŸå¸‚", "æ¬¢è¿é—®æˆ‘ä»»ä½•é—®é¢˜ã€‚"]],    top_p=0.9,    model_kwargs={"sample_model_args": False},)# turn on with_history only when you want the LLM object to keep track of the conversation history# and send the accumulated context to the backend model api, which make it stateful. By default it is stateless.# llm.with_history = Truellm_chain = LLMChain(prompt=prompt, llm=llm)question = "åŒ—äº¬å’Œä¸Šæµ·ä¸¤åº§åŸå¸‚æœ‰ä»€ä¹ˆä¸åŒï¼Ÿ"llm_chain.run(question)    ChatGLM payload: {'prompt': 'åŒ—äº¬å’Œä¸Šæµ·ä¸¤åº§åŸå¸‚æœ‰ä»€ä¹ˆä¸åŒï¼Ÿ', 'temperature': 0.1, 'history': [['æˆ‘å°†ä»ç¾å›½åˆ°ä¸­å›½æ¥æ—…æ¸¸ï¼Œå‡ºè¡Œå‰å¸Œæœ›äº†è§£ä¸­å›½çš„åŸå¸‚', 'æ¬¢è¿é—®æˆ‘ä»»ä½•é—®é¢˜ã€‚']], 'max_length': 80000, 'top_p': 0.9, 'sample_model_args': False}    'åŒ—äº¬å’Œä¸Šæµ·æ˜¯ä¸­å›½çš„ä¸¤ä¸ªé¦–éƒ½ï¼Œå®ƒä»¬åœ¨è®¸å¤šæ–¹é¢éƒ½æœ‰æ‰€ä¸åŒã€‚\n\nåŒ—äº¬æ˜¯ä¸­å›½çš„æ”¿æ²»å’Œæ–‡åŒ–ä¸­å¿ƒï¼Œæ‹¥æœ‰æ‚ ä¹…çš„å†å²å’Œç¿çƒ‚çš„æ–‡åŒ–ã€‚å®ƒæ˜¯ä¸­å›½æœ€é‡è¦çš„å¤éƒ½ä¹‹ä¸€ï¼Œä¹Ÿæ˜¯ä¸­å›½å†å²ä¸Šæœ€åä¸€ä¸ªå°å»ºç‹æœçš„éƒ½åŸã€‚åŒ—äº¬æœ‰è®¸å¤šè‘—åçš„å¤è¿¹å’Œæ™¯ç‚¹ï¼Œä¾‹å¦‚ç´«ç¦åŸã€å¤©å®‰é—¨å¹¿åœºå’Œé•¿åŸç­‰ã€‚\n\nä¸Šæµ·æ˜¯ä¸­å›½æœ€ç°ä»£åŒ–çš„åŸå¸‚ä¹‹ä¸€ï¼Œä¹Ÿæ˜¯ä¸­å›½å•†ä¸šå’Œé‡‘èä¸­å¿ƒã€‚ä¸Šæµ·æ‹¥æœ‰è®¸å¤šå›½é™…çŸ¥åçš„ä¼ä¸šå’Œé‡‘èæœºæ„ï¼ŒåŒæ—¶ä¹Ÿæœ‰è®¸å¤šè‘—åçš„æ™¯ç‚¹å’Œç¾é£Ÿã€‚ä¸Šæµ·çš„å¤–æ»©æ˜¯ä¸€ä¸ªå†å²æ‚ ä¹…çš„å•†ä¸šåŒºï¼Œæ‹¥æœ‰è®¸å¤šæ¬§å¼å»ºç­‘å’Œé¤é¦†ã€‚\n\né™¤æ­¤ä¹‹å¤–ï¼ŒåŒ—äº¬å’Œä¸Šæµ·åœ¨äº¤é€šå’Œäººå£æ–¹é¢ä¹Ÿæœ‰å¾ˆå¤§å·®å¼‚ã€‚åŒ—äº¬æ˜¯ä¸­å›½çš„é¦–éƒ½ï¼Œäººå£ä¼—å¤šï¼Œäº¤é€šæ‹¥å µé—®é¢˜è¾ƒä¸ºä¸¥é‡ã€‚è€Œä¸Šæµ·æ˜¯ä¸­å›½çš„å•†ä¸šå’Œé‡‘èä¸­å¿ƒï¼Œäººå£å¯†åº¦è¾ƒä½ï¼Œäº¤é€šç›¸å¯¹è¾ƒä¸ºä¾¿åˆ©ã€‚\n\næ€»çš„æ¥è¯´ï¼ŒåŒ—äº¬å’Œä¸Šæµ·æ˜¯ä¸¤ä¸ªæ‹¥æœ‰ç‹¬ç‰¹é­…åŠ›å’Œç‰¹ç‚¹çš„åŸå¸‚ï¼Œå¯ä»¥æ ¹æ®è‡ªå·±çš„å…´è¶£å’Œæ—¶é—´æ¥é€‰æ‹©å‰å¾€å…¶ä¸­ä¸€åº§åŸå¸‚æ—…æ¸¸ã€‚' from langchain.llms import ChatGLMfrom langchain import PromptTemplate, LLMChain# import os from langchain.llms import ChatGLMfrom langchain import PromptTemplate, LLMChain# import os  API Reference:ChatGLM template = """{question}"""prompt = PromptTemplate(template=template, input_variables=["question"]) template = """{question}"""prompt = PromptTemplate(template=template, input_variables=["question"])  # default endpoint_url for a local deployed ChatGLM api serverendpoint_url = "http://127.0.0.1:8000"# direct access endpoint in a proxied environment# os.environ['NO_PROXY'] = '127.0.0.1'llm = ChatGLM(    endpoint_url=endpoint_url,    max_token=80000,    history=[["æˆ‘å°†ä»ç¾å›½åˆ°ä¸­å›½æ¥æ—…æ¸¸ï¼Œå‡ºè¡Œå‰å¸Œæœ›äº†è§£ä¸­å›½çš„åŸå¸‚", "æ¬¢è¿é—®æˆ‘ä»»ä½•é—®é¢˜ã€‚"]],    top_p=0.9,    model_kwargs={"sample_model_args": False},)# turn on with_history only when you want the LLM object to keep track of the conversation history# and send the accumulated context to the backend model api, which make it stateful. By default it is stateless.# llm.with_history = True # default endpoint_url for a local deployed ChatGLM api serverendpoint_url = "http://127.0.0.1:8000"# direct access endpoint in a proxied environment# os.environ['NO_PROXY'] = '127.0.0.1'llm = ChatGLM(    endpoint_url=endpoint_url,    max_token=80000,    history=[["æˆ‘å°†ä»ç¾å›½åˆ°ä¸­å›½æ¥æ—…æ¸¸ï¼Œå‡ºè¡Œå‰å¸Œæœ›äº†è§£ä¸­å›½çš„åŸå¸‚", "æ¬¢è¿é—®æˆ‘ä»»ä½•é—®é¢˜ã€‚"]],    top_p=0.9,    model_kwargs={"sample_model_args": False},)# turn on with_history only when you want the LLM object to keep track of the conversation history# and send the accumulated context to the backend model api, which make it stateful. By default it is stateless.# llm.with_history = True  llm_chain = LLMChain(prompt=prompt, llm=llm) llm_chain = LLMChain(prompt=prompt, llm=llm)  question = "åŒ—äº¬å’Œä¸Šæµ·ä¸¤åº§åŸå¸‚æœ‰ä»€ä¹ˆä¸åŒï¼Ÿ"llm_chain.run(question) question = "åŒ—äº¬å’Œä¸Šæµ·ä¸¤åº§åŸå¸‚æœ‰ä»€ä¹ˆä¸åŒï¼Ÿ"llm_chain.run(question)      ChatGLM payload: {'prompt': 'åŒ—äº¬å’Œä¸Šæµ·ä¸¤åº§åŸå¸‚æœ‰ä»€ä¹ˆä¸åŒï¼Ÿ', 'temperature': 0.1, 'history': [['æˆ‘å°†ä»ç¾å›½åˆ°ä¸­å›½æ¥æ—…æ¸¸ï¼Œå‡ºè¡Œå‰å¸Œæœ›äº†è§£ä¸­å›½çš„åŸå¸‚', 'æ¬¢è¿é—®æˆ‘ä»»ä½•é—®é¢˜ã€‚']], 'max_length': 80000, 'top_p': 0.9, 'sample_model_args': False}    'åŒ—äº¬å’Œä¸Šæµ·æ˜¯ä¸­å›½çš„ä¸¤ä¸ªé¦–éƒ½ï¼Œå®ƒä»¬åœ¨è®¸å¤šæ–¹é¢éƒ½æœ‰æ‰€ä¸åŒã€‚\n\nåŒ—äº¬æ˜¯ä¸­å›½çš„æ”¿æ²»å’Œæ–‡åŒ–ä¸­å¿ƒï¼Œæ‹¥æœ‰æ‚ ä¹…çš„å†å²å’Œç¿çƒ‚çš„æ–‡åŒ–ã€‚å®ƒæ˜¯ä¸­å›½æœ€é‡è¦çš„å¤éƒ½ä¹‹ä¸€ï¼Œä¹Ÿæ˜¯ä¸­å›½å†å²ä¸Šæœ€åä¸€ä¸ªå°å»ºç‹æœçš„éƒ½åŸã€‚åŒ—äº¬æœ‰è®¸å¤šè‘—åçš„å¤è¿¹å’Œæ™¯ç‚¹ï¼Œä¾‹å¦‚ç´«ç¦åŸã€å¤©å®‰é—¨å¹¿åœºå’Œé•¿åŸç­‰ã€‚\n\nä¸Šæµ·æ˜¯ä¸­å›½æœ€ç°ä»£åŒ–çš„åŸå¸‚ä¹‹ä¸€ï¼Œä¹Ÿæ˜¯ä¸­å›½å•†ä¸šå’Œé‡‘èä¸­å¿ƒã€‚ä¸Šæµ·æ‹¥æœ‰è®¸å¤šå›½é™…çŸ¥åçš„ä¼ä¸šå’Œé‡‘èæœºæ„ï¼ŒåŒæ—¶ä¹Ÿæœ‰è®¸å¤šè‘—åçš„æ™¯ç‚¹å’Œç¾é£Ÿã€‚ä¸Šæµ·çš„å¤–æ»©æ˜¯ä¸€ä¸ªå†å²æ‚ ä¹…çš„å•†ä¸šåŒºï¼Œæ‹¥æœ‰è®¸å¤šæ¬§å¼å»ºç­‘å’Œé¤é¦†ã€‚\n\né™¤æ­¤ä¹‹å¤–ï¼ŒåŒ—äº¬å’Œä¸Šæµ·åœ¨äº¤é€šå’Œäººå£æ–¹é¢ä¹Ÿæœ‰å¾ˆå¤§å·®å¼‚ã€‚åŒ—äº¬æ˜¯ä¸­å›½çš„é¦–éƒ½ï¼Œäººå£ä¼—å¤šï¼Œäº¤é€šæ‹¥å µé—®é¢˜è¾ƒä¸ºä¸¥é‡ã€‚è€Œä¸Šæµ·æ˜¯ä¸­å›½çš„å•†ä¸šå’Œé‡‘èä¸­å¿ƒï¼Œäººå£å¯†åº¦è¾ƒä½ï¼Œäº¤é€šç›¸å¯¹è¾ƒä¸ºä¾¿åˆ©ã€‚\n\næ€»çš„æ¥è¯´ï¼ŒåŒ—äº¬å’Œä¸Šæµ·æ˜¯ä¸¤ä¸ªæ‹¥æœ‰ç‹¬ç‰¹é­…åŠ›å’Œç‰¹ç‚¹çš„åŸå¸‚ï¼Œå¯ä»¥æ ¹æ®è‡ªå·±çš„å…´è¶£å’Œæ—¶é—´æ¥é€‰æ‹©å‰å¾€å…¶ä¸­ä¸€åº§åŸå¸‚æ—…æ¸¸ã€‚'     ChatGLM payload: {'prompt': 'åŒ—äº¬å’Œä¸Šæµ·ä¸¤åº§åŸå¸‚æœ‰ä»€ä¹ˆä¸åŒï¼Ÿ', 'temperature': 0.1, 'history': [['æˆ‘å°†ä»ç¾å›½åˆ°ä¸­å›½æ¥æ—…æ¸¸ï¼Œå‡ºè¡Œå‰å¸Œæœ›äº†è§£ä¸­å›½çš„åŸå¸‚', 'æ¬¢è¿é—®æˆ‘ä»»ä½•é—®é¢˜ã€‚']], 'max_length': 80000, 'top_p': 0.9, 'sample_model_args': False}    'åŒ—äº¬å’Œä¸Šæµ·æ˜¯ä¸­å›½çš„ä¸¤ä¸ªé¦–éƒ½ï¼Œå®ƒä»¬åœ¨è®¸å¤šæ–¹é¢éƒ½æœ‰æ‰€ä¸åŒã€‚\n\nåŒ—äº¬æ˜¯ä¸­å›½çš„æ”¿æ²»å’Œæ–‡åŒ–ä¸­å¿ƒï¼Œæ‹¥æœ‰æ‚ ä¹…çš„å†å²å’Œç¿çƒ‚çš„æ–‡åŒ–ã€‚å®ƒæ˜¯ä¸­å›½æœ€é‡è¦çš„å¤éƒ½ä¹‹ä¸€ï¼Œä¹Ÿæ˜¯ä¸­å›½å†å²ä¸Šæœ€åä¸€ä¸ªå°å»ºç‹æœçš„éƒ½åŸã€‚åŒ—äº¬æœ‰è®¸å¤šè‘—åçš„å¤è¿¹å’Œæ™¯ç‚¹ï¼Œä¾‹å¦‚ç´«ç¦åŸã€å¤©å®‰é—¨å¹¿åœºå’Œé•¿åŸç­‰ã€‚\n\nä¸Šæµ·æ˜¯ä¸­å›½æœ€ç°ä»£åŒ–çš„åŸå¸‚ä¹‹ä¸€ï¼Œä¹Ÿæ˜¯ä¸­å›½å•†ä¸šå’Œé‡‘èä¸­å¿ƒã€‚ä¸Šæµ·æ‹¥æœ‰è®¸å¤šå›½é™…çŸ¥åçš„ä¼ä¸šå’Œé‡‘èæœºæ„ï¼ŒåŒæ—¶ä¹Ÿæœ‰è®¸å¤šè‘—åçš„æ™¯ç‚¹å’Œç¾é£Ÿã€‚ä¸Šæµ·çš„å¤–æ»©æ˜¯ä¸€ä¸ªå†å²æ‚ ä¹…çš„å•†ä¸šåŒºï¼Œæ‹¥æœ‰è®¸å¤šæ¬§å¼å»ºç­‘å’Œé¤é¦†ã€‚\n\né™¤æ­¤ä¹‹å¤–ï¼ŒåŒ—äº¬å’Œä¸Šæµ·åœ¨äº¤é€šå’Œäººå£æ–¹é¢ä¹Ÿæœ‰å¾ˆå¤§å·®å¼‚ã€‚åŒ—äº¬æ˜¯ä¸­å›½çš„é¦–éƒ½ï¼Œäººå£ä¼—å¤šï¼Œäº¤é€šæ‹¥å µé—®é¢˜è¾ƒä¸ºä¸¥é‡ã€‚è€Œä¸Šæµ·æ˜¯ä¸­å›½çš„å•†ä¸šå’Œé‡‘èä¸­å¿ƒï¼Œäººå£å¯†åº¦è¾ƒä½ï¼Œäº¤é€šç›¸å¯¹è¾ƒä¸ºä¾¿åˆ©ã€‚\n\næ€»çš„æ¥è¯´ï¼ŒåŒ—äº¬å’Œä¸Šæµ·æ˜¯ä¸¤ä¸ªæ‹¥æœ‰ç‹¬ç‰¹é­…åŠ›å’Œç‰¹ç‚¹çš„åŸå¸‚ï¼Œå¯ä»¥æ ¹æ®è‡ªå·±çš„å…´è¶£å’Œæ—¶é—´æ¥é€‰æ‹©å‰å¾€å…¶ä¸­ä¸€åº§åŸå¸‚æ—…æ¸¸ã€‚'     ChatGLM payload: {'prompt': 'åŒ—äº¬å’Œä¸Šæµ·ä¸¤åº§åŸå¸‚æœ‰ä»€ä¹ˆä¸åŒï¼Ÿ', 'temperature': 0.1, 'history': [['æˆ‘å°†ä»ç¾å›½åˆ°ä¸­å›½æ¥æ—…æ¸¸ï¼Œå‡ºè¡Œå‰å¸Œæœ›äº†è§£ä¸­å›½çš„åŸå¸‚', 'æ¬¢è¿é—®æˆ‘ä»»ä½•é—®é¢˜ã€‚']], 'max_length': 80000, 'top_p': 0.9, 'sample_model_args': False}    'åŒ—äº¬å’Œä¸Šæµ·æ˜¯ä¸­å›½çš„ä¸¤ä¸ªé¦–éƒ½ï¼Œå®ƒä»¬åœ¨è®¸å¤šæ–¹é¢éƒ½æœ‰æ‰€ä¸åŒã€‚\n\nåŒ—äº¬æ˜¯ä¸­å›½çš„æ”¿æ²»å’Œæ–‡åŒ–ä¸­å¿ƒï¼Œæ‹¥æœ‰æ‚ ä¹…çš„å†å²å’Œç¿çƒ‚çš„æ–‡åŒ–ã€‚å®ƒæ˜¯ä¸­å›½æœ€é‡è¦çš„å¤éƒ½ä¹‹ä¸€ï¼Œä¹Ÿæ˜¯ä¸­å›½å†å²ä¸Šæœ€åä¸€ä¸ªå°å»ºç‹æœçš„éƒ½åŸã€‚åŒ—äº¬æœ‰è®¸å¤šè‘—åçš„å¤è¿¹å’Œæ™¯ç‚¹ï¼Œä¾‹å¦‚ç´«ç¦åŸã€å¤©å®‰é—¨å¹¿åœºå’Œé•¿åŸç­‰ã€‚\n\nä¸Šæµ·æ˜¯ä¸­å›½æœ€ç°ä»£åŒ–çš„åŸå¸‚ä¹‹ä¸€ï¼Œä¹Ÿæ˜¯ä¸­å›½å•†ä¸šå’Œé‡‘èä¸­å¿ƒã€‚ä¸Šæµ·æ‹¥æœ‰è®¸å¤šå›½é™…çŸ¥åçš„ä¼ä¸šå’Œé‡‘èæœºæ„ï¼ŒåŒæ—¶ä¹Ÿæœ‰è®¸å¤šè‘—åçš„æ™¯ç‚¹å’Œç¾é£Ÿã€‚ä¸Šæµ·çš„å¤–æ»©æ˜¯ä¸€ä¸ªå†å²æ‚ ä¹…çš„å•†ä¸šåŒºï¼Œæ‹¥æœ‰è®¸å¤šæ¬§å¼å»ºç­‘å’Œé¤é¦†ã€‚\n\né™¤æ­¤ä¹‹å¤–ï¼ŒåŒ—äº¬å’Œä¸Šæµ·åœ¨äº¤é€šå’Œäººå£æ–¹é¢ä¹Ÿæœ‰å¾ˆå¤§å·®å¼‚ã€‚åŒ—äº¬æ˜¯ä¸­å›½çš„é¦–éƒ½ï¼Œäººå£ä¼—å¤šï¼Œäº¤é€šæ‹¥å µé—®é¢˜è¾ƒä¸ºä¸¥é‡ã€‚è€Œä¸Šæµ·æ˜¯ä¸­å›½çš„å•†ä¸šå’Œé‡‘èä¸­å¿ƒï¼Œäººå£å¯†åº¦è¾ƒä½ï¼Œäº¤é€šç›¸å¯¹è¾ƒä¸ºä¾¿åˆ©ã€‚\n\næ€»çš„æ¥è¯´ï¼ŒåŒ—äº¬å’Œä¸Šæµ·æ˜¯ä¸¤ä¸ªæ‹¥æœ‰ç‹¬ç‰¹é­…åŠ›å’Œç‰¹ç‚¹çš„åŸå¸‚ï¼Œå¯ä»¥æ ¹æ®è‡ªå·±çš„å…´è¶£å’Œæ—¶é—´æ¥é€‰æ‹©å‰å¾€å…¶ä¸­ä¸€åº§åŸå¸‚æ—…æ¸¸ã€‚'  Previous CerebriumAI Next Clarifai CommunityDiscordTwitterGitHubPythonJS/TSMoreHomepageBlogCopyright Â© 2023 LangChain, Inc. CommunityDiscordTwitterGitHubPythonJS/TSMoreHomepageBlog CommunityDiscordTwitter Community GitHubPythonJS/TS GitHub MoreHomepageBlog More Copyright Â© 2023 LangChain, Inc. Copyright Â© 2023 LangChain, Inc. Skip to main content (#docusaurus_skipToContent_fallback) ğŸ¦œï¸ğŸ”— LangChain (/) Docs (/docs/get_started/introduction) Use cases (/docs/use_cases) Integrations (/docs/integrations) API (https://api.python.langchain.com) LangSmith (https://smith.langchain.com) JS/TS Docs (https://js.langchain.com/docs)  (https://github.com/hwchase17/langchain) Integrations (/docs/integrations) Callbacks (/docs/integrations/callbacks/) Chat models (/docs/integrations/chat/) Chat loaders (/docs/integrations/chat_loaders/) Document loaders (/docs/integrations/document_loaders/) Document transformers (/docs/integrations/document_transformers/) LLMs (/docs/integrations/llms/) AI21 (/docs/integrations/llms/ai21) Aleph Alpha (/docs/integrations/llms/aleph_alpha) Amazon API Gateway (/docs/integrations/llms/amazon_api_gateway) Anyscale (/docs/integrations/llms/anyscale) Azure ML (/docs/integrations/llms/azure_ml) Azure OpenAI (/docs/integrations/llms/azure_openai) Banana (/docs/integrations/llms/banana) Baseten (/docs/integrations/llms/baseten) Beam (/docs/integrations/llms/beam) Bedrock (/docs/integrations/llms/bedrock) Bittensor (/docs/integrations/llms/bittensor) CerebriumAI (/docs/integrations/llms/cerebriumai) ChatGLM (/docs/integrations/llms/chatglm) Clarifai (/docs/integrations/llms/clarifai) Cohere (/docs/integrations/llms/cohere) C Transformers (/docs/integrations/llms/ctransformers) CTranslate2 (/docs/integrations/llms/ctranslate2) Databricks (/docs/integrations/llms/databricks) DeepInfra (/docs/integrations/llms/deepinfra) DeepSparse (/docs/integrations/llms/deepsparse) Eden AI (/docs/integrations/llms/edenai) Fireworks (/docs/integrations/llms/fireworks) ForefrontAI (/docs/integrations/llms/forefrontai) Google Vertex AI PaLM (/docs/integrations/llms/google_vertex_ai_palm) GooseAI (/docs/integrations/llms/gooseai) GPT4All (/docs/integrations/llms/gpt4all) Hugging Face Hub (/docs/integrations/llms/huggingface_hub) Hugging Face Local Pipelines (/docs/integrations/llms/huggingface_pipelines) Huggingface TextGen Inference (/docs/integrations/llms/huggingface_textgen_inference) JSONFormer (/docs/integrations/llms/jsonformer_experimental) KoboldAI API (/docs/integrations/llms/koboldai) Llama.cpp (/docs/integrations/llms/llamacpp) LLM Caching integrations (/docs/integrations/llms/llm_caching) Manifest (/docs/integrations/llms/manifest) Minimax (/docs/integrations/llms/minimax) Modal (/docs/integrations/llms/modal) MosaicML (/docs/integrations/llms/mosaicml) NLP Cloud (/docs/integrations/llms/nlpcloud) OctoAI (/docs/integrations/llms/octoai) Ollama (/docs/integrations/llms/ollama) OpaquePrompts (/docs/integrations/llms/opaqueprompts) OpenAI (/docs/integrations/llms/openai) OpenLLM (/docs/integrations/llms/openllm) OpenLM (/docs/integrations/llms/openlm) Petals (/docs/integrations/llms/petals) PipelineAI (/docs/integrations/llms/pipelineai) Predibase (/docs/integrations/llms/predibase) Prediction Guard (/docs/integrations/llms/predictionguard) PromptLayer OpenAI (/docs/integrations/llms/promptlayer_openai) RELLM (/docs/integrations/llms/rellm_experimental) Replicate (/docs/integrations/llms/replicate) Runhouse (/docs/integrations/llms/runhouse) SageMakerEndpoint (/docs/integrations/llms/sagemaker) StochasticAI (/docs/integrations/llms/stochasticai) Nebula (Symbl.ai) (/docs/integrations/llms/symblai_nebula) TextGen (/docs/integrations/llms/textgen) Titan Takeoff (/docs/integrations/llms/titan_takeoff) Tongyi Qwen (/docs/integrations/llms/tongyi) vLLM (/docs/integrations/llms/vllm) Writer (/docs/integrations/llms/writer) Xorbits Inference (Xinference) (/docs/integrations/llms/xinference) Memory (/docs/integrations/memory/) Retrievers (/docs/integrations/retrievers/) Text embedding models (/docs/integrations/text_embedding/) Agents & Toolkits (/docs/integrations/toolkits/) Tools (/docs/integrations/tools/) Vector stores (/docs/integrations/vectorstores/) Grouped by provider (/docs/integrations/providers/)  (/) Integrations (/docs/integrations) LLMs (/docs/integrations/llms/) ChatGLM-6B (https://github.com/THUDM/ChatGLM-6B) ChatGLM2-6B (https://github.com/THUDM/ChatGLM2-6B) ChatGLM (https://api.python.langchain.com/en/latest/llms/langchain.llms.chatglm.ChatGLM.html) PreviousCerebriumAI (/docs/integrations/llms/cerebriumai) NextClarifai (/docs/integrations/llms/clarifai) Discord (https://discord.gg/cU2adEyC7w) Twitter (https://twitter.com/LangChainAI) Python (https://github.com/hwchase17/langchain) JS/TS (https://github.com/hwchase17/langchainjs) Homepage (https://langchain.com) Blog (https://blog.langchain.dev)