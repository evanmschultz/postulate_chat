DocAI is a Google Cloud platform to transform unstructured data from documents into structured data, making it easier to understand, analyze, and consume. You can read more about it: https://cloud.google.com/document-ai/docs/overview  First, you need to set up a GCS bucket and create your own OCR processor as described here: https://cloud.google.com/document-ai/docs/create-processor The GCS_OUTPUT_PATH should be a path to a folder on GCS (starting with gs://) and a processor name should look like projects/PROJECT_NUMBER/locations/LOCATION/processors/PROCESSOR_ID. You can get it either programmatically or copy from the Prediction endpoint section of the Processor details tab in the Google Cloud Console. Now, let's create a parser: Let's go and parse an Alphabet's take from here: https://abc.xyz/assets/a7/5b/9e5ae0364b12b4c883f3cf748226/goog-exhibit-99-1-q1-2023-19.pdf. Copy it to your GCS bucket first, and adjust the path below. We'll get one document per page, 11 in total: You can run end-to-end parsing of a blob one-by-one. If you have many documents, it might be a better approach to batch them together and maybe even detach parsing from handling the results of parsing. You can check whether operations are finished: And when they're finished, you can parse the results: And now we can finally generate Documents from parsed results: IntegrationsCallbacksChat modelsChat loadersDocument loadersDocument transformersBeautiful SoupdocaiDoctran Extract PropertiesDoctran Interrogate DocumentsDoctran Translate Documentshtml2textNuclia Understanding API document transformerOpenAI Functions Metadata TaggerLLMsMemoryRetrieversText embedding modelsAgents & ToolkitsToolsVector storesGrouped by provider Callbacks Chat models Chat loaders Document loaders Document transformersBeautiful SoupdocaiDoctran Extract PropertiesDoctran Interrogate DocumentsDoctran Translate Documentshtml2textNuclia Understanding API document transformerOpenAI Functions Metadata Tagger Beautiful Soup docai Doctran Extract Properties Doctran Interrogate Documents Doctran Translate Documents html2text Nuclia Understanding API document transformer OpenAI Functions Metadata Tagger LLMs Memory Retrievers Text embedding models Agents & Toolkits Tools Vector stores Grouped by provider  Integrations Document transformers docai Blob DocAIParser Discord Twitter Python JS/TS Homepage Blog Skip to main contentü¶úÔ∏èüîó LangChainDocsUse casesIntegrationsAPILangSmithJS/TS DocsCTRLKIntegrationsCallbacksChat modelsChat loadersDocument loadersDocument transformersBeautiful SoupdocaiDoctran Extract PropertiesDoctran Interrogate DocumentsDoctran Translate Documentshtml2textNuclia Understanding API document transformerOpenAI Functions Metadata TaggerLLMsMemoryRetrieversText embedding modelsAgents & ToolkitsToolsVector storesGrouped by providerIntegrationsDocument transformersdocaidocaifrom langchain.document_loaders.blob_loaders import Blobfrom langchain.document_loaders.parsers import DocAIParserAPI Reference:BlobDocAIParserDocAI is a Google Cloud platform to transform unstructured data from documents into structured data, making it easier to understand, analyze, and consume. You can read more about it: https://cloud.google.com/document-ai/docs/overview First, you need to set up a GCS bucket and create your own OCR processor as described here: https://cloud.google.com/document-ai/docs/create-processor The GCS_OUTPUT_PATH should be a path to a folder on GCS (starting with gs://) and a processor name should look like projects/PROJECT_NUMBER/locations/LOCATION/processors/PROCESSOR_ID. You can get it either programmatically or copy from the Prediction endpoint section of the Processor details tab in the Google Cloud Console.PROJECT = "PUT_SOMETHING_HERE"GCS_OUTPUT_PATH = "PUT_SOMETHING_HERE"PROCESSOR_NAME = "PUT_SOMETHING_HERE"Now, let's create a parser:parser = DocAIParser(location="us", processor_name=PROCESSOR_NAME, gcs_output_path=GCS_OUTPUT_PATH)Let's go and parse an Alphabet's take from here: https://abc.xyz/assets/a7/5b/9e5ae0364b12b4c883f3cf748226/goog-exhibit-99-1-q1-2023-19.pdf. Copy it to your GCS bucket first, and adjust the path below.blob = Blob(path="gs://vertex-pgt/examples/goog-exhibit-99-1-q1-2023-19.pdf")docs = list(parser.lazy_parse(blob))We'll get one document per page, 11 in total:print(len(docs))    11You can run end-to-end parsing of a blob one-by-one. If you have many documents, it might be a better approach to batch them together and maybe even detach parsing from handling the results of parsing.operations = parser.docai_parse([blob])print([op.operation.name for op in operations])    ['projects/543079149601/locations/us/operations/16447136779727347991']You can check whether operations are finished:parser.is_running(operations)    TrueAnd when they're finished, you can parse the results:parser.is_running(operations)    Falseresults = parser.get_results(operations)print(results[0])    DocAIParsingResults(source_path='gs://vertex-pgt/examples/goog-exhibit-99-1-q1-2023-19.pdf', parsed_path='gs://vertex-pgt/test/run1/16447136779727347991/0')And now we can finally generate Documents from parsed results:docs = list(parser.parse_from_results(results))print(len(docs))    11PreviousBeautiful SoupNextDoctran Extract PropertiesCommunityDiscordTwitterGitHubPythonJS/TSMoreHomepageBlogCopyright ¬© 2023 LangChain, Inc. Skip to main content ü¶úÔ∏èüîó LangChainDocsUse casesIntegrationsAPILangSmithJS/TS DocsCTRLK ü¶úÔ∏èüîó LangChainDocsUse casesIntegrationsAPI LangSmithJS/TS DocsCTRLK  CTRLK CTRLK  CTRLK   IntegrationsCallbacksChat modelsChat loadersDocument loadersDocument transformersBeautiful SoupdocaiDoctran Extract PropertiesDoctran Interrogate DocumentsDoctran Translate Documentshtml2textNuclia Understanding API document transformerOpenAI Functions Metadata TaggerLLMsMemoryRetrieversText embedding modelsAgents & ToolkitsToolsVector storesGrouped by providerIntegrationsDocument transformersdocaidocaifrom langchain.document_loaders.blob_loaders import Blobfrom langchain.document_loaders.parsers import DocAIParserAPI Reference:BlobDocAIParserDocAI is a Google Cloud platform to transform unstructured data from documents into structured data, making it easier to understand, analyze, and consume. You can read more about it: https://cloud.google.com/document-ai/docs/overview First, you need to set up a GCS bucket and create your own OCR processor as described here: https://cloud.google.com/document-ai/docs/create-processor The GCS_OUTPUT_PATH should be a path to a folder on GCS (starting with gs://) and a processor name should look like projects/PROJECT_NUMBER/locations/LOCATION/processors/PROCESSOR_ID. You can get it either programmatically or copy from the Prediction endpoint section of the Processor details tab in the Google Cloud Console.PROJECT = "PUT_SOMETHING_HERE"GCS_OUTPUT_PATH = "PUT_SOMETHING_HERE"PROCESSOR_NAME = "PUT_SOMETHING_HERE"Now, let's create a parser:parser = DocAIParser(location="us", processor_name=PROCESSOR_NAME, gcs_output_path=GCS_OUTPUT_PATH)Let's go and parse an Alphabet's take from here: https://abc.xyz/assets/a7/5b/9e5ae0364b12b4c883f3cf748226/goog-exhibit-99-1-q1-2023-19.pdf. Copy it to your GCS bucket first, and adjust the path below.blob = Blob(path="gs://vertex-pgt/examples/goog-exhibit-99-1-q1-2023-19.pdf")docs = list(parser.lazy_parse(blob))We'll get one document per page, 11 in total:print(len(docs))    11You can run end-to-end parsing of a blob one-by-one. If you have many documents, it might be a better approach to batch them together and maybe even detach parsing from handling the results of parsing.operations = parser.docai_parse([blob])print([op.operation.name for op in operations])    ['projects/543079149601/locations/us/operations/16447136779727347991']You can check whether operations are finished:parser.is_running(operations)    TrueAnd when they're finished, you can parse the results:parser.is_running(operations)    Falseresults = parser.get_results(operations)print(results[0])    DocAIParsingResults(source_path='gs://vertex-pgt/examples/goog-exhibit-99-1-q1-2023-19.pdf', parsed_path='gs://vertex-pgt/test/run1/16447136779727347991/0')And now we can finally generate Documents from parsed results:docs = list(parser.parse_from_results(results))print(len(docs))    11PreviousBeautiful SoupNextDoctran Extract Properties IntegrationsCallbacksChat modelsChat loadersDocument loadersDocument transformersBeautiful SoupdocaiDoctran Extract PropertiesDoctran Interrogate DocumentsDoctran Translate Documentshtml2textNuclia Understanding API document transformerOpenAI Functions Metadata TaggerLLMsMemoryRetrieversText embedding modelsAgents & ToolkitsToolsVector storesGrouped by providerIntegrationsDocument transformersdocaidocaifrom langchain.document_loaders.blob_loaders import Blobfrom langchain.document_loaders.parsers import DocAIParserAPI Reference:BlobDocAIParserDocAI is a Google Cloud platform to transform unstructured data from documents into structured data, making it easier to understand, analyze, and consume. You can read more about it: https://cloud.google.com/document-ai/docs/overview First, you need to set up a GCS bucket and create your own OCR processor as described here: https://cloud.google.com/document-ai/docs/create-processor The GCS_OUTPUT_PATH should be a path to a folder on GCS (starting with gs://) and a processor name should look like projects/PROJECT_NUMBER/locations/LOCATION/processors/PROCESSOR_ID. You can get it either programmatically or copy from the Prediction endpoint section of the Processor details tab in the Google Cloud Console.PROJECT = "PUT_SOMETHING_HERE"GCS_OUTPUT_PATH = "PUT_SOMETHING_HERE"PROCESSOR_NAME = "PUT_SOMETHING_HERE"Now, let's create a parser:parser = DocAIParser(location="us", processor_name=PROCESSOR_NAME, gcs_output_path=GCS_OUTPUT_PATH)Let's go and parse an Alphabet's take from here: https://abc.xyz/assets/a7/5b/9e5ae0364b12b4c883f3cf748226/goog-exhibit-99-1-q1-2023-19.pdf. Copy it to your GCS bucket first, and adjust the path below.blob = Blob(path="gs://vertex-pgt/examples/goog-exhibit-99-1-q1-2023-19.pdf")docs = list(parser.lazy_parse(blob))We'll get one document per page, 11 in total:print(len(docs))    11You can run end-to-end parsing of a blob one-by-one. If you have many documents, it might be a better approach to batch them together and maybe even detach parsing from handling the results of parsing.operations = parser.docai_parse([blob])print([op.operation.name for op in operations])    ['projects/543079149601/locations/us/operations/16447136779727347991']You can check whether operations are finished:parser.is_running(operations)    TrueAnd when they're finished, you can parse the results:parser.is_running(operations)    Falseresults = parser.get_results(operations)print(results[0])    DocAIParsingResults(source_path='gs://vertex-pgt/examples/goog-exhibit-99-1-q1-2023-19.pdf', parsed_path='gs://vertex-pgt/test/run1/16447136779727347991/0')And now we can finally generate Documents from parsed results:docs = list(parser.parse_from_results(results))print(len(docs))    11PreviousBeautiful SoupNextDoctran Extract Properties IntegrationsCallbacksChat modelsChat loadersDocument loadersDocument transformersBeautiful SoupdocaiDoctran Extract PropertiesDoctran Interrogate DocumentsDoctran Translate Documentshtml2textNuclia Understanding API document transformerOpenAI Functions Metadata TaggerLLMsMemoryRetrieversText embedding modelsAgents & ToolkitsToolsVector storesGrouped by provider IntegrationsCallbacksChat modelsChat loadersDocument loadersDocument transformersBeautiful SoupdocaiDoctran Extract PropertiesDoctran Interrogate DocumentsDoctran Translate Documentshtml2textNuclia Understanding API document transformerOpenAI Functions Metadata TaggerLLMsMemoryRetrieversText embedding modelsAgents & ToolkitsToolsVector storesGrouped by provider Integrations Callbacks Chat models Chat loaders Document loaders Document transformers LLMs Memory Retrievers Text embedding models Agents & Toolkits Tools Vector stores Grouped by provider IntegrationsDocument transformersdocaidocaifrom langchain.document_loaders.blob_loaders import Blobfrom langchain.document_loaders.parsers import DocAIParserAPI Reference:BlobDocAIParserDocAI is a Google Cloud platform to transform unstructured data from documents into structured data, making it easier to understand, analyze, and consume. You can read more about it: https://cloud.google.com/document-ai/docs/overview First, you need to set up a GCS bucket and create your own OCR processor as described here: https://cloud.google.com/document-ai/docs/create-processor The GCS_OUTPUT_PATH should be a path to a folder on GCS (starting with gs://) and a processor name should look like projects/PROJECT_NUMBER/locations/LOCATION/processors/PROCESSOR_ID. You can get it either programmatically or copy from the Prediction endpoint section of the Processor details tab in the Google Cloud Console.PROJECT = "PUT_SOMETHING_HERE"GCS_OUTPUT_PATH = "PUT_SOMETHING_HERE"PROCESSOR_NAME = "PUT_SOMETHING_HERE"Now, let's create a parser:parser = DocAIParser(location="us", processor_name=PROCESSOR_NAME, gcs_output_path=GCS_OUTPUT_PATH)Let's go and parse an Alphabet's take from here: https://abc.xyz/assets/a7/5b/9e5ae0364b12b4c883f3cf748226/goog-exhibit-99-1-q1-2023-19.pdf. Copy it to your GCS bucket first, and adjust the path below.blob = Blob(path="gs://vertex-pgt/examples/goog-exhibit-99-1-q1-2023-19.pdf")docs = list(parser.lazy_parse(blob))We'll get one document per page, 11 in total:print(len(docs))    11You can run end-to-end parsing of a blob one-by-one. If you have many documents, it might be a better approach to batch them together and maybe even detach parsing from handling the results of parsing.operations = parser.docai_parse([blob])print([op.operation.name for op in operations])    ['projects/543079149601/locations/us/operations/16447136779727347991']You can check whether operations are finished:parser.is_running(operations)    TrueAnd when they're finished, you can parse the results:parser.is_running(operations)    Falseresults = parser.get_results(operations)print(results[0])    DocAIParsingResults(source_path='gs://vertex-pgt/examples/goog-exhibit-99-1-q1-2023-19.pdf', parsed_path='gs://vertex-pgt/test/run1/16447136779727347991/0')And now we can finally generate Documents from parsed results:docs = list(parser.parse_from_results(results))print(len(docs))    11PreviousBeautiful SoupNextDoctran Extract Properties IntegrationsDocument transformersdocaidocaifrom langchain.document_loaders.blob_loaders import Blobfrom langchain.document_loaders.parsers import DocAIParserAPI Reference:BlobDocAIParserDocAI is a Google Cloud platform to transform unstructured data from documents into structured data, making it easier to understand, analyze, and consume. You can read more about it: https://cloud.google.com/document-ai/docs/overview First, you need to set up a GCS bucket and create your own OCR processor as described here: https://cloud.google.com/document-ai/docs/create-processor The GCS_OUTPUT_PATH should be a path to a folder on GCS (starting with gs://) and a processor name should look like projects/PROJECT_NUMBER/locations/LOCATION/processors/PROCESSOR_ID. You can get it either programmatically or copy from the Prediction endpoint section of the Processor details tab in the Google Cloud Console.PROJECT = "PUT_SOMETHING_HERE"GCS_OUTPUT_PATH = "PUT_SOMETHING_HERE"PROCESSOR_NAME = "PUT_SOMETHING_HERE"Now, let's create a parser:parser = DocAIParser(location="us", processor_name=PROCESSOR_NAME, gcs_output_path=GCS_OUTPUT_PATH)Let's go and parse an Alphabet's take from here: https://abc.xyz/assets/a7/5b/9e5ae0364b12b4c883f3cf748226/goog-exhibit-99-1-q1-2023-19.pdf. Copy it to your GCS bucket first, and adjust the path below.blob = Blob(path="gs://vertex-pgt/examples/goog-exhibit-99-1-q1-2023-19.pdf")docs = list(parser.lazy_parse(blob))We'll get one document per page, 11 in total:print(len(docs))    11You can run end-to-end parsing of a blob one-by-one. If you have many documents, it might be a better approach to batch them together and maybe even detach parsing from handling the results of parsing.operations = parser.docai_parse([blob])print([op.operation.name for op in operations])    ['projects/543079149601/locations/us/operations/16447136779727347991']You can check whether operations are finished:parser.is_running(operations)    TrueAnd when they're finished, you can parse the results:parser.is_running(operations)    Falseresults = parser.get_results(operations)print(results[0])    DocAIParsingResults(source_path='gs://vertex-pgt/examples/goog-exhibit-99-1-q1-2023-19.pdf', parsed_path='gs://vertex-pgt/test/run1/16447136779727347991/0')And now we can finally generate Documents from parsed results:docs = list(parser.parse_from_results(results))print(len(docs))    11PreviousBeautiful SoupNextDoctran Extract Properties docaifrom langchain.document_loaders.blob_loaders import Blobfrom langchain.document_loaders.parsers import DocAIParserAPI Reference:BlobDocAIParserDocAI is a Google Cloud platform to transform unstructured data from documents into structured data, making it easier to understand, analyze, and consume. You can read more about it: https://cloud.google.com/document-ai/docs/overview First, you need to set up a GCS bucket and create your own OCR processor as described here: https://cloud.google.com/document-ai/docs/create-processor The GCS_OUTPUT_PATH should be a path to a folder on GCS (starting with gs://) and a processor name should look like projects/PROJECT_NUMBER/locations/LOCATION/processors/PROCESSOR_ID. You can get it either programmatically or copy from the Prediction endpoint section of the Processor details tab in the Google Cloud Console.PROJECT = "PUT_SOMETHING_HERE"GCS_OUTPUT_PATH = "PUT_SOMETHING_HERE"PROCESSOR_NAME = "PUT_SOMETHING_HERE"Now, let's create a parser:parser = DocAIParser(location="us", processor_name=PROCESSOR_NAME, gcs_output_path=GCS_OUTPUT_PATH)Let's go and parse an Alphabet's take from here: https://abc.xyz/assets/a7/5b/9e5ae0364b12b4c883f3cf748226/goog-exhibit-99-1-q1-2023-19.pdf. Copy it to your GCS bucket first, and adjust the path below.blob = Blob(path="gs://vertex-pgt/examples/goog-exhibit-99-1-q1-2023-19.pdf")docs = list(parser.lazy_parse(blob))We'll get one document per page, 11 in total:print(len(docs))    11You can run end-to-end parsing of a blob one-by-one. If you have many documents, it might be a better approach to batch them together and maybe even detach parsing from handling the results of parsing.operations = parser.docai_parse([blob])print([op.operation.name for op in operations])    ['projects/543079149601/locations/us/operations/16447136779727347991']You can check whether operations are finished:parser.is_running(operations)    TrueAnd when they're finished, you can parse the results:parser.is_running(operations)    Falseresults = parser.get_results(operations)print(results[0])    DocAIParsingResults(source_path='gs://vertex-pgt/examples/goog-exhibit-99-1-q1-2023-19.pdf', parsed_path='gs://vertex-pgt/test/run1/16447136779727347991/0')And now we can finally generate Documents from parsed results:docs = list(parser.parse_from_results(results))print(len(docs))    11 from langchain.document_loaders.blob_loaders import Blobfrom langchain.document_loaders.parsers import DocAIParser from langchain.document_loaders.blob_loaders import Blobfrom langchain.document_loaders.parsers import DocAIParser  API Reference:BlobDocAIParser PROJECT = "PUT_SOMETHING_HERE"GCS_OUTPUT_PATH = "PUT_SOMETHING_HERE"PROCESSOR_NAME = "PUT_SOMETHING_HERE" PROJECT = "PUT_SOMETHING_HERE"GCS_OUTPUT_PATH = "PUT_SOMETHING_HERE"PROCESSOR_NAME = "PUT_SOMETHING_HERE"  parser = DocAIParser(location="us", processor_name=PROCESSOR_NAME, gcs_output_path=GCS_OUTPUT_PATH) parser = DocAIParser(location="us", processor_name=PROCESSOR_NAME, gcs_output_path=GCS_OUTPUT_PATH)  blob = Blob(path="gs://vertex-pgt/examples/goog-exhibit-99-1-q1-2023-19.pdf") blob = Blob(path="gs://vertex-pgt/examples/goog-exhibit-99-1-q1-2023-19.pdf")  docs = list(parser.lazy_parse(blob)) docs = list(parser.lazy_parse(blob))  print(len(docs)) print(len(docs))      11     11     11  operations = parser.docai_parse([blob])print([op.operation.name for op in operations]) operations = parser.docai_parse([blob])print([op.operation.name for op in operations])      ['projects/543079149601/locations/us/operations/16447136779727347991']     ['projects/543079149601/locations/us/operations/16447136779727347991']     ['projects/543079149601/locations/us/operations/16447136779727347991']  parser.is_running(operations) parser.is_running(operations)      True     True     True  parser.is_running(operations) parser.is_running(operations)      False     False     False  results = parser.get_results(operations)print(results[0]) results = parser.get_results(operations)print(results[0])      DocAIParsingResults(source_path='gs://vertex-pgt/examples/goog-exhibit-99-1-q1-2023-19.pdf', parsed_path='gs://vertex-pgt/test/run1/16447136779727347991/0')     DocAIParsingResults(source_path='gs://vertex-pgt/examples/goog-exhibit-99-1-q1-2023-19.pdf', parsed_path='gs://vertex-pgt/test/run1/16447136779727347991/0')     DocAIParsingResults(source_path='gs://vertex-pgt/examples/goog-exhibit-99-1-q1-2023-19.pdf', parsed_path='gs://vertex-pgt/test/run1/16447136779727347991/0')  docs = list(parser.parse_from_results(results)) docs = list(parser.parse_from_results(results))  print(len(docs)) print(len(docs))      11     11     11  Previous Beautiful Soup Next Doctran Extract Properties CommunityDiscordTwitterGitHubPythonJS/TSMoreHomepageBlogCopyright ¬© 2023 LangChain, Inc. CommunityDiscordTwitterGitHubPythonJS/TSMoreHomepageBlog CommunityDiscordTwitter Community GitHubPythonJS/TS GitHub MoreHomepageBlog More Copyright ¬© 2023 LangChain, Inc. Copyright ¬© 2023 LangChain, Inc. Skip to main content (#docusaurus_skipToContent_fallback) ü¶úÔ∏èüîó LangChain (/) Docs (/docs/get_started/introduction) Use cases (/docs/use_cases) Integrations (/docs/integrations) API (https://api.python.langchain.com) LangSmith (https://smith.langchain.com) JS/TS Docs (https://js.langchain.com/docs)  (https://github.com/hwchase17/langchain) Integrations (/docs/integrations) Callbacks (/docs/integrations/callbacks/) Chat models (/docs/integrations/chat/) Chat loaders (/docs/integrations/chat_loaders/) Document loaders (/docs/integrations/document_loaders/) Document transformers (/docs/integrations/document_transformers/) Beautiful Soup (/docs/integrations/document_transformers/beautiful_soup) docai (/docs/integrations/document_transformers/docai) Doctran Extract Properties (/docs/integrations/document_transformers/doctran_extract_properties) Doctran Interrogate Documents (/docs/integrations/document_transformers/doctran_interrogate_document) Doctran Translate Documents (/docs/integrations/document_transformers/doctran_translate_document) html2text (/docs/integrations/document_transformers/html2text) Nuclia Understanding API document transformer (/docs/integrations/document_transformers/nuclia_transformer) OpenAI Functions Metadata Tagger (/docs/integrations/document_transformers/openai_metadata_tagger) LLMs (/docs/integrations/llms/) Memory (/docs/integrations/memory/) Retrievers (/docs/integrations/retrievers/) Text embedding models (/docs/integrations/text_embedding/) Agents & Toolkits (/docs/integrations/toolkits/) Tools (/docs/integrations/tools/) Vector stores (/docs/integrations/vectorstores/) Grouped by provider (/docs/integrations/providers/)  (/) Integrations (/docs/integrations) Document transformers (/docs/integrations/document_transformers/) Blob (https://api.python.langchain.com/en/latest/document_loaders/langchain.document_loaders.blob_loaders.schema.Blob.html) DocAIParser (https://api.python.langchain.com/en/latest/document_loaders/langchain.document_loaders.parsers.docai.DocAIParser.html) https://cloud.google.com/document-ai/docs/overview (https://cloud.google.com/document-ai/docs/overview) https://cloud.google.com/document-ai/docs/create-processor (https://cloud.google.com/document-ai/docs/create-processor) https://abc.xyz/assets/a7/5b/9e5ae0364b12b4c883f3cf748226/goog-exhibit-99-1-q1-2023-19.pdf (https://abc.xyz/assets/a7/5b/9e5ae0364b12b4c883f3cf748226/goog-exhibit-99-1-q1-2023-19.pdf) PreviousBeautiful Soup (/docs/integrations/document_transformers/beautiful_soup) NextDoctran Extract Properties (/docs/integrations/document_transformers/doctran_extract_properties) Discord (https://discord.gg/cU2adEyC7w) Twitter (https://twitter.com/LangChainAI) Python (https://github.com/hwchase17/langchain) JS/TS (https://github.com/hwchase17/langchainjs) Homepage (https://langchain.com) Blog (https://blog.langchain.dev)