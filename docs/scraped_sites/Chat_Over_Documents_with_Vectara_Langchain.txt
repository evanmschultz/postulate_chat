This notebook is based on the chat_vector_db notebook, but using Vectara as the vector database. Load in documents. You can replace this with a loader for whatever type of data you want We now split the documents, create embeddings for them, and put them in a vectorstore. This allows us to do semantic search over them. We can now create a memory object, which is neccessary to track the inputs/outputs and hold a conversation. We now initialize the ConversationalRetrievalChain In the above example, we used a Memory object to track chat history. We can also just pass it in explicitly. In order to do this, we need to initialize a chain without any memory object. Here's an example of asking a question with no chat history Here's an example of asking a question with some chat history You can also easily return source documents from the ConversationalRetrievalChain. This is useful for when you want to inspect what documents were returned. If you are using a vector store that supports filtering by search distance, you can add a threshold value parameter. We can also use different types of combine document chains with the ConversationalRetrievalChain chain. You can also use this chain with the question answering with sources chain. Output from the chain will be streamed to stdout token by token in this example. You can also specify a get_chat_history function, which can be used to format the chat_history string. IntegrationsCallbacksChat modelsChat loadersDocument loadersDocument transformersLLMsMemoryRetrieversText embedding modelsAgents & ToolkitsToolsVector storesGrouped by providerActiveloop Deep LakeAI21 LabsAimAINetworkAirbyteAirtableAleph AlphaAlibaba Cloud OpensearchAmazon API GatewayAnalyticDBAnnoyAnyscaleApifyArangoDBArgillaArthurArxivAtlasAwaDBAWS S3 DirectoryAZLyricsAzure Blob StorageAzure Cognitive SearchAzure OpenAIBagelDBBananaBasetenBeamBedrockBiliBiliNIBittensorBlackboardBrave SearchCassandraCerebriumAIChaindeskChromaClarifaiClearMLClickHouseCnosDBCohereCollege ConfidentialCometConfident AIConfluenceC TransformersDashVectorDatabricksDatadog TracingDatadog LogsDataForSEODeepInfraDeepSparseDiffbotDingoDiscordDocArrayDocugamiDuckDBElasticsearchEpsillaEverNoteFacebook ChatFacebook FaissFigmaFireworksFlyteForefrontAIGitGitBookGoldenGoogle BigQueryGoogle Cloud StorageGoogle DriveGoogle SearchGoogle SerperGoogle Vertex AI MatchingEngineGooseAIGPT4AllGraphsignalGrobidGutenbergHacker NewsHazy ResearchHeliconeHologresHugging FaceiFixitIMSDbInfinoJinaKonkoLanceDBLangChain Decorators âœ¨Llama.cppLog10MarqoMediaWikiDumpMeilisearchMetalMicrosoft OneDriveMicrosoft PowerPointMicrosoft WordMilvusMinimaxMLflow AI GatewayMLflowModalModelScopeModern TreasuryMomentoMongoDB AtlasMotherduckMyScaleNeo4jNLPCloudNotion DBObsidianOpenAIOpenLLMOpenSearchOpenWeatherMapPetalsPostgres EmbeddingPGVectorPineconePipelineAIPortkeyPredibasePrediction GuardPromptLayerPsychicPubMedQdrantRay ServeRebuffRedditRedisReplicateRoamRocksetRunhouseRWKV-4SageMaker EndpointSageMaker TrackingScaNNSearxNG Search APISerpAPIShale ProtocolSingleStoreDBscikit-learnSlackspaCySpreedlyStarRocksStochasticAIStripeSupabase (Postgres)NebulaTairTelegramTencentVectorDBTensorFlow DatasetsTigris2MarkdownTrelloTruLensTwitterTypesenseUnstructuredUSearchVearchVectaraChat Over Documents with VectaraVectara Text GenerationVespaWandB TracingWeights & BiasesWeatherWeaviateWhatsAppWhyLabsWikipediaWolfram AlphaWriterXataXorbits Inference (Xinference)Yeager.aiYouTubeZepZilliz Callbacks Chat models Chat loaders Document loaders Document transformers LLMs Memory Retrievers Text embedding models Agents & Toolkits Tools Vector stores Grouped by providerActiveloop Deep LakeAI21 LabsAimAINetworkAirbyteAirtableAleph AlphaAlibaba Cloud OpensearchAmazon API GatewayAnalyticDBAnnoyAnyscaleApifyArangoDBArgillaArthurArxivAtlasAwaDBAWS S3 DirectoryAZLyricsAzure Blob StorageAzure Cognitive SearchAzure OpenAIBagelDBBananaBasetenBeamBedrockBiliBiliNIBittensorBlackboardBrave SearchCassandraCerebriumAIChaindeskChromaClarifaiClearMLClickHouseCnosDBCohereCollege ConfidentialCometConfident AIConfluenceC TransformersDashVectorDatabricksDatadog TracingDatadog LogsDataForSEODeepInfraDeepSparseDiffbotDingoDiscordDocArrayDocugamiDuckDBElasticsearchEpsillaEverNoteFacebook ChatFacebook FaissFigmaFireworksFlyteForefrontAIGitGitBookGoldenGoogle BigQueryGoogle Cloud StorageGoogle DriveGoogle SearchGoogle SerperGoogle Vertex AI MatchingEngineGooseAIGPT4AllGraphsignalGrobidGutenbergHacker NewsHazy ResearchHeliconeHologresHugging FaceiFixitIMSDbInfinoJinaKonkoLanceDBLangChain Decorators âœ¨Llama.cppLog10MarqoMediaWikiDumpMeilisearchMetalMicrosoft OneDriveMicrosoft PowerPointMicrosoft WordMilvusMinimaxMLflow AI GatewayMLflowModalModelScopeModern TreasuryMomentoMongoDB AtlasMotherduckMyScaleNeo4jNLPCloudNotion DBObsidianOpenAIOpenLLMOpenSearchOpenWeatherMapPetalsPostgres EmbeddingPGVectorPineconePipelineAIPortkeyPredibasePrediction GuardPromptLayerPsychicPubMedQdrantRay ServeRebuffRedditRedisReplicateRoamRocksetRunhouseRWKV-4SageMaker EndpointSageMaker TrackingScaNNSearxNG Search APISerpAPIShale ProtocolSingleStoreDBscikit-learnSlackspaCySpreedlyStarRocksStochasticAIStripeSupabase (Postgres)NebulaTairTelegramTencentVectorDBTensorFlow DatasetsTigris2MarkdownTrelloTruLensTwitterTypesenseUnstructuredUSearchVearchVectaraChat Over Documents with VectaraVectara Text GenerationVespaWandB TracingWeights & BiasesWeatherWeaviateWhatsAppWhyLabsWikipediaWolfram AlphaWriterXataXorbits Inference (Xinference)Yeager.aiYouTubeZepZilliz Activeloop Deep Lake AI21 Labs Aim AINetwork Airbyte Airtable Aleph Alpha Alibaba Cloud Opensearch Amazon API Gateway AnalyticDB Annoy Anyscale Apify ArangoDB Argilla Arthur Arxiv Atlas AwaDB AWS S3 Directory AZLyrics Azure Blob Storage Azure Cognitive Search Azure OpenAI BagelDB Banana Baseten Beam Bedrock BiliBili NIBittensor Blackboard Brave Search Cassandra CerebriumAI Chaindesk Chroma Clarifai ClearML ClickHouse CnosDB Cohere College Confidential Comet Confident AI Confluence C Transformers DashVector Databricks Datadog Tracing Datadog Logs DataForSEO DeepInfra DeepSparse Diffbot Dingo Discord DocArray Docugami DuckDB Elasticsearch Epsilla EverNote Facebook Chat Facebook Faiss Figma Fireworks Flyte ForefrontAI Git GitBook Golden Google BigQuery Google Cloud Storage Google Drive Google Search Google Serper Google Vertex AI MatchingEngine GooseAI GPT4All Graphsignal Grobid Gutenberg Hacker News Hazy Research Helicone Hologres Hugging Face iFixit IMSDb Infino Jina Konko LanceDB LangChain Decorators âœ¨ Llama.cpp Log10 Marqo MediaWikiDump Meilisearch Metal Microsoft OneDrive Microsoft PowerPoint Microsoft Word Milvus Minimax MLflow AI Gateway MLflow Modal ModelScope Modern Treasury Momento MongoDB Atlas Motherduck MyScale Neo4j NLPCloud Notion DB Obsidian OpenAI OpenLLM OpenSearch OpenWeatherMap Petals Postgres Embedding PGVector Pinecone PipelineAI Portkey Predibase Prediction Guard PromptLayer Psychic PubMed Qdrant Ray Serve Rebuff Reddit Redis Replicate Roam Rockset Runhouse RWKV-4 SageMaker Endpoint SageMaker Tracking ScaNN SearxNG Search API SerpAPI Shale Protocol SingleStoreDB scikit-learn Slack spaCy Spreedly StarRocks StochasticAI Stripe Supabase (Postgres) Nebula Tair Telegram TencentVectorDB TensorFlow Datasets Tigris 2Markdown Trello TruLens Twitter Typesense Unstructured USearch Vearch VectaraChat Over Documents with VectaraVectara Text Generation Chat Over Documents with Vectara Vectara Text Generation Vespa WandB Tracing Weights & Biases Weather Weaviate WhatsApp WhyLabs Wikipedia Wolfram Alpha Writer Xata Xorbits Inference (Xinference) Yeager.ai YouTube Zep Zilliz  Integrations Grouped by provider Vectara Chat Over Documents with Vectara Vectara VectaraRetriever OpenAI ConversationalRetrievalChain TextLoader ConversationBufferMemory LLMChain load_qa_chain CONDENSE_QUESTION_PROMPT load_qa_with_sources_chain LLMChain StreamingStdOutCallbackHandler CONDENSE_QUESTION_PROMPT QA_PROMPT load_qa_chain Pass in chat history Return Source Documents ConversationalRetrievalChain with search_distance ConversationalRetrievalChain with map_reduce ConversationalRetrievalChain with Question Answering with sources ConversationalRetrievalChain with streaming to stdout get_chat_history Function Discord Twitter Python JS/TS Homepage Blog Skip to main contentðŸ¦œï¸ðŸ”— LangChainDocsUse casesIntegrationsAPILangSmithJS/TS DocsCTRLKIntegrationsCallbacksChat modelsChat loadersDocument loadersDocument transformersLLMsMemoryRetrieversText embedding modelsAgents & ToolkitsToolsVector storesGrouped by providerActiveloop Deep LakeAI21 LabsAimAINetworkAirbyteAirtableAleph AlphaAlibaba Cloud OpensearchAmazon API GatewayAnalyticDBAnnoyAnyscaleApifyArangoDBArgillaArthurArxivAtlasAwaDBAWS S3 DirectoryAZLyricsAzure Blob StorageAzure Cognitive SearchAzure OpenAIBagelDBBananaBasetenBeamBedrockBiliBiliNIBittensorBlackboardBrave SearchCassandraCerebriumAIChaindeskChromaClarifaiClearMLClickHouseCnosDBCohereCollege ConfidentialCometConfident AIConfluenceC TransformersDashVectorDatabricksDatadog TracingDatadog LogsDataForSEODeepInfraDeepSparseDiffbotDingoDiscordDocArrayDocugamiDuckDBElasticsearchEpsillaEverNoteFacebook ChatFacebook FaissFigmaFireworksFlyteForefrontAIGitGitBookGoldenGoogle BigQueryGoogle Cloud StorageGoogle DriveGoogle SearchGoogle SerperGoogle Vertex AI MatchingEngineGooseAIGPT4AllGraphsignalGrobidGutenbergHacker NewsHazy ResearchHeliconeHologresHugging FaceiFixitIMSDbInfinoJinaKonkoLanceDBLangChain Decorators âœ¨Llama.cppLog10MarqoMediaWikiDumpMeilisearchMetalMicrosoft OneDriveMicrosoft PowerPointMicrosoft WordMilvusMinimaxMLflow AI GatewayMLflowModalModelScopeModern TreasuryMomentoMongoDB AtlasMotherduckMyScaleNeo4jNLPCloudNotion DBObsidianOpenAIOpenLLMOpenSearchOpenWeatherMapPetalsPostgres EmbeddingPGVectorPineconePipelineAIPortkeyPredibasePrediction GuardPromptLayerPsychicPubMedQdrantRay ServeRebuffRedditRedisReplicateRoamRocksetRunhouseRWKV-4SageMaker EndpointSageMaker TrackingScaNNSearxNG Search APISerpAPIShale ProtocolSingleStoreDBscikit-learnSlackspaCySpreedlyStarRocksStochasticAIStripeSupabase (Postgres)NebulaTairTelegramTencentVectorDBTensorFlow DatasetsTigris2MarkdownTrelloTruLensTwitterTypesenseUnstructuredUSearchVearchVectaraChat Over Documents with VectaraVectara Text GenerationVespaWandB TracingWeights & BiasesWeatherWeaviateWhatsAppWhyLabsWikipediaWolfram AlphaWriterXataXorbits Inference (Xinference)Yeager.aiYouTubeZepZillizIntegrationsGrouped by providerVectaraChat Over Documents with VectaraOn this pageChat Over Documents with VectaraThis notebook is based on the chat_vector_db notebook, but using Vectara as the vector database.import osfrom langchain.vectorstores import Vectarafrom langchain.vectorstores.vectara import VectaraRetrieverfrom langchain.llms import OpenAIfrom langchain.chains import ConversationalRetrievalChainAPI Reference:VectaraVectaraRetrieverOpenAIConversationalRetrievalChainLoad in documents. You can replace this with a loader for whatever type of data you wantfrom langchain.document_loaders import TextLoaderloader = TextLoader("../../../modules/state_of_the_union.txt")documents = loader.load()API Reference:TextLoaderWe now split the documents, create embeddings for them, and put them in a vectorstore. This allows us to do semantic search over them.vectorstore = Vectara.from_documents(documents, embedding=None)We can now create a memory object, which is neccessary to track the inputs/outputs and hold a conversation.from langchain.memory import ConversationBufferMemorymemory = ConversationBufferMemory(memory_key="chat_history", return_messages=True)API Reference:ConversationBufferMemoryWe now initialize the ConversationalRetrievalChainopenai_api_key = os.environ["OPENAI_API_KEY"]llm = OpenAI(openai_api_key=openai_api_key, temperature=0)retriever = vectorstore.as_retriever(lambda_val=0.025, k=5, filter=None)d = retriever.get_relevant_documents(    "What did the president say about Ketanji Brown Jackson")qa = ConversationalRetrievalChain.from_llm(llm, retriever, memory=memory)query = "What did the president say about Ketanji Brown Jackson"result = qa({"question": query})result["answer"]    " The president said that Ketanji Brown Jackson is one of the nation's top legal minds and a former top litigator in private practice, and that she will continue Justice Breyer's legacy of excellence."query = "Did he mention who she suceeded"result = qa({"question": query})result["answer"]    ' Ketanji Brown Jackson succeeded Justice Breyer.'Pass in chat historyâ€‹In the above example, we used a Memory object to track chat history. We can also just pass it in explicitly. In order to do this, we need to initialize a chain without any memory object.qa = ConversationalRetrievalChain.from_llm(    OpenAI(temperature=0), vectorstore.as_retriever())Here's an example of asking a question with no chat historychat_history = []query = "What did the president say about Ketanji Brown Jackson"result = qa({"question": query, "chat_history": chat_history})result["answer"]    " The president said that Ketanji Brown Jackson is one of the nation's top legal minds and a former top litigator in private practice, and that she will continue Justice Breyer's legacy of excellence."Here's an example of asking a question with some chat historychat_history = [(query, result["answer"])]query = "Did he mention who she suceeded"result = qa({"question": query, "chat_history": chat_history})result["answer"]    ' Ketanji Brown Jackson succeeded Justice Breyer.'Return Source Documentsâ€‹You can also easily return source documents from the ConversationalRetrievalChain. This is useful for when you want to inspect what documents were returned.qa = ConversationalRetrievalChain.from_llm(    llm, vectorstore.as_retriever(), return_source_documents=True)chat_history = []query = "What did the president say about Ketanji Brown Jackson"result = qa({"question": query, "chat_history": chat_history})result["source_documents"][0]    Document(page_content='Justice Breyer, thank you for your service. One of the most serious constitutional responsibilities a President has is nominating someone to serve on the United States Supreme Court. And I did that 4 days ago, when I nominated Circuit Court of Appeals Judge Ketanji Brown Jackson. One of our nationâ€™s top legal minds, who will continue Justice Breyerâ€™s legacy of excellence. A former top litigator in private practice.', metadata={'source': '../../../modules/state_of_the_union.txt'})ConversationalRetrievalChain with search_distanceâ€‹If you are using a vector store that supports filtering by search distance, you can add a threshold value parameter.vectordbkwargs = {"search_distance": 0.9}qa = ConversationalRetrievalChain.from_llm(    OpenAI(temperature=0), vectorstore.as_retriever(), return_source_documents=True)chat_history = []query = "What did the president say about Ketanji Brown Jackson"result = qa(    {"question": query, "chat_history": chat_history, "vectordbkwargs": vectordbkwargs})print(result["answer"])     The president said that Ketanji Brown Jackson is one of the nation's top legal minds and a former top litigator in private practice, and that she will continue Justice Breyer's legacy of excellence.ConversationalRetrievalChain with map_reduceâ€‹We can also use different types of combine document chains with the ConversationalRetrievalChain chain.from langchain.chains import LLMChainfrom langchain.chains.question_answering import load_qa_chainfrom langchain.chains.conversational_retrieval.prompts import CONDENSE_QUESTION_PROMPTAPI Reference:LLMChainload_qa_chainCONDENSE_QUESTION_PROMPTquestion_generator = LLMChain(llm=llm, prompt=CONDENSE_QUESTION_PROMPT)doc_chain = load_qa_chain(llm, chain_type="map_reduce")chain = ConversationalRetrievalChain(    retriever=vectorstore.as_retriever(),    question_generator=question_generator,    combine_docs_chain=doc_chain,)chat_history = []query = "What did the president say about Ketanji Brown Jackson"result = chain({"question": query, "chat_history": chat_history})result["answer"]    " The president said that he nominated Circuit Court of Appeals Judge Ketanji Brown Jackson, who is one of the nation's top legal minds and a former top litigator in private practice."ConversationalRetrievalChain with Question Answering with sourcesâ€‹You can also use this chain with the question answering with sources chain.from langchain.chains.qa_with_sources import load_qa_with_sources_chainAPI Reference:load_qa_with_sources_chainquestion_generator = LLMChain(llm=llm, prompt=CONDENSE_QUESTION_PROMPT)doc_chain = load_qa_with_sources_chain(llm, chain_type="map_reduce")chain = ConversationalRetrievalChain(    retriever=vectorstore.as_retriever(),    question_generator=question_generator,    combine_docs_chain=doc_chain,)chat_history = []query = "What did the president say about Ketanji Brown Jackson"result = chain({"question": query, "chat_history": chat_history})result["answer"]    " The president said that Ketanji Brown Jackson is one of the nation's top legal minds and a former top litigator in private practice.\nSOURCES: ../../../modules/state_of_the_union.txt"ConversationalRetrievalChain with streaming to stdoutâ€‹Output from the chain will be streamed to stdout token by token in this example.from langchain.chains.llm import LLMChainfrom langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandlerfrom langchain.chains.conversational_retrieval.prompts import (    CONDENSE_QUESTION_PROMPT,    QA_PROMPT,)from langchain.chains.question_answering import load_qa_chain# Construct a ConversationalRetrievalChain with a streaming llm for combine docs# and a separate, non-streaming llm for question generationllm = OpenAI(temperature=0, openai_api_key=openai_api_key)streaming_llm = OpenAI(    streaming=True,    callbacks=[StreamingStdOutCallbackHandler()],    temperature=0,    openai_api_key=openai_api_key,)question_generator = LLMChain(llm=llm, prompt=CONDENSE_QUESTION_PROMPT)doc_chain = load_qa_chain(streaming_llm, chain_type="stuff", prompt=QA_PROMPT)qa = ConversationalRetrievalChain(    retriever=vectorstore.as_retriever(),    combine_docs_chain=doc_chain,    question_generator=question_generator,)API Reference:LLMChainStreamingStdOutCallbackHandlerCONDENSE_QUESTION_PROMPTQA_PROMPTload_qa_chainchat_history = []query = "What did the president say about Ketanji Brown Jackson"result = qa({"question": query, "chat_history": chat_history})     The president said that Ketanji Brown Jackson is one of the nation's top legal minds and a former top litigator in private practice, and that she will continue Justice Breyer's legacy of excellence.chat_history = [(query, result["answer"])]query = "Did he mention who she suceeded"result = qa({"question": query, "chat_history": chat_history})     Justice Breyerget_chat_history Functionâ€‹You can also specify a get_chat_history function, which can be used to format the chat_history string.def get_chat_history(inputs) -> str:    res = []    for human, ai in inputs:        res.append(f"Human:{human}\nAI:{ai}")    return "\n".join(res)qa = ConversationalRetrievalChain.from_llm(    llm, vectorstore.as_retriever(), get_chat_history=get_chat_history)chat_history = []query = "What did the president say about Ketanji Brown Jackson"result = qa({"question": query, "chat_history": chat_history})result["answer"]    " The president said that Ketanji Brown Jackson is one of the nation's top legal minds and a former top litigator in private practice, and that she will continue Justice Breyer's legacy of excellence."PreviousVectaraNextVectara Text GenerationPass in chat historyReturn Source DocumentsConversationalRetrievalChain with search_distanceConversationalRetrievalChain with map_reduceConversationalRetrievalChain with Question Answering with sourcesConversationalRetrievalChain with streaming to stdoutget_chat_history FunctionCommunityDiscordTwitterGitHubPythonJS/TSMoreHomepageBlogCopyright Â© 2023 LangChain, Inc. Skip to main content ðŸ¦œï¸ðŸ”— LangChainDocsUse casesIntegrationsAPILangSmithJS/TS DocsCTRLK ðŸ¦œï¸ðŸ”— LangChainDocsUse casesIntegrationsAPI LangSmithJS/TS DocsCTRLK  CTRLK CTRLK  CTRLK   IntegrationsCallbacksChat modelsChat loadersDocument loadersDocument transformersLLMsMemoryRetrieversText embedding modelsAgents & ToolkitsToolsVector storesGrouped by providerActiveloop Deep LakeAI21 LabsAimAINetworkAirbyteAirtableAleph AlphaAlibaba Cloud OpensearchAmazon API GatewayAnalyticDBAnnoyAnyscaleApifyArangoDBArgillaArthurArxivAtlasAwaDBAWS S3 DirectoryAZLyricsAzure Blob StorageAzure Cognitive SearchAzure OpenAIBagelDBBananaBasetenBeamBedrockBiliBiliNIBittensorBlackboardBrave SearchCassandraCerebriumAIChaindeskChromaClarifaiClearMLClickHouseCnosDBCohereCollege ConfidentialCometConfident AIConfluenceC TransformersDashVectorDatabricksDatadog TracingDatadog LogsDataForSEODeepInfraDeepSparseDiffbotDingoDiscordDocArrayDocugamiDuckDBElasticsearchEpsillaEverNoteFacebook ChatFacebook FaissFigmaFireworksFlyteForefrontAIGitGitBookGoldenGoogle BigQueryGoogle Cloud StorageGoogle DriveGoogle SearchGoogle SerperGoogle Vertex AI MatchingEngineGooseAIGPT4AllGraphsignalGrobidGutenbergHacker NewsHazy ResearchHeliconeHologresHugging FaceiFixitIMSDbInfinoJinaKonkoLanceDBLangChain Decorators âœ¨Llama.cppLog10MarqoMediaWikiDumpMeilisearchMetalMicrosoft OneDriveMicrosoft PowerPointMicrosoft WordMilvusMinimaxMLflow AI GatewayMLflowModalModelScopeModern TreasuryMomentoMongoDB AtlasMotherduckMyScaleNeo4jNLPCloudNotion DBObsidianOpenAIOpenLLMOpenSearchOpenWeatherMapPetalsPostgres EmbeddingPGVectorPineconePipelineAIPortkeyPredibasePrediction GuardPromptLayerPsychicPubMedQdrantRay ServeRebuffRedditRedisReplicateRoamRocksetRunhouseRWKV-4SageMaker EndpointSageMaker TrackingScaNNSearxNG Search APISerpAPIShale ProtocolSingleStoreDBscikit-learnSlackspaCySpreedlyStarRocksStochasticAIStripeSupabase (Postgres)NebulaTairTelegramTencentVectorDBTensorFlow DatasetsTigris2MarkdownTrelloTruLensTwitterTypesenseUnstructuredUSearchVearchVectaraChat Over Documents with VectaraVectara Text GenerationVespaWandB TracingWeights & BiasesWeatherWeaviateWhatsAppWhyLabsWikipediaWolfram AlphaWriterXataXorbits Inference (Xinference)Yeager.aiYouTubeZepZillizIntegrationsGrouped by providerVectaraChat Over Documents with VectaraOn this pageChat Over Documents with VectaraThis notebook is based on the chat_vector_db notebook, but using Vectara as the vector database.import osfrom langchain.vectorstores import Vectarafrom langchain.vectorstores.vectara import VectaraRetrieverfrom langchain.llms import OpenAIfrom langchain.chains import ConversationalRetrievalChainAPI Reference:VectaraVectaraRetrieverOpenAIConversationalRetrievalChainLoad in documents. You can replace this with a loader for whatever type of data you wantfrom langchain.document_loaders import TextLoaderloader = TextLoader("../../../modules/state_of_the_union.txt")documents = loader.load()API Reference:TextLoaderWe now split the documents, create embeddings for them, and put them in a vectorstore. This allows us to do semantic search over them.vectorstore = Vectara.from_documents(documents, embedding=None)We can now create a memory object, which is neccessary to track the inputs/outputs and hold a conversation.from langchain.memory import ConversationBufferMemorymemory = ConversationBufferMemory(memory_key="chat_history", return_messages=True)API Reference:ConversationBufferMemoryWe now initialize the ConversationalRetrievalChainopenai_api_key = os.environ["OPENAI_API_KEY"]llm = OpenAI(openai_api_key=openai_api_key, temperature=0)retriever = vectorstore.as_retriever(lambda_val=0.025, k=5, filter=None)d = retriever.get_relevant_documents(    "What did the president say about Ketanji Brown Jackson")qa = ConversationalRetrievalChain.from_llm(llm, retriever, memory=memory)query = "What did the president say about Ketanji Brown Jackson"result = qa({"question": query})result["answer"]    " The president said that Ketanji Brown Jackson is one of the nation's top legal minds and a former top litigator in private practice, and that she will continue Justice Breyer's legacy of excellence."query = "Did he mention who she suceeded"result = qa({"question": query})result["answer"]    ' Ketanji Brown Jackson succeeded Justice Breyer.'Pass in chat historyâ€‹In the above example, we used a Memory object to track chat history. We can also just pass it in explicitly. In order to do this, we need to initialize a chain without any memory object.qa = ConversationalRetrievalChain.from_llm(    OpenAI(temperature=0), vectorstore.as_retriever())Here's an example of asking a question with no chat historychat_history = []query = "What did the president say about Ketanji Brown Jackson"result = qa({"question": query, "chat_history": chat_history})result["answer"]    " The president said that Ketanji Brown Jackson is one of the nation's top legal minds and a former top litigator in private practice, and that she will continue Justice Breyer's legacy of excellence."Here's an example of asking a question with some chat historychat_history = [(query, result["answer"])]query = "Did he mention who she suceeded"result = qa({"question": query, "chat_history": chat_history})result["answer"]    ' Ketanji Brown Jackson succeeded Justice Breyer.'Return Source Documentsâ€‹You can also easily return source documents from the ConversationalRetrievalChain. This is useful for when you want to inspect what documents were returned.qa = ConversationalRetrievalChain.from_llm(    llm, vectorstore.as_retriever(), return_source_documents=True)chat_history = []query = "What did the president say about Ketanji Brown Jackson"result = qa({"question": query, "chat_history": chat_history})result["source_documents"][0]    Document(page_content='Justice Breyer, thank you for your service. One of the most serious constitutional responsibilities a President has is nominating someone to serve on the United States Supreme Court. And I did that 4 days ago, when I nominated Circuit Court of Appeals Judge Ketanji Brown Jackson. One of our nationâ€™s top legal minds, who will continue Justice Breyerâ€™s legacy of excellence. A former top litigator in private practice.', metadata={'source': '../../../modules/state_of_the_union.txt'})ConversationalRetrievalChain with search_distanceâ€‹If you are using a vector store that supports filtering by search distance, you can add a threshold value parameter.vectordbkwargs = {"search_distance": 0.9}qa = ConversationalRetrievalChain.from_llm(    OpenAI(temperature=0), vectorstore.as_retriever(), return_source_documents=True)chat_history = []query = "What did the president say about Ketanji Brown Jackson"result = qa(    {"question": query, "chat_history": chat_history, "vectordbkwargs": vectordbkwargs})print(result["answer"])     The president said that Ketanji Brown Jackson is one of the nation's top legal minds and a former top litigator in private practice, and that she will continue Justice Breyer's legacy of excellence.ConversationalRetrievalChain with map_reduceâ€‹We can also use different types of combine document chains with the ConversationalRetrievalChain chain.from langchain.chains import LLMChainfrom langchain.chains.question_answering import load_qa_chainfrom langchain.chains.conversational_retrieval.prompts import CONDENSE_QUESTION_PROMPTAPI Reference:LLMChainload_qa_chainCONDENSE_QUESTION_PROMPTquestion_generator = LLMChain(llm=llm, prompt=CONDENSE_QUESTION_PROMPT)doc_chain = load_qa_chain(llm, chain_type="map_reduce")chain = ConversationalRetrievalChain(    retriever=vectorstore.as_retriever(),    question_generator=question_generator,    combine_docs_chain=doc_chain,)chat_history = []query = "What did the president say about Ketanji Brown Jackson"result = chain({"question": query, "chat_history": chat_history})result["answer"]    " The president said that he nominated Circuit Court of Appeals Judge Ketanji Brown Jackson, who is one of the nation's top legal minds and a former top litigator in private practice."ConversationalRetrievalChain with Question Answering with sourcesâ€‹You can also use this chain with the question answering with sources chain.from langchain.chains.qa_with_sources import load_qa_with_sources_chainAPI Reference:load_qa_with_sources_chainquestion_generator = LLMChain(llm=llm, prompt=CONDENSE_QUESTION_PROMPT)doc_chain = load_qa_with_sources_chain(llm, chain_type="map_reduce")chain = ConversationalRetrievalChain(    retriever=vectorstore.as_retriever(),    question_generator=question_generator,    combine_docs_chain=doc_chain,)chat_history = []query = "What did the president say about Ketanji Brown Jackson"result = chain({"question": query, "chat_history": chat_history})result["answer"]    " The president said that Ketanji Brown Jackson is one of the nation's top legal minds and a former top litigator in private practice.\nSOURCES: ../../../modules/state_of_the_union.txt"ConversationalRetrievalChain with streaming to stdoutâ€‹Output from the chain will be streamed to stdout token by token in this example.from langchain.chains.llm import LLMChainfrom langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandlerfrom langchain.chains.conversational_retrieval.prompts import (    CONDENSE_QUESTION_PROMPT,    QA_PROMPT,)from langchain.chains.question_answering import load_qa_chain# Construct a ConversationalRetrievalChain with a streaming llm for combine docs# and a separate, non-streaming llm for question generationllm = OpenAI(temperature=0, openai_api_key=openai_api_key)streaming_llm = OpenAI(    streaming=True,    callbacks=[StreamingStdOutCallbackHandler()],    temperature=0,    openai_api_key=openai_api_key,)question_generator = LLMChain(llm=llm, prompt=CONDENSE_QUESTION_PROMPT)doc_chain = load_qa_chain(streaming_llm, chain_type="stuff", prompt=QA_PROMPT)qa = ConversationalRetrievalChain(    retriever=vectorstore.as_retriever(),    combine_docs_chain=doc_chain,    question_generator=question_generator,)API Reference:LLMChainStreamingStdOutCallbackHandlerCONDENSE_QUESTION_PROMPTQA_PROMPTload_qa_chainchat_history = []query = "What did the president say about Ketanji Brown Jackson"result = qa({"question": query, "chat_history": chat_history})     The president said that Ketanji Brown Jackson is one of the nation's top legal minds and a former top litigator in private practice, and that she will continue Justice Breyer's legacy of excellence.chat_history = [(query, result["answer"])]query = "Did he mention who she suceeded"result = qa({"question": query, "chat_history": chat_history})     Justice Breyerget_chat_history Functionâ€‹You can also specify a get_chat_history function, which can be used to format the chat_history string.def get_chat_history(inputs) -> str:    res = []    for human, ai in inputs:        res.append(f"Human:{human}\nAI:{ai}")    return "\n".join(res)qa = ConversationalRetrievalChain.from_llm(    llm, vectorstore.as_retriever(), get_chat_history=get_chat_history)chat_history = []query = "What did the president say about Ketanji Brown Jackson"result = qa({"question": query, "chat_history": chat_history})result["answer"]    " The president said that Ketanji Brown Jackson is one of the nation's top legal minds and a former top litigator in private practice, and that she will continue Justice Breyer's legacy of excellence."PreviousVectaraNextVectara Text GenerationPass in chat historyReturn Source DocumentsConversationalRetrievalChain with search_distanceConversationalRetrievalChain with map_reduceConversationalRetrievalChain with Question Answering with sourcesConversationalRetrievalChain with streaming to stdoutget_chat_history Function IntegrationsCallbacksChat modelsChat loadersDocument loadersDocument transformersLLMsMemoryRetrieversText embedding modelsAgents & ToolkitsToolsVector storesGrouped by providerActiveloop Deep LakeAI21 LabsAimAINetworkAirbyteAirtableAleph AlphaAlibaba Cloud OpensearchAmazon API GatewayAnalyticDBAnnoyAnyscaleApifyArangoDBArgillaArthurArxivAtlasAwaDBAWS S3 DirectoryAZLyricsAzure Blob StorageAzure Cognitive SearchAzure OpenAIBagelDBBananaBasetenBeamBedrockBiliBiliNIBittensorBlackboardBrave SearchCassandraCerebriumAIChaindeskChromaClarifaiClearMLClickHouseCnosDBCohereCollege ConfidentialCometConfident AIConfluenceC TransformersDashVectorDatabricksDatadog TracingDatadog LogsDataForSEODeepInfraDeepSparseDiffbotDingoDiscordDocArrayDocugamiDuckDBElasticsearchEpsillaEverNoteFacebook ChatFacebook FaissFigmaFireworksFlyteForefrontAIGitGitBookGoldenGoogle BigQueryGoogle Cloud StorageGoogle DriveGoogle SearchGoogle SerperGoogle Vertex AI MatchingEngineGooseAIGPT4AllGraphsignalGrobidGutenbergHacker NewsHazy ResearchHeliconeHologresHugging FaceiFixitIMSDbInfinoJinaKonkoLanceDBLangChain Decorators âœ¨Llama.cppLog10MarqoMediaWikiDumpMeilisearchMetalMicrosoft OneDriveMicrosoft PowerPointMicrosoft WordMilvusMinimaxMLflow AI GatewayMLflowModalModelScopeModern TreasuryMomentoMongoDB AtlasMotherduckMyScaleNeo4jNLPCloudNotion DBObsidianOpenAIOpenLLMOpenSearchOpenWeatherMapPetalsPostgres EmbeddingPGVectorPineconePipelineAIPortkeyPredibasePrediction GuardPromptLayerPsychicPubMedQdrantRay ServeRebuffRedditRedisReplicateRoamRocksetRunhouseRWKV-4SageMaker EndpointSageMaker TrackingScaNNSearxNG Search APISerpAPIShale ProtocolSingleStoreDBscikit-learnSlackspaCySpreedlyStarRocksStochasticAIStripeSupabase (Postgres)NebulaTairTelegramTencentVectorDBTensorFlow DatasetsTigris2MarkdownTrelloTruLensTwitterTypesenseUnstructuredUSearchVearchVectaraChat Over Documents with VectaraVectara Text GenerationVespaWandB TracingWeights & BiasesWeatherWeaviateWhatsAppWhyLabsWikipediaWolfram AlphaWriterXataXorbits Inference (Xinference)Yeager.aiYouTubeZepZillizIntegrationsGrouped by providerVectaraChat Over Documents with VectaraOn this pageChat Over Documents with VectaraThis notebook is based on the chat_vector_db notebook, but using Vectara as the vector database.import osfrom langchain.vectorstores import Vectarafrom langchain.vectorstores.vectara import VectaraRetrieverfrom langchain.llms import OpenAIfrom langchain.chains import ConversationalRetrievalChainAPI Reference:VectaraVectaraRetrieverOpenAIConversationalRetrievalChainLoad in documents. You can replace this with a loader for whatever type of data you wantfrom langchain.document_loaders import TextLoaderloader = TextLoader("../../../modules/state_of_the_union.txt")documents = loader.load()API Reference:TextLoaderWe now split the documents, create embeddings for them, and put them in a vectorstore. This allows us to do semantic search over them.vectorstore = Vectara.from_documents(documents, embedding=None)We can now create a memory object, which is neccessary to track the inputs/outputs and hold a conversation.from langchain.memory import ConversationBufferMemorymemory = ConversationBufferMemory(memory_key="chat_history", return_messages=True)API Reference:ConversationBufferMemoryWe now initialize the ConversationalRetrievalChainopenai_api_key = os.environ["OPENAI_API_KEY"]llm = OpenAI(openai_api_key=openai_api_key, temperature=0)retriever = vectorstore.as_retriever(lambda_val=0.025, k=5, filter=None)d = retriever.get_relevant_documents(    "What did the president say about Ketanji Brown Jackson")qa = ConversationalRetrievalChain.from_llm(llm, retriever, memory=memory)query = "What did the president say about Ketanji Brown Jackson"result = qa({"question": query})result["answer"]    " The president said that Ketanji Brown Jackson is one of the nation's top legal minds and a former top litigator in private practice, and that she will continue Justice Breyer's legacy of excellence."query = "Did he mention who she suceeded"result = qa({"question": query})result["answer"]    ' Ketanji Brown Jackson succeeded Justice Breyer.'Pass in chat historyâ€‹In the above example, we used a Memory object to track chat history. We can also just pass it in explicitly. In order to do this, we need to initialize a chain without any memory object.qa = ConversationalRetrievalChain.from_llm(    OpenAI(temperature=0), vectorstore.as_retriever())Here's an example of asking a question with no chat historychat_history = []query = "What did the president say about Ketanji Brown Jackson"result = qa({"question": query, "chat_history": chat_history})result["answer"]    " The president said that Ketanji Brown Jackson is one of the nation's top legal minds and a former top litigator in private practice, and that she will continue Justice Breyer's legacy of excellence."Here's an example of asking a question with some chat historychat_history = [(query, result["answer"])]query = "Did he mention who she suceeded"result = qa({"question": query, "chat_history": chat_history})result["answer"]    ' Ketanji Brown Jackson succeeded Justice Breyer.'Return Source Documentsâ€‹You can also easily return source documents from the ConversationalRetrievalChain. This is useful for when you want to inspect what documents were returned.qa = ConversationalRetrievalChain.from_llm(    llm, vectorstore.as_retriever(), return_source_documents=True)chat_history = []query = "What did the president say about Ketanji Brown Jackson"result = qa({"question": query, "chat_history": chat_history})result["source_documents"][0]    Document(page_content='Justice Breyer, thank you for your service. One of the most serious constitutional responsibilities a President has is nominating someone to serve on the United States Supreme Court. And I did that 4 days ago, when I nominated Circuit Court of Appeals Judge Ketanji Brown Jackson. One of our nationâ€™s top legal minds, who will continue Justice Breyerâ€™s legacy of excellence. A former top litigator in private practice.', metadata={'source': '../../../modules/state_of_the_union.txt'})ConversationalRetrievalChain with search_distanceâ€‹If you are using a vector store that supports filtering by search distance, you can add a threshold value parameter.vectordbkwargs = {"search_distance": 0.9}qa = ConversationalRetrievalChain.from_llm(    OpenAI(temperature=0), vectorstore.as_retriever(), return_source_documents=True)chat_history = []query = "What did the president say about Ketanji Brown Jackson"result = qa(    {"question": query, "chat_history": chat_history, "vectordbkwargs": vectordbkwargs})print(result["answer"])     The president said that Ketanji Brown Jackson is one of the nation's top legal minds and a former top litigator in private practice, and that she will continue Justice Breyer's legacy of excellence.ConversationalRetrievalChain with map_reduceâ€‹We can also use different types of combine document chains with the ConversationalRetrievalChain chain.from langchain.chains import LLMChainfrom langchain.chains.question_answering import load_qa_chainfrom langchain.chains.conversational_retrieval.prompts import CONDENSE_QUESTION_PROMPTAPI Reference:LLMChainload_qa_chainCONDENSE_QUESTION_PROMPTquestion_generator = LLMChain(llm=llm, prompt=CONDENSE_QUESTION_PROMPT)doc_chain = load_qa_chain(llm, chain_type="map_reduce")chain = ConversationalRetrievalChain(    retriever=vectorstore.as_retriever(),    question_generator=question_generator,    combine_docs_chain=doc_chain,)chat_history = []query = "What did the president say about Ketanji Brown Jackson"result = chain({"question": query, "chat_history": chat_history})result["answer"]    " The president said that he nominated Circuit Court of Appeals Judge Ketanji Brown Jackson, who is one of the nation's top legal minds and a former top litigator in private practice."ConversationalRetrievalChain with Question Answering with sourcesâ€‹You can also use this chain with the question answering with sources chain.from langchain.chains.qa_with_sources import load_qa_with_sources_chainAPI Reference:load_qa_with_sources_chainquestion_generator = LLMChain(llm=llm, prompt=CONDENSE_QUESTION_PROMPT)doc_chain = load_qa_with_sources_chain(llm, chain_type="map_reduce")chain = ConversationalRetrievalChain(    retriever=vectorstore.as_retriever(),    question_generator=question_generator,    combine_docs_chain=doc_chain,)chat_history = []query = "What did the president say about Ketanji Brown Jackson"result = chain({"question": query, "chat_history": chat_history})result["answer"]    " The president said that Ketanji Brown Jackson is one of the nation's top legal minds and a former top litigator in private practice.\nSOURCES: ../../../modules/state_of_the_union.txt"ConversationalRetrievalChain with streaming to stdoutâ€‹Output from the chain will be streamed to stdout token by token in this example.from langchain.chains.llm import LLMChainfrom langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandlerfrom langchain.chains.conversational_retrieval.prompts import (    CONDENSE_QUESTION_PROMPT,    QA_PROMPT,)from langchain.chains.question_answering import load_qa_chain# Construct a ConversationalRetrievalChain with a streaming llm for combine docs# and a separate, non-streaming llm for question generationllm = OpenAI(temperature=0, openai_api_key=openai_api_key)streaming_llm = OpenAI(    streaming=True,    callbacks=[StreamingStdOutCallbackHandler()],    temperature=0,    openai_api_key=openai_api_key,)question_generator = LLMChain(llm=llm, prompt=CONDENSE_QUESTION_PROMPT)doc_chain = load_qa_chain(streaming_llm, chain_type="stuff", prompt=QA_PROMPT)qa = ConversationalRetrievalChain(    retriever=vectorstore.as_retriever(),    combine_docs_chain=doc_chain,    question_generator=question_generator,)API Reference:LLMChainStreamingStdOutCallbackHandlerCONDENSE_QUESTION_PROMPTQA_PROMPTload_qa_chainchat_history = []query = "What did the president say about Ketanji Brown Jackson"result = qa({"question": query, "chat_history": chat_history})     The president said that Ketanji Brown Jackson is one of the nation's top legal minds and a former top litigator in private practice, and that she will continue Justice Breyer's legacy of excellence.chat_history = [(query, result["answer"])]query = "Did he mention who she suceeded"result = qa({"question": query, "chat_history": chat_history})     Justice Breyerget_chat_history Functionâ€‹You can also specify a get_chat_history function, which can be used to format the chat_history string.def get_chat_history(inputs) -> str:    res = []    for human, ai in inputs:        res.append(f"Human:{human}\nAI:{ai}")    return "\n".join(res)qa = ConversationalRetrievalChain.from_llm(    llm, vectorstore.as_retriever(), get_chat_history=get_chat_history)chat_history = []query = "What did the president say about Ketanji Brown Jackson"result = qa({"question": query, "chat_history": chat_history})result["answer"]    " The president said that Ketanji Brown Jackson is one of the nation's top legal minds and a former top litigator in private practice, and that she will continue Justice Breyer's legacy of excellence."PreviousVectaraNextVectara Text GenerationPass in chat historyReturn Source DocumentsConversationalRetrievalChain with search_distanceConversationalRetrievalChain with map_reduceConversationalRetrievalChain with Question Answering with sourcesConversationalRetrievalChain with streaming to stdoutget_chat_history Function IntegrationsCallbacksChat modelsChat loadersDocument loadersDocument transformersLLMsMemoryRetrieversText embedding modelsAgents & ToolkitsToolsVector storesGrouped by providerActiveloop Deep LakeAI21 LabsAimAINetworkAirbyteAirtableAleph AlphaAlibaba Cloud OpensearchAmazon API GatewayAnalyticDBAnnoyAnyscaleApifyArangoDBArgillaArthurArxivAtlasAwaDBAWS S3 DirectoryAZLyricsAzure Blob StorageAzure Cognitive SearchAzure OpenAIBagelDBBananaBasetenBeamBedrockBiliBiliNIBittensorBlackboardBrave SearchCassandraCerebriumAIChaindeskChromaClarifaiClearMLClickHouseCnosDBCohereCollege ConfidentialCometConfident AIConfluenceC TransformersDashVectorDatabricksDatadog TracingDatadog LogsDataForSEODeepInfraDeepSparseDiffbotDingoDiscordDocArrayDocugamiDuckDBElasticsearchEpsillaEverNoteFacebook ChatFacebook FaissFigmaFireworksFlyteForefrontAIGitGitBookGoldenGoogle BigQueryGoogle Cloud StorageGoogle DriveGoogle SearchGoogle SerperGoogle Vertex AI MatchingEngineGooseAIGPT4AllGraphsignalGrobidGutenbergHacker NewsHazy ResearchHeliconeHologresHugging FaceiFixitIMSDbInfinoJinaKonkoLanceDBLangChain Decorators âœ¨Llama.cppLog10MarqoMediaWikiDumpMeilisearchMetalMicrosoft OneDriveMicrosoft PowerPointMicrosoft WordMilvusMinimaxMLflow AI GatewayMLflowModalModelScopeModern TreasuryMomentoMongoDB AtlasMotherduckMyScaleNeo4jNLPCloudNotion DBObsidianOpenAIOpenLLMOpenSearchOpenWeatherMapPetalsPostgres EmbeddingPGVectorPineconePipelineAIPortkeyPredibasePrediction GuardPromptLayerPsychicPubMedQdrantRay ServeRebuffRedditRedisReplicateRoamRocksetRunhouseRWKV-4SageMaker EndpointSageMaker TrackingScaNNSearxNG Search APISerpAPIShale ProtocolSingleStoreDBscikit-learnSlackspaCySpreedlyStarRocksStochasticAIStripeSupabase (Postgres)NebulaTairTelegramTencentVectorDBTensorFlow DatasetsTigris2MarkdownTrelloTruLensTwitterTypesenseUnstructuredUSearchVearchVectaraChat Over Documents with VectaraVectara Text GenerationVespaWandB TracingWeights & BiasesWeatherWeaviateWhatsAppWhyLabsWikipediaWolfram AlphaWriterXataXorbits Inference (Xinference)Yeager.aiYouTubeZepZilliz IntegrationsCallbacksChat modelsChat loadersDocument loadersDocument transformersLLMsMemoryRetrieversText embedding modelsAgents & ToolkitsToolsVector storesGrouped by providerActiveloop Deep LakeAI21 LabsAimAINetworkAirbyteAirtableAleph AlphaAlibaba Cloud OpensearchAmazon API GatewayAnalyticDBAnnoyAnyscaleApifyArangoDBArgillaArthurArxivAtlasAwaDBAWS S3 DirectoryAZLyricsAzure Blob StorageAzure Cognitive SearchAzure OpenAIBagelDBBananaBasetenBeamBedrockBiliBiliNIBittensorBlackboardBrave SearchCassandraCerebriumAIChaindeskChromaClarifaiClearMLClickHouseCnosDBCohereCollege ConfidentialCometConfident AIConfluenceC TransformersDashVectorDatabricksDatadog TracingDatadog LogsDataForSEODeepInfraDeepSparseDiffbotDingoDiscordDocArrayDocugamiDuckDBElasticsearchEpsillaEverNoteFacebook ChatFacebook FaissFigmaFireworksFlyteForefrontAIGitGitBookGoldenGoogle BigQueryGoogle Cloud StorageGoogle DriveGoogle SearchGoogle SerperGoogle Vertex AI MatchingEngineGooseAIGPT4AllGraphsignalGrobidGutenbergHacker NewsHazy ResearchHeliconeHologresHugging FaceiFixitIMSDbInfinoJinaKonkoLanceDBLangChain Decorators âœ¨Llama.cppLog10MarqoMediaWikiDumpMeilisearchMetalMicrosoft OneDriveMicrosoft PowerPointMicrosoft WordMilvusMinimaxMLflow AI GatewayMLflowModalModelScopeModern TreasuryMomentoMongoDB AtlasMotherduckMyScaleNeo4jNLPCloudNotion DBObsidianOpenAIOpenLLMOpenSearchOpenWeatherMapPetalsPostgres EmbeddingPGVectorPineconePipelineAIPortkeyPredibasePrediction GuardPromptLayerPsychicPubMedQdrantRay ServeRebuffRedditRedisReplicateRoamRocksetRunhouseRWKV-4SageMaker EndpointSageMaker TrackingScaNNSearxNG Search APISerpAPIShale ProtocolSingleStoreDBscikit-learnSlackspaCySpreedlyStarRocksStochasticAIStripeSupabase (Postgres)NebulaTairTelegramTencentVectorDBTensorFlow DatasetsTigris2MarkdownTrelloTruLensTwitterTypesenseUnstructuredUSearchVearchVectaraChat Over Documents with VectaraVectara Text GenerationVespaWandB TracingWeights & BiasesWeatherWeaviateWhatsAppWhyLabsWikipediaWolfram AlphaWriterXataXorbits Inference (Xinference)Yeager.aiYouTubeZepZilliz Integrations Callbacks Chat models Chat loaders Document loaders Document transformers LLMs Memory Retrievers Text embedding models Agents & Toolkits Tools Vector stores Grouped by provider Portkey Vectara IntegrationsGrouped by providerVectaraChat Over Documents with VectaraOn this pageChat Over Documents with VectaraThis notebook is based on the chat_vector_db notebook, but using Vectara as the vector database.import osfrom langchain.vectorstores import Vectarafrom langchain.vectorstores.vectara import VectaraRetrieverfrom langchain.llms import OpenAIfrom langchain.chains import ConversationalRetrievalChainAPI Reference:VectaraVectaraRetrieverOpenAIConversationalRetrievalChainLoad in documents. You can replace this with a loader for whatever type of data you wantfrom langchain.document_loaders import TextLoaderloader = TextLoader("../../../modules/state_of_the_union.txt")documents = loader.load()API Reference:TextLoaderWe now split the documents, create embeddings for them, and put them in a vectorstore. This allows us to do semantic search over them.vectorstore = Vectara.from_documents(documents, embedding=None)We can now create a memory object, which is neccessary to track the inputs/outputs and hold a conversation.from langchain.memory import ConversationBufferMemorymemory = ConversationBufferMemory(memory_key="chat_history", return_messages=True)API Reference:ConversationBufferMemoryWe now initialize the ConversationalRetrievalChainopenai_api_key = os.environ["OPENAI_API_KEY"]llm = OpenAI(openai_api_key=openai_api_key, temperature=0)retriever = vectorstore.as_retriever(lambda_val=0.025, k=5, filter=None)d = retriever.get_relevant_documents(    "What did the president say about Ketanji Brown Jackson")qa = ConversationalRetrievalChain.from_llm(llm, retriever, memory=memory)query = "What did the president say about Ketanji Brown Jackson"result = qa({"question": query})result["answer"]    " The president said that Ketanji Brown Jackson is one of the nation's top legal minds and a former top litigator in private practice, and that she will continue Justice Breyer's legacy of excellence."query = "Did he mention who she suceeded"result = qa({"question": query})result["answer"]    ' Ketanji Brown Jackson succeeded Justice Breyer.'Pass in chat historyâ€‹In the above example, we used a Memory object to track chat history. We can also just pass it in explicitly. In order to do this, we need to initialize a chain without any memory object.qa = ConversationalRetrievalChain.from_llm(    OpenAI(temperature=0), vectorstore.as_retriever())Here's an example of asking a question with no chat historychat_history = []query = "What did the president say about Ketanji Brown Jackson"result = qa({"question": query, "chat_history": chat_history})result["answer"]    " The president said that Ketanji Brown Jackson is one of the nation's top legal minds and a former top litigator in private practice, and that she will continue Justice Breyer's legacy of excellence."Here's an example of asking a question with some chat historychat_history = [(query, result["answer"])]query = "Did he mention who she suceeded"result = qa({"question": query, "chat_history": chat_history})result["answer"]    ' Ketanji Brown Jackson succeeded Justice Breyer.'Return Source Documentsâ€‹You can also easily return source documents from the ConversationalRetrievalChain. This is useful for when you want to inspect what documents were returned.qa = ConversationalRetrievalChain.from_llm(    llm, vectorstore.as_retriever(), return_source_documents=True)chat_history = []query = "What did the president say about Ketanji Brown Jackson"result = qa({"question": query, "chat_history": chat_history})result["source_documents"][0]    Document(page_content='Justice Breyer, thank you for your service. One of the most serious constitutional responsibilities a President has is nominating someone to serve on the United States Supreme Court. And I did that 4 days ago, when I nominated Circuit Court of Appeals Judge Ketanji Brown Jackson. One of our nationâ€™s top legal minds, who will continue Justice Breyerâ€™s legacy of excellence. A former top litigator in private practice.', metadata={'source': '../../../modules/state_of_the_union.txt'})ConversationalRetrievalChain with search_distanceâ€‹If you are using a vector store that supports filtering by search distance, you can add a threshold value parameter.vectordbkwargs = {"search_distance": 0.9}qa = ConversationalRetrievalChain.from_llm(    OpenAI(temperature=0), vectorstore.as_retriever(), return_source_documents=True)chat_history = []query = "What did the president say about Ketanji Brown Jackson"result = qa(    {"question": query, "chat_history": chat_history, "vectordbkwargs": vectordbkwargs})print(result["answer"])     The president said that Ketanji Brown Jackson is one of the nation's top legal minds and a former top litigator in private practice, and that she will continue Justice Breyer's legacy of excellence.ConversationalRetrievalChain with map_reduceâ€‹We can also use different types of combine document chains with the ConversationalRetrievalChain chain.from langchain.chains import LLMChainfrom langchain.chains.question_answering import load_qa_chainfrom langchain.chains.conversational_retrieval.prompts import CONDENSE_QUESTION_PROMPTAPI Reference:LLMChainload_qa_chainCONDENSE_QUESTION_PROMPTquestion_generator = LLMChain(llm=llm, prompt=CONDENSE_QUESTION_PROMPT)doc_chain = load_qa_chain(llm, chain_type="map_reduce")chain = ConversationalRetrievalChain(    retriever=vectorstore.as_retriever(),    question_generator=question_generator,    combine_docs_chain=doc_chain,)chat_history = []query = "What did the president say about Ketanji Brown Jackson"result = chain({"question": query, "chat_history": chat_history})result["answer"]    " The president said that he nominated Circuit Court of Appeals Judge Ketanji Brown Jackson, who is one of the nation's top legal minds and a former top litigator in private practice."ConversationalRetrievalChain with Question Answering with sourcesâ€‹You can also use this chain with the question answering with sources chain.from langchain.chains.qa_with_sources import load_qa_with_sources_chainAPI Reference:load_qa_with_sources_chainquestion_generator = LLMChain(llm=llm, prompt=CONDENSE_QUESTION_PROMPT)doc_chain = load_qa_with_sources_chain(llm, chain_type="map_reduce")chain = ConversationalRetrievalChain(    retriever=vectorstore.as_retriever(),    question_generator=question_generator,    combine_docs_chain=doc_chain,)chat_history = []query = "What did the president say about Ketanji Brown Jackson"result = chain({"question": query, "chat_history": chat_history})result["answer"]    " The president said that Ketanji Brown Jackson is one of the nation's top legal minds and a former top litigator in private practice.\nSOURCES: ../../../modules/state_of_the_union.txt"ConversationalRetrievalChain with streaming to stdoutâ€‹Output from the chain will be streamed to stdout token by token in this example.from langchain.chains.llm import LLMChainfrom langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandlerfrom langchain.chains.conversational_retrieval.prompts import (    CONDENSE_QUESTION_PROMPT,    QA_PROMPT,)from langchain.chains.question_answering import load_qa_chain# Construct a ConversationalRetrievalChain with a streaming llm for combine docs# and a separate, non-streaming llm for question generationllm = OpenAI(temperature=0, openai_api_key=openai_api_key)streaming_llm = OpenAI(    streaming=True,    callbacks=[StreamingStdOutCallbackHandler()],    temperature=0,    openai_api_key=openai_api_key,)question_generator = LLMChain(llm=llm, prompt=CONDENSE_QUESTION_PROMPT)doc_chain = load_qa_chain(streaming_llm, chain_type="stuff", prompt=QA_PROMPT)qa = ConversationalRetrievalChain(    retriever=vectorstore.as_retriever(),    combine_docs_chain=doc_chain,    question_generator=question_generator,)API Reference:LLMChainStreamingStdOutCallbackHandlerCONDENSE_QUESTION_PROMPTQA_PROMPTload_qa_chainchat_history = []query = "What did the president say about Ketanji Brown Jackson"result = qa({"question": query, "chat_history": chat_history})     The president said that Ketanji Brown Jackson is one of the nation's top legal minds and a former top litigator in private practice, and that she will continue Justice Breyer's legacy of excellence.chat_history = [(query, result["answer"])]query = "Did he mention who she suceeded"result = qa({"question": query, "chat_history": chat_history})     Justice Breyerget_chat_history Functionâ€‹You can also specify a get_chat_history function, which can be used to format the chat_history string.def get_chat_history(inputs) -> str:    res = []    for human, ai in inputs:        res.append(f"Human:{human}\nAI:{ai}")    return "\n".join(res)qa = ConversationalRetrievalChain.from_llm(    llm, vectorstore.as_retriever(), get_chat_history=get_chat_history)chat_history = []query = "What did the president say about Ketanji Brown Jackson"result = qa({"question": query, "chat_history": chat_history})result["answer"]    " The president said that Ketanji Brown Jackson is one of the nation's top legal minds and a former top litigator in private practice, and that she will continue Justice Breyer's legacy of excellence."PreviousVectaraNextVectara Text GenerationPass in chat historyReturn Source DocumentsConversationalRetrievalChain with search_distanceConversationalRetrievalChain with map_reduceConversationalRetrievalChain with Question Answering with sourcesConversationalRetrievalChain with streaming to stdoutget_chat_history Function IntegrationsGrouped by providerVectaraChat Over Documents with VectaraOn this pageChat Over Documents with VectaraThis notebook is based on the chat_vector_db notebook, but using Vectara as the vector database.import osfrom langchain.vectorstores import Vectarafrom langchain.vectorstores.vectara import VectaraRetrieverfrom langchain.llms import OpenAIfrom langchain.chains import ConversationalRetrievalChainAPI Reference:VectaraVectaraRetrieverOpenAIConversationalRetrievalChainLoad in documents. You can replace this with a loader for whatever type of data you wantfrom langchain.document_loaders import TextLoaderloader = TextLoader("../../../modules/state_of_the_union.txt")documents = loader.load()API Reference:TextLoaderWe now split the documents, create embeddings for them, and put them in a vectorstore. This allows us to do semantic search over them.vectorstore = Vectara.from_documents(documents, embedding=None)We can now create a memory object, which is neccessary to track the inputs/outputs and hold a conversation.from langchain.memory import ConversationBufferMemorymemory = ConversationBufferMemory(memory_key="chat_history", return_messages=True)API Reference:ConversationBufferMemoryWe now initialize the ConversationalRetrievalChainopenai_api_key = os.environ["OPENAI_API_KEY"]llm = OpenAI(openai_api_key=openai_api_key, temperature=0)retriever = vectorstore.as_retriever(lambda_val=0.025, k=5, filter=None)d = retriever.get_relevant_documents(    "What did the president say about Ketanji Brown Jackson")qa = ConversationalRetrievalChain.from_llm(llm, retriever, memory=memory)query = "What did the president say about Ketanji Brown Jackson"result = qa({"question": query})result["answer"]    " The president said that Ketanji Brown Jackson is one of the nation's top legal minds and a former top litigator in private practice, and that she will continue Justice Breyer's legacy of excellence."query = "Did he mention who she suceeded"result = qa({"question": query})result["answer"]    ' Ketanji Brown Jackson succeeded Justice Breyer.'Pass in chat historyâ€‹In the above example, we used a Memory object to track chat history. We can also just pass it in explicitly. In order to do this, we need to initialize a chain without any memory object.qa = ConversationalRetrievalChain.from_llm(    OpenAI(temperature=0), vectorstore.as_retriever())Here's an example of asking a question with no chat historychat_history = []query = "What did the president say about Ketanji Brown Jackson"result = qa({"question": query, "chat_history": chat_history})result["answer"]    " The president said that Ketanji Brown Jackson is one of the nation's top legal minds and a former top litigator in private practice, and that she will continue Justice Breyer's legacy of excellence."Here's an example of asking a question with some chat historychat_history = [(query, result["answer"])]query = "Did he mention who she suceeded"result = qa({"question": query, "chat_history": chat_history})result["answer"]    ' Ketanji Brown Jackson succeeded Justice Breyer.'Return Source Documentsâ€‹You can also easily return source documents from the ConversationalRetrievalChain. This is useful for when you want to inspect what documents were returned.qa = ConversationalRetrievalChain.from_llm(    llm, vectorstore.as_retriever(), return_source_documents=True)chat_history = []query = "What did the president say about Ketanji Brown Jackson"result = qa({"question": query, "chat_history": chat_history})result["source_documents"][0]    Document(page_content='Justice Breyer, thank you for your service. One of the most serious constitutional responsibilities a President has is nominating someone to serve on the United States Supreme Court. And I did that 4 days ago, when I nominated Circuit Court of Appeals Judge Ketanji Brown Jackson. One of our nationâ€™s top legal minds, who will continue Justice Breyerâ€™s legacy of excellence. A former top litigator in private practice.', metadata={'source': '../../../modules/state_of_the_union.txt'})ConversationalRetrievalChain with search_distanceâ€‹If you are using a vector store that supports filtering by search distance, you can add a threshold value parameter.vectordbkwargs = {"search_distance": 0.9}qa = ConversationalRetrievalChain.from_llm(    OpenAI(temperature=0), vectorstore.as_retriever(), return_source_documents=True)chat_history = []query = "What did the president say about Ketanji Brown Jackson"result = qa(    {"question": query, "chat_history": chat_history, "vectordbkwargs": vectordbkwargs})print(result["answer"])     The president said that Ketanji Brown Jackson is one of the nation's top legal minds and a former top litigator in private practice, and that she will continue Justice Breyer's legacy of excellence.ConversationalRetrievalChain with map_reduceâ€‹We can also use different types of combine document chains with the ConversationalRetrievalChain chain.from langchain.chains import LLMChainfrom langchain.chains.question_answering import load_qa_chainfrom langchain.chains.conversational_retrieval.prompts import CONDENSE_QUESTION_PROMPTAPI Reference:LLMChainload_qa_chainCONDENSE_QUESTION_PROMPTquestion_generator = LLMChain(llm=llm, prompt=CONDENSE_QUESTION_PROMPT)doc_chain = load_qa_chain(llm, chain_type="map_reduce")chain = ConversationalRetrievalChain(    retriever=vectorstore.as_retriever(),    question_generator=question_generator,    combine_docs_chain=doc_chain,)chat_history = []query = "What did the president say about Ketanji Brown Jackson"result = chain({"question": query, "chat_history": chat_history})result["answer"]    " The president said that he nominated Circuit Court of Appeals Judge Ketanji Brown Jackson, who is one of the nation's top legal minds and a former top litigator in private practice."ConversationalRetrievalChain with Question Answering with sourcesâ€‹You can also use this chain with the question answering with sources chain.from langchain.chains.qa_with_sources import load_qa_with_sources_chainAPI Reference:load_qa_with_sources_chainquestion_generator = LLMChain(llm=llm, prompt=CONDENSE_QUESTION_PROMPT)doc_chain = load_qa_with_sources_chain(llm, chain_type="map_reduce")chain = ConversationalRetrievalChain(    retriever=vectorstore.as_retriever(),    question_generator=question_generator,    combine_docs_chain=doc_chain,)chat_history = []query = "What did the president say about Ketanji Brown Jackson"result = chain({"question": query, "chat_history": chat_history})result["answer"]    " The president said that Ketanji Brown Jackson is one of the nation's top legal minds and a former top litigator in private practice.\nSOURCES: ../../../modules/state_of_the_union.txt"ConversationalRetrievalChain with streaming to stdoutâ€‹Output from the chain will be streamed to stdout token by token in this example.from langchain.chains.llm import LLMChainfrom langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandlerfrom langchain.chains.conversational_retrieval.prompts import (    CONDENSE_QUESTION_PROMPT,    QA_PROMPT,)from langchain.chains.question_answering import load_qa_chain# Construct a ConversationalRetrievalChain with a streaming llm for combine docs# and a separate, non-streaming llm for question generationllm = OpenAI(temperature=0, openai_api_key=openai_api_key)streaming_llm = OpenAI(    streaming=True,    callbacks=[StreamingStdOutCallbackHandler()],    temperature=0,    openai_api_key=openai_api_key,)question_generator = LLMChain(llm=llm, prompt=CONDENSE_QUESTION_PROMPT)doc_chain = load_qa_chain(streaming_llm, chain_type="stuff", prompt=QA_PROMPT)qa = ConversationalRetrievalChain(    retriever=vectorstore.as_retriever(),    combine_docs_chain=doc_chain,    question_generator=question_generator,)API Reference:LLMChainStreamingStdOutCallbackHandlerCONDENSE_QUESTION_PROMPTQA_PROMPTload_qa_chainchat_history = []query = "What did the president say about Ketanji Brown Jackson"result = qa({"question": query, "chat_history": chat_history})     The president said that Ketanji Brown Jackson is one of the nation's top legal minds and a former top litigator in private practice, and that she will continue Justice Breyer's legacy of excellence.chat_history = [(query, result["answer"])]query = "Did he mention who she suceeded"result = qa({"question": query, "chat_history": chat_history})     Justice Breyerget_chat_history Functionâ€‹You can also specify a get_chat_history function, which can be used to format the chat_history string.def get_chat_history(inputs) -> str:    res = []    for human, ai in inputs:        res.append(f"Human:{human}\nAI:{ai}")    return "\n".join(res)qa = ConversationalRetrievalChain.from_llm(    llm, vectorstore.as_retriever(), get_chat_history=get_chat_history)chat_history = []query = "What did the president say about Ketanji Brown Jackson"result = qa({"question": query, "chat_history": chat_history})result["answer"]    " The president said that Ketanji Brown Jackson is one of the nation's top legal minds and a former top litigator in private practice, and that she will continue Justice Breyer's legacy of excellence."PreviousVectaraNextVectara Text GenerationPass in chat historyReturn Source DocumentsConversationalRetrievalChain with search_distanceConversationalRetrievalChain with map_reduceConversationalRetrievalChain with Question Answering with sourcesConversationalRetrievalChain with streaming to stdoutget_chat_history Function IntegrationsGrouped by providerVectaraChat Over Documents with VectaraOn this pageChat Over Documents with VectaraThis notebook is based on the chat_vector_db notebook, but using Vectara as the vector database.import osfrom langchain.vectorstores import Vectarafrom langchain.vectorstores.vectara import VectaraRetrieverfrom langchain.llms import OpenAIfrom langchain.chains import ConversationalRetrievalChainAPI Reference:VectaraVectaraRetrieverOpenAIConversationalRetrievalChainLoad in documents. You can replace this with a loader for whatever type of data you wantfrom langchain.document_loaders import TextLoaderloader = TextLoader("../../../modules/state_of_the_union.txt")documents = loader.load()API Reference:TextLoaderWe now split the documents, create embeddings for them, and put them in a vectorstore. This allows us to do semantic search over them.vectorstore = Vectara.from_documents(documents, embedding=None)We can now create a memory object, which is neccessary to track the inputs/outputs and hold a conversation.from langchain.memory import ConversationBufferMemorymemory = ConversationBufferMemory(memory_key="chat_history", return_messages=True)API Reference:ConversationBufferMemoryWe now initialize the ConversationalRetrievalChainopenai_api_key = os.environ["OPENAI_API_KEY"]llm = OpenAI(openai_api_key=openai_api_key, temperature=0)retriever = vectorstore.as_retriever(lambda_val=0.025, k=5, filter=None)d = retriever.get_relevant_documents(    "What did the president say about Ketanji Brown Jackson")qa = ConversationalRetrievalChain.from_llm(llm, retriever, memory=memory)query = "What did the president say about Ketanji Brown Jackson"result = qa({"question": query})result["answer"]    " The president said that Ketanji Brown Jackson is one of the nation's top legal minds and a former top litigator in private practice, and that she will continue Justice Breyer's legacy of excellence."query = "Did he mention who she suceeded"result = qa({"question": query})result["answer"]    ' Ketanji Brown Jackson succeeded Justice Breyer.'Pass in chat historyâ€‹In the above example, we used a Memory object to track chat history. We can also just pass it in explicitly. In order to do this, we need to initialize a chain without any memory object.qa = ConversationalRetrievalChain.from_llm(    OpenAI(temperature=0), vectorstore.as_retriever())Here's an example of asking a question with no chat historychat_history = []query = "What did the president say about Ketanji Brown Jackson"result = qa({"question": query, "chat_history": chat_history})result["answer"]    " The president said that Ketanji Brown Jackson is one of the nation's top legal minds and a former top litigator in private practice, and that she will continue Justice Breyer's legacy of excellence."Here's an example of asking a question with some chat historychat_history = [(query, result["answer"])]query = "Did he mention who she suceeded"result = qa({"question": query, "chat_history": chat_history})result["answer"]    ' Ketanji Brown Jackson succeeded Justice Breyer.'Return Source Documentsâ€‹You can also easily return source documents from the ConversationalRetrievalChain. This is useful for when you want to inspect what documents were returned.qa = ConversationalRetrievalChain.from_llm(    llm, vectorstore.as_retriever(), return_source_documents=True)chat_history = []query = "What did the president say about Ketanji Brown Jackson"result = qa({"question": query, "chat_history": chat_history})result["source_documents"][0]    Document(page_content='Justice Breyer, thank you for your service. One of the most serious constitutional responsibilities a President has is nominating someone to serve on the United States Supreme Court. And I did that 4 days ago, when I nominated Circuit Court of Appeals Judge Ketanji Brown Jackson. One of our nationâ€™s top legal minds, who will continue Justice Breyerâ€™s legacy of excellence. A former top litigator in private practice.', metadata={'source': '../../../modules/state_of_the_union.txt'})ConversationalRetrievalChain with search_distanceâ€‹If you are using a vector store that supports filtering by search distance, you can add a threshold value parameter.vectordbkwargs = {"search_distance": 0.9}qa = ConversationalRetrievalChain.from_llm(    OpenAI(temperature=0), vectorstore.as_retriever(), return_source_documents=True)chat_history = []query = "What did the president say about Ketanji Brown Jackson"result = qa(    {"question": query, "chat_history": chat_history, "vectordbkwargs": vectordbkwargs})print(result["answer"])     The president said that Ketanji Brown Jackson is one of the nation's top legal minds and a former top litigator in private practice, and that she will continue Justice Breyer's legacy of excellence.ConversationalRetrievalChain with map_reduceâ€‹We can also use different types of combine document chains with the ConversationalRetrievalChain chain.from langchain.chains import LLMChainfrom langchain.chains.question_answering import load_qa_chainfrom langchain.chains.conversational_retrieval.prompts import CONDENSE_QUESTION_PROMPTAPI Reference:LLMChainload_qa_chainCONDENSE_QUESTION_PROMPTquestion_generator = LLMChain(llm=llm, prompt=CONDENSE_QUESTION_PROMPT)doc_chain = load_qa_chain(llm, chain_type="map_reduce")chain = ConversationalRetrievalChain(    retriever=vectorstore.as_retriever(),    question_generator=question_generator,    combine_docs_chain=doc_chain,)chat_history = []query = "What did the president say about Ketanji Brown Jackson"result = chain({"question": query, "chat_history": chat_history})result["answer"]    " The president said that he nominated Circuit Court of Appeals Judge Ketanji Brown Jackson, who is one of the nation's top legal minds and a former top litigator in private practice."ConversationalRetrievalChain with Question Answering with sourcesâ€‹You can also use this chain with the question answering with sources chain.from langchain.chains.qa_with_sources import load_qa_with_sources_chainAPI Reference:load_qa_with_sources_chainquestion_generator = LLMChain(llm=llm, prompt=CONDENSE_QUESTION_PROMPT)doc_chain = load_qa_with_sources_chain(llm, chain_type="map_reduce")chain = ConversationalRetrievalChain(    retriever=vectorstore.as_retriever(),    question_generator=question_generator,    combine_docs_chain=doc_chain,)chat_history = []query = "What did the president say about Ketanji Brown Jackson"result = chain({"question": query, "chat_history": chat_history})result["answer"]    " The president said that Ketanji Brown Jackson is one of the nation's top legal minds and a former top litigator in private practice.\nSOURCES: ../../../modules/state_of_the_union.txt"ConversationalRetrievalChain with streaming to stdoutâ€‹Output from the chain will be streamed to stdout token by token in this example.from langchain.chains.llm import LLMChainfrom langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandlerfrom langchain.chains.conversational_retrieval.prompts import (    CONDENSE_QUESTION_PROMPT,    QA_PROMPT,)from langchain.chains.question_answering import load_qa_chain# Construct a ConversationalRetrievalChain with a streaming llm for combine docs# and a separate, non-streaming llm for question generationllm = OpenAI(temperature=0, openai_api_key=openai_api_key)streaming_llm = OpenAI(    streaming=True,    callbacks=[StreamingStdOutCallbackHandler()],    temperature=0,    openai_api_key=openai_api_key,)question_generator = LLMChain(llm=llm, prompt=CONDENSE_QUESTION_PROMPT)doc_chain = load_qa_chain(streaming_llm, chain_type="stuff", prompt=QA_PROMPT)qa = ConversationalRetrievalChain(    retriever=vectorstore.as_retriever(),    combine_docs_chain=doc_chain,    question_generator=question_generator,)API Reference:LLMChainStreamingStdOutCallbackHandlerCONDENSE_QUESTION_PROMPTQA_PROMPTload_qa_chainchat_history = []query = "What did the president say about Ketanji Brown Jackson"result = qa({"question": query, "chat_history": chat_history})     The president said that Ketanji Brown Jackson is one of the nation's top legal minds and a former top litigator in private practice, and that she will continue Justice Breyer's legacy of excellence.chat_history = [(query, result["answer"])]query = "Did he mention who she suceeded"result = qa({"question": query, "chat_history": chat_history})     Justice Breyerget_chat_history Functionâ€‹You can also specify a get_chat_history function, which can be used to format the chat_history string.def get_chat_history(inputs) -> str:    res = []    for human, ai in inputs:        res.append(f"Human:{human}\nAI:{ai}")    return "\n".join(res)qa = ConversationalRetrievalChain.from_llm(    llm, vectorstore.as_retriever(), get_chat_history=get_chat_history)chat_history = []query = "What did the president say about Ketanji Brown Jackson"result = qa({"question": query, "chat_history": chat_history})result["answer"]    " The president said that Ketanji Brown Jackson is one of the nation's top legal minds and a former top litigator in private practice, and that she will continue Justice Breyer's legacy of excellence."PreviousVectaraNextVectara Text Generation IntegrationsGrouped by providerVectaraChat Over Documents with VectaraOn this pageChat Over Documents with VectaraThis notebook is based on the chat_vector_db notebook, but using Vectara as the vector database.import osfrom langchain.vectorstores import Vectarafrom langchain.vectorstores.vectara import VectaraRetrieverfrom langchain.llms import OpenAIfrom langchain.chains import ConversationalRetrievalChainAPI Reference:VectaraVectaraRetrieverOpenAIConversationalRetrievalChainLoad in documents. You can replace this with a loader for whatever type of data you wantfrom langchain.document_loaders import TextLoaderloader = TextLoader("../../../modules/state_of_the_union.txt")documents = loader.load()API Reference:TextLoaderWe now split the documents, create embeddings for them, and put them in a vectorstore. This allows us to do semantic search over them.vectorstore = Vectara.from_documents(documents, embedding=None)We can now create a memory object, which is neccessary to track the inputs/outputs and hold a conversation.from langchain.memory import ConversationBufferMemorymemory = ConversationBufferMemory(memory_key="chat_history", return_messages=True)API Reference:ConversationBufferMemoryWe now initialize the ConversationalRetrievalChainopenai_api_key = os.environ["OPENAI_API_KEY"]llm = OpenAI(openai_api_key=openai_api_key, temperature=0)retriever = vectorstore.as_retriever(lambda_val=0.025, k=5, filter=None)d = retriever.get_relevant_documents(    "What did the president say about Ketanji Brown Jackson")qa = ConversationalRetrievalChain.from_llm(llm, retriever, memory=memory)query = "What did the president say about Ketanji Brown Jackson"result = qa({"question": query})result["answer"]    " The president said that Ketanji Brown Jackson is one of the nation's top legal minds and a former top litigator in private practice, and that she will continue Justice Breyer's legacy of excellence."query = "Did he mention who she suceeded"result = qa({"question": query})result["answer"]    ' Ketanji Brown Jackson succeeded Justice Breyer.'Pass in chat historyâ€‹In the above example, we used a Memory object to track chat history. We can also just pass it in explicitly. In order to do this, we need to initialize a chain without any memory object.qa = ConversationalRetrievalChain.from_llm(    OpenAI(temperature=0), vectorstore.as_retriever())Here's an example of asking a question with no chat historychat_history = []query = "What did the president say about Ketanji Brown Jackson"result = qa({"question": query, "chat_history": chat_history})result["answer"]    " The president said that Ketanji Brown Jackson is one of the nation's top legal minds and a former top litigator in private practice, and that she will continue Justice Breyer's legacy of excellence."Here's an example of asking a question with some chat historychat_history = [(query, result["answer"])]query = "Did he mention who she suceeded"result = qa({"question": query, "chat_history": chat_history})result["answer"]    ' Ketanji Brown Jackson succeeded Justice Breyer.'Return Source Documentsâ€‹You can also easily return source documents from the ConversationalRetrievalChain. This is useful for when you want to inspect what documents were returned.qa = ConversationalRetrievalChain.from_llm(    llm, vectorstore.as_retriever(), return_source_documents=True)chat_history = []query = "What did the president say about Ketanji Brown Jackson"result = qa({"question": query, "chat_history": chat_history})result["source_documents"][0]    Document(page_content='Justice Breyer, thank you for your service. One of the most serious constitutional responsibilities a President has is nominating someone to serve on the United States Supreme Court. And I did that 4 days ago, when I nominated Circuit Court of Appeals Judge Ketanji Brown Jackson. One of our nationâ€™s top legal minds, who will continue Justice Breyerâ€™s legacy of excellence. A former top litigator in private practice.', metadata={'source': '../../../modules/state_of_the_union.txt'})ConversationalRetrievalChain with search_distanceâ€‹If you are using a vector store that supports filtering by search distance, you can add a threshold value parameter.vectordbkwargs = {"search_distance": 0.9}qa = ConversationalRetrievalChain.from_llm(    OpenAI(temperature=0), vectorstore.as_retriever(), return_source_documents=True)chat_history = []query = "What did the president say about Ketanji Brown Jackson"result = qa(    {"question": query, "chat_history": chat_history, "vectordbkwargs": vectordbkwargs})print(result["answer"])     The president said that Ketanji Brown Jackson is one of the nation's top legal minds and a former top litigator in private practice, and that she will continue Justice Breyer's legacy of excellence.ConversationalRetrievalChain with map_reduceâ€‹We can also use different types of combine document chains with the ConversationalRetrievalChain chain.from langchain.chains import LLMChainfrom langchain.chains.question_answering import load_qa_chainfrom langchain.chains.conversational_retrieval.prompts import CONDENSE_QUESTION_PROMPTAPI Reference:LLMChainload_qa_chainCONDENSE_QUESTION_PROMPTquestion_generator = LLMChain(llm=llm, prompt=CONDENSE_QUESTION_PROMPT)doc_chain = load_qa_chain(llm, chain_type="map_reduce")chain = ConversationalRetrievalChain(    retriever=vectorstore.as_retriever(),    question_generator=question_generator,    combine_docs_chain=doc_chain,)chat_history = []query = "What did the president say about Ketanji Brown Jackson"result = chain({"question": query, "chat_history": chat_history})result["answer"]    " The president said that he nominated Circuit Court of Appeals Judge Ketanji Brown Jackson, who is one of the nation's top legal minds and a former top litigator in private practice."ConversationalRetrievalChain with Question Answering with sourcesâ€‹You can also use this chain with the question answering with sources chain.from langchain.chains.qa_with_sources import load_qa_with_sources_chainAPI Reference:load_qa_with_sources_chainquestion_generator = LLMChain(llm=llm, prompt=CONDENSE_QUESTION_PROMPT)doc_chain = load_qa_with_sources_chain(llm, chain_type="map_reduce")chain = ConversationalRetrievalChain(    retriever=vectorstore.as_retriever(),    question_generator=question_generator,    combine_docs_chain=doc_chain,)chat_history = []query = "What did the president say about Ketanji Brown Jackson"result = chain({"question": query, "chat_history": chat_history})result["answer"]    " The president said that Ketanji Brown Jackson is one of the nation's top legal minds and a former top litigator in private practice.\nSOURCES: ../../../modules/state_of_the_union.txt"ConversationalRetrievalChain with streaming to stdoutâ€‹Output from the chain will be streamed to stdout token by token in this example.from langchain.chains.llm import LLMChainfrom langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandlerfrom langchain.chains.conversational_retrieval.prompts import (    CONDENSE_QUESTION_PROMPT,    QA_PROMPT,)from langchain.chains.question_answering import load_qa_chain# Construct a ConversationalRetrievalChain with a streaming llm for combine docs# and a separate, non-streaming llm for question generationllm = OpenAI(temperature=0, openai_api_key=openai_api_key)streaming_llm = OpenAI(    streaming=True,    callbacks=[StreamingStdOutCallbackHandler()],    temperature=0,    openai_api_key=openai_api_key,)question_generator = LLMChain(llm=llm, prompt=CONDENSE_QUESTION_PROMPT)doc_chain = load_qa_chain(streaming_llm, chain_type="stuff", prompt=QA_PROMPT)qa = ConversationalRetrievalChain(    retriever=vectorstore.as_retriever(),    combine_docs_chain=doc_chain,    question_generator=question_generator,)API Reference:LLMChainStreamingStdOutCallbackHandlerCONDENSE_QUESTION_PROMPTQA_PROMPTload_qa_chainchat_history = []query = "What did the president say about Ketanji Brown Jackson"result = qa({"question": query, "chat_history": chat_history})     The president said that Ketanji Brown Jackson is one of the nation's top legal minds and a former top litigator in private practice, and that she will continue Justice Breyer's legacy of excellence.chat_history = [(query, result["answer"])]query = "Did he mention who she suceeded"result = qa({"question": query, "chat_history": chat_history})     Justice Breyerget_chat_history Functionâ€‹You can also specify a get_chat_history function, which can be used to format the chat_history string.def get_chat_history(inputs) -> str:    res = []    for human, ai in inputs:        res.append(f"Human:{human}\nAI:{ai}")    return "\n".join(res)qa = ConversationalRetrievalChain.from_llm(    llm, vectorstore.as_retriever(), get_chat_history=get_chat_history)chat_history = []query = "What did the president say about Ketanji Brown Jackson"result = qa({"question": query, "chat_history": chat_history})result["answer"]    " The president said that Ketanji Brown Jackson is one of the nation's top legal minds and a former top litigator in private practice, and that she will continue Justice Breyer's legacy of excellence."PreviousVectaraNextVectara Text Generation On this page Chat Over Documents with VectaraThis notebook is based on the chat_vector_db notebook, but using Vectara as the vector database.import osfrom langchain.vectorstores import Vectarafrom langchain.vectorstores.vectara import VectaraRetrieverfrom langchain.llms import OpenAIfrom langchain.chains import ConversationalRetrievalChainAPI Reference:VectaraVectaraRetrieverOpenAIConversationalRetrievalChainLoad in documents. You can replace this with a loader for whatever type of data you wantfrom langchain.document_loaders import TextLoaderloader = TextLoader("../../../modules/state_of_the_union.txt")documents = loader.load()API Reference:TextLoaderWe now split the documents, create embeddings for them, and put them in a vectorstore. This allows us to do semantic search over them.vectorstore = Vectara.from_documents(documents, embedding=None)We can now create a memory object, which is neccessary to track the inputs/outputs and hold a conversation.from langchain.memory import ConversationBufferMemorymemory = ConversationBufferMemory(memory_key="chat_history", return_messages=True)API Reference:ConversationBufferMemoryWe now initialize the ConversationalRetrievalChainopenai_api_key = os.environ["OPENAI_API_KEY"]llm = OpenAI(openai_api_key=openai_api_key, temperature=0)retriever = vectorstore.as_retriever(lambda_val=0.025, k=5, filter=None)d = retriever.get_relevant_documents(    "What did the president say about Ketanji Brown Jackson")qa = ConversationalRetrievalChain.from_llm(llm, retriever, memory=memory)query = "What did the president say about Ketanji Brown Jackson"result = qa({"question": query})result["answer"]    " The president said that Ketanji Brown Jackson is one of the nation's top legal minds and a former top litigator in private practice, and that she will continue Justice Breyer's legacy of excellence."query = "Did he mention who she suceeded"result = qa({"question": query})result["answer"]    ' Ketanji Brown Jackson succeeded Justice Breyer.'Pass in chat historyâ€‹In the above example, we used a Memory object to track chat history. We can also just pass it in explicitly. In order to do this, we need to initialize a chain without any memory object.qa = ConversationalRetrievalChain.from_llm(    OpenAI(temperature=0), vectorstore.as_retriever())Here's an example of asking a question with no chat historychat_history = []query = "What did the president say about Ketanji Brown Jackson"result = qa({"question": query, "chat_history": chat_history})result["answer"]    " The president said that Ketanji Brown Jackson is one of the nation's top legal minds and a former top litigator in private practice, and that she will continue Justice Breyer's legacy of excellence."Here's an example of asking a question with some chat historychat_history = [(query, result["answer"])]query = "Did he mention who she suceeded"result = qa({"question": query, "chat_history": chat_history})result["answer"]    ' Ketanji Brown Jackson succeeded Justice Breyer.'Return Source Documentsâ€‹You can also easily return source documents from the ConversationalRetrievalChain. This is useful for when you want to inspect what documents were returned.qa = ConversationalRetrievalChain.from_llm(    llm, vectorstore.as_retriever(), return_source_documents=True)chat_history = []query = "What did the president say about Ketanji Brown Jackson"result = qa({"question": query, "chat_history": chat_history})result["source_documents"][0]    Document(page_content='Justice Breyer, thank you for your service. One of the most serious constitutional responsibilities a President has is nominating someone to serve on the United States Supreme Court. And I did that 4 days ago, when I nominated Circuit Court of Appeals Judge Ketanji Brown Jackson. One of our nationâ€™s top legal minds, who will continue Justice Breyerâ€™s legacy of excellence. A former top litigator in private practice.', metadata={'source': '../../../modules/state_of_the_union.txt'})ConversationalRetrievalChain with search_distanceâ€‹If you are using a vector store that supports filtering by search distance, you can add a threshold value parameter.vectordbkwargs = {"search_distance": 0.9}qa = ConversationalRetrievalChain.from_llm(    OpenAI(temperature=0), vectorstore.as_retriever(), return_source_documents=True)chat_history = []query = "What did the president say about Ketanji Brown Jackson"result = qa(    {"question": query, "chat_history": chat_history, "vectordbkwargs": vectordbkwargs})print(result["answer"])     The president said that Ketanji Brown Jackson is one of the nation's top legal minds and a former top litigator in private practice, and that she will continue Justice Breyer's legacy of excellence.ConversationalRetrievalChain with map_reduceâ€‹We can also use different types of combine document chains with the ConversationalRetrievalChain chain.from langchain.chains import LLMChainfrom langchain.chains.question_answering import load_qa_chainfrom langchain.chains.conversational_retrieval.prompts import CONDENSE_QUESTION_PROMPTAPI Reference:LLMChainload_qa_chainCONDENSE_QUESTION_PROMPTquestion_generator = LLMChain(llm=llm, prompt=CONDENSE_QUESTION_PROMPT)doc_chain = load_qa_chain(llm, chain_type="map_reduce")chain = ConversationalRetrievalChain(    retriever=vectorstore.as_retriever(),    question_generator=question_generator,    combine_docs_chain=doc_chain,)chat_history = []query = "What did the president say about Ketanji Brown Jackson"result = chain({"question": query, "chat_history": chat_history})result["answer"]    " The president said that he nominated Circuit Court of Appeals Judge Ketanji Brown Jackson, who is one of the nation's top legal minds and a former top litigator in private practice."ConversationalRetrievalChain with Question Answering with sourcesâ€‹You can also use this chain with the question answering with sources chain.from langchain.chains.qa_with_sources import load_qa_with_sources_chainAPI Reference:load_qa_with_sources_chainquestion_generator = LLMChain(llm=llm, prompt=CONDENSE_QUESTION_PROMPT)doc_chain = load_qa_with_sources_chain(llm, chain_type="map_reduce")chain = ConversationalRetrievalChain(    retriever=vectorstore.as_retriever(),    question_generator=question_generator,    combine_docs_chain=doc_chain,)chat_history = []query = "What did the president say about Ketanji Brown Jackson"result = chain({"question": query, "chat_history": chat_history})result["answer"]    " The president said that Ketanji Brown Jackson is one of the nation's top legal minds and a former top litigator in private practice.\nSOURCES: ../../../modules/state_of_the_union.txt"ConversationalRetrievalChain with streaming to stdoutâ€‹Output from the chain will be streamed to stdout token by token in this example.from langchain.chains.llm import LLMChainfrom langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandlerfrom langchain.chains.conversational_retrieval.prompts import (    CONDENSE_QUESTION_PROMPT,    QA_PROMPT,)from langchain.chains.question_answering import load_qa_chain# Construct a ConversationalRetrievalChain with a streaming llm for combine docs# and a separate, non-streaming llm for question generationllm = OpenAI(temperature=0, openai_api_key=openai_api_key)streaming_llm = OpenAI(    streaming=True,    callbacks=[StreamingStdOutCallbackHandler()],    temperature=0,    openai_api_key=openai_api_key,)question_generator = LLMChain(llm=llm, prompt=CONDENSE_QUESTION_PROMPT)doc_chain = load_qa_chain(streaming_llm, chain_type="stuff", prompt=QA_PROMPT)qa = ConversationalRetrievalChain(    retriever=vectorstore.as_retriever(),    combine_docs_chain=doc_chain,    question_generator=question_generator,)API Reference:LLMChainStreamingStdOutCallbackHandlerCONDENSE_QUESTION_PROMPTQA_PROMPTload_qa_chainchat_history = []query = "What did the president say about Ketanji Brown Jackson"result = qa({"question": query, "chat_history": chat_history})     The president said that Ketanji Brown Jackson is one of the nation's top legal minds and a former top litigator in private practice, and that she will continue Justice Breyer's legacy of excellence.chat_history = [(query, result["answer"])]query = "Did he mention who she suceeded"result = qa({"question": query, "chat_history": chat_history})     Justice Breyerget_chat_history Functionâ€‹You can also specify a get_chat_history function, which can be used to format the chat_history string.def get_chat_history(inputs) -> str:    res = []    for human, ai in inputs:        res.append(f"Human:{human}\nAI:{ai}")    return "\n".join(res)qa = ConversationalRetrievalChain.from_llm(    llm, vectorstore.as_retriever(), get_chat_history=get_chat_history)chat_history = []query = "What did the president say about Ketanji Brown Jackson"result = qa({"question": query, "chat_history": chat_history})result["answer"]    " The president said that Ketanji Brown Jackson is one of the nation's top legal minds and a former top litigator in private practice, and that she will continue Justice Breyer's legacy of excellence." import osfrom langchain.vectorstores import Vectarafrom langchain.vectorstores.vectara import VectaraRetrieverfrom langchain.llms import OpenAIfrom langchain.chains import ConversationalRetrievalChain import osfrom langchain.vectorstores import Vectarafrom langchain.vectorstores.vectara import VectaraRetrieverfrom langchain.llms import OpenAIfrom langchain.chains import ConversationalRetrievalChain  API Reference:VectaraVectaraRetrieverOpenAIConversationalRetrievalChain from langchain.document_loaders import TextLoaderloader = TextLoader("../../../modules/state_of_the_union.txt")documents = loader.load() from langchain.document_loaders import TextLoaderloader = TextLoader("../../../modules/state_of_the_union.txt")documents = loader.load()  API Reference:TextLoader vectorstore = Vectara.from_documents(documents, embedding=None) vectorstore = Vectara.from_documents(documents, embedding=None)  from langchain.memory import ConversationBufferMemorymemory = ConversationBufferMemory(memory_key="chat_history", return_messages=True) from langchain.memory import ConversationBufferMemorymemory = ConversationBufferMemory(memory_key="chat_history", return_messages=True)  API Reference:ConversationBufferMemory openai_api_key = os.environ["OPENAI_API_KEY"]llm = OpenAI(openai_api_key=openai_api_key, temperature=0)retriever = vectorstore.as_retriever(lambda_val=0.025, k=5, filter=None)d = retriever.get_relevant_documents(    "What did the president say about Ketanji Brown Jackson")qa = ConversationalRetrievalChain.from_llm(llm, retriever, memory=memory) openai_api_key = os.environ["OPENAI_API_KEY"]llm = OpenAI(openai_api_key=openai_api_key, temperature=0)retriever = vectorstore.as_retriever(lambda_val=0.025, k=5, filter=None)d = retriever.get_relevant_documents(    "What did the president say about Ketanji Brown Jackson")qa = ConversationalRetrievalChain.from_llm(llm, retriever, memory=memory)  query = "What did the president say about Ketanji Brown Jackson"result = qa({"question": query}) query = "What did the president say about Ketanji Brown Jackson"result = qa({"question": query})  result["answer"] result["answer"]      " The president said that Ketanji Brown Jackson is one of the nation's top legal minds and a former top litigator in private practice, and that she will continue Justice Breyer's legacy of excellence."     " The president said that Ketanji Brown Jackson is one of the nation's top legal minds and a former top litigator in private practice, and that she will continue Justice Breyer's legacy of excellence."     " The president said that Ketanji Brown Jackson is one of the nation's top legal minds and a former top litigator in private practice, and that she will continue Justice Breyer's legacy of excellence."  query = "Did he mention who she suceeded"result = qa({"question": query}) query = "Did he mention who she suceeded"result = qa({"question": query})  result["answer"] result["answer"]      ' Ketanji Brown Jackson succeeded Justice Breyer.'     ' Ketanji Brown Jackson succeeded Justice Breyer.'     ' Ketanji Brown Jackson succeeded Justice Breyer.'  qa = ConversationalRetrievalChain.from_llm(    OpenAI(temperature=0), vectorstore.as_retriever()) qa = ConversationalRetrievalChain.from_llm(    OpenAI(temperature=0), vectorstore.as_retriever())  chat_history = []query = "What did the president say about Ketanji Brown Jackson"result = qa({"question": query, "chat_history": chat_history}) chat_history = []query = "What did the president say about Ketanji Brown Jackson"result = qa({"question": query, "chat_history": chat_history})  result["answer"] result["answer"]      " The president said that Ketanji Brown Jackson is one of the nation's top legal minds and a former top litigator in private practice, and that she will continue Justice Breyer's legacy of excellence."     " The president said that Ketanji Brown Jackson is one of the nation's top legal minds and a former top litigator in private practice, and that she will continue Justice Breyer's legacy of excellence."     " The president said that Ketanji Brown Jackson is one of the nation's top legal minds and a former top litigator in private practice, and that she will continue Justice Breyer's legacy of excellence."  chat_history = [(query, result["answer"])]query = "Did he mention who she suceeded"result = qa({"question": query, "chat_history": chat_history}) chat_history = [(query, result["answer"])]query = "Did he mention who she suceeded"result = qa({"question": query, "chat_history": chat_history})  result["answer"] result["answer"]      ' Ketanji Brown Jackson succeeded Justice Breyer.'     ' Ketanji Brown Jackson succeeded Justice Breyer.'     ' Ketanji Brown Jackson succeeded Justice Breyer.'  qa = ConversationalRetrievalChain.from_llm(    llm, vectorstore.as_retriever(), return_source_documents=True) qa = ConversationalRetrievalChain.from_llm(    llm, vectorstore.as_retriever(), return_source_documents=True)  chat_history = []query = "What did the president say about Ketanji Brown Jackson"result = qa({"question": query, "chat_history": chat_history}) chat_history = []query = "What did the president say about Ketanji Brown Jackson"result = qa({"question": query, "chat_history": chat_history})  result["source_documents"][0] result["source_documents"][0]      Document(page_content='Justice Breyer, thank you for your service. One of the most serious constitutional responsibilities a President has is nominating someone to serve on the United States Supreme Court. And I did that 4 days ago, when I nominated Circuit Court of Appeals Judge Ketanji Brown Jackson. One of our nationâ€™s top legal minds, who will continue Justice Breyerâ€™s legacy of excellence. A former top litigator in private practice.', metadata={'source': '../../../modules/state_of_the_union.txt'})     Document(page_content='Justice Breyer, thank you for your service. One of the most serious constitutional responsibilities a President has is nominating someone to serve on the United States Supreme Court. And I did that 4 days ago, when I nominated Circuit Court of Appeals Judge Ketanji Brown Jackson. One of our nationâ€™s top legal minds, who will continue Justice Breyerâ€™s legacy of excellence. A former top litigator in private practice.', metadata={'source': '../../../modules/state_of_the_union.txt'})     Document(page_content='Justice Breyer, thank you for your service. One of the most serious constitutional responsibilities a President has is nominating someone to serve on the United States Supreme Court. And I did that 4 days ago, when I nominated Circuit Court of Appeals Judge Ketanji Brown Jackson. One of our nationâ€™s top legal minds, who will continue Justice Breyerâ€™s legacy of excellence. A former top litigator in private practice.', metadata={'source': '../../../modules/state_of_the_union.txt'})  vectordbkwargs = {"search_distance": 0.9} vectordbkwargs = {"search_distance": 0.9}  qa = ConversationalRetrievalChain.from_llm(    OpenAI(temperature=0), vectorstore.as_retriever(), return_source_documents=True)chat_history = []query = "What did the president say about Ketanji Brown Jackson"result = qa(    {"question": query, "chat_history": chat_history, "vectordbkwargs": vectordbkwargs}) qa = ConversationalRetrievalChain.from_llm(    OpenAI(temperature=0), vectorstore.as_retriever(), return_source_documents=True)chat_history = []query = "What did the president say about Ketanji Brown Jackson"result = qa(    {"question": query, "chat_history": chat_history, "vectordbkwargs": vectordbkwargs})  print(result["answer"]) print(result["answer"])       The president said that Ketanji Brown Jackson is one of the nation's top legal minds and a former top litigator in private practice, and that she will continue Justice Breyer's legacy of excellence.      The president said that Ketanji Brown Jackson is one of the nation's top legal minds and a former top litigator in private practice, and that she will continue Justice Breyer's legacy of excellence.      The president said that Ketanji Brown Jackson is one of the nation's top legal minds and a former top litigator in private practice, and that she will continue Justice Breyer's legacy of excellence.  from langchain.chains import LLMChainfrom langchain.chains.question_answering import load_qa_chainfrom langchain.chains.conversational_retrieval.prompts import CONDENSE_QUESTION_PROMPT from langchain.chains import LLMChainfrom langchain.chains.question_answering import load_qa_chainfrom langchain.chains.conversational_retrieval.prompts import CONDENSE_QUESTION_PROMPT  API Reference:LLMChainload_qa_chainCONDENSE_QUESTION_PROMPT question_generator = LLMChain(llm=llm, prompt=CONDENSE_QUESTION_PROMPT)doc_chain = load_qa_chain(llm, chain_type="map_reduce")chain = ConversationalRetrievalChain(    retriever=vectorstore.as_retriever(),    question_generator=question_generator,    combine_docs_chain=doc_chain,) question_generator = LLMChain(llm=llm, prompt=CONDENSE_QUESTION_PROMPT)doc_chain = load_qa_chain(llm, chain_type="map_reduce")chain = ConversationalRetrievalChain(    retriever=vectorstore.as_retriever(),    question_generator=question_generator,    combine_docs_chain=doc_chain,)  chat_history = []query = "What did the president say about Ketanji Brown Jackson"result = chain({"question": query, "chat_history": chat_history}) chat_history = []query = "What did the president say about Ketanji Brown Jackson"result = chain({"question": query, "chat_history": chat_history})  result["answer"] result["answer"]      " The president said that he nominated Circuit Court of Appeals Judge Ketanji Brown Jackson, who is one of the nation's top legal minds and a former top litigator in private practice."     " The president said that he nominated Circuit Court of Appeals Judge Ketanji Brown Jackson, who is one of the nation's top legal minds and a former top litigator in private practice."     " The president said that he nominated Circuit Court of Appeals Judge Ketanji Brown Jackson, who is one of the nation's top legal minds and a former top litigator in private practice."  from langchain.chains.qa_with_sources import load_qa_with_sources_chain from langchain.chains.qa_with_sources import load_qa_with_sources_chain  API Reference:load_qa_with_sources_chain question_generator = LLMChain(llm=llm, prompt=CONDENSE_QUESTION_PROMPT)doc_chain = load_qa_with_sources_chain(llm, chain_type="map_reduce")chain = ConversationalRetrievalChain(    retriever=vectorstore.as_retriever(),    question_generator=question_generator,    combine_docs_chain=doc_chain,) question_generator = LLMChain(llm=llm, prompt=CONDENSE_QUESTION_PROMPT)doc_chain = load_qa_with_sources_chain(llm, chain_type="map_reduce")chain = ConversationalRetrievalChain(    retriever=vectorstore.as_retriever(),    question_generator=question_generator,    combine_docs_chain=doc_chain,)  chat_history = []query = "What did the president say about Ketanji Brown Jackson"result = chain({"question": query, "chat_history": chat_history}) chat_history = []query = "What did the president say about Ketanji Brown Jackson"result = chain({"question": query, "chat_history": chat_history})  result["answer"] result["answer"]      " The president said that Ketanji Brown Jackson is one of the nation's top legal minds and a former top litigator in private practice.\nSOURCES: ../../../modules/state_of_the_union.txt"     " The president said that Ketanji Brown Jackson is one of the nation's top legal minds and a former top litigator in private practice.\nSOURCES: ../../../modules/state_of_the_union.txt"     " The president said that Ketanji Brown Jackson is one of the nation's top legal minds and a former top litigator in private practice.\nSOURCES: ../../../modules/state_of_the_union.txt"  from langchain.chains.llm import LLMChainfrom langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandlerfrom langchain.chains.conversational_retrieval.prompts import (    CONDENSE_QUESTION_PROMPT,    QA_PROMPT,)from langchain.chains.question_answering import load_qa_chain# Construct a ConversationalRetrievalChain with a streaming llm for combine docs# and a separate, non-streaming llm for question generationllm = OpenAI(temperature=0, openai_api_key=openai_api_key)streaming_llm = OpenAI(    streaming=True,    callbacks=[StreamingStdOutCallbackHandler()],    temperature=0,    openai_api_key=openai_api_key,)question_generator = LLMChain(llm=llm, prompt=CONDENSE_QUESTION_PROMPT)doc_chain = load_qa_chain(streaming_llm, chain_type="stuff", prompt=QA_PROMPT)qa = ConversationalRetrievalChain(    retriever=vectorstore.as_retriever(),    combine_docs_chain=doc_chain,    question_generator=question_generator,) from langchain.chains.llm import LLMChainfrom langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandlerfrom langchain.chains.conversational_retrieval.prompts import (    CONDENSE_QUESTION_PROMPT,    QA_PROMPT,)from langchain.chains.question_answering import load_qa_chain# Construct a ConversationalRetrievalChain with a streaming llm for combine docs# and a separate, non-streaming llm for question generationllm = OpenAI(temperature=0, openai_api_key=openai_api_key)streaming_llm = OpenAI(    streaming=True,    callbacks=[StreamingStdOutCallbackHandler()],    temperature=0,    openai_api_key=openai_api_key,)question_generator = LLMChain(llm=llm, prompt=CONDENSE_QUESTION_PROMPT)doc_chain = load_qa_chain(streaming_llm, chain_type="stuff", prompt=QA_PROMPT)qa = ConversationalRetrievalChain(    retriever=vectorstore.as_retriever(),    combine_docs_chain=doc_chain,    question_generator=question_generator,)  API Reference:LLMChainStreamingStdOutCallbackHandlerCONDENSE_QUESTION_PROMPTQA_PROMPTload_qa_chain chat_history = []query = "What did the president say about Ketanji Brown Jackson"result = qa({"question": query, "chat_history": chat_history}) chat_history = []query = "What did the president say about Ketanji Brown Jackson"result = qa({"question": query, "chat_history": chat_history})       The president said that Ketanji Brown Jackson is one of the nation's top legal minds and a former top litigator in private practice, and that she will continue Justice Breyer's legacy of excellence.      The president said that Ketanji Brown Jackson is one of the nation's top legal minds and a former top litigator in private practice, and that she will continue Justice Breyer's legacy of excellence.      The president said that Ketanji Brown Jackson is one of the nation's top legal minds and a former top litigator in private practice, and that she will continue Justice Breyer's legacy of excellence.  chat_history = [(query, result["answer"])]query = "Did he mention who she suceeded"result = qa({"question": query, "chat_history": chat_history}) chat_history = [(query, result["answer"])]query = "Did he mention who she suceeded"result = qa({"question": query, "chat_history": chat_history})       Justice Breyer      Justice Breyer      Justice Breyer  def get_chat_history(inputs) -> str:    res = []    for human, ai in inputs:        res.append(f"Human:{human}\nAI:{ai}")    return "\n".join(res)qa = ConversationalRetrievalChain.from_llm(    llm, vectorstore.as_retriever(), get_chat_history=get_chat_history) def get_chat_history(inputs) -> str:    res = []    for human, ai in inputs:        res.append(f"Human:{human}\nAI:{ai}")    return "\n".join(res)qa = ConversationalRetrievalChain.from_llm(    llm, vectorstore.as_retriever(), get_chat_history=get_chat_history)  chat_history = []query = "What did the president say about Ketanji Brown Jackson"result = qa({"question": query, "chat_history": chat_history}) chat_history = []query = "What did the president say about Ketanji Brown Jackson"result = qa({"question": query, "chat_history": chat_history})  result["answer"] result["answer"]      " The president said that Ketanji Brown Jackson is one of the nation's top legal minds and a former top litigator in private practice, and that she will continue Justice Breyer's legacy of excellence."     " The president said that Ketanji Brown Jackson is one of the nation's top legal minds and a former top litigator in private practice, and that she will continue Justice Breyer's legacy of excellence."     " The president said that Ketanji Brown Jackson is one of the nation's top legal minds and a former top litigator in private practice, and that she will continue Justice Breyer's legacy of excellence."  Previous Vectara Next Vectara Text Generation Pass in chat historyReturn Source DocumentsConversationalRetrievalChain with search_distanceConversationalRetrievalChain with map_reduceConversationalRetrievalChain with Question Answering with sourcesConversationalRetrievalChain with streaming to stdoutget_chat_history Function Pass in chat historyReturn Source DocumentsConversationalRetrievalChain with search_distanceConversationalRetrievalChain with map_reduceConversationalRetrievalChain with Question Answering with sourcesConversationalRetrievalChain with streaming to stdoutget_chat_history Function CommunityDiscordTwitterGitHubPythonJS/TSMoreHomepageBlogCopyright Â© 2023 LangChain, Inc. CommunityDiscordTwitterGitHubPythonJS/TSMoreHomepageBlog CommunityDiscordTwitter Community GitHubPythonJS/TS GitHub MoreHomepageBlog More Copyright Â© 2023 LangChain, Inc. Copyright Â© 2023 LangChain, Inc. Skip to main content (#docusaurus_skipToContent_fallback) ðŸ¦œï¸ðŸ”— LangChain (/) Docs (/docs/get_started/introduction) Use cases (/docs/use_cases) Integrations (/docs/integrations) API (https://api.python.langchain.com) LangSmith (https://smith.langchain.com) JS/TS Docs (https://js.langchain.com/docs)  (https://github.com/hwchase17/langchain) Integrations (/docs/integrations) Callbacks (/docs/integrations/callbacks/) Chat models (/docs/integrations/chat/) Chat loaders (/docs/integrations/chat_loaders/) Document loaders (/docs/integrations/document_loaders/) Document transformers (/docs/integrations/document_transformers/) LLMs (/docs/integrations/llms/) Memory (/docs/integrations/memory/) Retrievers (/docs/integrations/retrievers/) Text embedding models (/docs/integrations/text_embedding/) Agents & Toolkits (/docs/integrations/toolkits/) Tools (/docs/integrations/tools/) Vector stores (/docs/integrations/vectorstores/) Grouped by provider (/docs/integrations/providers/) Activeloop Deep Lake (/docs/integrations/providers/activeloop_deeplake) AI21 Labs (/docs/integrations/providers/ai21) Aim (/docs/integrations/providers/aim_tracking) AINetwork (/docs/integrations/providers/ainetwork) Airbyte (/docs/integrations/providers/airbyte) Airtable (/docs/integrations/providers/airtable) Aleph Alpha (/docs/integrations/providers/aleph_alpha) Alibaba Cloud Opensearch (/docs/integrations/providers/alibabacloud_opensearch) Amazon API Gateway (/docs/integrations/providers/amazon_api_gateway) AnalyticDB (/docs/integrations/providers/analyticdb) Annoy (/docs/integrations/providers/annoy) Anyscale (/docs/integrations/providers/anyscale) Apify (/docs/integrations/providers/apify) ArangoDB (/docs/integrations/providers/arangodb) Argilla (/docs/integrations/providers/argilla) Arthur (/docs/integrations/providers/arthur_tracking) Arxiv (/docs/integrations/providers/arxiv) Atlas (/docs/integrations/providers/atlas) AwaDB (/docs/integrations/providers/awadb) AWS S3 Directory (/docs/integrations/providers/aws_s3) AZLyrics (/docs/integrations/providers/azlyrics) Azure Blob Storage (/docs/integrations/providers/azure_blob_storage) Azure Cognitive Search (/docs/integrations/providers/azure_cognitive_search_) Azure OpenAI (/docs/integrations/providers/azure_openai) BagelDB (/docs/integrations/providers/bageldb) Banana (/docs/integrations/providers/bananadev) Baseten (/docs/integrations/providers/baseten) Beam (/docs/integrations/providers/beam) Bedrock (/docs/integrations/providers/bedrock) BiliBili (/docs/integrations/providers/bilibili) NIBittensor (/docs/integrations/providers/bittensor) Blackboard (/docs/integrations/providers/blackboard) Brave Search (/docs/integrations/providers/brave_search) Cassandra (/docs/integrations/providers/cassandra) CerebriumAI (/docs/integrations/providers/cerebriumai) Chaindesk (/docs/integrations/providers/chaindesk) Chroma (/docs/integrations/providers/chroma) Clarifai (/docs/integrations/providers/clarifai) ClearML (/docs/integrations/providers/clearml_tracking) ClickHouse (/docs/integrations/providers/clickhouse) CnosDB (/docs/integrations/providers/cnosdb) Cohere (/docs/integrations/providers/cohere) College Confidential (/docs/integrations/providers/college_confidential) Comet (/docs/integrations/providers/comet_tracking) Confident AI (/docs/integrations/providers/confident) Confluence (/docs/integrations/providers/confluence) C Transformers (/docs/integrations/providers/ctransformers) DashVector (/docs/integrations/providers/dashvector) Databricks (/docs/integrations/providers/databricks) Datadog Tracing (/docs/integrations/providers/datadog) Datadog Logs (/docs/integrations/providers/datadog_logs) DataForSEO (/docs/integrations/providers/dataforseo) DeepInfra (/docs/integrations/providers/deepinfra) DeepSparse (/docs/integrations/providers/deepsparse) Diffbot (/docs/integrations/providers/diffbot) Dingo (/docs/integrations/providers/dingo) Discord (/docs/integrations/providers/discord) DocArray (/docs/integrations/providers/docarray) Docugami (/docs/integrations/providers/docugami) DuckDB (/docs/integrations/providers/duckdb) Elasticsearch (/docs/integrations/providers/elasticsearch) Epsilla (/docs/integrations/providers/epsilla) EverNote (/docs/integrations/providers/evernote) Facebook Chat (/docs/integrations/providers/facebook_chat) Facebook Faiss (/docs/integrations/providers/facebook_faiss) Figma (/docs/integrations/providers/figma) Fireworks (/docs/integrations/providers/fireworks) Flyte (/docs/integrations/providers/flyte) ForefrontAI (/docs/integrations/providers/forefrontai) Git (/docs/integrations/providers/git) GitBook (/docs/integrations/providers/gitbook) Golden (/docs/integrations/providers/golden) Google BigQuery (/docs/integrations/providers/google_bigquery) Google Cloud Storage (/docs/integrations/providers/google_cloud_storage) Google Drive (/docs/integrations/providers/google_drive) Google Search (/docs/integrations/providers/google_search) Google Serper (/docs/integrations/providers/google_serper) Google Vertex AI MatchingEngine (/docs/integrations/providers/google_vertex_ai_matchingengine) GooseAI (/docs/integrations/providers/gooseai) GPT4All (/docs/integrations/providers/gpt4all) Graphsignal (/docs/integrations/providers/graphsignal) Grobid (/docs/integrations/providers/grobid) Gutenberg (/docs/integrations/providers/gutenberg) Hacker News (/docs/integrations/providers/hacker_news) Hazy Research (/docs/integrations/providers/hazy_research) Helicone (/docs/integrations/providers/helicone) Hologres (/docs/integrations/providers/hologres) Hugging Face (/docs/integrations/providers/huggingface) iFixit (/docs/integrations/providers/ifixit) IMSDb (/docs/integrations/providers/imsdb) Infino (/docs/integrations/providers/infino) Jina (/docs/integrations/providers/jina) Konko (/docs/integrations/providers/konko) LanceDB (/docs/integrations/providers/lancedb) LangChain Decorators âœ¨ (/docs/integrations/providers/langchain_decorators) Llama.cpp (/docs/integrations/providers/llamacpp) Log10 (/docs/integrations/providers/log10) Marqo (/docs/integrations/providers/marqo) MediaWikiDump (/docs/integrations/providers/mediawikidump) Meilisearch (/docs/integrations/providers/meilisearch) Metal (/docs/integrations/providers/metal) Microsoft OneDrive (/docs/integrations/providers/microsoft_onedrive) Microsoft PowerPoint (/docs/integrations/providers/microsoft_powerpoint) Microsoft Word (/docs/integrations/providers/microsoft_word) Milvus (/docs/integrations/providers/milvus) Minimax (/docs/integrations/providers/minimax) MLflow AI Gateway (/docs/integrations/providers/mlflow_ai_gateway) MLflow (/docs/integrations/providers/mlflow_tracking) Modal (/docs/integrations/providers/modal) ModelScope (/docs/integrations/providers/modelscope) Modern Treasury (/docs/integrations/providers/modern_treasury) Momento (/docs/integrations/providers/momento) MongoDB Atlas (/docs/integrations/providers/mongodb_atlas) Motherduck (/docs/integrations/providers/motherduck) MyScale (/docs/integrations/providers/myscale) Neo4j (/docs/integrations/providers/neo4j) NLPCloud (/docs/integrations/providers/nlpcloud) Notion DB (/docs/integrations/providers/notion) Obsidian (/docs/integrations/providers/obsidian) OpenAI (/docs/integrations/providers/openai) OpenLLM (/docs/integrations/providers/openllm) OpenSearch (/docs/integrations/providers/opensearch) OpenWeatherMap (/docs/integrations/providers/openweathermap) Petals (/docs/integrations/providers/petals) Postgres Embedding (/docs/integrations/providers/pg_embedding) PGVector (/docs/integrations/providers/pgvector) Pinecone (/docs/integrations/providers/pinecone) PipelineAI (/docs/integrations/providers/pipelineai) Portkey (/docs/integrations/providers/portkey/) Predibase (/docs/integrations/providers/predibase) Prediction Guard (/docs/integrations/providers/predictionguard) PromptLayer (/docs/integrations/providers/promptlayer) Psychic (/docs/integrations/providers/psychic) PubMed (/docs/integrations/providers/pubmed) Qdrant (/docs/integrations/providers/qdrant) Ray Serve (/docs/integrations/providers/ray_serve) Rebuff (/docs/integrations/providers/rebuff) Reddit (/docs/integrations/providers/reddit) Redis (/docs/integrations/providers/redis) Replicate (/docs/integrations/providers/replicate) Roam (/docs/integrations/providers/roam) Rockset (/docs/integrations/providers/rockset) Runhouse (/docs/integrations/providers/runhouse) RWKV-4 (/docs/integrations/providers/rwkv) SageMaker Endpoint (/docs/integrations/providers/sagemaker_endpoint) SageMaker Tracking (/docs/integrations/providers/sagemaker_tracking) ScaNN (/docs/integrations/providers/scann) SearxNG Search API (/docs/integrations/providers/searx) SerpAPI (/docs/integrations/providers/serpapi) Shale Protocol (/docs/integrations/providers/shaleprotocol) SingleStoreDB (/docs/integrations/providers/singlestoredb) scikit-learn (/docs/integrations/providers/sklearn) Slack (/docs/integrations/providers/slack) spaCy (/docs/integrations/providers/spacy) Spreedly (/docs/integrations/providers/spreedly) StarRocks (/docs/integrations/providers/starrocks) StochasticAI (/docs/integrations/providers/stochasticai) Stripe (/docs/integrations/providers/stripe) Supabase (Postgres) (/docs/integrations/providers/supabase) Nebula (/docs/integrations/providers/symblai_nebula) Tair (/docs/integrations/providers/tair) Telegram (/docs/integrations/providers/telegram) TencentVectorDB (/docs/integrations/providers/tencentvectordb) TensorFlow Datasets (/docs/integrations/providers/tensorflow_datasets) Tigris (/docs/integrations/providers/tigris) 2Markdown (/docs/integrations/providers/tomarkdown) Trello (/docs/integrations/providers/trello) TruLens (/docs/integrations/providers/trulens) Twitter (/docs/integrations/providers/twitter) Typesense (/docs/integrations/providers/typesense) Unstructured (/docs/integrations/providers/unstructured) USearch (/docs/integrations/providers/usearch) Vearch (/docs/integrations/providers/vearch) Vectara (/docs/integrations/providers/vectara/) Chat Over Documents with Vectara (/docs/integrations/providers/vectara/vectara_chat) Vectara Text Generation (/docs/integrations/providers/vectara/vectara_text_generation) Vespa (/docs/integrations/providers/vespa) WandB Tracing (/docs/integrations/providers/wandb_tracing) Weights & Biases (/docs/integrations/providers/wandb_tracking) Weather (/docs/integrations/providers/weather) Weaviate (/docs/integrations/providers/weaviate) WhatsApp (/docs/integrations/providers/whatsapp) WhyLabs (/docs/integrations/providers/whylabs_profiling) Wikipedia (/docs/integrations/providers/wikipedia) Wolfram Alpha (/docs/integrations/providers/wolfram_alpha) Writer (/docs/integrations/providers/writer) Xata (/docs/integrations/providers/xata) Xorbits Inference (Xinference) (/docs/integrations/providers/xinference) Yeager.ai (/docs/integrations/providers/yeagerai) YouTube (/docs/integrations/providers/youtube) Zep (/docs/integrations/providers/zep) Zilliz (/docs/integrations/providers/zilliz)  (/) Integrations (/docs/integrations) Grouped by provider (/docs/integrations/providers/) Vectara (/docs/integrations/providers/vectara/) chat_vector_db (https://github.com/hwchase17/langchain/blob/master/docs/modules/chains/index_examples/chat_vector_db.html) Vectara (https://api.python.langchain.com/en/latest/vectorstores/langchain.vectorstores.vectara.Vectara.html) VectaraRetriever (https://api.python.langchain.com/en/latest/vectorstores/langchain.vectorstores.vectara.VectaraRetriever.html) OpenAI (https://api.python.langchain.com/en/latest/llms/langchain.llms.openai.OpenAI.html) ConversationalRetrievalChain (https://api.python.langchain.com/en/latest/chains/langchain.chains.conversational_retrieval.base.ConversationalRetrievalChain.html) TextLoader (https://api.python.langchain.com/en/latest/document_loaders/langchain.document_loaders.text.TextLoader.html) ConversationBufferMemory (https://api.python.langchain.com/en/latest/memory/langchain.memory.buffer.ConversationBufferMemory.html) â€‹ (#pass-in-chat-history) â€‹ (#return-source-documents) â€‹ (#conversationalretrievalchain-with-search_distance) â€‹ (#conversationalretrievalchain-with-map_reduce) LLMChain (https://api.python.langchain.com/en/latest/chains/langchain.chains.llm.LLMChain.html) load_qa_chain (https://api.python.langchain.com/en/latest/chains/langchain.chains.question_answering.load_qa_chain.html) CONDENSE_QUESTION_PROMPT (https://api.python.langchain.com/en/latest/prompts/langchain.prompts.prompt.CONDENSE_QUESTION_PROMPT.html) â€‹ (#conversationalretrievalchain-with-question-answering-with-sources) load_qa_with_sources_chain (https://api.python.langchain.com/en/latest/chains/langchain.chains.qa_with_sources.loading.load_qa_with_sources_chain.html) â€‹ (#conversationalretrievalchain-with-streaming-to-stdout) LLMChain (https://api.python.langchain.com/en/latest/chains/langchain.chains.llm.LLMChain.html) StreamingStdOutCallbackHandler (https://api.python.langchain.com/en/latest/callbacks/langchain.callbacks.streaming_stdout.StreamingStdOutCallbackHandler.html) CONDENSE_QUESTION_PROMPT (https://api.python.langchain.com/en/latest/prompts/langchain.prompts.prompt.CONDENSE_QUESTION_PROMPT.html) QA_PROMPT (https://api.python.langchain.com/en/latest/prompts/langchain.prompts.prompt.QA_PROMPT.html) load_qa_chain (https://api.python.langchain.com/en/latest/chains/langchain.chains.question_answering.load_qa_chain.html) â€‹ (#get_chat_history-function) PreviousVectara (/docs/integrations/providers/vectara/) NextVectara Text Generation (/docs/integrations/providers/vectara/vectara_text_generation) Pass in chat history (#pass-in-chat-history) Return Source Documents (#return-source-documents) ConversationalRetrievalChain with search_distance (#conversationalretrievalchain-with-search_distance) ConversationalRetrievalChain with map_reduce (#conversationalretrievalchain-with-map_reduce) ConversationalRetrievalChain with Question Answering with sources (#conversationalretrievalchain-with-question-answering-with-sources) ConversationalRetrievalChain with streaming to stdout (#conversationalretrievalchain-with-streaming-to-stdout) get_chat_history Function (#get_chat_history-function) Discord (https://discord.gg/cU2adEyC7w) Twitter (https://twitter.com/LangChainAI) Python (https://github.com/hwchase17/langchain) JS/TS (https://github.com/hwchase17/langchainjs) Homepage (https://langchain.com) Blog (https://blog.langchain.dev)