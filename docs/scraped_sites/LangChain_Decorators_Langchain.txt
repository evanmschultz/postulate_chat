lanchchain decorators is a layer on the top of LangChain that provides syntactic sugar üç≠ for writing custom langchain prompts and chains For Feedback, Issues, Contributions - please raise an issue here: ju-bezdek/langchain-decorators Main principles and benefits: Here is a simple example of a code written with LangChain Decorators ‚ú® Good idea on how to start is to review the examples here: Here we are just marking a function as a prompt with llm_prompt decorator, turning it effectively into a LLMChain. Instead of running it  Standard LLMchain takes much more init parameter than just inputs_variables and prompt... here is this implementation detail hidden in the decorator. Here is how it works: To pass any of these, just declare them in the function (or use kwargs to pass anything) If we want to leverage streaming: This way we just mark which prompt should be streamed, not needing to tinker with what LLM should we use, passing around the creating and distribute streaming handler into particular part of our chain... just turn the streaming on/off on prompt/prompt type... The streaming will happen only if we call it in streaming context ... there we can define a simple function to handle the stream By default the prompt is is the whole function docs, unless you mark your prompt  We can specify what part of our docs is the prompt definition, by specifying a code block with <prompt> language tag For chat models is very useful to define prompt as a set of message templates... here is how to do it: the roles here are model native roles (assistant, user, system for chatGPT) the syntax for this is as follows: for dict / pydantic you need to specify the formatting instructions... this can be tedious, that's why you can let the output parser gegnerate you the instructions based on the model (pydantic) IntegrationsCallbacksChat modelsChat loadersDocument loadersDocument transformersLLMsMemoryRetrieversText embedding modelsAgents & ToolkitsToolsVector storesGrouped by providerActiveloop Deep LakeAI21 LabsAimAINetworkAirbyteAirtableAleph AlphaAlibaba Cloud OpensearchAmazon API GatewayAnalyticDBAnnoyAnyscaleApifyArangoDBArgillaArthurArxivAtlasAwaDBAWS S3 DirectoryAZLyricsAzure Blob StorageAzure Cognitive SearchAzure OpenAIBagelDBBananaBasetenBeamBedrockBiliBiliNIBittensorBlackboardBrave SearchCassandraCerebriumAIChaindeskChromaClarifaiClearMLClickHouseCnosDBCohereCollege ConfidentialCometConfident AIConfluenceC TransformersDashVectorDatabricksDatadog TracingDatadog LogsDataForSEODeepInfraDeepSparseDiffbotDingoDiscordDocArrayDocugamiDuckDBElasticsearchEpsillaEverNoteFacebook ChatFacebook FaissFigmaFireworksFlyteForefrontAIGitGitBookGoldenGoogle BigQueryGoogle Cloud StorageGoogle DriveGoogle SearchGoogle SerperGoogle Vertex AI MatchingEngineGooseAIGPT4AllGraphsignalGrobidGutenbergHacker NewsHazy ResearchHeliconeHologresHugging FaceiFixitIMSDbInfinoJinaKonkoLanceDBLangChain Decorators ‚ú®Llama.cppLog10MarqoMediaWikiDumpMeilisearchMetalMicrosoft OneDriveMicrosoft PowerPointMicrosoft WordMilvusMinimaxMLflow AI GatewayMLflowModalModelScopeModern TreasuryMomentoMongoDB AtlasMotherduckMyScaleNeo4jNLPCloudNotion DBObsidianOpenAIOpenLLMOpenSearchOpenWeatherMapPetalsPostgres EmbeddingPGVectorPineconePipelineAIPortkeyPredibasePrediction GuardPromptLayerPsychicPubMedQdrantRay ServeRebuffRedditRedisReplicateRoamRocksetRunhouseRWKV-4SageMaker EndpointSageMaker TrackingScaNNSearxNG Search APISerpAPIShale ProtocolSingleStoreDBscikit-learnSlackspaCySpreedlyStarRocksStochasticAIStripeSupabase (Postgres)NebulaTairTelegramTencentVectorDBTensorFlow DatasetsTigris2MarkdownTrelloTruLensTwitterTypesenseUnstructuredUSearchVearchVectaraVespaWandB TracingWeights & BiasesWeatherWeaviateWhatsAppWhyLabsWikipediaWolfram AlphaWriterXataXorbits Inference (Xinference)Yeager.aiYouTubeZepZilliz Callbacks Chat models Chat loaders Document loaders Document transformers LLMs Memory Retrievers Text embedding models Agents & Toolkits Tools Vector stores Grouped by providerActiveloop Deep LakeAI21 LabsAimAINetworkAirbyteAirtableAleph AlphaAlibaba Cloud OpensearchAmazon API GatewayAnalyticDBAnnoyAnyscaleApifyArangoDBArgillaArthurArxivAtlasAwaDBAWS S3 DirectoryAZLyricsAzure Blob StorageAzure Cognitive SearchAzure OpenAIBagelDBBananaBasetenBeamBedrockBiliBiliNIBittensorBlackboardBrave SearchCassandraCerebriumAIChaindeskChromaClarifaiClearMLClickHouseCnosDBCohereCollege ConfidentialCometConfident AIConfluenceC TransformersDashVectorDatabricksDatadog TracingDatadog LogsDataForSEODeepInfraDeepSparseDiffbotDingoDiscordDocArrayDocugamiDuckDBElasticsearchEpsillaEverNoteFacebook ChatFacebook FaissFigmaFireworksFlyteForefrontAIGitGitBookGoldenGoogle BigQueryGoogle Cloud StorageGoogle DriveGoogle SearchGoogle SerperGoogle Vertex AI MatchingEngineGooseAIGPT4AllGraphsignalGrobidGutenbergHacker NewsHazy ResearchHeliconeHologresHugging FaceiFixitIMSDbInfinoJinaKonkoLanceDBLangChain Decorators ‚ú®Llama.cppLog10MarqoMediaWikiDumpMeilisearchMetalMicrosoft OneDriveMicrosoft PowerPointMicrosoft WordMilvusMinimaxMLflow AI GatewayMLflowModalModelScopeModern TreasuryMomentoMongoDB AtlasMotherduckMyScaleNeo4jNLPCloudNotion DBObsidianOpenAIOpenLLMOpenSearchOpenWeatherMapPetalsPostgres EmbeddingPGVectorPineconePipelineAIPortkeyPredibasePrediction GuardPromptLayerPsychicPubMedQdrantRay ServeRebuffRedditRedisReplicateRoamRocksetRunhouseRWKV-4SageMaker EndpointSageMaker TrackingScaNNSearxNG Search APISerpAPIShale ProtocolSingleStoreDBscikit-learnSlackspaCySpreedlyStarRocksStochasticAIStripeSupabase (Postgres)NebulaTairTelegramTencentVectorDBTensorFlow DatasetsTigris2MarkdownTrelloTruLensTwitterTypesenseUnstructuredUSearchVearchVectaraVespaWandB TracingWeights & BiasesWeatherWeaviateWhatsAppWhyLabsWikipediaWolfram AlphaWriterXataXorbits Inference (Xinference)Yeager.aiYouTubeZepZilliz Activeloop Deep Lake AI21 Labs Aim AINetwork Airbyte Airtable Aleph Alpha Alibaba Cloud Opensearch Amazon API Gateway AnalyticDB Annoy Anyscale Apify ArangoDB Argilla Arthur Arxiv Atlas AwaDB AWS S3 Directory AZLyrics Azure Blob Storage Azure Cognitive Search Azure OpenAI BagelDB Banana Baseten Beam Bedrock BiliBili NIBittensor Blackboard Brave Search Cassandra CerebriumAI Chaindesk Chroma Clarifai ClearML ClickHouse CnosDB Cohere College Confidential Comet Confident AI Confluence C Transformers DashVector Databricks Datadog Tracing Datadog Logs DataForSEO DeepInfra DeepSparse Diffbot Dingo Discord DocArray Docugami DuckDB Elasticsearch Epsilla EverNote Facebook Chat Facebook Faiss Figma Fireworks Flyte ForefrontAI Git GitBook Golden Google BigQuery Google Cloud Storage Google Drive Google Search Google Serper Google Vertex AI MatchingEngine GooseAI GPT4All Graphsignal Grobid Gutenberg Hacker News Hazy Research Helicone Hologres Hugging Face iFixit IMSDb Infino Jina Konko LanceDB LangChain Decorators ‚ú® Llama.cpp Log10 Marqo MediaWikiDump Meilisearch Metal Microsoft OneDrive Microsoft PowerPoint Microsoft Word Milvus Minimax MLflow AI Gateway MLflow Modal ModelScope Modern Treasury Momento MongoDB Atlas Motherduck MyScale Neo4j NLPCloud Notion DB Obsidian OpenAI OpenLLM OpenSearch OpenWeatherMap Petals Postgres Embedding PGVector Pinecone PipelineAI Portkey Predibase Prediction Guard PromptLayer Psychic PubMed Qdrant Ray Serve Rebuff Reddit Redis Replicate Roam Rockset Runhouse RWKV-4 SageMaker Endpoint SageMaker Tracking ScaNN SearxNG Search API SerpAPI Shale Protocol SingleStoreDB scikit-learn Slack spaCy Spreedly StarRocks StochasticAI Stripe Supabase (Postgres) Nebula Tair Telegram TencentVectorDB TensorFlow Datasets Tigris 2Markdown Trello TruLens Twitter Typesense Unstructured USearch Vearch Vectara Vespa WandB Tracing Weights & Biases Weather Weaviate WhatsApp WhyLabs Wikipedia Wolfram Alpha Writer Xata Xorbits Inference (Xinference) Yeager.ai YouTube Zep Zilliz  Integrations Grouped by provider LangChain Decorators ‚ú® more pythonic way of writing code write multiline prompts that won't break your code flow with indentation making use of IDE in-built support for hinting, type checking and popup with docs to quickly peek in the function to see the prompt, parameters it consumes etc. leverage all the power of ü¶úüîó LangChain ecosystem adding support for optional parameters easily share parameters between the prompts by binding them to one class jupyter notebook colab notebook Using Global settings: Using predefined prompt types Define the settings directly in the decorator we need to define prompt as async function  turn on the streaming on the decorator, or we can define PromptType with streaming on capture the stream using StreamingContext you can define a whole sections of your prompt that should be optional if any input in the section is missing, the whole section won't be rendered llm_prompt decorator natively tries to detect the best output parser based on the output type. (if not set, it returns the raw string) list, dict and pydantic outputs are also supported natively (automatically) these and few more examples are also available in the colab notebook here including the ReAct Agent re-implementation using purely langchain decorators Installation Examples Passing a memory and/or callbacks: Documenting your prompt Chat messages prompt More complex structures Discord Twitter Python JS/TS Homepage Blog Skip to main contentü¶úÔ∏èüîó LangChainDocsUse casesIntegrationsAPILangSmithJS/TS DocsCTRLKIntegrationsCallbacksChat modelsChat loadersDocument loadersDocument transformersLLMsMemoryRetrieversText embedding modelsAgents & ToolkitsToolsVector storesGrouped by providerActiveloop Deep LakeAI21 LabsAimAINetworkAirbyteAirtableAleph AlphaAlibaba Cloud OpensearchAmazon API GatewayAnalyticDBAnnoyAnyscaleApifyArangoDBArgillaArthurArxivAtlasAwaDBAWS S3 DirectoryAZLyricsAzure Blob StorageAzure Cognitive SearchAzure OpenAIBagelDBBananaBasetenBeamBedrockBiliBiliNIBittensorBlackboardBrave SearchCassandraCerebriumAIChaindeskChromaClarifaiClearMLClickHouseCnosDBCohereCollege ConfidentialCometConfident AIConfluenceC TransformersDashVectorDatabricksDatadog TracingDatadog LogsDataForSEODeepInfraDeepSparseDiffbotDingoDiscordDocArrayDocugamiDuckDBElasticsearchEpsillaEverNoteFacebook ChatFacebook FaissFigmaFireworksFlyteForefrontAIGitGitBookGoldenGoogle BigQueryGoogle Cloud StorageGoogle DriveGoogle SearchGoogle SerperGoogle Vertex AI MatchingEngineGooseAIGPT4AllGraphsignalGrobidGutenbergHacker NewsHazy ResearchHeliconeHologresHugging FaceiFixitIMSDbInfinoJinaKonkoLanceDBLangChain Decorators ‚ú®Llama.cppLog10MarqoMediaWikiDumpMeilisearchMetalMicrosoft OneDriveMicrosoft PowerPointMicrosoft WordMilvusMinimaxMLflow AI GatewayMLflowModalModelScopeModern TreasuryMomentoMongoDB AtlasMotherduckMyScaleNeo4jNLPCloudNotion DBObsidianOpenAIOpenLLMOpenSearchOpenWeatherMapPetalsPostgres EmbeddingPGVectorPineconePipelineAIPortkeyPredibasePrediction GuardPromptLayerPsychicPubMedQdrantRay ServeRebuffRedditRedisReplicateRoamRocksetRunhouseRWKV-4SageMaker EndpointSageMaker TrackingScaNNSearxNG Search APISerpAPIShale ProtocolSingleStoreDBscikit-learnSlackspaCySpreedlyStarRocksStochasticAIStripeSupabase (Postgres)NebulaTairTelegramTencentVectorDBTensorFlow DatasetsTigris2MarkdownTrelloTruLensTwitterTypesenseUnstructuredUSearchVearchVectaraVespaWandB TracingWeights & BiasesWeatherWeaviateWhatsAppWhyLabsWikipediaWolfram AlphaWriterXataXorbits Inference (Xinference)Yeager.aiYouTubeZepZillizIntegrationsGrouped by providerLangChain Decorators ‚ú®On this pageLangChain Decorators ‚ú®lanchchain decorators is a layer on the top of LangChain that provides syntactic sugar üç≠ for writing custom langchain prompts and chainsFor Feedback, Issues, Contributions - please raise an issue here: ju-bezdek/langchain-decoratorsMain principles and benefits:more pythonic way of writing codewrite multiline prompts that won't break your code flow with indentationmaking use of IDE in-built support for hinting, type checking and popup with docs to quickly peek in the function to see the prompt, parameters it consumes etc.leverage all the power of ü¶úüîó LangChain ecosystemadding support for optional parameterseasily share parameters between the prompts by binding them to one classHere is a simple example of a code written with LangChain Decorators ‚ú®@llm_promptdef write_me_short_post(topic:str, platform:str="twitter", audience:str = "developers")->str:    """    Write me a short header for my post about {topic} for {platform} platform.     It should be for {audience} audience.    (Max 15 words)    """    return# run it naturallywrite_me_short_post(topic="starwars")# orwrite_me_short_post(topic="starwars", platform="redit")Quick startInstallation‚Äãpip install langchain_decoratorsExamples‚ÄãGood idea on how to start is to review the examples here:jupyter notebookcolab notebookDefining other parametersHere we are just marking a function as a prompt with llm_prompt decorator, turning it effectively into a LLMChain. Instead of running it Standard LLMchain takes much more init parameter than just inputs_variables and prompt... here is this implementation detail hidden in the decorator. Here is how it works:Using Global settings:# define global settings for all prompty (if not set - chatGPT is the current default)from langchain_decorators import GlobalSettingsGlobalSettings.define_settings(    default_llm=ChatOpenAI(temperature=0.0), this is default... can change it here globally    default_streaming_llm=ChatOpenAI(temperature=0.0,streaming=True), this is default... can change it here for all ... will be used for streaming)Using predefined prompt types#You can change the default prompt typesfrom langchain_decorators import PromptTypes, PromptTypeSettingsPromptTypes.AGENT_REASONING.llm = ChatOpenAI()# Or you can just define your own ones:class MyCustomPromptTypes(PromptTypes):    GPT4=PromptTypeSettings(llm=ChatOpenAI(model="gpt-4"))@llm_prompt(prompt_type=MyCustomPromptTypes.GPT4) def write_a_complicated_code(app_idea:str)->str:    ...Define the settings directly in the decoratorfrom langchain.llms import OpenAI@llm_prompt(    llm=OpenAI(temperature=0.7),    stop_tokens=["\nObservation"],    ...    )def creative_writer(book_title:str)->str:    ...Passing a memory and/or callbacks:‚ÄãTo pass any of these, just declare them in the function (or use kwargs to pass anything)@llm_prompt()async def write_me_short_post(topic:str, platform:str="twitter", memory:SimpleMemory = None):    """    {history_key}    Write me a short header for my post about {topic} for {platform} platform.     It should be for {audience} audience.    (Max 15 words)    """    passawait write_me_short_post(topic="old movies")Simplified streamingIf we want to leverage streaming:we need to define prompt as async function turn on the streaming on the decorator, or we can define PromptType with streaming oncapture the stream using StreamingContextThis way we just mark which prompt should be streamed, not needing to tinker with what LLM should we use, passing around the creating and distribute streaming handler into particular part of our chain... just turn the streaming on/off on prompt/prompt type...The streaming will happen only if we call it in streaming context ... there we can define a simple function to handle the stream# this code example is complete and should run as it isfrom langchain_decorators import StreamingContext, llm_prompt# this will mark the prompt for streaming (useful if we want stream just some prompts in our app... but don't want to pass distribute the callback handlers)# note that only async functions can be streamed (will get an error if it's not)@llm_prompt(capture_stream=True) async def write_me_short_post(topic:str, platform:str="twitter", audience:str = "developers"):    """    Write me a short header for my post about {topic} for {platform} platform.     It should be for {audience} audience.    (Max 15 words)    """    pass# just an arbitrary  function to demonstrate the streaming... will be some websockets code in the real worldtokens=[]def capture_stream_func(new_token:str):    tokens.append(new_token)# if we want to capture the stream, we need to wrap the execution into StreamingContext... # this will allow us to capture the stream even if the prompt call is hidden inside higher level method# only the prompts marked with capture_stream will be captured herewith StreamingContext(stream_to_stdout=True, callback=capture_stream_func):    result = await run_prompt()    print("Stream finished ... we can distinguish tokens thanks to alternating colors")print("\nWe've captured",len(tokens),"tokensüéâ\n")print("Here is the result:")print(result)Prompt declarationsBy default the prompt is is the whole function docs, unless you mark your prompt Documenting your prompt‚ÄãWe can specify what part of our docs is the prompt definition, by specifying a code block with <prompt> language tag@llm_promptdef write_me_short_post(topic:str, platform:str="twitter", audience:str = "developers"):    """    Here is a good way to write a prompt as part of a function docstring, with additional documentation for devs.    It needs to be a code block, marked as a `<prompt>` language    ```<prompt>    Write me a short header for my post about {topic} for {platform} platform.     It should be for {audience} audience.    (Max 15 words)    ```    Now only to code block above will be used as a prompt, and the rest of the docstring will be used as a description for developers.    (It has also a nice benefit that IDE (like VS code) will display the prompt properly (not trying to parse it as markdown, and thus not showing new lines properly))    """    return Chat messages prompt‚ÄãFor chat models is very useful to define prompt as a set of message templates... here is how to do it:@llm_promptdef simulate_conversation(human_input:str, agent_role:str="a pirate"):    """    ## System message     - note the `:system` sufix inside the <prompt:_role_> tag         ```<prompt:system>    You are a {agent_role} hacker. You mus act like one.    You reply always in code, using python or javascript code block...    for example:        ... do not reply with anything else.. just with code - respecting your role.    ```    # human message     (we are using the real role that are enforced by the LLM - GPT supports system, assistant, user)    ``` <prompt:user>    Helo, who are you    ```    a reply:        ``` <prompt:assistant>    \``` python <<- escaping inner code block with \ that should be part of the prompt    def hello():        print("Argh... hello you pesky pirate")    \```    ```        we can also add some history using placeholder    ```<prompt:placeholder>    {history}    ```    ```<prompt:user>    {human_input}    ```    Now only to code block above will be used as a prompt, and the rest of the docstring will be used as a description for developers.    (It has also a nice benefit that IDE (like VS code) will display the prompt properly (not trying to parse it as markdown, and thus not showing new lines properly))    """    passthe roles here are model native roles (assistant, user, system for chatGPT)Optional sectionsyou can define a whole sections of your prompt that should be optionalif any input in the section is missing, the whole section won't be renderedthe syntax for this is as follows:@llm_promptdef prompt_with_optional_partials():    """    this text will be rendered always, but    {? anything inside this block will be rendered only if all the {value}s parameters are not empty (None | "")   ?}    you can also place it in between the words    this too will be rendered{? , but        this  block will be rendered only if {this_value} and {this_value}        is not empty?} !    """Output parsersllm_prompt decorator natively tries to detect the best output parser based on the output type. (if not set, it returns the raw string)list, dict and pydantic outputs are also supported natively (automatically)# this code example is complete and should run as it isfrom langchain_decorators import llm_prompt@llm_promptdef write_name_suggestions(company_business:str, count:int)->list:    """ Write me {count} good name suggestions for company that {company_business}    """    passwrite_name_suggestions(company_business="sells cookies", count=5)More complex structures‚Äãfor dict / pydantic you need to specify the formatting instructions... this can be tedious, that's why you can let the output parser gegnerate you the instructions based on the model (pydantic)from langchain_decorators import llm_promptfrom pydantic import BaseModel, Fieldclass TheOutputStructureWeExpect(BaseModel):    name:str = Field (description="The name of the company")    headline:str = Field( description="The description of the company (for landing page)")    employees:list[str] = Field(description="5-8 fake employee names with their positions")@llm_prompt()def fake_company_generator(company_business:str)->TheOutputStructureWeExpect:    """ Generate a fake company that {company_business}    {FORMAT_INSTRUCTIONS}    """    returncompany = fake_company_generator(company_business="sells cookies")# print the result nicely formattedprint("Company name: ",company.name)print("company headline: ",company.headline)print("company employees: ",company.employees)Binding the prompt to an objectfrom pydantic import BaseModelfrom langchain_decorators import llm_promptclass AssistantPersonality(BaseModel):    assistant_name:str    assistant_role:str    field:str    @property    def a_property(self):        return "whatever"    def hello_world(self, function_kwarg:str=None):        """        We can reference any {field} or {a_property} inside our prompt... and combine it with {function_kwarg} in the method        """        @llm_prompt    def introduce_your_self(self)->str:        """        ```¬†<prompt:system>        You are an assistant named {assistant_name}.         Your role is to act as {assistant_role}        ```        ```<prompt:user>        Introduce your self (in less than 20 words)        ```        """    personality = AssistantPersonality(assistant_name="John", assistant_role="a pirate")print(personality.introduce_your_self(personality))More examples:these and few more examples are also available in the colab notebook hereincluding the ReAct Agent re-implementation using purely langchain decoratorsPreviousLanceDBNextLlama.cppInstallationExamplesPassing a memory and/or callbacks:Documenting your promptChat messages promptMore complex structuresCommunityDiscordTwitterGitHubPythonJS/TSMoreHomepageBlogCopyright ¬© 2023 LangChain, Inc. Skip to main content ü¶úÔ∏èüîó LangChainDocsUse casesIntegrationsAPILangSmithJS/TS DocsCTRLK ü¶úÔ∏èüîó LangChainDocsUse casesIntegrationsAPI LangSmithJS/TS DocsCTRLK  CTRLK CTRLK  CTRLK   IntegrationsCallbacksChat modelsChat loadersDocument loadersDocument transformersLLMsMemoryRetrieversText embedding modelsAgents & ToolkitsToolsVector storesGrouped by providerActiveloop Deep LakeAI21 LabsAimAINetworkAirbyteAirtableAleph AlphaAlibaba Cloud OpensearchAmazon API GatewayAnalyticDBAnnoyAnyscaleApifyArangoDBArgillaArthurArxivAtlasAwaDBAWS S3 DirectoryAZLyricsAzure Blob StorageAzure Cognitive SearchAzure OpenAIBagelDBBananaBasetenBeamBedrockBiliBiliNIBittensorBlackboardBrave SearchCassandraCerebriumAIChaindeskChromaClarifaiClearMLClickHouseCnosDBCohereCollege ConfidentialCometConfident AIConfluenceC TransformersDashVectorDatabricksDatadog TracingDatadog LogsDataForSEODeepInfraDeepSparseDiffbotDingoDiscordDocArrayDocugamiDuckDBElasticsearchEpsillaEverNoteFacebook ChatFacebook FaissFigmaFireworksFlyteForefrontAIGitGitBookGoldenGoogle BigQueryGoogle Cloud StorageGoogle DriveGoogle SearchGoogle SerperGoogle Vertex AI MatchingEngineGooseAIGPT4AllGraphsignalGrobidGutenbergHacker NewsHazy ResearchHeliconeHologresHugging FaceiFixitIMSDbInfinoJinaKonkoLanceDBLangChain Decorators ‚ú®Llama.cppLog10MarqoMediaWikiDumpMeilisearchMetalMicrosoft OneDriveMicrosoft PowerPointMicrosoft WordMilvusMinimaxMLflow AI GatewayMLflowModalModelScopeModern TreasuryMomentoMongoDB AtlasMotherduckMyScaleNeo4jNLPCloudNotion DBObsidianOpenAIOpenLLMOpenSearchOpenWeatherMapPetalsPostgres EmbeddingPGVectorPineconePipelineAIPortkeyPredibasePrediction GuardPromptLayerPsychicPubMedQdrantRay ServeRebuffRedditRedisReplicateRoamRocksetRunhouseRWKV-4SageMaker EndpointSageMaker TrackingScaNNSearxNG Search APISerpAPIShale ProtocolSingleStoreDBscikit-learnSlackspaCySpreedlyStarRocksStochasticAIStripeSupabase (Postgres)NebulaTairTelegramTencentVectorDBTensorFlow DatasetsTigris2MarkdownTrelloTruLensTwitterTypesenseUnstructuredUSearchVearchVectaraVespaWandB TracingWeights & BiasesWeatherWeaviateWhatsAppWhyLabsWikipediaWolfram AlphaWriterXataXorbits Inference (Xinference)Yeager.aiYouTubeZepZillizIntegrationsGrouped by providerLangChain Decorators ‚ú®On this pageLangChain Decorators ‚ú®lanchchain decorators is a layer on the top of LangChain that provides syntactic sugar üç≠ for writing custom langchain prompts and chainsFor Feedback, Issues, Contributions - please raise an issue here: this can be tedious, that's why you can let the output parser gegnerate you the instructions based on the model (pydantic)from langchain_decorators import llm_promptfrom pydantic import BaseModel, Fieldclass TheOutputStructureWeExpect(BaseModel):    name:str = Field (description="The name of the company")    headline:str = Field( description="The description of the company (for landing page)")    employees:list[str] = Field(description="5-8 fake employee names with their positions")@llm_prompt()def fake_company_generator(company_business:str)->TheOutputStructureWeExpect:    """ Generate a fake company that {company_business}    {FORMAT_INSTRUCTIONS}    """    returncompany = fake_company_generator(company_business="sells cookies")# print the result nicely formattedprint("Company name: ",company.name)print("company headline: ",company.headline)print("company employees: ",company.employees)Binding the prompt to an objectfrom pydantic import BaseModelfrom langchain_decorators import llm_promptclass AssistantPersonality(BaseModel):    assistant_name:str    assistant_role:str    field:str    @property    def a_property(self):        return "whatever"    def hello_world(self, function_kwarg:str=None):        """        We can reference any {field} or {a_property} inside our prompt... and combine it with {function_kwarg} in the method        """        @llm_prompt    def introduce_your_self(self)->str:        """        ```¬†<prompt:system>        You are an assistant named {assistant_name}.         Your role is to act as {assistant_role}        ```        ```<prompt:user>        Introduce your self (in less than 20 words)        ```        """    personality = AssistantPersonality(assistant_name="John", assistant_role="a pirate")print(personality.introduce_your_self(personality))More examples:these and few more examples are also available in the colab notebook hereincluding the ReAct Agent re-implementation using purely langchain decoratorsPreviousLanceDBNextLlama.cppInstallationExamplesPassing a memory and/or callbacks:Documenting your promptChat messages promptMore complex structures IntegrationsCallbacksChat modelsChat loadersDocument loadersDocument transformersLLMsMemoryRetrieversText embedding modelsAgents & ToolkitsToolsVector storesGrouped by providerActiveloop Deep LakeAI21 LabsAimAINetworkAirbyteAirtableAleph AlphaAlibaba Cloud OpensearchAmazon API GatewayAnalyticDBAnnoyAnyscaleApifyArangoDBArgillaArthurArxivAtlasAwaDBAWS S3 DirectoryAZLyricsAzure Blob StorageAzure Cognitive SearchAzure OpenAIBagelDBBananaBasetenBeamBedrockBiliBiliNIBittensorBlackboardBrave SearchCassandraCerebriumAIChaindeskChromaClarifaiClearMLClickHouseCnosDBCohereCollege ConfidentialCometConfident AIConfluenceC TransformersDashVectorDatabricksDatadog TracingDatadog LogsDataForSEODeepInfraDeepSparseDiffbotDingoDiscordDocArrayDocugamiDuckDBElasticsearchEpsillaEverNoteFacebook ChatFacebook FaissFigmaFireworksFlyteForefrontAIGitGitBookGoldenGoogle BigQueryGoogle Cloud StorageGoogle DriveGoogle SearchGoogle SerperGoogle Vertex AI MatchingEngineGooseAIGPT4AllGraphsignalGrobidGutenbergHacker NewsHazy ResearchHeliconeHologresHugging FaceiFixitIMSDbInfinoJinaKonkoLanceDBLangChain Decorators ‚ú®Llama.cppLog10MarqoMediaWikiDumpMeilisearchMetalMicrosoft OneDriveMicrosoft PowerPointMicrosoft WordMilvusMinimaxMLflow AI GatewayMLflowModalModelScopeModern TreasuryMomentoMongoDB AtlasMotherduckMyScaleNeo4jNLPCloudNotion DBObsidianOpenAIOpenLLMOpenSearchOpenWeatherMapPetalsPostgres EmbeddingPGVectorPineconePipelineAIPortkeyPredibasePrediction GuardPromptLayerPsychicPubMedQdrantRay ServeRebuffRedditRedisReplicateRoamRocksetRunhouseRWKV-4SageMaker EndpointSageMaker TrackingScaNNSearxNG Search APISerpAPIShale ProtocolSingleStoreDBscikit-learnSlackspaCySpreedlyStarRocksStochasticAIStripeSupabase (Postgres)NebulaTairTelegramTencentVectorDBTensorFlow DatasetsTigris2MarkdownTrelloTruLensTwitterTypesenseUnstructuredUSearchVearchVectaraVespaWandB TracingWeights & BiasesWeatherWeaviateWhatsAppWhyLabsWikipediaWolfram AlphaWriterXataXorbits Inference (Xinference)Yeager.aiYouTubeZepZillizIntegrationsGrouped by providerLangChain Decorators ‚ú®On this pageLangChain Decorators ‚ú®lanchchain decorators is a layer on the top of LangChain that provides syntactic sugar üç≠ for writing custom langchain prompts and chainsFor Feedback, Issues, Contributions - please raise an issue here: this can be tedious, that's why you can let the output parser gegnerate you the instructions based on the model (pydantic)from langchain_decorators import llm_promptfrom pydantic import BaseModel, Fieldclass TheOutputStructureWeExpect(BaseModel):    name:str = Field (description="The name of the company")    headline:str = Field( description="The description of the company (for landing page)")    employees:list[str] = Field(description="5-8 fake employee names with their positions")@llm_prompt()def fake_company_generator(company_business:str)->TheOutputStructureWeExpect:    """ Generate a fake company that {company_business}    {FORMAT_INSTRUCTIONS}    """    returncompany = fake_company_generator(company_business="sells cookies")# print the result nicely formattedprint("Company name: ",company.name)print("company headline: ",company.headline)print("company employees: ",company.employees)Binding the prompt to an objectfrom pydantic import BaseModelfrom langchain_decorators import llm_promptclass AssistantPersonality(BaseModel):    assistant_name:str    assistant_role:str    field:str    @property    def a_property(self):        return "whatever"    def hello_world(self, function_kwarg:str=None):        """        We can reference any {field} or {a_property} inside our prompt... and combine it with {function_kwarg} in the method        """        @llm_prompt    def introduce_your_self(self)->str:        """        ```¬†<prompt:system>        You are an assistant named {assistant_name}.         Your role is to act as {assistant_role}        ```        ```<prompt:user>        Introduce your self (in less than 20 words)        ```        """    personality = AssistantPersonality(assistant_name="John", assistant_role="a pirate")print(personality.introduce_your_self(personality))More examples:these and few more examples are also available in the colab notebook hereincluding the ReAct Agent re-implementation using purely langchain decoratorsPreviousLanceDBNextLlama.cppInstallationExamplesPassing a memory and/or callbacks:Documenting your promptChat messages promptMore complex structures IntegrationsCallbacksChat modelsChat loadersDocument loadersDocument transformersLLMsMemoryRetrieversText embedding modelsAgents & ToolkitsToolsVector storesGrouped by providerActiveloop Deep LakeAI21 LabsAimAINetworkAirbyteAirtableAleph AlphaAlibaba Cloud OpensearchAmazon API GatewayAnalyticDBAnnoyAnyscaleApifyArangoDBArgillaArthurArxivAtlasAwaDBAWS S3 DirectoryAZLyricsAzure Blob StorageAzure Cognitive SearchAzure OpenAIBagelDBBananaBasetenBeamBedrockBiliBiliNIBittensorBlackboardBrave SearchCassandraCerebriumAIChaindeskChromaClarifaiClearMLClickHouseCnosDBCohereCollege ConfidentialCometConfident AIConfluenceC TransformersDashVectorDatabricksDatadog TracingDatadog LogsDataForSEODeepInfraDeepSparseDiffbotDingoDiscordDocArrayDocugamiDuckDBElasticsearchEpsillaEverNoteFacebook ChatFacebook FaissFigmaFireworksFlyteForefrontAIGitGitBookGoldenGoogle BigQueryGoogle Cloud StorageGoogle DriveGoogle SearchGoogle SerperGoogle Vertex AI MatchingEngineGooseAIGPT4AllGraphsignalGrobidGutenbergHacker NewsHazy ResearchHeliconeHologresHugging FaceiFixitIMSDbInfinoJinaKonkoLanceDBLangChain Decorators ‚ú®Llama.cppLog10MarqoMediaWikiDumpMeilisearchMetalMicrosoft OneDriveMicrosoft PowerPointMicrosoft WordMilvusMinimaxMLflow AI GatewayMLflowModalModelScopeModern TreasuryMomentoMongoDB AtlasMotherduckMyScaleNeo4jNLPCloudNotion DBObsidianOpenAIOpenLLMOpenSearchOpenWeatherMapPetalsPostgres EmbeddingPGVectorPineconePipelineAIPortkeyPredibasePrediction GuardPromptLayerPsychicPubMedQdrantRay ServeRebuffRedditRedisReplicateRoamRocksetRunhouseRWKV-4SageMaker EndpointSageMaker TrackingScaNNSearxNG Search APISerpAPIShale ProtocolSingleStoreDBscikit-learnSlackspaCySpreedlyStarRocksStochasticAIStripeSupabase (Postgres)NebulaTairTelegramTencentVectorDBTensorFlow DatasetsTigris2MarkdownTrelloTruLensTwitterTypesenseUnstructuredUSearchVearchVectaraVespaWandB TracingWeights & BiasesWeatherWeaviateWhatsAppWhyLabsWikipediaWolfram AlphaWriterXataXorbits Inference (Xinference)Yeager.aiYouTubeZepZilliz IntegrationsCallbacksChat modelsChat loadersDocument loadersDocument transformersLLMsMemoryRetrieversText embedding modelsAgents & ToolkitsToolsVector storesGrouped by providerActiveloop Deep LakeAI21 LabsAimAINetworkAirbyteAirtableAleph AlphaAlibaba Cloud OpensearchAmazon API GatewayAnalyticDBAnnoyAnyscaleApifyArangoDBArgillaArthurArxivAtlasAwaDBAWS S3 DirectoryAZLyricsAzure Blob StorageAzure Cognitive SearchAzure OpenAIBagelDBBananaBasetenBeamBedrockBiliBiliNIBittensorBlackboardBrave SearchCassandraCerebriumAIChaindeskChromaClarifaiClearMLClickHouseCnosDBCohereCollege ConfidentialCometConfident AIConfluenceC TransformersDashVectorDatabricksDatadog TracingDatadog LogsDataForSEODeepInfraDeepSparseDiffbotDingoDiscordDocArrayDocugamiDuckDBElasticsearchEpsillaEverNoteFacebook ChatFacebook FaissFigmaFireworksFlyteForefrontAIGitGitBookGoldenGoogle BigQueryGoogle Cloud StorageGoogle DriveGoogle SearchGoogle SerperGoogle Vertex AI MatchingEngineGooseAIGPT4AllGraphsignalGrobidGutenbergHacker NewsHazy ResearchHeliconeHologresHugging FaceiFixitIMSDbInfinoJinaKonkoLanceDBLangChain Decorators ‚ú®Llama.cppLog10MarqoMediaWikiDumpMeilisearchMetalMicrosoft OneDriveMicrosoft PowerPointMicrosoft WordMilvusMinimaxMLflow AI GatewayMLflowModalModelScopeModern TreasuryMomentoMongoDB AtlasMotherduckMyScaleNeo4jNLPCloudNotion DBObsidianOpenAIOpenLLMOpenSearchOpenWeatherMapPetalsPostgres EmbeddingPGVectorPineconePipelineAIPortkeyPredibasePrediction GuardPromptLayerPsychicPubMedQdrantRay ServeRebuffRedditRedisReplicateRoamRocksetRunhouseRWKV-4SageMaker EndpointSageMaker TrackingScaNNSearxNG Search APISerpAPIShale ProtocolSingleStoreDBscikit-learnSlackspaCySpreedlyStarRocksStochasticAIStripeSupabase (Postgres)NebulaTairTelegramTencentVectorDBTensorFlow DatasetsTigris2MarkdownTrelloTruLensTwitterTypesenseUnstructuredUSearchVearchVectaraVespaWandB TracingWeights & BiasesWeatherWeaviateWhatsAppWhyLabsWikipediaWolfram AlphaWriterXataXorbits Inference (Xinference)Yeager.aiYouTubeZepZilliz Integrations Callbacks Chat models Chat loaders Document loaders Document transformers LLMs Memory Retrievers Text embedding models Agents & Toolkits Tools Vector stores Grouped by provider Portkey Vectara IntegrationsGrouped by providerLangChain Decorators ‚ú®On this pageLangChain Decorators ‚ú®lanchchain decorators is a layer on the top of LangChain that provides syntactic sugar üç≠ for writing custom langchain prompts and chainsFor Feedback, Issues, Contributions - please raise an issue here: this can be tedious, that's why you can let the output parser gegnerate you the instructions based on the model (pydantic)from langchain_decorators import llm_promptfrom pydantic import BaseModel, Fieldclass TheOutputStructureWeExpect(BaseModel):    name:str = Field (description="The name of the company")    headline:str = Field( description="The description of the company (for landing page)")    employees:list[str] = Field(description="5-8 fake employee names with their positions")@llm_prompt()def fake_company_generator(company_business:str)->TheOutputStructureWeExpect:    """ Generate a fake company that {company_business}    {FORMAT_INSTRUCTIONS}    """    returncompany = fake_company_generator(company_business="sells cookies")# print the result nicely formattedprint("Company name: ",company.name)print("company headline: ",company.headline)print("company employees: ",company.employees)Binding the prompt to an objectfrom pydantic import BaseModelfrom langchain_decorators import llm_promptclass AssistantPersonality(BaseModel):    assistant_name:str    assistant_role:str    field:str    @property    def a_property(self):        return "whatever"    def hello_world(self, function_kwarg:str=None):        """        We can reference any {field} or {a_property} inside our prompt... and combine it with {function_kwarg} in the method        """        @llm_prompt    def introduce_your_self(self)->str:        """        ```¬†<prompt:system>        You are an assistant named {assistant_name}.         Your role is to act as {assistant_role}        ```        ```<prompt:user>        Introduce your self (in less than 20 words)        ```        """    personality = AssistantPersonality(assistant_name="John", assistant_role="a pirate")print(personality.introduce_your_self(personality))More examples:these and few more examples are also available in the colab notebook hereincluding the ReAct Agent re-implementation using purely langchain decoratorsPreviousLanceDBNextLlama.cppInstallationExamplesPassing a memory and/or callbacks:Documenting your promptChat messages promptMore complex structures IntegrationsGrouped by providerLangChain Decorators ‚ú®On this pageLangChain Decorators ‚ú®lanchchain decorators is a layer on the top of LangChain that provides syntactic sugar üç≠ for writing custom langchain prompts and chainsFor Feedback, Issues, Contributions - please raise an issue here: this can be tedious, that's why you can let the output parser gegnerate you the instructions based on the model (pydantic)from langchain_decorators import llm_promptfrom pydantic import BaseModel, Fieldclass TheOutputStructureWeExpect(BaseModel):    name:str = Field (description="The name of the company")    headline:str = Field( description="The description of the company (for landing page)")    employees:list[str] = Field(description="5-8 fake employee names with their positions")@llm_prompt()def fake_company_generator(company_business:str)->TheOutputStructureWeExpect:    """ Generate a fake company that {company_business}    {FORMAT_INSTRUCTIONS}    """    returncompany = fake_company_generator(company_business="sells cookies")# print the result nicely formattedprint("Company name: ",company.name)print("company headline: ",company.headline)print("company employees: ",company.employees)Binding the prompt to an objectfrom pydantic import BaseModelfrom langchain_decorators import llm_promptclass AssistantPersonality(BaseModel):    assistant_name:str    assistant_role:str    field:str    @property    def a_property(self):        return "whatever"    def hello_world(self, function_kwarg:str=None):        """        We can reference any {field} or {a_property} inside our prompt... and combine it with {function_kwarg} in the method        """        @llm_prompt    def introduce_your_self(self)->str:        """        ```¬†<prompt:system>        You are an assistant named {assistant_name}.         Your role is to act as {assistant_role}        ```        ```<prompt:user>        Introduce your self (in less than 20 words)        ```        """    personality = AssistantPersonality(assistant_name="John", assistant_role="a pirate")print(personality.introduce_your_self(personality))More examples:these and few more examples are also available in the colab notebook hereincluding the ReAct Agent re-implementation using purely langchain decoratorsPreviousLanceDBNextLlama.cpp IntegrationsGrouped by providerLangChain Decorators ‚ú®On this pageLangChain Decorators ‚ú®lanchchain decorators is a layer on the top of LangChain that provides syntactic sugar üç≠ for writing custom langchain prompts and chainsFor Feedback, Issues, Contributions - please raise an issue here: this can be tedious, that's why you can let the output parser gegnerate you the instructions based on the model (pydantic)from langchain_decorators import llm_promptfrom pydantic import BaseModel, Fieldclass TheOutputStructureWeExpect(BaseModel):    name:str = Field (description="The name of the company")    headline:str = Field( description="The description of the company (for landing page)")    employees:list[str] = Field(description="5-8 fake employee names with their positions")@llm_prompt()def fake_company_generator(company_business:str)->TheOutputStructureWeExpect:    """ Generate a fake company that {company_business}    {FORMAT_INSTRUCTIONS}    """    returncompany = fake_company_generator(company_business="sells cookies")# print the result nicely formattedprint("Company name: ",company.name)print("company headline: ",company.headline)print("company employees: ",company.employees)Binding the prompt to an objectfrom pydantic import BaseModelfrom langchain_decorators import llm_promptclass AssistantPersonality(BaseModel):    assistant_name:str    assistant_role:str    field:str    @property    def a_property(self):        return "whatever"    def hello_world(self, function_kwarg:str=None):        """        We can reference any {field} or {a_property} inside our prompt... and combine it with {function_kwarg} in the method        """        @llm_prompt    def introduce_your_self(self)->str:        """        ```¬†<prompt:system>        You are an assistant named {assistant_name}.         Your role is to act as {assistant_role}        ```        ```<prompt:user>        Introduce your self (in less than 20 words)        ```        """    personality = AssistantPersonality(assistant_name="John", assistant_role="a pirate")print(personality.introduce_your_self(personality))More examples:these and few more examples are also available in the colab notebook hereincluding the ReAct Agent re-implementation using purely langchain decoratorsPreviousLanceDBNextLlama.cpp On this page LangChain Decorators ‚ú®lanchchain decorators is a layer on the top of LangChain that provides syntactic sugar üç≠ for writing custom langchain prompts and chainsFor Feedback, Issues, Contributions - please raise an issue here: this can be tedious, that's why you can let the output parser gegnerate you the instructions based on the model (pydantic)from langchain_decorators import llm_promptfrom pydantic import BaseModel, Fieldclass TheOutputStructureWeExpect(BaseModel):    name:str = Field (description="The name of the company")    headline:str = Field( description="The description of the company (for landing page)")    employees:list[str] = Field(description="5-8 fake employee names with their positions")@llm_prompt()def fake_company_generator(company_business:str)->TheOutputStructureWeExpect:    """ Generate a fake company that {company_business}    {FORMAT_INSTRUCTIONS}    """    returncompany = fake_company_generator(company_business="sells cookies")# print the result nicely formattedprint("Company name: ",company.name)print("company headline: ",company.headline)print("company employees: ",company.employees)Binding the prompt to an objectfrom pydantic import BaseModelfrom langchain_decorators import llm_promptclass AssistantPersonality(BaseModel):    assistant_name:str    assistant_role:str    field:str    @property    def a_property(self):        return "whatever"    def hello_world(self, function_kwarg:str=None):        """        We can reference any {field} or {a_property} inside our prompt... and combine it with {function_kwarg} in the method        """        @llm_prompt    def introduce_your_self(self)->str:        """        ```¬†<prompt:system>        You are an assistant named {assistant_name}.         Your role is to act as {assistant_role}        ```        ```<prompt:user>        Introduce your self (in less than 20 words)        ```        """    personality = AssistantPersonality(assistant_name="John", assistant_role="a pirate")print(personality.introduce_your_self(personality))More examples:these and few more examples are also available in the colab notebook hereincluding the ReAct Agent re-implementation using purely langchain decorators @llm_promptdef write_me_short_post(topic:str, platform:str="twitter", audience:str = "developers")->str:    """    Write me a short header for my post about {topic} for {platform} platform.     It should be for {audience} audience.    (Max 15 words)    """    return# run it naturallywrite_me_short_post(topic="starwars")# orwrite_me_short_post(topic="starwars", platform="redit") @llm_promptdef write_me_short_post(topic:str, platform:str="twitter", audience:str = "developers")->str:    """    Write me a short header for my post about {topic} for {platform} platform.     It should be for {audience} audience.    (Max 15 words)    """    return# run it naturallywrite_me_short_post(topic="starwars")# orwrite_me_short_post(topic="starwars", platform="redit")  pip install langchain_decorators pip install langchain_decorators  # define global settings for all prompty (if not set - chatGPT is the current default)from langchain_decorators import GlobalSettingsGlobalSettings.define_settings(    default_llm=ChatOpenAI(temperature=0.0), this is default... can change it here globally    default_streaming_llm=ChatOpenAI(temperature=0.0,streaming=True), this is default... can change it here for all ... will be used for streaming) # define global settings for all prompty (if not set - chatGPT is the current default)from langchain_decorators import GlobalSettingsGlobalSettings.define_settings(    default_llm=ChatOpenAI(temperature=0.0), this is default... can change it here globally    default_streaming_llm=ChatOpenAI(temperature=0.0,streaming=True), this is default... can change it here for all ... will be used for streaming)  #You can change the default prompt typesfrom langchain_decorators import PromptTypes, PromptTypeSettingsPromptTypes.AGENT_REASONING.llm = ChatOpenAI()# Or you can just define your own ones:class MyCustomPromptTypes(PromptTypes):    GPT4=PromptTypeSettings(llm=ChatOpenAI(model="gpt-4"))@llm_prompt(prompt_type=MyCustomPromptTypes.GPT4) def write_a_complicated_code(app_idea:str)->str:    ... #You can change the default prompt typesfrom langchain_decorators import PromptTypes, PromptTypeSettingsPromptTypes.AGENT_REASONING.llm = ChatOpenAI()# Or you can just define your own ones:class MyCustomPromptTypes(PromptTypes):    GPT4=PromptTypeSettings(llm=ChatOpenAI(model="gpt-4"))@llm_prompt(prompt_type=MyCustomPromptTypes.GPT4) def write_a_complicated_code(app_idea:str)->str:    ...  from langchain.llms import OpenAI@llm_prompt(    llm=OpenAI(temperature=0.7),    stop_tokens=["\nObservation"],    ...    )def creative_writer(book_title:str)->str:    ... from langchain.llms import OpenAI@llm_prompt(    llm=OpenAI(temperature=0.7),    stop_tokens=["\nObservation"],    ...    )def creative_writer(book_title:str)->str:    ...  @llm_prompt()async def write_me_short_post(topic:str, platform:str="twitter", memory:SimpleMemory = None):    """    {history_key}    Write me a short header for my post about {topic} for {platform} platform.     It should be for {audience} audience.    (Max 15 words)    """    passawait write_me_short_post(topic="old movies") @llm_prompt()async def write_me_short_post(topic:str, platform:str="twitter", memory:SimpleMemory = None):    """    {history_key}    Write me a short header for my post about {topic} for {platform} platform.     It should be for {audience} audience.    (Max 15 words)    """    passawait write_me_short_post(topic="old movies")  # this code example is complete and should run as it isfrom langchain_decorators import StreamingContext, llm_prompt# this will mark the prompt for streaming (useful if we want stream just some prompts in our app... but don't want to pass distribute the callback handlers)# note that only async functions can be streamed (will get an error if it's not)@llm_prompt(capture_stream=True) async def write_me_short_post(topic:str, platform:str="twitter", audience:str = "developers"):    """    Write me a short header for my post about {topic} for {platform} platform.     It should be for {audience} audience.    (Max 15 words)    """    pass# just an arbitrary  function to demonstrate the streaming... will be some websockets code in the real worldtokens=[]def capture_stream_func(new_token:str):    tokens.append(new_token)# if we want to capture the stream, we need to wrap the execution into StreamingContext... # this will allow us to capture the stream even if the prompt call is hidden inside higher level method# only the prompts marked with capture_stream will be captured herewith StreamingContext(stream_to_stdout=True, callback=capture_stream_func):    result = await run_prompt()    print("Stream finished ... we can distinguish tokens thanks to alternating colors")print("\nWe've captured",len(tokens),"tokensüéâ\n")print("Here is the result:")print(result) # this code example is complete and should run as it isfrom langchain_decorators import StreamingContext, llm_prompt# this will mark the prompt for streaming (useful if we want stream just some prompts in our app... but don't want to pass distribute the callback handlers)# note that only async functions can be streamed (will get an error if it's not)@llm_prompt(capture_stream=True) async def write_me_short_post(topic:str, platform:str="twitter", audience:str = "developers"):    """    Write me a short header for my post about {topic} for {platform} platform.     It should be for {audience} audience.    (Max 15 words)    """    pass# just an arbitrary  function to demonstrate the streaming... will be some websockets code in the real worldtokens=[]def capture_stream_func(new_token:str):    tokens.append(new_token)# if we want to capture the stream, we need to wrap the execution into StreamingContext... # this will allow us to capture the stream even if the prompt call is hidden inside higher level method# only the prompts marked with capture_stream will be captured herewith StreamingContext(stream_to_stdout=True, callback=capture_stream_func):    result = await run_prompt()    print("Stream finished ... we can distinguish tokens thanks to alternating colors")print("\nWe've captured",len(tokens),"tokensüéâ\n")print("Here is the result:")print(result)  @llm_promptdef write_me_short_post(topic:str, platform:str="twitter", audience:str = "developers"):    """    Here is a good way to write a prompt as part of a function docstring, with additional documentation for devs.    It needs to be a code block, marked as a `<prompt>` language    ```<prompt>    Write me a short header for my post about {topic} for {platform} platform.     It should be for {audience} audience.    (Max 15 words)    ```    Now only to code block above will be used as a prompt, and the rest of the docstring will be used as a description for developers.    (It has also a nice benefit that IDE (like VS code) will display the prompt properly (not trying to parse it as markdown, and thus not showing new lines properly))    """    return  @llm_promptdef write_me_short_post(topic:str, platform:str="twitter", audience:str = "developers"):    """    Here is a good way to write a prompt as part of a function docstring, with additional documentation for devs.    It needs to be a code block, marked as a `<prompt>` language    ```<prompt>    Write me a short header for my post about {topic} for {platform} platform.     It should be for {audience} audience.    (Max 15 words)    ```    Now only to code block above will be used as a prompt, and the rest of the docstring will be used as a description for developers.    (It has also a nice benefit that IDE (like VS code) will display the prompt properly (not trying to parse it as markdown, and thus not showing new lines properly))    """    return   @llm_promptdef simulate_conversation(human_input:str, agent_role:str="a pirate"):    """    ## System message     - note the `:system` sufix inside the <prompt:_role_> tag         ```<prompt:system>    You are a {agent_role} hacker. You mus act like one.    You reply always in code, using python or javascript code block...    for example:        ... do not reply with anything else.. just with code - respecting your role.    ```    # human message     (we are using the real role that are enforced by the LLM - GPT supports system, assistant, user)    ``` <prompt:user>    Helo, who are you    ```    a reply:        ``` <prompt:assistant>    \``` python <<- escaping inner code block with \ that should be part of the prompt    def hello():        print("Argh... hello you pesky pirate")    \```    ```        we can also add some history using placeholder    ```<prompt:placeholder>    {history}    ```    ```<prompt:user>    {human_input}    ```    Now only to code block above will be used as a prompt, and the rest of the docstring will be used as a description for developers.    (It has also a nice benefit that IDE (like VS code) will display the prompt properly (not trying to parse it as markdown, and thus not showing new lines properly))    """    pass @llm_promptdef simulate_conversation(human_input:str, agent_role:str="a pirate"):    """    ## System message     - note the `:system` sufix inside the <prompt:_role_> tag         ```<prompt:system>    You are a {agent_role} hacker. You mus act like one.    You reply always in code, using python or javascript code block...    for example:        ... do not reply with anything else.. just with code - respecting your role.    ```    # human message     (we are using the real role that are enforced by the LLM - GPT supports system, assistant, user)    ``` <prompt:user>    Helo, who are you    ```    a reply:        ``` <prompt:assistant>    \``` python <<- escaping inner code block with \ that should be part of the prompt    def hello():        print("Argh... hello you pesky pirate")    \```    ```        we can also add some history using placeholder    ```<prompt:placeholder>    {history}    ```    ```<prompt:user>    {human_input}    ```    Now only to code block above will be used as a prompt, and the rest of the docstring will be used as a description for developers.    (It has also a nice benefit that IDE (like VS code) will display the prompt properly (not trying to parse it as markdown, and thus not showing new lines properly))    """    pass  @llm_promptdef prompt_with_optional_partials():    """    this text will be rendered always, but    {? anything inside this block will be rendered only if all the {value}s parameters are not empty (None | "")   ?}    you can also place it in between the words    this too will be rendered{? , but        this  block will be rendered only if {this_value} and {this_value}        is not empty?} !    """ @llm_promptdef prompt_with_optional_partials():    """    this text will be rendered always, but    {? anything inside this block will be rendered only if all the {value}s parameters are not empty (None | "")   ?}    you can also place it in between the words    this too will be rendered{? , but        this  block will be rendered only if {this_value} and {this_value}        is not empty?} !    """  # this code example is complete and should run as it isfrom langchain_decorators import llm_prompt@llm_promptdef write_name_suggestions(company_business:str, count:int)->list:    """ Write me {count} good name suggestions for company that {company_business}    """    passwrite_name_suggestions(company_business="sells cookies", count=5) # this code example is complete and should run as it isfrom langchain_decorators import llm_prompt@llm_promptdef write_name_suggestions(company_business:str, count:int)->list:    """ Write me {count} good name suggestions for company that {company_business}    """    passwrite_name_suggestions(company_business="sells cookies", count=5)  from langchain_decorators import llm_promptfrom pydantic import BaseModel, Fieldclass TheOutputStructureWeExpect(BaseModel):    name:str = Field (description="The name of the company")    headline:str = Field( description="The description of the company (for landing page)")    employees:list[str] = Field(description="5-8 fake employee names with their positions")@llm_prompt()def fake_company_generator(company_business:str)->TheOutputStructureWeExpect:    """ Generate a fake company that {company_business}    {FORMAT_INSTRUCTIONS}    """    returncompany = fake_company_generator(company_business="sells cookies")# print the result nicely formattedprint("Company name: ",company.name)print("company headline: ",company.headline)print("company employees: ",company.employees) from langchain_decorators import llm_promptfrom pydantic import BaseModel, Fieldclass TheOutputStructureWeExpect(BaseModel):    name:str = Field (description="The name of the company")    headline:str = Field( description="The description of the company (for landing page)")    employees:list[str] = Field(description="5-8 fake employee names with their positions")@llm_prompt()def fake_company_generator(company_business:str)->TheOutputStructureWeExpect:    """ Generate a fake company that {company_business}    {FORMAT_INSTRUCTIONS}    """    returncompany = fake_company_generator(company_business="sells cookies")# print the result nicely formattedprint("Company name: ",company.name)print("company headline: ",company.headline)print("company employees: ",company.employees)  from pydantic import BaseModelfrom langchain_decorators import llm_promptclass AssistantPersonality(BaseModel):    assistant_name:str    assistant_role:str    field:str    @property    def a_property(self):        return "whatever"    def hello_world(self, function_kwarg:str=None):        """        We can reference any {field} or {a_property} inside our prompt... and combine it with {function_kwarg} in the method        """        @llm_prompt    def introduce_your_self(self)->str:        """        ```¬†<prompt:system>        You are an assistant named {assistant_name}.         Your role is to act as {assistant_role}        ```        ```<prompt:user>        Introduce your self (in less than 20 words)        ```        """    personality = AssistantPersonality(assistant_name="John", assistant_role="a pirate")print(personality.introduce_your_self(personality)) from pydantic import BaseModelfrom langchain_decorators import llm_promptclass AssistantPersonality(BaseModel):    assistant_name:str    assistant_role:str    field:str    @property    def a_property(self):        return "whatever"    def hello_world(self, function_kwarg:str=None):        """        We can reference any {field} or {a_property} inside our prompt... and combine it with {function_kwarg} in the method        """        @llm_prompt    def introduce_your_self(self)->str:        """        ```¬†<prompt:system>        You are an assistant named {assistant_name}.         Your role is to act as {assistant_role}        ```        ```<prompt:user>        Introduce your self (in less than 20 words)        ```        """    personality = AssistantPersonality(assistant_name="John", assistant_role="a pirate")print(personality.introduce_your_self(personality))  Previous LanceDB Next Llama.cpp InstallationExamplesPassing a memory and/or callbacks:Documenting your promptChat messages promptMore complex structures InstallationExamplesPassing a memory and/or callbacks:Documenting your promptChat messages promptMore complex structures CommunityDiscordTwitterGitHubPythonJS/TSMoreHomepageBlogCopyright ¬© 2023 LangChain, Inc. CommunityDiscordTwitterGitHubPythonJS/TSMoreHomepageBlog CommunityDiscordTwitter Community GitHubPythonJS/TS GitHub MoreHomepageBlog More Copyright ¬© 2023 LangChain, Inc. Copyright ¬© 2023 LangChain, Inc. Skip to main content (#docusaurus_skipToContent_fallback) ü¶úÔ∏èüîó LangChain (/) Docs (/docs/get_started/introduction) Use cases (/docs/use_cases) Integrations (/docs/integrations) API (https://api.python.langchain.com) LangSmith (https://smith.langchain.com) JS/TS Docs (https://js.langchain.com/docs)  (https://github.com/hwchase17/langchain) Integrations (/docs/integrations) Callbacks (/docs/integrations/callbacks/) Chat models (/docs/integrations/chat/) Chat loaders (/docs/integrations/chat_loaders/) Document loaders (/docs/integrations/document_loaders/) Document transformers (/docs/integrations/document_transformers/) LLMs (/docs/integrations/llms/) Memory (/docs/integrations/memory/) Retrievers (/docs/integrations/retrievers/) Text embedding models (/docs/integrations/text_embedding/) Agents & Toolkits (/docs/integrations/toolkits/) Tools (/docs/integrations/tools/) Vector stores (/docs/integrations/vectorstores/) Grouped by provider (/docs/integrations/providers/) Activeloop Deep Lake (/docs/integrations/providers/activeloop_deeplake) AI21 Labs (/docs/integrations/providers/ai21) Aim (/docs/integrations/providers/aim_tracking) AINetwork (/docs/integrations/providers/ainetwork) Airbyte (/docs/integrations/providers/airbyte) Airtable (/docs/integrations/providers/airtable) Aleph Alpha (/docs/integrations/providers/aleph_alpha) Alibaba Cloud Opensearch (/docs/integrations/providers/alibabacloud_opensearch) Amazon API Gateway (/docs/integrations/providers/amazon_api_gateway) AnalyticDB (/docs/integrations/providers/analyticdb) Annoy (/docs/integrations/providers/annoy) Anyscale (/docs/integrations/providers/anyscale) Apify (/docs/integrations/providers/apify) ArangoDB (/docs/integrations/providers/arangodb) Argilla (/docs/integrations/providers/argilla) Arthur (/docs/integrations/providers/arthur_tracking) Arxiv (/docs/integrations/providers/arxiv) Atlas (/docs/integrations/providers/atlas) AwaDB (/docs/integrations/providers/awadb) AWS S3 Directory (/docs/integrations/providers/aws_s3) AZLyrics (/docs/integrations/providers/azlyrics) Azure Blob Storage (/docs/integrations/providers/azure_blob_storage) Azure Cognitive Search (/docs/integrations/providers/azure_cognitive_search_) Azure OpenAI (/docs/integrations/providers/azure_openai) BagelDB (/docs/integrations/providers/bageldb) Banana (/docs/integrations/providers/bananadev) Baseten (/docs/integrations/providers/baseten) Beam (/docs/integrations/providers/beam) Bedrock (/docs/integrations/providers/bedrock) BiliBili (/docs/integrations/providers/bilibili) NIBittensor (/docs/integrations/providers/bittensor) Blackboard (/docs/integrations/providers/blackboard) Brave Search (/docs/integrations/providers/brave_search) Cassandra (/docs/integrations/providers/cassandra) CerebriumAI (/docs/integrations/providers/cerebriumai) Chaindesk (/docs/integrations/providers/chaindesk) Chroma (/docs/integrations/providers/chroma) Clarifai (/docs/integrations/providers/clarifai) ClearML (/docs/integrations/providers/clearml_tracking) ClickHouse (/docs/integrations/providers/clickhouse) CnosDB (/docs/integrations/providers/cnosdb) Cohere (/docs/integrations/providers/cohere) College Confidential (/docs/integrations/providers/college_confidential) Comet (/docs/integrations/providers/comet_tracking) Confident AI (/docs/integrations/providers/confident) Confluence (/docs/integrations/providers/confluence) C Transformers (/docs/integrations/providers/ctransformers) DashVector (/docs/integrations/providers/dashvector) Databricks (/docs/integrations/providers/databricks) Datadog Tracing (/docs/integrations/providers/datadog) Datadog Logs (/docs/integrations/providers/datadog_logs) DataForSEO (/docs/integrations/providers/dataforseo) DeepInfra (/docs/integrations/providers/deepinfra) DeepSparse (/docs/integrations/providers/deepsparse) Diffbot (/docs/integrations/providers/diffbot) Dingo (/docs/integrations/providers/dingo) Discord (/docs/integrations/providers/discord) DocArray (/docs/integrations/providers/docarray) Docugami (/docs/integrations/providers/docugami) DuckDB (/docs/integrations/providers/duckdb) Elasticsearch (/docs/integrations/providers/elasticsearch) Epsilla (/docs/integrations/providers/epsilla) EverNote (/docs/integrations/providers/evernote) Facebook Chat (/docs/integrations/providers/facebook_chat) Facebook Faiss (/docs/integrations/providers/facebook_faiss) Figma (/docs/integrations/providers/figma) Fireworks (/docs/integrations/providers/fireworks) Flyte (/docs/integrations/providers/flyte) ForefrontAI (/docs/integrations/providers/forefrontai) Git (/docs/integrations/providers/git) GitBook (/docs/integrations/providers/gitbook) Golden (/docs/integrations/providers/golden) Google BigQuery (/docs/integrations/providers/google_bigquery) Google Cloud Storage (/docs/integrations/providers/google_cloud_storage) Google Drive (/docs/integrations/providers/google_drive) Google Search (/docs/integrations/providers/google_search) Google Serper (/docs/integrations/providers/google_serper) Google Vertex AI MatchingEngine (/docs/integrations/providers/google_vertex_ai_matchingengine) GooseAI (/docs/integrations/providers/gooseai) GPT4All (/docs/integrations/providers/gpt4all) Graphsignal (/docs/integrations/providers/graphsignal) Grobid (/docs/integrations/providers/grobid) Gutenberg (/docs/integrations/providers/gutenberg) Hacker News (/docs/integrations/providers/hacker_news) Hazy Research (/docs/integrations/providers/hazy_research) Helicone (/docs/integrations/providers/helicone) Hologres (/docs/integrations/providers/hologres) Hugging Face (/docs/integrations/providers/huggingface) iFixit (/docs/integrations/providers/ifixit) IMSDb (/docs/integrations/providers/imsdb) Infino (/docs/integrations/providers/infino) Jina (/docs/integrations/providers/jina) Konko (/docs/integrations/providers/konko) LanceDB (/docs/integrations/providers/lancedb) LangChain Decorators ‚ú® (/docs/integrations/providers/langchain_decorators) Llama.cpp (/docs/integrations/providers/llamacpp) Log10 (/docs/integrations/providers/log10) Marqo (/docs/integrations/providers/marqo) MediaWikiDump (/docs/integrations/providers/mediawikidump) Meilisearch (/docs/integrations/providers/meilisearch) Metal (/docs/integrations/providers/metal) Microsoft OneDrive (/docs/integrations/providers/microsoft_onedrive) Microsoft PowerPoint (/docs/integrations/providers/microsoft_powerpoint) Microsoft Word (/docs/integrations/providers/microsoft_word) Milvus (/docs/integrations/providers/milvus) Minimax (/docs/integrations/providers/minimax) MLflow AI Gateway (/docs/integrations/providers/mlflow_ai_gateway) MLflow (/docs/integrations/providers/mlflow_tracking) Modal (/docs/integrations/providers/modal) ModelScope (/docs/integrations/providers/modelscope) Modern Treasury (/docs/integrations/providers/modern_treasury) Momento (/docs/integrations/providers/momento) MongoDB Atlas (/docs/integrations/providers/mongodb_atlas) Motherduck (/docs/integrations/providers/motherduck) MyScale (/docs/integrations/providers/myscale) Neo4j (/docs/integrations/providers/neo4j) NLPCloud (/docs/integrations/providers/nlpcloud) Notion DB (/docs/integrations/providers/notion) Obsidian (/docs/integrations/providers/obsidian) OpenAI (/docs/integrations/providers/openai) OpenLLM (/docs/integrations/providers/openllm) OpenSearch (/docs/integrations/providers/opensearch) OpenWeatherMap (/docs/integrations/providers/openweathermap) Petals (/docs/integrations/providers/petals) Postgres Embedding (/docs/integrations/providers/pg_embedding) PGVector (/docs/integrations/providers/pgvector) Pinecone (/docs/integrations/providers/pinecone) PipelineAI (/docs/integrations/providers/pipelineai) Portkey (/docs/integrations/providers/portkey/) Predibase (/docs/integrations/providers/predibase) Prediction Guard (/docs/integrations/providers/predictionguard) PromptLayer (/docs/integrations/providers/promptlayer) Psychic (/docs/integrations/providers/psychic) PubMed (/docs/integrations/providers/pubmed) Qdrant (/docs/integrations/providers/qdrant) Ray Serve (/docs/integrations/providers/ray_serve) Rebuff (/docs/integrations/providers/rebuff) Reddit (/docs/integrations/providers/reddit) Redis (/docs/integrations/providers/redis) Replicate (/docs/integrations/providers/replicate) Roam (/docs/integrations/providers/roam) Rockset (/docs/integrations/providers/rockset) Runhouse (/docs/integrations/providers/runhouse) RWKV-4 (/docs/integrations/providers/rwkv) SageMaker Endpoint (/docs/integrations/providers/sagemaker_endpoint) SageMaker Tracking (/docs/integrations/providers/sagemaker_tracking) ScaNN (/docs/integrations/providers/scann) SearxNG Search API (/docs/integrations/providers/searx) SerpAPI (/docs/integrations/providers/serpapi) Shale Protocol (/docs/integrations/providers/shaleprotocol) SingleStoreDB (/docs/integrations/providers/singlestoredb) scikit-learn (/docs/integrations/providers/sklearn) Slack (/docs/integrations/providers/slack) spaCy (/docs/integrations/providers/spacy) Spreedly (/docs/integrations/providers/spreedly) StarRocks (/docs/integrations/providers/starrocks) StochasticAI (/docs/integrations/providers/stochasticai) Stripe (/docs/integrations/providers/stripe) Supabase (Postgres) (/docs/integrations/providers/supabase) Nebula (/docs/integrations/providers/symblai_nebula) Tair (/docs/integrations/providers/tair) Telegram (/docs/integrations/providers/telegram) TencentVectorDB (/docs/integrations/providers/tencentvectordb) TensorFlow Datasets (/docs/integrations/providers/tensorflow_datasets) Tigris (/docs/integrations/providers/tigris) 2Markdown (/docs/integrations/providers/tomarkdown) Trello (/docs/integrations/providers/trello) TruLens (/docs/integrations/providers/trulens) Twitter (/docs/integrations/providers/twitter) Typesense (/docs/integrations/providers/typesense) Unstructured (/docs/integrations/providers/unstructured) USearch (/docs/integrations/providers/usearch) Vearch (/docs/integrations/providers/vearch) Vectara (/docs/integrations/providers/vectara/) Vespa (/docs/integrations/providers/vespa) WandB Tracing (/docs/integrations/providers/wandb_tracing) Weights & Biases (/docs/integrations/providers/wandb_tracking) Weather (/docs/integrations/providers/weather) Weaviate (/docs/integrations/providers/weaviate) WhatsApp (/docs/integrations/providers/whatsapp) WhyLabs (/docs/integrations/providers/whylabs_profiling) Wikipedia (/docs/integrations/providers/wikipedia) Wolfram Alpha (/docs/integrations/providers/wolfram_alpha) Writer (/docs/integrations/providers/writer) Xata (/docs/integrations/providers/xata) Xorbits Inference (Xinference) (/docs/integrations/providers/xinference) Yeager.ai (/docs/integrations/providers/yeagerai) YouTube (/docs/integrations/providers/youtube) Zep (/docs/integrations/providers/zep) Zilliz (/docs/integrations/providers/zilliz)  (/) Integrations (/docs/integrations) Grouped by provider (/docs/integrations/providers/) ju-bezdek/langchain-decorators (https://github.com/ju-bezdek/langchain-decorators) ‚Äã (#installation) ‚Äã (#examples) jupyter notebook (https://github.com/ju-bezdek/langchain-decorators/blob/main/example_notebook.ipynb) colab notebook (https://colab.research.google.com/drive/1no-8WfeP6JaLD9yUtkPgym6x0G9ZYZOG#scrollTo=N4cf__D0E2Yk) ‚Äã (#passing-a-memory-andor-callbacks) ‚Äã (#documenting-your-prompt) ‚Äã (#chat-messages-prompt) ‚Äã (#more-complex-structures) colab notebook here (https://colab.research.google.com/drive/1no-8WfeP6JaLD9yUtkPgym6x0G9ZYZOG#scrollTo=N4cf__D0E2Yk) ReAct Agent re-implementation (https://colab.research.google.com/drive/1no-8WfeP6JaLD9yUtkPgym6x0G9ZYZOG#scrollTo=3bID5fryE2Yp) PreviousLanceDB (/docs/integrations/providers/lancedb) NextLlama.cpp (/docs/integrations/providers/llamacpp) Installation (#installation) Examples (#examples) Passing a memory and/or callbacks: (#passing-a-memory-andor-callbacks) Documenting your prompt (#documenting-your-prompt) Chat messages prompt (#chat-messages-prompt) More complex structures (#more-complex-structures) Discord (https://discord.gg/cU2adEyC7w) Twitter (https://twitter.com/LangChainAI) Python (https://github.com/hwchase17/langchain) JS/TS (https://github.com/hwchase17/langchainjs) Homepage (https://langchain.com) Blog (https://blog.langchain.dev)