Postgres Embedding is an open-source vector similarity search for Postgres that uses  Hierarchical Navigable Small Worlds (HNSW) for approximate nearest neighbor search. It supports: This notebook shows how to use the Postgres vector database (PGEmbedding). The PGEmbedding integration creates the pg_embedding extension for you, but you run the following Postgres query to add it: Add the OpenAI API Key to the environment variables to use OpenAIEmbeddings. By default, the extension performs a sequential scan search, with 100% recall. You might consider creating an HNSW index for approximate nearest neighbor (ANN) search to speed up similarity_search_with_score execution time. To create the HNSW index on your vector column, use a create_hnsw_index function: The function above is equivalent to running the below SQL query: The HNSW index options used in the statement above include: maxelements: Defines the maximum number of elements indexed. This is a required parameter. The example shown above has a value of 3. A real-world example would have a much large value, such as 1000000. An "element" refers to a data point (a vector) in the dataset, which is represented as a node in the HNSW graph. Typically, you would set this option to a value able to accommodate the number of rows in your in your dataset. dims: Defines the number of dimensions in your vector data. This is a required parameter. A small value is used in the example above. If you are storing data generated using OpenAI's text-embedding-ada-002 model, which supports 1536 dimensions, you would define a value of 1536, for example. m: Defines the maximum number of bi-directional links (also referred to as "edges") created for each node during graph construction. The following additional index options are supported: efConstruction: Defines the number of nearest neighbors considered during index construction. The default value is 32. efsearch: Defines the number of nearest neighbors considered during index search. The default value is 32. For information about how you can configure these options to influence the HNSW algorithm, refer to Tuning the HNSW algorithm. IntegrationsCallbacksChat modelsChat loadersDocument loadersDocument transformersLLMsMemoryRetrieversText embedding modelsAgents & ToolkitsToolsVector storesActiveloop Deep LakeAlibaba Cloud OpenSearchAnalyticDBAnnoyAtlasAwaDBAzure Cognitive SearchBagelDBCassandraChromaClickHouseDashVectorDingoDocArray HnswSearchDocArray InMemorySearchElasticsearchEpsillaFaissHologresLanceDBMarqoGoogle Vertex AI MatchingEngineMeilisearchMilvusMongoDB AtlasMyScaleNeo4j Vector IndexNucliaDBOpenSearchPostgres EmbeddingPGVectorPineconeQdrantRedisRocksetScaNNSingleStoreDBscikit-learnsqlite-vssStarRocksSupabase (Postgres)TairTencent Cloud VectorDBTigrisTypesenseUSearchvearchVectaraWeaviateXataZepZillizGrouped by provider Callbacks Chat models Chat loaders Document loaders Document transformers LLMs Memory Retrievers Text embedding models Agents & Toolkits Tools Vector storesActiveloop Deep LakeAlibaba Cloud OpenSearchAnalyticDBAnnoyAtlasAwaDBAzure Cognitive SearchBagelDBCassandraChromaClickHouseDashVectorDingoDocArray HnswSearchDocArray InMemorySearchElasticsearchEpsillaFaissHologresLanceDBMarqoGoogle Vertex AI MatchingEngineMeilisearchMilvusMongoDB AtlasMyScaleNeo4j Vector IndexNucliaDBOpenSearchPostgres EmbeddingPGVectorPineconeQdrantRedisRocksetScaNNSingleStoreDBscikit-learnsqlite-vssStarRocksSupabase (Postgres)TairTencent Cloud VectorDBTigrisTypesenseUSearchvearchVectaraWeaviateXataZepZilliz Activeloop Deep Lake Alibaba Cloud OpenSearch AnalyticDB Annoy Atlas AwaDB Azure Cognitive Search BagelDB Cassandra Chroma ClickHouse DashVector Dingo DocArray HnswSearch DocArray InMemorySearch Elasticsearch Epsilla Faiss Hologres LanceDB Marqo Google Vertex AI MatchingEngine Meilisearch Milvus MongoDB Atlas MyScale Neo4j Vector Index NucliaDB OpenSearch Postgres Embedding PGVector Pinecone Qdrant Redis Rockset ScaNN SingleStoreDB scikit-learn sqlite-vss StarRocks Supabase (Postgres) Tair Tencent Cloud VectorDB Tigris Typesense USearch vearch Vectara Weaviate Xata Zep Zilliz Grouped by provider  Integrations Vector stores Postgres Embedding exact and approximate nearest neighbor search using HNSW L2 distance OpenAIEmbeddings CharacterTextSplitter PGEmbedding TextLoader Document maxelements: Defines the maximum number of elements indexed. This is a required parameter. The example shown above has a value of 3. A real-world example would have a much large value, such as 1000000. An "element" refers to a data point (a vector) in the dataset, which is represented as a node in the HNSW graph. Typically, you would set this option to a value able to accommodate the number of rows in your in your dataset. dims: Defines the number of dimensions in your vector data. This is a required parameter. A small value is used in the example above. If you are storing data generated using OpenAI's text-embedding-ada-002 model, which supports 1536 dimensions, you would define a value of 1536, for example. m: Defines the maximum number of bi-directional links (also referred to as "edges") created for each node during graph construction. For information about how you can configure these options to influence the HNSW algorithm, refer to Tuning the HNSW algorithm. Working with vectorstore in PostgresUploading a vectorstore in PGCreate HNSW IndexRetrieving a vectorstore in PG Uploading a vectorstore in PG Create HNSW Index Retrieving a vectorstore in PG Discord Twitter Python JS/TS Homepage Blog Skip to main contentü¶úÔ∏èüîó LangChainDocsUse casesIntegrationsAPILangSmithJS/TS DocsCTRLKIntegrationsCallbacksChat modelsChat loadersDocument loadersDocument transformersLLMsMemoryRetrieversText embedding modelsAgents & ToolkitsToolsVector storesActiveloop Deep LakeAlibaba Cloud OpenSearchAnalyticDBAnnoyAtlasAwaDBAzure Cognitive SearchBagelDBCassandraChromaClickHouseDashVectorDingoDocArray HnswSearchDocArray InMemorySearchElasticsearchEpsillaFaissHologresLanceDBMarqoGoogle Vertex AI MatchingEngineMeilisearchMilvusMongoDB AtlasMyScaleNeo4j Vector IndexNucliaDBOpenSearchPostgres EmbeddingPGVectorPineconeQdrantRedisRocksetScaNNSingleStoreDBscikit-learnsqlite-vssStarRocksSupabase (Postgres)TairTencent Cloud VectorDBTigrisTypesenseUSearchvearchVectaraWeaviateXataZepZillizGrouped by providerIntegrationsVector storesPostgres EmbeddingOn this pagePostgres EmbeddingPostgres Embedding is an open-source vector similarity search for Postgres that uses  Hierarchical Navigable Small Worlds (HNSW) for approximate nearest neighbor search.It supports:exact and approximate nearest neighbor search using HNSWL2 distanceThis notebook shows how to use the Postgres vector database (PGEmbedding).The PGEmbedding integration creates the pg_embedding extension for you, but you run the following Postgres query to add it:CREATE EXTENSION embedding;# Pip install necessary packagepip install openaipip install psycopg2-binarypip install tiktokenAdd the OpenAI API Key to the environment variables to use OpenAIEmbeddings.import osimport getpassos.environ["OPENAI_API_KEY"] = getpass.getpass("OpenAI API Key:")    OpenAI API Key:¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑## Loading Environment Variablesfrom typing import List, Tuplefrom langchain.embeddings.openai import OpenAIEmbeddingsfrom langchain.text_splitter import CharacterTextSplitterfrom langchain.vectorstores import PGEmbeddingfrom langchain.document_loaders import TextLoaderfrom langchain.docstore.document import DocumentAPI Reference:OpenAIEmbeddingsCharacterTextSplitterPGEmbeddingTextLoaderDocumentos.environ["DATABASE_URL"] = getpass.getpass("Database Url:")    Database Url:¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑loader = TextLoader("state_of_the_union.txt")documents = loader.load()text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)docs = text_splitter.split_documents(documents)embeddings = OpenAIEmbeddings()connection_string = os.environ.get("DATABASE_URL")collection_name = "state_of_the_union"db = PGEmbedding.from_documents(    embedding=embeddings,    documents=docs,    collection_name=collection_name,    connection_string=connection_string,)query = "What did the president say about Ketanji Brown Jackson"docs_with_score: List[Tuple[Document, float]] = db.similarity_search_with_score(query)for doc, score in docs_with_score:    print("-" * 80)    print("Score: ", score)    print(doc.page_content)    print("-" * 80)Working with vectorstore in Postgres‚ÄãUploading a vectorstore in PG‚Äãdb = PGEmbedding.from_documents(    embedding=embeddings,    documents=docs,    collection_name=collection_name,    connection_string=connection_string,    pre_delete_collection=False,)Create HNSW Index‚ÄãBy default, the extension performs a sequential scan search, with 100% recall. You might consider creating an HNSW index for approximate nearest neighbor (ANN) search to speed up similarity_search_with_score execution time. To create the HNSW index on your vector column, use a create_hnsw_index function:PGEmbedding.create_hnsw_index(    max_elements=10000, dims=1536, m=8, ef_construction=16, ef_search=16)The function above is equivalent to running the below SQL query:CREATE INDEX ON vectors USING hnsw(vec) WITH (maxelements=10000, dims=1536, m=3, efconstruction=16, efsearch=16);The HNSW index options used in the statement above include:maxelements: Defines the maximum number of elements indexed. This is a required parameter. The example shown above has a value of 3. A real-world example would have a much large value, such as 1000000. An "element" refers to a data point (a vector) in the dataset, which is represented as a node in the HNSW graph. Typically, you would set this option to a value able to accommodate the number of rows in your in your dataset.dims: Defines the number of dimensions in your vector data. This is a required parameter. A small value is used in the example above. If you are storing data generated using OpenAI's text-embedding-ada-002 model, which supports 1536 dimensions, you would define a value of 1536, for example.m: Defines the maximum number of bi-directional links (also referred to as "edges") created for each node during graph construction. The following additional index options are supported:efConstruction: Defines the number of nearest neighbors considered during index construction. The default value is 32.efsearch: Defines the number of nearest neighbors considered during index search. The default value is 32. For information about how you can configure these options to influence the HNSW algorithm, refer to Tuning the HNSW algorithm.Retrieving a vectorstore in PG‚Äãstore = PGEmbedding(    connection_string=connection_string,    embedding_function=embeddings,    collection_name=collection_name,)retriever = store.as_retriever()retriever    VectorStoreRetriever(vectorstore=<langchain.vectorstores.pghnsw.HNSWVectoreStore object at 0x121d3c8b0>, search_type='similarity', search_kwargs={})db1 = PGEmbedding.from_existing_index(    embedding=embeddings,    collection_name=collection_name,    pre_delete_collection=False,    connection_string=connection_string,)query = "What did the president say about Ketanji Brown Jackson"docs_with_score: List[Tuple[Document, float]] = db1.similarity_search_with_score(query)for doc, score in docs_with_score:    print("-" * 80)    print("Score: ", score)    print(doc.page_content)    print("-" * 80)PreviousOpenSearchNextPGVectorWorking with vectorstore in PostgresUploading a vectorstore in PGCreate HNSW IndexRetrieving a vectorstore in PGCommunityDiscordTwitterGitHubPythonJS/TSMoreHomepageBlogCopyright ¬© 2023 LangChain, Inc. Skip to main content ü¶úÔ∏èüîó LangChainDocsUse casesIntegrationsAPILangSmithJS/TS DocsCTRLK ü¶úÔ∏èüîó LangChainDocsUse casesIntegrationsAPI LangSmithJS/TS DocsCTRLK  CTRLK CTRLK  CTRLK   IntegrationsCallbacksChat modelsChat loadersDocument loadersDocument transformersLLMsMemoryRetrieversText embedding modelsAgents & ToolkitsToolsVector storesActiveloop Deep LakeAlibaba Cloud OpenSearchAnalyticDBAnnoyAtlasAwaDBAzure Cognitive SearchBagelDBCassandraChromaClickHouseDashVectorDingoDocArray HnswSearchDocArray InMemorySearchElasticsearchEpsillaFaissHologresLanceDBMarqoGoogle Vertex AI MatchingEngineMeilisearchMilvusMongoDB AtlasMyScaleNeo4j Vector IndexNucliaDBOpenSearchPostgres EmbeddingPGVectorPineconeQdrantRedisRocksetScaNNSingleStoreDBscikit-learnsqlite-vssStarRocksSupabase (Postgres)TairTencent Cloud VectorDBTigrisTypesenseUSearchvearchVectaraWeaviateXataZepZillizGrouped by providerIntegrationsVector storesPostgres EmbeddingOn this pagePostgres EmbeddingPostgres Embedding is an open-source vector similarity search for Postgres that uses  Hierarchical Navigable Small Worlds (HNSW) for approximate nearest neighbor search.It supports:exact and approximate nearest neighbor search using HNSWL2 distanceThis notebook shows how to use the Postgres vector database (PGEmbedding).The PGEmbedding integration creates the pg_embedding extension for you, but you run the following Postgres query to add it:CREATE EXTENSION embedding;# Pip install necessary packagepip install openaipip install psycopg2-binarypip install tiktokenAdd the OpenAI API Key to the environment variables to use OpenAIEmbeddings.import osimport getpassos.environ["OPENAI_API_KEY"] = getpass.getpass("OpenAI API Key:")    OpenAI API Key:¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑## Loading Environment Variablesfrom typing import List, Tuplefrom langchain.embeddings.openai import OpenAIEmbeddingsfrom langchain.text_splitter import CharacterTextSplitterfrom langchain.vectorstores import PGEmbeddingfrom langchain.document_loaders import TextLoaderfrom langchain.docstore.document import DocumentAPI Reference:OpenAIEmbeddingsCharacterTextSplitterPGEmbeddingTextLoaderDocumentos.environ["DATABASE_URL"] = getpass.getpass("Database Url:")    Database Url:¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑loader = TextLoader("state_of_the_union.txt")documents = loader.load()text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)docs = text_splitter.split_documents(documents)embeddings = OpenAIEmbeddings()connection_string = os.environ.get("DATABASE_URL")collection_name = "state_of_the_union"db = PGEmbedding.from_documents(    embedding=embeddings,    documents=docs,    collection_name=collection_name,    connection_string=connection_string,)query = "What did the president say about Ketanji Brown Jackson"docs_with_score: List[Tuple[Document, float]] = db.similarity_search_with_score(query)for doc, score in docs_with_score:    print("-" * 80)    print("Score: ", score)    print(doc.page_content)    print("-" * 80)Working with vectorstore in Postgres‚ÄãUploading a vectorstore in PG‚Äãdb = PGEmbedding.from_documents(    embedding=embeddings,    documents=docs,    collection_name=collection_name,    connection_string=connection_string,    pre_delete_collection=False,)Create HNSW Index‚ÄãBy default, the extension performs a sequential scan search, with 100% recall. You might consider creating an HNSW index for approximate nearest neighbor (ANN) search to speed up similarity_search_with_score execution time. To create the HNSW index on your vector column, use a create_hnsw_index function:PGEmbedding.create_hnsw_index(    max_elements=10000, dims=1536, m=8, ef_construction=16, ef_search=16)The function above is equivalent to running the below SQL query:CREATE INDEX ON vectors USING hnsw(vec) WITH (maxelements=10000, dims=1536, m=3, efconstruction=16, efsearch=16);The HNSW index options used in the statement above include:maxelements: Defines the maximum number of elements indexed. This is a required parameter. The example shown above has a value of 3. A real-world example would have a much large value, such as 1000000. An "element" refers to a data point (a vector) in the dataset, which is represented as a node in the HNSW graph. Typically, you would set this option to a value able to accommodate the number of rows in your in your dataset.dims: Defines the number of dimensions in your vector data. This is a required parameter. A small value is used in the example above. If you are storing data generated using OpenAI's text-embedding-ada-002 model, which supports 1536 dimensions, you would define a value of 1536, for example.m: Defines the maximum number of bi-directional links (also referred to as "edges") created for each node during graph construction. For information about how you can configure these options to influence the HNSW algorithm, refer to Tuning the HNSW algorithm.Retrieving a vectorstore in PG‚Äãstore = PGEmbedding(    connection_string=connection_string,    embedding_function=embeddings,    collection_name=collection_name,)retriever = store.as_retriever()retriever    VectorStoreRetriever(vectorstore=<langchain.vectorstores.pghnsw.HNSWVectoreStore object at 0x121d3c8b0>, search_type='similarity', search_kwargs={})db1 = PGEmbedding.from_existing_index(    embedding=embeddings,    collection_name=collection_name,    pre_delete_collection=False,    connection_string=connection_string,)query = "What did the president say about Ketanji Brown Jackson"docs_with_score: List[Tuple[Document, float]] = db1.similarity_search_with_score(query)for doc, score in docs_with_score:    print("-" * 80)    print("Score: ", score)    print(doc.page_content)    print("-" * 80)PreviousOpenSearchNextPGVectorWorking with vectorstore in PostgresUploading a vectorstore in PGCreate HNSW IndexRetrieving a vectorstore in PG IntegrationsCallbacksChat modelsChat loadersDocument loadersDocument transformersLLMsMemoryRetrieversText embedding modelsAgents & ToolkitsToolsVector storesActiveloop Deep LakeAlibaba Cloud OpenSearchAnalyticDBAnnoyAtlasAwaDBAzure Cognitive SearchBagelDBCassandraChromaClickHouseDashVectorDingoDocArray HnswSearchDocArray InMemorySearchElasticsearchEpsillaFaissHologresLanceDBMarqoGoogle Vertex AI MatchingEngineMeilisearchMilvusMongoDB AtlasMyScaleNeo4j Vector IndexNucliaDBOpenSearchPostgres EmbeddingPGVectorPineconeQdrantRedisRocksetScaNNSingleStoreDBscikit-learnsqlite-vssStarRocksSupabase (Postgres)TairTencent Cloud VectorDBTigrisTypesenseUSearchvearchVectaraWeaviateXataZepZillizGrouped by providerIntegrationsVector storesPostgres EmbeddingOn this pagePostgres EmbeddingPostgres Embedding is an open-source vector similarity search for Postgres that uses  Hierarchical Navigable Small Worlds (HNSW) for approximate nearest neighbor search.It supports:exact and approximate nearest neighbor search using HNSWL2 distanceThis notebook shows how to use the Postgres vector database (PGEmbedding).The PGEmbedding integration creates the pg_embedding extension for you, but you run the following Postgres query to add it:CREATE EXTENSION embedding;# Pip install necessary packagepip install openaipip install psycopg2-binarypip install tiktokenAdd the OpenAI API Key to the environment variables to use OpenAIEmbeddings.import osimport getpassos.environ["OPENAI_API_KEY"] = getpass.getpass("OpenAI API Key:")    OpenAI API Key:¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑## Loading Environment Variablesfrom typing import List, Tuplefrom langchain.embeddings.openai import OpenAIEmbeddingsfrom langchain.text_splitter import CharacterTextSplitterfrom langchain.vectorstores import PGEmbeddingfrom langchain.document_loaders import TextLoaderfrom langchain.docstore.document import DocumentAPI Reference:OpenAIEmbeddingsCharacterTextSplitterPGEmbeddingTextLoaderDocumentos.environ["DATABASE_URL"] = getpass.getpass("Database Url:")    Database Url:¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑loader = TextLoader("state_of_the_union.txt")documents = loader.load()text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)docs = text_splitter.split_documents(documents)embeddings = OpenAIEmbeddings()connection_string = os.environ.get("DATABASE_URL")collection_name = "state_of_the_union"db = PGEmbedding.from_documents(    embedding=embeddings,    documents=docs,    collection_name=collection_name,    connection_string=connection_string,)query = "What did the president say about Ketanji Brown Jackson"docs_with_score: List[Tuple[Document, float]] = db.similarity_search_with_score(query)for doc, score in docs_with_score:    print("-" * 80)    print("Score: ", score)    print(doc.page_content)    print("-" * 80)Working with vectorstore in Postgres‚ÄãUploading a vectorstore in PG‚Äãdb = PGEmbedding.from_documents(    embedding=embeddings,    documents=docs,    collection_name=collection_name,    connection_string=connection_string,    pre_delete_collection=False,)Create HNSW Index‚ÄãBy default, the extension performs a sequential scan search, with 100% recall. You might consider creating an HNSW index for approximate nearest neighbor (ANN) search to speed up similarity_search_with_score execution time. To create the HNSW index on your vector column, use a create_hnsw_index function:PGEmbedding.create_hnsw_index(    max_elements=10000, dims=1536, m=8, ef_construction=16, ef_search=16)The function above is equivalent to running the below SQL query:CREATE INDEX ON vectors USING hnsw(vec) WITH (maxelements=10000, dims=1536, m=3, efconstruction=16, efsearch=16);The HNSW index options used in the statement above include:maxelements: Defines the maximum number of elements indexed. This is a required parameter. The example shown above has a value of 3. A real-world example would have a much large value, such as 1000000. An "element" refers to a data point (a vector) in the dataset, which is represented as a node in the HNSW graph. Typically, you would set this option to a value able to accommodate the number of rows in your in your dataset.dims: Defines the number of dimensions in your vector data. This is a required parameter. A small value is used in the example above. If you are storing data generated using OpenAI's text-embedding-ada-002 model, which supports 1536 dimensions, you would define a value of 1536, for example.m: Defines the maximum number of bi-directional links (also referred to as "edges") created for each node during graph construction. For information about how you can configure these options to influence the HNSW algorithm, refer to Tuning the HNSW algorithm.Retrieving a vectorstore in PG‚Äãstore = PGEmbedding(    connection_string=connection_string,    embedding_function=embeddings,    collection_name=collection_name,)retriever = store.as_retriever()retriever    VectorStoreRetriever(vectorstore=<langchain.vectorstores.pghnsw.HNSWVectoreStore object at 0x121d3c8b0>, search_type='similarity', search_kwargs={})db1 = PGEmbedding.from_existing_index(    embedding=embeddings,    collection_name=collection_name,    pre_delete_collection=False,    connection_string=connection_string,)query = "What did the president say about Ketanji Brown Jackson"docs_with_score: List[Tuple[Document, float]] = db1.similarity_search_with_score(query)for doc, score in docs_with_score:    print("-" * 80)    print("Score: ", score)    print(doc.page_content)    print("-" * 80)PreviousOpenSearchNextPGVectorWorking with vectorstore in PostgresUploading a vectorstore in PGCreate HNSW IndexRetrieving a vectorstore in PG IntegrationsCallbacksChat modelsChat loadersDocument loadersDocument transformersLLMsMemoryRetrieversText embedding modelsAgents & ToolkitsToolsVector storesActiveloop Deep LakeAlibaba Cloud OpenSearchAnalyticDBAnnoyAtlasAwaDBAzure Cognitive SearchBagelDBCassandraChromaClickHouseDashVectorDingoDocArray HnswSearchDocArray InMemorySearchElasticsearchEpsillaFaissHologresLanceDBMarqoGoogle Vertex AI MatchingEngineMeilisearchMilvusMongoDB AtlasMyScaleNeo4j Vector IndexNucliaDBOpenSearchPostgres EmbeddingPGVectorPineconeQdrantRedisRocksetScaNNSingleStoreDBscikit-learnsqlite-vssStarRocksSupabase (Postgres)TairTencent Cloud VectorDBTigrisTypesenseUSearchvearchVectaraWeaviateXataZepZillizGrouped by provider IntegrationsCallbacksChat modelsChat loadersDocument loadersDocument transformersLLMsMemoryRetrieversText embedding modelsAgents & ToolkitsToolsVector storesActiveloop Deep LakeAlibaba Cloud OpenSearchAnalyticDBAnnoyAtlasAwaDBAzure Cognitive SearchBagelDBCassandraChromaClickHouseDashVectorDingoDocArray HnswSearchDocArray InMemorySearchElasticsearchEpsillaFaissHologresLanceDBMarqoGoogle Vertex AI MatchingEngineMeilisearchMilvusMongoDB AtlasMyScaleNeo4j Vector IndexNucliaDBOpenSearchPostgres EmbeddingPGVectorPineconeQdrantRedisRocksetScaNNSingleStoreDBscikit-learnsqlite-vssStarRocksSupabase (Postgres)TairTencent Cloud VectorDBTigrisTypesenseUSearchvearchVectaraWeaviateXataZepZillizGrouped by provider Integrations Callbacks Chat models Chat loaders Document loaders Document transformers LLMs Memory Retrievers Text embedding models Agents & Toolkits Tools Vector stores Grouped by provider IntegrationsVector storesPostgres EmbeddingOn this pagePostgres EmbeddingPostgres Embedding is an open-source vector similarity search for Postgres that uses  Hierarchical Navigable Small Worlds (HNSW) for approximate nearest neighbor search.It supports:exact and approximate nearest neighbor search using HNSWL2 distanceThis notebook shows how to use the Postgres vector database (PGEmbedding).The PGEmbedding integration creates the pg_embedding extension for you, but you run the following Postgres query to add it:CREATE EXTENSION embedding;# Pip install necessary packagepip install openaipip install psycopg2-binarypip install tiktokenAdd the OpenAI API Key to the environment variables to use OpenAIEmbeddings.import osimport getpassos.environ["OPENAI_API_KEY"] = getpass.getpass("OpenAI API Key:")    OpenAI API Key:¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑## Loading Environment Variablesfrom typing import List, Tuplefrom langchain.embeddings.openai import OpenAIEmbeddingsfrom langchain.text_splitter import CharacterTextSplitterfrom langchain.vectorstores import PGEmbeddingfrom langchain.document_loaders import TextLoaderfrom langchain.docstore.document import DocumentAPI Reference:OpenAIEmbeddingsCharacterTextSplitterPGEmbeddingTextLoaderDocumentos.environ["DATABASE_URL"] = getpass.getpass("Database Url:")    Database Url:¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑loader = TextLoader("state_of_the_union.txt")documents = loader.load()text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)docs = text_splitter.split_documents(documents)embeddings = OpenAIEmbeddings()connection_string = os.environ.get("DATABASE_URL")collection_name = "state_of_the_union"db = PGEmbedding.from_documents(    embedding=embeddings,    documents=docs,    collection_name=collection_name,    connection_string=connection_string,)query = "What did the president say about Ketanji Brown Jackson"docs_with_score: List[Tuple[Document, float]] = db.similarity_search_with_score(query)for doc, score in docs_with_score:    print("-" * 80)    print("Score: ", score)    print(doc.page_content)    print("-" * 80)Working with vectorstore in Postgres‚ÄãUploading a vectorstore in PG‚Äãdb = PGEmbedding.from_documents(    embedding=embeddings,    documents=docs,    collection_name=collection_name,    connection_string=connection_string,    pre_delete_collection=False,)Create HNSW Index‚ÄãBy default, the extension performs a sequential scan search, with 100% recall. You might consider creating an HNSW index for approximate nearest neighbor (ANN) search to speed up similarity_search_with_score execution time. To create the HNSW index on your vector column, use a create_hnsw_index function:PGEmbedding.create_hnsw_index(    max_elements=10000, dims=1536, m=8, ef_construction=16, ef_search=16)The function above is equivalent to running the below SQL query:CREATE INDEX ON vectors USING hnsw(vec) WITH (maxelements=10000, dims=1536, m=3, efconstruction=16, efsearch=16);The HNSW index options used in the statement above include:maxelements: Defines the maximum number of elements indexed. This is a required parameter. The example shown above has a value of 3. A real-world example would have a much large value, such as 1000000. An "element" refers to a data point (a vector) in the dataset, which is represented as a node in the HNSW graph. Typically, you would set this option to a value able to accommodate the number of rows in your in your dataset.dims: Defines the number of dimensions in your vector data. This is a required parameter. A small value is used in the example above. If you are storing data generated using OpenAI's text-embedding-ada-002 model, which supports 1536 dimensions, you would define a value of 1536, for example.m: Defines the maximum number of bi-directional links (also referred to as "edges") created for each node during graph construction. For information about how you can configure these options to influence the HNSW algorithm, refer to Tuning the HNSW algorithm.Retrieving a vectorstore in PG‚Äãstore = PGEmbedding(    connection_string=connection_string,    embedding_function=embeddings,    collection_name=collection_name,)retriever = store.as_retriever()retriever    VectorStoreRetriever(vectorstore=<langchain.vectorstores.pghnsw.HNSWVectoreStore object at 0x121d3c8b0>, search_type='similarity', search_kwargs={})db1 = PGEmbedding.from_existing_index(    embedding=embeddings,    collection_name=collection_name,    pre_delete_collection=False,    connection_string=connection_string,)query = "What did the president say about Ketanji Brown Jackson"docs_with_score: List[Tuple[Document, float]] = db1.similarity_search_with_score(query)for doc, score in docs_with_score:    print("-" * 80)    print("Score: ", score)    print(doc.page_content)    print("-" * 80)PreviousOpenSearchNextPGVectorWorking with vectorstore in PostgresUploading a vectorstore in PGCreate HNSW IndexRetrieving a vectorstore in PG IntegrationsVector storesPostgres EmbeddingOn this pagePostgres EmbeddingPostgres Embedding is an open-source vector similarity search for Postgres that uses  Hierarchical Navigable Small Worlds (HNSW) for approximate nearest neighbor search.It supports:exact and approximate nearest neighbor search using HNSWL2 distanceThis notebook shows how to use the Postgres vector database (PGEmbedding).The PGEmbedding integration creates the pg_embedding extension for you, but you run the following Postgres query to add it:CREATE EXTENSION embedding;# Pip install necessary packagepip install openaipip install psycopg2-binarypip install tiktokenAdd the OpenAI API Key to the environment variables to use OpenAIEmbeddings.import osimport getpassos.environ["OPENAI_API_KEY"] = getpass.getpass("OpenAI API Key:")    OpenAI API Key:¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑## Loading Environment Variablesfrom typing import List, Tuplefrom langchain.embeddings.openai import OpenAIEmbeddingsfrom langchain.text_splitter import CharacterTextSplitterfrom langchain.vectorstores import PGEmbeddingfrom langchain.document_loaders import TextLoaderfrom langchain.docstore.document import DocumentAPI Reference:OpenAIEmbeddingsCharacterTextSplitterPGEmbeddingTextLoaderDocumentos.environ["DATABASE_URL"] = getpass.getpass("Database Url:")    Database Url:¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑loader = TextLoader("state_of_the_union.txt")documents = loader.load()text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)docs = text_splitter.split_documents(documents)embeddings = OpenAIEmbeddings()connection_string = os.environ.get("DATABASE_URL")collection_name = "state_of_the_union"db = PGEmbedding.from_documents(    embedding=embeddings,    documents=docs,    collection_name=collection_name,    connection_string=connection_string,)query = "What did the president say about Ketanji Brown Jackson"docs_with_score: List[Tuple[Document, float]] = db.similarity_search_with_score(query)for doc, score in docs_with_score:    print("-" * 80)    print("Score: ", score)    print(doc.page_content)    print("-" * 80)Working with vectorstore in Postgres‚ÄãUploading a vectorstore in PG‚Äãdb = PGEmbedding.from_documents(    embedding=embeddings,    documents=docs,    collection_name=collection_name,    connection_string=connection_string,    pre_delete_collection=False,)Create HNSW Index‚ÄãBy default, the extension performs a sequential scan search, with 100% recall. You might consider creating an HNSW index for approximate nearest neighbor (ANN) search to speed up similarity_search_with_score execution time. To create the HNSW index on your vector column, use a create_hnsw_index function:PGEmbedding.create_hnsw_index(    max_elements=10000, dims=1536, m=8, ef_construction=16, ef_search=16)The function above is equivalent to running the below SQL query:CREATE INDEX ON vectors USING hnsw(vec) WITH (maxelements=10000, dims=1536, m=3, efconstruction=16, efsearch=16);The HNSW index options used in the statement above include:maxelements: Defines the maximum number of elements indexed. This is a required parameter. The example shown above has a value of 3. A real-world example would have a much large value, such as 1000000. An "element" refers to a data point (a vector) in the dataset, which is represented as a node in the HNSW graph. Typically, you would set this option to a value able to accommodate the number of rows in your in your dataset.dims: Defines the number of dimensions in your vector data. This is a required parameter. A small value is used in the example above. If you are storing data generated using OpenAI's text-embedding-ada-002 model, which supports 1536 dimensions, you would define a value of 1536, for example.m: Defines the maximum number of bi-directional links (also referred to as "edges") created for each node during graph construction. For information about how you can configure these options to influence the HNSW algorithm, refer to Tuning the HNSW algorithm.Retrieving a vectorstore in PG‚Äãstore = PGEmbedding(    connection_string=connection_string,    embedding_function=embeddings,    collection_name=collection_name,)retriever = store.as_retriever()retriever    VectorStoreRetriever(vectorstore=<langchain.vectorstores.pghnsw.HNSWVectoreStore object at 0x121d3c8b0>, search_type='similarity', search_kwargs={})db1 = PGEmbedding.from_existing_index(    embedding=embeddings,    collection_name=collection_name,    pre_delete_collection=False,    connection_string=connection_string,)query = "What did the president say about Ketanji Brown Jackson"docs_with_score: List[Tuple[Document, float]] = db1.similarity_search_with_score(query)for doc, score in docs_with_score:    print("-" * 80)    print("Score: ", score)    print(doc.page_content)    print("-" * 80)PreviousOpenSearchNextPGVector IntegrationsVector storesPostgres EmbeddingOn this pagePostgres EmbeddingPostgres Embedding is an open-source vector similarity search for Postgres that uses  Hierarchical Navigable Small Worlds (HNSW) for approximate nearest neighbor search.It supports:exact and approximate nearest neighbor search using HNSWL2 distanceThis notebook shows how to use the Postgres vector database (PGEmbedding).The PGEmbedding integration creates the pg_embedding extension for you, but you run the following Postgres query to add it:CREATE EXTENSION embedding;# Pip install necessary packagepip install openaipip install psycopg2-binarypip install tiktokenAdd the OpenAI API Key to the environment variables to use OpenAIEmbeddings.import osimport getpassos.environ["OPENAI_API_KEY"] = getpass.getpass("OpenAI API Key:")    OpenAI API Key:¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑## Loading Environment Variablesfrom typing import List, Tuplefrom langchain.embeddings.openai import OpenAIEmbeddingsfrom langchain.text_splitter import CharacterTextSplitterfrom langchain.vectorstores import PGEmbeddingfrom langchain.document_loaders import TextLoaderfrom langchain.docstore.document import DocumentAPI Reference:OpenAIEmbeddingsCharacterTextSplitterPGEmbeddingTextLoaderDocumentos.environ["DATABASE_URL"] = getpass.getpass("Database Url:")    Database Url:¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑loader = TextLoader("state_of_the_union.txt")documents = loader.load()text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)docs = text_splitter.split_documents(documents)embeddings = OpenAIEmbeddings()connection_string = os.environ.get("DATABASE_URL")collection_name = "state_of_the_union"db = PGEmbedding.from_documents(    embedding=embeddings,    documents=docs,    collection_name=collection_name,    connection_string=connection_string,)query = "What did the president say about Ketanji Brown Jackson"docs_with_score: List[Tuple[Document, float]] = db.similarity_search_with_score(query)for doc, score in docs_with_score:    print("-" * 80)    print("Score: ", score)    print(doc.page_content)    print("-" * 80)Working with vectorstore in Postgres‚ÄãUploading a vectorstore in PG‚Äãdb = PGEmbedding.from_documents(    embedding=embeddings,    documents=docs,    collection_name=collection_name,    connection_string=connection_string,    pre_delete_collection=False,)Create HNSW Index‚ÄãBy default, the extension performs a sequential scan search, with 100% recall. You might consider creating an HNSW index for approximate nearest neighbor (ANN) search to speed up similarity_search_with_score execution time. To create the HNSW index on your vector column, use a create_hnsw_index function:PGEmbedding.create_hnsw_index(    max_elements=10000, dims=1536, m=8, ef_construction=16, ef_search=16)The function above is equivalent to running the below SQL query:CREATE INDEX ON vectors USING hnsw(vec) WITH (maxelements=10000, dims=1536, m=3, efconstruction=16, efsearch=16);The HNSW index options used in the statement above include:maxelements: Defines the maximum number of elements indexed. This is a required parameter. The example shown above has a value of 3. A real-world example would have a much large value, such as 1000000. An "element" refers to a data point (a vector) in the dataset, which is represented as a node in the HNSW graph. Typically, you would set this option to a value able to accommodate the number of rows in your in your dataset.dims: Defines the number of dimensions in your vector data. This is a required parameter. A small value is used in the example above. If you are storing data generated using OpenAI's text-embedding-ada-002 model, which supports 1536 dimensions, you would define a value of 1536, for example.m: Defines the maximum number of bi-directional links (also referred to as "edges") created for each node during graph construction. For information about how you can configure these options to influence the HNSW algorithm, refer to Tuning the HNSW algorithm.Retrieving a vectorstore in PG‚Äãstore = PGEmbedding(    connection_string=connection_string,    embedding_function=embeddings,    collection_name=collection_name,)retriever = store.as_retriever()retriever    VectorStoreRetriever(vectorstore=<langchain.vectorstores.pghnsw.HNSWVectoreStore object at 0x121d3c8b0>, search_type='similarity', search_kwargs={})db1 = PGEmbedding.from_existing_index(    embedding=embeddings,    collection_name=collection_name,    pre_delete_collection=False,    connection_string=connection_string,)query = "What did the president say about Ketanji Brown Jackson"docs_with_score: List[Tuple[Document, float]] = db1.similarity_search_with_score(query)for doc, score in docs_with_score:    print("-" * 80)    print("Score: ", score)    print(doc.page_content)    print("-" * 80)PreviousOpenSearchNextPGVector On this page Postgres EmbeddingPostgres Embedding is an open-source vector similarity search for Postgres that uses  Hierarchical Navigable Small Worlds (HNSW) for approximate nearest neighbor search.It supports:exact and approximate nearest neighbor search using HNSWL2 distanceThis notebook shows how to use the Postgres vector database (PGEmbedding).The PGEmbedding integration creates the pg_embedding extension for you, but you run the following Postgres query to add it:CREATE EXTENSION embedding;# Pip install necessary packagepip install openaipip install psycopg2-binarypip install tiktokenAdd the OpenAI API Key to the environment variables to use OpenAIEmbeddings.import osimport getpassos.environ["OPENAI_API_KEY"] = getpass.getpass("OpenAI API Key:")    OpenAI API Key:¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑## Loading Environment Variablesfrom typing import List, Tuplefrom langchain.embeddings.openai import OpenAIEmbeddingsfrom langchain.text_splitter import CharacterTextSplitterfrom langchain.vectorstores import PGEmbeddingfrom langchain.document_loaders import TextLoaderfrom langchain.docstore.document import DocumentAPI Reference:OpenAIEmbeddingsCharacterTextSplitterPGEmbeddingTextLoaderDocumentos.environ["DATABASE_URL"] = getpass.getpass("Database Url:")    Database Url:¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑loader = TextLoader("state_of_the_union.txt")documents = loader.load()text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)docs = text_splitter.split_documents(documents)embeddings = OpenAIEmbeddings()connection_string = os.environ.get("DATABASE_URL")collection_name = "state_of_the_union"db = PGEmbedding.from_documents(    embedding=embeddings,    documents=docs,    collection_name=collection_name,    connection_string=connection_string,)query = "What did the president say about Ketanji Brown Jackson"docs_with_score: List[Tuple[Document, float]] = db.similarity_search_with_score(query)for doc, score in docs_with_score:    print("-" * 80)    print("Score: ", score)    print(doc.page_content)    print("-" * 80)Working with vectorstore in Postgres‚ÄãUploading a vectorstore in PG‚Äãdb = PGEmbedding.from_documents(    embedding=embeddings,    documents=docs,    collection_name=collection_name,    connection_string=connection_string,    pre_delete_collection=False,)Create HNSW Index‚ÄãBy default, the extension performs a sequential scan search, with 100% recall. You might consider creating an HNSW index for approximate nearest neighbor (ANN) search to speed up similarity_search_with_score execution time. To create the HNSW index on your vector column, use a create_hnsw_index function:PGEmbedding.create_hnsw_index(    max_elements=10000, dims=1536, m=8, ef_construction=16, ef_search=16)The function above is equivalent to running the below SQL query:CREATE INDEX ON vectors USING hnsw(vec) WITH (maxelements=10000, dims=1536, m=3, efconstruction=16, efsearch=16);The HNSW index options used in the statement above include:maxelements: Defines the maximum number of elements indexed. This is a required parameter. The example shown above has a value of 3. A real-world example would have a much large value, such as 1000000. An "element" refers to a data point (a vector) in the dataset, which is represented as a node in the HNSW graph. Typically, you would set this option to a value able to accommodate the number of rows in your in your dataset.dims: Defines the number of dimensions in your vector data. This is a required parameter. A small value is used in the example above. If you are storing data generated using OpenAI's text-embedding-ada-002 model, which supports 1536 dimensions, you would define a value of 1536, for example.m: Defines the maximum number of bi-directional links (also referred to as "edges") created for each node during graph construction. For information about how you can configure these options to influence the HNSW algorithm, refer to Tuning the HNSW algorithm.Retrieving a vectorstore in PG‚Äãstore = PGEmbedding(    connection_string=connection_string,    embedding_function=embeddings,    collection_name=collection_name,)retriever = store.as_retriever()retriever    VectorStoreRetriever(vectorstore=<langchain.vectorstores.pghnsw.HNSWVectoreStore object at 0x121d3c8b0>, search_type='similarity', search_kwargs={})db1 = PGEmbedding.from_existing_index(    embedding=embeddings,    collection_name=collection_name,    pre_delete_collection=False,    connection_string=connection_string,)query = "What did the president say about Ketanji Brown Jackson"docs_with_score: List[Tuple[Document, float]] = db1.similarity_search_with_score(query)for doc, score in docs_with_score:    print("-" * 80)    print("Score: ", score)    print(doc.page_content)    print("-" * 80) CREATE EXTENSION embedding; CREATE EXTENSION embedding;  # Pip install necessary packagepip install openaipip install psycopg2-binarypip install tiktoken # Pip install necessary packagepip install openaipip install psycopg2-binarypip install tiktoken  import osimport getpassos.environ["OPENAI_API_KEY"] = getpass.getpass("OpenAI API Key:") import osimport getpassos.environ["OPENAI_API_KEY"] = getpass.getpass("OpenAI API Key:")      OpenAI API Key:¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑     OpenAI API Key:¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑     OpenAI API Key:¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑  ## Loading Environment Variablesfrom typing import List, Tuple ## Loading Environment Variablesfrom typing import List, Tuple  from langchain.embeddings.openai import OpenAIEmbeddingsfrom langchain.text_splitter import CharacterTextSplitterfrom langchain.vectorstores import PGEmbeddingfrom langchain.document_loaders import TextLoaderfrom langchain.docstore.document import Document from langchain.embeddings.openai import OpenAIEmbeddingsfrom langchain.text_splitter import CharacterTextSplitterfrom langchain.vectorstores import PGEmbeddingfrom langchain.document_loaders import TextLoaderfrom langchain.docstore.document import Document  API Reference:OpenAIEmbeddingsCharacterTextSplitterPGEmbeddingTextLoaderDocument os.environ["DATABASE_URL"] = getpass.getpass("Database Url:") os.environ["DATABASE_URL"] = getpass.getpass("Database Url:")      Database Url:¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑     Database Url:¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑     Database Url:¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑  loader = TextLoader("state_of_the_union.txt")documents = loader.load()text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)docs = text_splitter.split_documents(documents)embeddings = OpenAIEmbeddings()connection_string = os.environ.get("DATABASE_URL")collection_name = "state_of_the_union" loader = TextLoader("state_of_the_union.txt")documents = loader.load()text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)docs = text_splitter.split_documents(documents)embeddings = OpenAIEmbeddings()connection_string = os.environ.get("DATABASE_URL")collection_name = "state_of_the_union"  db = PGEmbedding.from_documents(    embedding=embeddings,    documents=docs,    collection_name=collection_name,    connection_string=connection_string,)query = "What did the president say about Ketanji Brown Jackson"docs_with_score: List[Tuple[Document, float]] = db.similarity_search_with_score(query) db = PGEmbedding.from_documents(    embedding=embeddings,    documents=docs,    collection_name=collection_name,    connection_string=connection_string,)query = "What did the president say about Ketanji Brown Jackson"docs_with_score: List[Tuple[Document, float]] = db.similarity_search_with_score(query)  for doc, score in docs_with_score:    print("-" * 80)    print("Score: ", score)    print(doc.page_content)    print("-" * 80) for doc, score in docs_with_score:    print("-" * 80)    print("Score: ", score)    print(doc.page_content)    print("-" * 80)  db = PGEmbedding.from_documents(    embedding=embeddings,    documents=docs,    collection_name=collection_name,    connection_string=connection_string,    pre_delete_collection=False,) db = PGEmbedding.from_documents(    embedding=embeddings,    documents=docs,    collection_name=collection_name,    connection_string=connection_string,    pre_delete_collection=False,)  PGEmbedding.create_hnsw_index(    max_elements=10000, dims=1536, m=8, ef_construction=16, ef_search=16) PGEmbedding.create_hnsw_index(    max_elements=10000, dims=1536, m=8, ef_construction=16, ef_search=16)  CREATE INDEX ON vectors USING hnsw(vec) WITH (maxelements=10000, dims=1536, m=3, efconstruction=16, efsearch=16); CREATE INDEX ON vectors USING hnsw(vec) WITH (maxelements=10000, dims=1536, m=3, efconstruction=16, efsearch=16);  store = PGEmbedding(    connection_string=connection_string,    embedding_function=embeddings,    collection_name=collection_name,)retriever = store.as_retriever() store = PGEmbedding(    connection_string=connection_string,    embedding_function=embeddings,    collection_name=collection_name,)retriever = store.as_retriever()  retriever retriever      VectorStoreRetriever(vectorstore=<langchain.vectorstores.pghnsw.HNSWVectoreStore object at 0x121d3c8b0>, search_type='similarity', search_kwargs={})     VectorStoreRetriever(vectorstore=<langchain.vectorstores.pghnsw.HNSWVectoreStore object at 0x121d3c8b0>, search_type='similarity', search_kwargs={})     VectorStoreRetriever(vectorstore=<langchain.vectorstores.pghnsw.HNSWVectoreStore object at 0x121d3c8b0>, search_type='similarity', search_kwargs={})  db1 = PGEmbedding.from_existing_index(    embedding=embeddings,    collection_name=collection_name,    pre_delete_collection=False,    connection_string=connection_string,)query = "What did the president say about Ketanji Brown Jackson"docs_with_score: List[Tuple[Document, float]] = db1.similarity_search_with_score(query) db1 = PGEmbedding.from_existing_index(    embedding=embeddings,    collection_name=collection_name,    pre_delete_collection=False,    connection_string=connection_string,)query = "What did the president say about Ketanji Brown Jackson"docs_with_score: List[Tuple[Document, float]] = db1.similarity_search_with_score(query)  for doc, score in docs_with_score:    print("-" * 80)    print("Score: ", score)    print(doc.page_content)    print("-" * 80) for doc, score in docs_with_score:    print("-" * 80)    print("Score: ", score)    print(doc.page_content)    print("-" * 80)  Previous OpenSearch Next PGVector Working with vectorstore in PostgresUploading a vectorstore in PGCreate HNSW IndexRetrieving a vectorstore in PG Working with vectorstore in PostgresUploading a vectorstore in PGCreate HNSW IndexRetrieving a vectorstore in PG CommunityDiscordTwitterGitHubPythonJS/TSMoreHomepageBlogCopyright ¬© 2023 LangChain, Inc. CommunityDiscordTwitterGitHubPythonJS/TSMoreHomepageBlog CommunityDiscordTwitter Community GitHubPythonJS/TS GitHub MoreHomepageBlog More Copyright ¬© 2023 LangChain, Inc. Copyright ¬© 2023 LangChain, Inc. Skip to main content (#docusaurus_skipToContent_fallback) ü¶úÔ∏èüîó LangChain (/) Docs (/docs/get_started/introduction) Use cases (/docs/use_cases) Integrations (/docs/integrations) API (https://api.python.langchain.com) LangSmith (https://smith.langchain.com) JS/TS Docs (https://js.langchain.com/docs)  (https://github.com/hwchase17/langchain) Integrations (/docs/integrations) Callbacks (/docs/integrations/callbacks/) Chat models (/docs/integrations/chat/) Chat loaders (/docs/integrations/chat_loaders/) Document loaders (/docs/integrations/document_loaders/) Document transformers (/docs/integrations/document_transformers/) LLMs (/docs/integrations/llms/) Memory (/docs/integrations/memory/) Retrievers (/docs/integrations/retrievers/) Text embedding models (/docs/integrations/text_embedding/) Agents & Toolkits (/docs/integrations/toolkits/) Tools (/docs/integrations/tools/) Vector stores (/docs/integrations/vectorstores/) Activeloop Deep Lake (/docs/integrations/vectorstores/activeloop_deeplake) Alibaba Cloud OpenSearch (/docs/integrations/vectorstores/alibabacloud_opensearch) AnalyticDB (/docs/integrations/vectorstores/analyticdb) Annoy (/docs/integrations/vectorstores/annoy) Atlas (/docs/integrations/vectorstores/atlas) AwaDB (/docs/integrations/vectorstores/awadb) Azure Cognitive Search (/docs/integrations/vectorstores/azuresearch) BagelDB (/docs/integrations/vectorstores/bageldb) Cassandra (/docs/integrations/vectorstores/cassandra) Chroma (/docs/integrations/vectorstores/chroma) ClickHouse (/docs/integrations/vectorstores/clickhouse) DashVector (/docs/integrations/vectorstores/dashvector) Dingo (/docs/integrations/vectorstores/dingo) DocArray HnswSearch (/docs/integrations/vectorstores/docarray_hnsw) DocArray InMemorySearch (/docs/integrations/vectorstores/docarray_in_memory) Elasticsearch (/docs/integrations/vectorstores/elasticsearch) Epsilla (/docs/integrations/vectorstores/epsilla) Faiss (/docs/integrations/vectorstores/faiss) Hologres (/docs/integrations/vectorstores/hologres) LanceDB (/docs/integrations/vectorstores/lancedb) Marqo (/docs/integrations/vectorstores/marqo) Google Vertex AI MatchingEngine (/docs/integrations/vectorstores/matchingengine) Meilisearch (/docs/integrations/vectorstores/meilisearch) Milvus (/docs/integrations/vectorstores/milvus) MongoDB Atlas (/docs/integrations/vectorstores/mongodb_atlas) MyScale (/docs/integrations/vectorstores/myscale) Neo4j Vector Index (/docs/integrations/vectorstores/neo4jvector) NucliaDB (/docs/integrations/vectorstores/nucliadb) OpenSearch (/docs/integrations/vectorstores/opensearch) Postgres Embedding (/docs/integrations/vectorstores/pgembedding) PGVector (/docs/integrations/vectorstores/pgvector) Pinecone (/docs/integrations/vectorstores/pinecone) Qdrant (/docs/integrations/vectorstores/qdrant) Redis (/docs/integrations/vectorstores/redis) Rockset (/docs/integrations/vectorstores/rockset) ScaNN (/docs/integrations/vectorstores/scann) SingleStoreDB (/docs/integrations/vectorstores/singlestoredb) scikit-learn (/docs/integrations/vectorstores/sklearn) sqlite-vss (/docs/integrations/vectorstores/sqlitevss) StarRocks (/docs/integrations/vectorstores/starrocks) Supabase (Postgres) (/docs/integrations/vectorstores/supabase) Tair (/docs/integrations/vectorstores/tair) Tencent Cloud VectorDB (/docs/integrations/vectorstores/tencentvectordb) Tigris (/docs/integrations/vectorstores/tigris) Typesense (/docs/integrations/vectorstores/typesense) USearch (/docs/integrations/vectorstores/usearch) vearch (/docs/integrations/vectorstores/vearch) Vectara (/docs/integrations/vectorstores/vectara) Weaviate (/docs/integrations/vectorstores/weaviate) Xata (/docs/integrations/vectorstores/xata) Zep (/docs/integrations/vectorstores/zep) Zilliz (/docs/integrations/vectorstores/zilliz) Grouped by provider (/docs/integrations/providers/)  (/) Integrations (/docs/integrations) Vector stores (/docs/integrations/vectorstores/) Postgres Embedding (https://github.com/neondatabase/pg_embedding) OpenAIEmbeddings (https://api.python.langchain.com/en/latest/embeddings/langchain.embeddings.openai.OpenAIEmbeddings.html) CharacterTextSplitter (https://api.python.langchain.com/en/latest/text_splitter/langchain.text_splitter.CharacterTextSplitter.html) PGEmbedding (https://api.python.langchain.com/en/latest/vectorstores/langchain.vectorstores.pgembedding.PGEmbedding.html) TextLoader (https://api.python.langchain.com/en/latest/document_loaders/langchain.document_loaders.text.TextLoader.html) Document (https://api.python.langchain.com/en/latest/schema/langchain.schema.document.Document.html) ‚Äã (#working-with-vectorstore-in-postgres) ‚Äã (#uploading-a-vectorstore-in-pg) ‚Äã (#create-hnsw-index) Tuning the HNSW algorithm (https://neon.tech/docs/extensions/pg_embedding#tuning-the-hnsw-algorithm) ‚Äã (#retrieving-a-vectorstore-in-pg) PreviousOpenSearch (/docs/integrations/vectorstores/opensearch) NextPGVector (/docs/integrations/vectorstores/pgvector) Working with vectorstore in Postgres (#working-with-vectorstore-in-postgres) Uploading a vectorstore in PG (#uploading-a-vectorstore-in-pg) Create HNSW Index (#create-hnsw-index) Retrieving a vectorstore in PG (#retrieving-a-vectorstore-in-pg) Discord (https://discord.gg/cU2adEyC7w) Twitter (https://twitter.com/LangChainAI) Python (https://github.com/hwchase17/langchain) JS/TS (https://github.com/hwchase17/langchainjs) Homepage (https://langchain.com) Blog (https://blog.langchain.dev)