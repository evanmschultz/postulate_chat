Azure ML is a platform used to build, train, and deploy machine learning models. Users can explore the types of models to deploy in the Model Catalog, which provides Azure Foundation Models and OpenAI Models. Azure Foundation Models include various open-source models and popular Hugging Face models. Users can also import models of their liking into AzureML. This notebook goes over how to use an LLM hosted on an AzureML online endpoint To use the wrapper, you must deploy a model on AzureML and obtain the following parameters: The content_formatter parameter is a handler class for transforming the request and response of an AzureML endpoint to match with required schema. Since there are a wide range of models in the model catalog, each of which may process data differently from one another, a ContentFormatterBase class is provided to allow users to transform data to their liking. The following content formatters are provided: Note: OSSContentFormatter is being deprecated and replaced with GPT2ContentFormatter. The logic is the same but GPT2ContentFormatter is a more suitable name. You can still continue to use OSSContentFormatter as the changes are backwards compatibile. Below is an example using a summarization model from Hugging Face. You can also save and load LLM configurations IntegrationsCallbacksChat modelsChat loadersDocument loadersDocument transformersLLMsAI21Aleph AlphaAmazon API GatewayAnyscaleAzure MLAzure OpenAIBananaBasetenBeamBedrockBittensorCerebriumAIChatGLMClarifaiCohereC TransformersCTranslate2DatabricksDeepInfraDeepSparseEden AIFireworksForefrontAIGoogle Vertex AI PaLMGooseAIGPT4AllHugging Face HubHugging Face Local PipelinesHuggingface TextGen InferenceJSONFormerKoboldAI APILlama.cppLLM Caching integrationsManifestMinimaxModalMosaicMLNLP CloudOctoAIOllamaOpaquePromptsOpenAIOpenLLMOpenLMPetalsPipelineAIPredibasePrediction GuardPromptLayer OpenAIRELLMReplicateRunhouseSageMakerEndpointStochasticAINebula (Symbl.ai)TextGenTitan TakeoffTongyi QwenvLLMWriterXorbits Inference (Xinference)MemoryRetrieversText embedding modelsAgents & ToolkitsToolsVector storesGrouped by provider Callbacks Chat models Chat loaders Document loaders Document transformers LLMsAI21Aleph AlphaAmazon API GatewayAnyscaleAzure MLAzure OpenAIBananaBasetenBeamBedrockBittensorCerebriumAIChatGLMClarifaiCohereC TransformersCTranslate2DatabricksDeepInfraDeepSparseEden AIFireworksForefrontAIGoogle Vertex AI PaLMGooseAIGPT4AllHugging Face HubHugging Face Local PipelinesHuggingface TextGen InferenceJSONFormerKoboldAI APILlama.cppLLM Caching integrationsManifestMinimaxModalMosaicMLNLP CloudOctoAIOllamaOpaquePromptsOpenAIOpenLLMOpenLMPetalsPipelineAIPredibasePrediction GuardPromptLayer OpenAIRELLMReplicateRunhouseSageMakerEndpointStochasticAINebula (Symbl.ai)TextGenTitan TakeoffTongyi QwenvLLMWriterXorbits Inference (Xinference) AI21 Aleph Alpha Amazon API Gateway Anyscale Azure ML Azure OpenAI Banana Baseten Beam Bedrock Bittensor CerebriumAI ChatGLM Clarifai Cohere C Transformers CTranslate2 Databricks DeepInfra DeepSparse Eden AI Fireworks ForefrontAI Google Vertex AI PaLM GooseAI GPT4All Hugging Face Hub Hugging Face Local Pipelines Huggingface TextGen Inference JSONFormer KoboldAI API Llama.cpp LLM Caching integrations Manifest Minimax Modal MosaicML NLP Cloud OctoAI Ollama OpaquePrompts OpenAI OpenLLM OpenLM Petals PipelineAI Predibase Prediction Guard PromptLayer OpenAI RELLM Replicate Runhouse SageMakerEndpoint StochasticAI Nebula (Symbl.ai) TextGen Titan Takeoff Tongyi Qwen vLLM Writer Xorbits Inference (Xinference) Memory Retrievers Text embedding models Agents & Toolkits Tools Vector stores Grouped by provider  Integrations LLMs Azure ML AzureMLOnlineEndpoint endpoint_api_key: Required - The API key provided by the endpoint endpoint_url: Required - The REST endpoint url provided by the endpoint deployment_name: Not required - The deployment name of the model using the endpoint GPT2ContentFormatter: Formats request and response data for GPT2 DollyContentFormatter: Formats request and response data for the Dolly-v2 HFContentFormatter: Formats request and response data for text-generation Hugging Face models LLamaContentFormatter: Formats request and response data for LLaMa2 AzureMLOnlineEndpoint ContentFormatterBase DollyContentFormatter LLMChain load_llm AzureMLEndpointClient Set up Content FormatterCustom Content FormatterDolly with LLMChainSerializing an LLM Custom Content Formatter Dolly with LLMChain Serializing an LLM Discord Twitter Python JS/TS Homepage Blog Skip to main content🦜️🔗 LangChainDocsUse casesIntegrationsAPILangSmithJS/TS DocsCTRLKIntegrationsCallbacksChat modelsChat loadersDocument loadersDocument transformersLLMsAI21Aleph AlphaAmazon API GatewayAnyscaleAzure MLAzure OpenAIBananaBasetenBeamBedrockBittensorCerebriumAIChatGLMClarifaiCohereC TransformersCTranslate2DatabricksDeepInfraDeepSparseEden AIFireworksForefrontAIGoogle Vertex AI PaLMGooseAIGPT4AllHugging Face HubHugging Face Local PipelinesHuggingface TextGen InferenceJSONFormerKoboldAI APILlama.cppLLM Caching integrationsManifestMinimaxModalMosaicMLNLP CloudOctoAIOllamaOpaquePromptsOpenAIOpenLLMOpenLMPetalsPipelineAIPredibasePrediction GuardPromptLayer OpenAIRELLMReplicateRunhouseSageMakerEndpointStochasticAINebula (Symbl.ai)TextGenTitan TakeoffTongyi QwenvLLMWriterXorbits Inference (Xinference)MemoryRetrieversText embedding modelsAgents & ToolkitsToolsVector storesGrouped by providerIntegrationsLLMsAzure MLOn this pageAzure MLAzure ML is a platform used to build, train, and deploy machine learning models. Users can explore the types of models to deploy in the Model Catalog, which provides Azure Foundation Models and OpenAI Models. Azure Foundation Models include various open-source models and popular Hugging Face models. Users can also import models of their liking into AzureML.This notebook goes over how to use an LLM hosted on an AzureML online endpointfrom langchain.llms.azureml_endpoint import AzureMLOnlineEndpointAPI Reference:AzureMLOnlineEndpointSet up​To use the wrapper, you must deploy a model on AzureML and obtain the following parameters:endpoint_api_key: Required - The API key provided by the endpointendpoint_url: Required - The REST endpoint url provided by the endpointdeployment_name: Not required - The deployment name of the model using the endpointContent Formatter​The content_formatter parameter is a handler class for transforming the request and response of an AzureML endpoint to match with required schema. Since there are a wide range of models in the model catalog, each of which may process data differently from one another, a ContentFormatterBase class is provided to allow users to transform data to their liking. The following content formatters are provided:GPT2ContentFormatter: Formats request and response data for GPT2DollyContentFormatter: Formats request and response data for the Dolly-v2HFContentFormatter: Formats request and response data for text-generation Hugging Face modelsLLamaContentFormatter: Formats request and response data for LLaMa2Note: OSSContentFormatter is being deprecated and replaced with GPT2ContentFormatter. The logic is the same but GPT2ContentFormatter is a more suitable name. You can still continue to use OSSContentFormatter as the changes are backwards compatibile.Below is an example using a summarization model from Hugging Face.Custom Content Formatter​from typing import Dictfrom langchain.llms.azureml_endpoint import AzureMLOnlineEndpoint, ContentFormatterBaseimport osimport jsonclass CustomFormatter(ContentFormatterBase):    content_type = "application/json"    accepts = "application/json"    def format_request_payload(self, prompt: str, model_kwargs: Dict) -> bytes:        input_str = json.dumps(            {                "inputs": [prompt],                "parameters": model_kwargs,                "options": {"use_cache": False, "wait_for_model": True},            }        )        return str.encode(input_str)    def format_response_payload(self, output: bytes) -> str:        response_json = json.loads(output)        return response_json[0]["summary_text"]content_formatter = CustomFormatter()llm = AzureMLOnlineEndpoint(    endpoint_api_key=os.getenv("BART_ENDPOINT_API_KEY"),    endpoint_url=os.getenv("BART_ENDPOINT_URL"),    model_kwargs={"temperature": 0.8, "max_new_tokens": 400},    content_formatter=content_formatter,)large_text = """On January 7, 2020, Blockberry Creative announced that HaSeul would not participate in the promotion for Loona's next album because of mental health concerns. She was said to be diagnosed with "intermittent anxiety symptoms" and would be taking time to focus on her health.[39] On February 5, 2020, Loona released their second EP titled [#] (read as hash), along with the title track "So What".[40] Although HaSeul did not appear in the title track, her vocals are featured on three other songs on the album, including "365". Once peaked at number 1 on the daily Gaon Retail Album Chart,[41] the EP then debuted at number 2 on the weekly Gaon Album Chart. On March 12, 2020, Loona won their first music show trophy with "So What" on Mnet's M Countdown.[42]On October 19, 2020, Loona released their third EP titled [12:00] (read as midnight),[43] accompanied by its first single "Why Not?". HaSeul was again not involved in the album, out of her own decision to focus on the recovery of her health.[44] The EP then became their first album to enter the Billboard 200, debuting at number 112.[45] On November 18, Loona released the music video for "Star", another song on [12:00].[46] Peaking at number 40, "Star" is Loona's first entry on the Billboard Mainstream Top 40, making them the second K-pop girl group to enter the chart.[47]On June 1, 2021, Loona announced that they would be having a comeback on June 28, with their fourth EP, [&] (read as and).[48] The following day, on June 2, a teaser was posted to Loona's official social media accounts showing twelve sets of eyes, confirming the return of member HaSeul who had been on hiatus since early 2020.[49] On June 12, group members YeoJin, Kim Lip, Choerry, and Go Won released the song "Yum-Yum" as a collaboration with Cocomong.[50] On September 8, they released another collaboration song named "Yummy-Yummy".[51] On June 27, 2021, Loona announced at the end of their special clip that they are making their Japanese debut on September 15 under Universal Music Japan sublabel EMI Records.[52] On August 27, it was announced that Loona will release the double A-side single, "Hula Hoop / Star Seed" on September 15, with a physical CD release on October 20.[53] In December, Chuu filed an injunction to suspend her exclusive contract with Blockberry Creative.[54][55]"""summarized_text = llm(large_text)print(summarized_text)API Reference:AzureMLOnlineEndpointContentFormatterBase    HaSeul won her first music show trophy with "So What" on Mnet's M Countdown. Loona released their second EP titled [#] (read as hash] on February 5, 2020. HaSeul did not take part in the promotion of the album because of mental health issues. On October 19, 2020, they released their third EP called [12:00]. It was their first album to enter the Billboard 200, debuting at number 112. On June 2, 2021, the group released their fourth EP called Yummy-Yummy. On August 27, it was announced that they are making their Japanese debut on September 15 under Universal Music Japan sublabel EMI Records.Dolly with LLMChain​from langchain import PromptTemplatefrom langchain.llms.azureml_endpoint import DollyContentFormatterfrom langchain.chains import LLMChainformatter_template = "Write a {word_count} word essay about {topic}."prompt = PromptTemplate(    input_variables=["word_count", "topic"], template=formatter_template)content_formatter = DollyContentFormatter()llm = AzureMLOnlineEndpoint(    endpoint_api_key=os.getenv("DOLLY_ENDPOINT_API_KEY"),    endpoint_url=os.getenv("DOLLY_ENDPOINT_URL"),    model_kwargs={"temperature": 0.8, "max_tokens": 300},    content_formatter=content_formatter,)chain = LLMChain(llm=llm, prompt=prompt)print(chain.run({"word_count": 100, "topic": "how to make friends"}))API Reference:DollyContentFormatterLLMChain    Many people are willing to talk about themselves; it's others who seem to be stuck up. Try to understand others where they're coming from. Like minded people can build a tribe together.Serializing an LLM​You can also save and load LLM configurationsfrom langchain.llms.loading import load_llmfrom langchain.llms.azureml_endpoint import AzureMLEndpointClientsave_llm = AzureMLOnlineEndpoint(    deployment_name="databricks-dolly-v2-12b-4",    model_kwargs={        "temperature": 0.2,        "max_tokens": 150,        "top_p": 0.8,        "frequency_penalty": 0.32,        "presence_penalty": 72e-3,    },)save_llm.save("azureml.json")loaded_llm = load_llm("azureml.json")print(loaded_llm)API Reference:load_llmAzureMLEndpointClient    AzureMLOnlineEndpoint    Params: {'deployment_name': 'databricks-dolly-v2-12b-4', 'model_kwargs': {'temperature': 0.2, 'max_tokens': 150, 'top_p': 0.8, 'frequency_penalty': 0.32, 'presence_penalty': 0.072}}PreviousAnyscaleNextAzure OpenAISet upContent FormatterCustom Content FormatterDolly with LLMChainSerializing an LLMCommunityDiscordTwitterGitHubPythonJS/TSMoreHomepageBlogCopyright © 2023 LangChain, Inc. Skip to main content 🦜️🔗 LangChainDocsUse casesIntegrationsAPILangSmithJS/TS DocsCTRLK 🦜️🔗 LangChainDocsUse casesIntegrationsAPI LangSmithJS/TS DocsCTRLK  CTRLK CTRLK  CTRLK   IntegrationsCallbacksChat modelsChat loadersDocument loadersDocument transformersLLMsAI21Aleph AlphaAmazon API GatewayAnyscaleAzure MLAzure OpenAIBananaBasetenBeamBedrockBittensorCerebriumAIChatGLMClarifaiCohereC TransformersCTranslate2DatabricksDeepInfraDeepSparseEden AIFireworksForefrontAIGoogle Vertex AI PaLMGooseAIGPT4AllHugging Face HubHugging Face Local PipelinesHuggingface TextGen InferenceJSONFormerKoboldAI APILlama.cppLLM Caching integrationsManifestMinimaxModalMosaicMLNLP CloudOctoAIOllamaOpaquePromptsOpenAIOpenLLMOpenLMPetalsPipelineAIPredibasePrediction GuardPromptLayer OpenAIRELLMReplicateRunhouseSageMakerEndpointStochasticAINebula (Symbl.ai)TextGenTitan TakeoffTongyi QwenvLLMWriterXorbits Inference (Xinference)MemoryRetrieversText embedding modelsAgents & ToolkitsToolsVector storesGrouped by providerIntegrationsLLMsAzure MLOn this pageAzure MLAzure ML is a platform used to build, train, and deploy machine learning models. Users can explore the types of models to deploy in the Model Catalog, which provides Azure Foundation Models and OpenAI Models. Azure Foundation Models include various open-source models and popular Hugging Face models. Users can also import models of their liking into AzureML.This notebook goes over how to use an LLM hosted on an AzureML online endpointfrom langchain.llms.azureml_endpoint import AzureMLOnlineEndpointAPI Reference:AzureMLOnlineEndpointSet up​To use the wrapper, you must deploy a model on AzureML and obtain the following parameters:endpoint_api_key: Required - The API key provided by the endpointendpoint_url: Required - The REST endpoint url provided by the endpointdeployment_name: Not required - The deployment name of the model using the endpointContent Formatter​The content_formatter parameter is a handler class for transforming the request and response of an AzureML endpoint to match with required schema. Since there are a wide range of models in the model catalog, each of which may process data differently from one another, a ContentFormatterBase class is provided to allow users to transform data to their liking. The following content formatters are provided:GPT2ContentFormatter: Formats request and response data for GPT2DollyContentFormatter: Formats request and response data for the Dolly-v2HFContentFormatter: Formats request and response data for text-generation Hugging Face modelsLLamaContentFormatter: Formats request and response data for LLaMa2Note: OSSContentFormatter is being deprecated and replaced with GPT2ContentFormatter. The logic is the same but GPT2ContentFormatter is a more suitable name. You can still continue to use OSSContentFormatter as the changes are backwards compatibile.Below is an example using a summarization model from Hugging Face.Custom Content Formatter​from typing import Dictfrom langchain.llms.azureml_endpoint import AzureMLOnlineEndpoint, ContentFormatterBaseimport osimport jsonclass CustomFormatter(ContentFormatterBase):    content_type = "application/json"    accepts = "application/json"    def format_request_payload(self, prompt: str, model_kwargs: Dict) -> bytes:        input_str = json.dumps(            {                "inputs": [prompt],                "parameters": model_kwargs,                "options": {"use_cache": False, "wait_for_model": True},            }        )        return str.encode(input_str)    def format_response_payload(self, output: bytes) -> str:        response_json = json.loads(output)        return response_json[0]["summary_text"]content_formatter = CustomFormatter()llm = AzureMLOnlineEndpoint(    endpoint_api_key=os.getenv("BART_ENDPOINT_API_KEY"),    endpoint_url=os.getenv("BART_ENDPOINT_URL"),    model_kwargs={"temperature": 0.8, "max_new_tokens": 400},    content_formatter=content_formatter,)large_text = """On January 7, 2020, Blockberry Creative announced that HaSeul would not participate in the promotion for Loona's next album because of mental health concerns. She was said to be diagnosed with "intermittent anxiety symptoms" and would be taking time to focus on her health.[39] On February 5, 2020, Loona released their second EP titled [#] (read as hash), along with the title track "So What".[40] Although HaSeul did not appear in the title track, her vocals are featured on three other songs on the album, including "365". Once peaked at number 1 on the daily Gaon Retail Album Chart,[41] the EP then debuted at number 2 on the weekly Gaon Album Chart. On March 12, 2020, Loona won their first music show trophy with "So What" on Mnet's M Countdown.[42]On October 19, 2020, Loona released their third EP titled [12:00] (read as midnight),[43] accompanied by its first single "Why Not?". HaSeul was again not involved in the album, out of her own decision to focus on the recovery of her health.[44] The EP then became their first album to enter the Billboard 200, debuting at number 112.[45] On November 18, Loona released the music video for "Star", another song on [12:00].[46] Peaking at number 40, "Star" is Loona's first entry on the Billboard Mainstream Top 40, making them the second K-pop girl group to enter the chart.[47]On June 1, 2021, Loona announced that they would be having a comeback on June 28, with their fourth EP, [&] (read as and).[48] The following day, on June 2, a teaser was posted to Loona's official social media accounts showing twelve sets of eyes, confirming the return of member HaSeul who had been on hiatus since early 2020.[49] On June 12, group members YeoJin, Kim Lip, Choerry, and Go Won released the song "Yum-Yum" as a collaboration with Cocomong.[50] On September 8, they released another collaboration song named "Yummy-Yummy".[51] On June 27, 2021, Loona announced at the end of their special clip that they are making their Japanese debut on September 15 under Universal Music Japan sublabel EMI Records.[52] On August 27, it was announced that Loona will release the double A-side single, "Hula Hoop / Star Seed" on September 15, with a physical CD release on October 20.[53] In December, Chuu filed an injunction to suspend her exclusive contract with Blockberry Creative.[54][55]"""summarized_text = llm(large_text)print(summarized_text)API Reference:AzureMLOnlineEndpointContentFormatterBase    HaSeul won her first music show trophy with "So What" on Mnet's M Countdown. Loona released their second EP titled [#] (read as hash] on February 5, 2020. HaSeul did not take part in the promotion of the album because of mental health issues. On October 19, 2020, they released their third EP called [12:00]. It was their first album to enter the Billboard 200, debuting at number 112. On June 2, 2021, the group released their fourth EP called Yummy-Yummy. On August 27, it was announced that they are making their Japanese debut on September 15 under Universal Music Japan sublabel EMI Records.Dolly with LLMChain​from langchain import PromptTemplatefrom langchain.llms.azureml_endpoint import DollyContentFormatterfrom langchain.chains import LLMChainformatter_template = "Write a {word_count} word essay about {topic}."prompt = PromptTemplate(    input_variables=["word_count", "topic"], template=formatter_template)content_formatter = DollyContentFormatter()llm = AzureMLOnlineEndpoint(    endpoint_api_key=os.getenv("DOLLY_ENDPOINT_API_KEY"),    endpoint_url=os.getenv("DOLLY_ENDPOINT_URL"),    model_kwargs={"temperature": 0.8, "max_tokens": 300},    content_formatter=content_formatter,)chain = LLMChain(llm=llm, prompt=prompt)print(chain.run({"word_count": 100, "topic": "how to make friends"}))API Reference:DollyContentFormatterLLMChain    Many people are willing to talk about themselves; it's others who seem to be stuck up. Try to understand others where they're coming from. Like minded people can build a tribe together.Serializing an LLM​You can also save and load LLM configurationsfrom langchain.llms.loading import load_llmfrom langchain.llms.azureml_endpoint import AzureMLEndpointClientsave_llm = AzureMLOnlineEndpoint(    deployment_name="databricks-dolly-v2-12b-4",    model_kwargs={        "temperature": 0.2,        "max_tokens": 150,        "top_p": 0.8,        "frequency_penalty": 0.32,        "presence_penalty": 72e-3,    },)save_llm.save("azureml.json")loaded_llm = load_llm("azureml.json")print(loaded_llm)API Reference:load_llmAzureMLEndpointClient    AzureMLOnlineEndpoint    Params: {'deployment_name': 'databricks-dolly-v2-12b-4', 'model_kwargs': {'temperature': 0.2, 'max_tokens': 150, 'top_p': 0.8, 'frequency_penalty': 0.32, 'presence_penalty': 0.072}}PreviousAnyscaleNextAzure OpenAISet upContent FormatterCustom Content FormatterDolly with LLMChainSerializing an LLM IntegrationsCallbacksChat modelsChat loadersDocument loadersDocument transformersLLMsAI21Aleph AlphaAmazon API GatewayAnyscaleAzure MLAzure OpenAIBananaBasetenBeamBedrockBittensorCerebriumAIChatGLMClarifaiCohereC TransformersCTranslate2DatabricksDeepInfraDeepSparseEden AIFireworksForefrontAIGoogle Vertex AI PaLMGooseAIGPT4AllHugging Face HubHugging Face Local PipelinesHuggingface TextGen InferenceJSONFormerKoboldAI APILlama.cppLLM Caching integrationsManifestMinimaxModalMosaicMLNLP CloudOctoAIOllamaOpaquePromptsOpenAIOpenLLMOpenLMPetalsPipelineAIPredibasePrediction GuardPromptLayer OpenAIRELLMReplicateRunhouseSageMakerEndpointStochasticAINebula (Symbl.ai)TextGenTitan TakeoffTongyi QwenvLLMWriterXorbits Inference (Xinference)MemoryRetrieversText embedding modelsAgents & ToolkitsToolsVector storesGrouped by providerIntegrationsLLMsAzure MLOn this pageAzure MLAzure ML is a platform used to build, train, and deploy machine learning models. Users can explore the types of models to deploy in the Model Catalog, which provides Azure Foundation Models and OpenAI Models. Azure Foundation Models include various open-source models and popular Hugging Face models. Users can also import models of their liking into AzureML.This notebook goes over how to use an LLM hosted on an AzureML online endpointfrom langchain.llms.azureml_endpoint import AzureMLOnlineEndpointAPI Reference:AzureMLOnlineEndpointSet up​To use the wrapper, you must deploy a model on AzureML and obtain the following parameters:endpoint_api_key: Required - The API key provided by the endpointendpoint_url: Required - The REST endpoint url provided by the endpointdeployment_name: Not required - The deployment name of the model using the endpointContent Formatter​The content_formatter parameter is a handler class for transforming the request and response of an AzureML endpoint to match with required schema. Since there are a wide range of models in the model catalog, each of which may process data differently from one another, a ContentFormatterBase class is provided to allow users to transform data to their liking. The following content formatters are provided:GPT2ContentFormatter: Formats request and response data for GPT2DollyContentFormatter: Formats request and response data for the Dolly-v2HFContentFormatter: Formats request and response data for text-generation Hugging Face modelsLLamaContentFormatter: Formats request and response data for LLaMa2Note: OSSContentFormatter is being deprecated and replaced with GPT2ContentFormatter. The logic is the same but GPT2ContentFormatter is a more suitable name. You can still continue to use OSSContentFormatter as the changes are backwards compatibile.Below is an example using a summarization model from Hugging Face.Custom Content Formatter​from typing import Dictfrom langchain.llms.azureml_endpoint import AzureMLOnlineEndpoint, ContentFormatterBaseimport osimport jsonclass CustomFormatter(ContentFormatterBase):    content_type = "application/json"    accepts = "application/json"    def format_request_payload(self, prompt: str, model_kwargs: Dict) -> bytes:        input_str = json.dumps(            {                "inputs": [prompt],                "parameters": model_kwargs,                "options": {"use_cache": False, "wait_for_model": True},            }        )        return str.encode(input_str)    def format_response_payload(self, output: bytes) -> str:        response_json = json.loads(output)        return response_json[0]["summary_text"]content_formatter = CustomFormatter()llm = AzureMLOnlineEndpoint(    endpoint_api_key=os.getenv("BART_ENDPOINT_API_KEY"),    endpoint_url=os.getenv("BART_ENDPOINT_URL"),    model_kwargs={"temperature": 0.8, "max_new_tokens": 400},    content_formatter=content_formatter,)large_text = """On January 7, 2020, Blockberry Creative announced that HaSeul would not participate in the promotion for Loona's next album because of mental health concerns. She was said to be diagnosed with "intermittent anxiety symptoms" and would be taking time to focus on her health.[39] On February 5, 2020, Loona released their second EP titled [#] (read as hash), along with the title track "So What".[40] Although HaSeul did not appear in the title track, her vocals are featured on three other songs on the album, including "365". Once peaked at number 1 on the daily Gaon Retail Album Chart,[41] the EP then debuted at number 2 on the weekly Gaon Album Chart. On March 12, 2020, Loona won their first music show trophy with "So What" on Mnet's M Countdown.[42]On October 19, 2020, Loona released their third EP titled [12:00] (read as midnight),[43] accompanied by its first single "Why Not?". HaSeul was again not involved in the album, out of her own decision to focus on the recovery of her health.[44] The EP then became their first album to enter the Billboard 200, debuting at number 112.[45] On November 18, Loona released the music video for "Star", another song on [12:00].[46] Peaking at number 40, "Star" is Loona's first entry on the Billboard Mainstream Top 40, making them the second K-pop girl group to enter the chart.[47]On June 1, 2021, Loona announced that they would be having a comeback on June 28, with their fourth EP, [&] (read as and).[48] The following day, on June 2, a teaser was posted to Loona's official social media accounts showing twelve sets of eyes, confirming the return of member HaSeul who had been on hiatus since early 2020.[49] On June 12, group members YeoJin, Kim Lip, Choerry, and Go Won released the song "Yum-Yum" as a collaboration with Cocomong.[50] On September 8, they released another collaboration song named "Yummy-Yummy".[51] On June 27, 2021, Loona announced at the end of their special clip that they are making their Japanese debut on September 15 under Universal Music Japan sublabel EMI Records.[52] On August 27, it was announced that Loona will release the double A-side single, "Hula Hoop / Star Seed" on September 15, with a physical CD release on October 20.[53] In December, Chuu filed an injunction to suspend her exclusive contract with Blockberry Creative.[54][55]"""summarized_text = llm(large_text)print(summarized_text)API Reference:AzureMLOnlineEndpointContentFormatterBase    HaSeul won her first music show trophy with "So What" on Mnet's M Countdown. Loona released their second EP titled [#] (read as hash] on February 5, 2020. HaSeul did not take part in the promotion of the album because of mental health issues. On October 19, 2020, they released their third EP called [12:00]. It was their first album to enter the Billboard 200, debuting at number 112. On June 2, 2021, the group released their fourth EP called Yummy-Yummy. On August 27, it was announced that they are making their Japanese debut on September 15 under Universal Music Japan sublabel EMI Records.Dolly with LLMChain​from langchain import PromptTemplatefrom langchain.llms.azureml_endpoint import DollyContentFormatterfrom langchain.chains import LLMChainformatter_template = "Write a {word_count} word essay about {topic}."prompt = PromptTemplate(    input_variables=["word_count", "topic"], template=formatter_template)content_formatter = DollyContentFormatter()llm = AzureMLOnlineEndpoint(    endpoint_api_key=os.getenv("DOLLY_ENDPOINT_API_KEY"),    endpoint_url=os.getenv("DOLLY_ENDPOINT_URL"),    model_kwargs={"temperature": 0.8, "max_tokens": 300},    content_formatter=content_formatter,)chain = LLMChain(llm=llm, prompt=prompt)print(chain.run({"word_count": 100, "topic": "how to make friends"}))API Reference:DollyContentFormatterLLMChain    Many people are willing to talk about themselves; it's others who seem to be stuck up. Try to understand others where they're coming from. Like minded people can build a tribe together.Serializing an LLM​You can also save and load LLM configurationsfrom langchain.llms.loading import load_llmfrom langchain.llms.azureml_endpoint import AzureMLEndpointClientsave_llm = AzureMLOnlineEndpoint(    deployment_name="databricks-dolly-v2-12b-4",    model_kwargs={        "temperature": 0.2,        "max_tokens": 150,        "top_p": 0.8,        "frequency_penalty": 0.32,        "presence_penalty": 72e-3,    },)save_llm.save("azureml.json")loaded_llm = load_llm("azureml.json")print(loaded_llm)API Reference:load_llmAzureMLEndpointClient    AzureMLOnlineEndpoint    Params: {'deployment_name': 'databricks-dolly-v2-12b-4', 'model_kwargs': {'temperature': 0.2, 'max_tokens': 150, 'top_p': 0.8, 'frequency_penalty': 0.32, 'presence_penalty': 0.072}}PreviousAnyscaleNextAzure OpenAISet upContent FormatterCustom Content FormatterDolly with LLMChainSerializing an LLM IntegrationsCallbacksChat modelsChat loadersDocument loadersDocument transformersLLMsAI21Aleph AlphaAmazon API GatewayAnyscaleAzure MLAzure OpenAIBananaBasetenBeamBedrockBittensorCerebriumAIChatGLMClarifaiCohereC TransformersCTranslate2DatabricksDeepInfraDeepSparseEden AIFireworksForefrontAIGoogle Vertex AI PaLMGooseAIGPT4AllHugging Face HubHugging Face Local PipelinesHuggingface TextGen InferenceJSONFormerKoboldAI APILlama.cppLLM Caching integrationsManifestMinimaxModalMosaicMLNLP CloudOctoAIOllamaOpaquePromptsOpenAIOpenLLMOpenLMPetalsPipelineAIPredibasePrediction GuardPromptLayer OpenAIRELLMReplicateRunhouseSageMakerEndpointStochasticAINebula (Symbl.ai)TextGenTitan TakeoffTongyi QwenvLLMWriterXorbits Inference (Xinference)MemoryRetrieversText embedding modelsAgents & ToolkitsToolsVector storesGrouped by provider IntegrationsCallbacksChat modelsChat loadersDocument loadersDocument transformersLLMsAI21Aleph AlphaAmazon API GatewayAnyscaleAzure MLAzure OpenAIBananaBasetenBeamBedrockBittensorCerebriumAIChatGLMClarifaiCohereC TransformersCTranslate2DatabricksDeepInfraDeepSparseEden AIFireworksForefrontAIGoogle Vertex AI PaLMGooseAIGPT4AllHugging Face HubHugging Face Local PipelinesHuggingface TextGen InferenceJSONFormerKoboldAI APILlama.cppLLM Caching integrationsManifestMinimaxModalMosaicMLNLP CloudOctoAIOllamaOpaquePromptsOpenAIOpenLLMOpenLMPetalsPipelineAIPredibasePrediction GuardPromptLayer OpenAIRELLMReplicateRunhouseSageMakerEndpointStochasticAINebula (Symbl.ai)TextGenTitan TakeoffTongyi QwenvLLMWriterXorbits Inference (Xinference)MemoryRetrieversText embedding modelsAgents & ToolkitsToolsVector storesGrouped by provider Integrations Callbacks Chat models Chat loaders Document loaders Document transformers LLMs Memory Retrievers Text embedding models Agents & Toolkits Tools Vector stores Grouped by provider IntegrationsLLMsAzure MLOn this pageAzure MLAzure ML is a platform used to build, train, and deploy machine learning models. Users can explore the types of models to deploy in the Model Catalog, which provides Azure Foundation Models and OpenAI Models. Azure Foundation Models include various open-source models and popular Hugging Face models. Users can also import models of their liking into AzureML.This notebook goes over how to use an LLM hosted on an AzureML online endpointfrom langchain.llms.azureml_endpoint import AzureMLOnlineEndpointAPI Reference:AzureMLOnlineEndpointSet up​To use the wrapper, you must deploy a model on AzureML and obtain the following parameters:endpoint_api_key: Required - The API key provided by the endpointendpoint_url: Required - The REST endpoint url provided by the endpointdeployment_name: Not required - The deployment name of the model using the endpointContent Formatter​The content_formatter parameter is a handler class for transforming the request and response of an AzureML endpoint to match with required schema. Since there are a wide range of models in the model catalog, each of which may process data differently from one another, a ContentFormatterBase class is provided to allow users to transform data to their liking. The following content formatters are provided:GPT2ContentFormatter: Formats request and response data for GPT2DollyContentFormatter: Formats request and response data for the Dolly-v2HFContentFormatter: Formats request and response data for text-generation Hugging Face modelsLLamaContentFormatter: Formats request and response data for LLaMa2Note: OSSContentFormatter is being deprecated and replaced with GPT2ContentFormatter. The logic is the same but GPT2ContentFormatter is a more suitable name. You can still continue to use OSSContentFormatter as the changes are backwards compatibile.Below is an example using a summarization model from Hugging Face.Custom Content Formatter​from typing import Dictfrom langchain.llms.azureml_endpoint import AzureMLOnlineEndpoint, ContentFormatterBaseimport osimport jsonclass CustomFormatter(ContentFormatterBase):    content_type = "application/json"    accepts = "application/json"    def format_request_payload(self, prompt: str, model_kwargs: Dict) -> bytes:        input_str = json.dumps(            {                "inputs": [prompt],                "parameters": model_kwargs,                "options": {"use_cache": False, "wait_for_model": True},            }        )        return str.encode(input_str)    def format_response_payload(self, output: bytes) -> str:        response_json = json.loads(output)        return response_json[0]["summary_text"]content_formatter = CustomFormatter()llm = AzureMLOnlineEndpoint(    endpoint_api_key=os.getenv("BART_ENDPOINT_API_KEY"),    endpoint_url=os.getenv("BART_ENDPOINT_URL"),    model_kwargs={"temperature": 0.8, "max_new_tokens": 400},    content_formatter=content_formatter,)large_text = """On January 7, 2020, Blockberry Creative announced that HaSeul would not participate in the promotion for Loona's next album because of mental health concerns. She was said to be diagnosed with "intermittent anxiety symptoms" and would be taking time to focus on her health.[39] On February 5, 2020, Loona released their second EP titled [#] (read as hash), along with the title track "So What".[40] Although HaSeul did not appear in the title track, her vocals are featured on three other songs on the album, including "365". Once peaked at number 1 on the daily Gaon Retail Album Chart,[41] the EP then debuted at number 2 on the weekly Gaon Album Chart. On March 12, 2020, Loona won their first music show trophy with "So What" on Mnet's M Countdown.[42]On October 19, 2020, Loona released their third EP titled [12:00] (read as midnight),[43] accompanied by its first single "Why Not?". HaSeul was again not involved in the album, out of her own decision to focus on the recovery of her health.[44] The EP then became their first album to enter the Billboard 200, debuting at number 112.[45] On November 18, Loona released the music video for "Star", another song on [12:00].[46] Peaking at number 40, "Star" is Loona's first entry on the Billboard Mainstream Top 40, making them the second K-pop girl group to enter the chart.[47]On June 1, 2021, Loona announced that they would be having a comeback on June 28, with their fourth EP, [&] (read as and).[48] The following day, on June 2, a teaser was posted to Loona's official social media accounts showing twelve sets of eyes, confirming the return of member HaSeul who had been on hiatus since early 2020.[49] On June 12, group members YeoJin, Kim Lip, Choerry, and Go Won released the song "Yum-Yum" as a collaboration with Cocomong.[50] On September 8, they released another collaboration song named "Yummy-Yummy".[51] On June 27, 2021, Loona announced at the end of their special clip that they are making their Japanese debut on September 15 under Universal Music Japan sublabel EMI Records.[52] On August 27, it was announced that Loona will release the double A-side single, "Hula Hoop / Star Seed" on September 15, with a physical CD release on October 20.[53] In December, Chuu filed an injunction to suspend her exclusive contract with Blockberry Creative.[54][55]"""summarized_text = llm(large_text)print(summarized_text)API Reference:AzureMLOnlineEndpointContentFormatterBase    HaSeul won her first music show trophy with "So What" on Mnet's M Countdown. Loona released their second EP titled [#] (read as hash] on February 5, 2020. HaSeul did not take part in the promotion of the album because of mental health issues. On October 19, 2020, they released their third EP called [12:00]. It was their first album to enter the Billboard 200, debuting at number 112. On June 2, 2021, the group released their fourth EP called Yummy-Yummy. On August 27, it was announced that they are making their Japanese debut on September 15 under Universal Music Japan sublabel EMI Records.Dolly with LLMChain​from langchain import PromptTemplatefrom langchain.llms.azureml_endpoint import DollyContentFormatterfrom langchain.chains import LLMChainformatter_template = "Write a {word_count} word essay about {topic}."prompt = PromptTemplate(    input_variables=["word_count", "topic"], template=formatter_template)content_formatter = DollyContentFormatter()llm = AzureMLOnlineEndpoint(    endpoint_api_key=os.getenv("DOLLY_ENDPOINT_API_KEY"),    endpoint_url=os.getenv("DOLLY_ENDPOINT_URL"),    model_kwargs={"temperature": 0.8, "max_tokens": 300},    content_formatter=content_formatter,)chain = LLMChain(llm=llm, prompt=prompt)print(chain.run({"word_count": 100, "topic": "how to make friends"}))API Reference:DollyContentFormatterLLMChain    Many people are willing to talk about themselves; it's others who seem to be stuck up. Try to understand others where they're coming from. Like minded people can build a tribe together.Serializing an LLM​You can also save and load LLM configurationsfrom langchain.llms.loading import load_llmfrom langchain.llms.azureml_endpoint import AzureMLEndpointClientsave_llm = AzureMLOnlineEndpoint(    deployment_name="databricks-dolly-v2-12b-4",    model_kwargs={        "temperature": 0.2,        "max_tokens": 150,        "top_p": 0.8,        "frequency_penalty": 0.32,        "presence_penalty": 72e-3,    },)save_llm.save("azureml.json")loaded_llm = load_llm("azureml.json")print(loaded_llm)API Reference:load_llmAzureMLEndpointClient    AzureMLOnlineEndpoint    Params: {'deployment_name': 'databricks-dolly-v2-12b-4', 'model_kwargs': {'temperature': 0.2, 'max_tokens': 150, 'top_p': 0.8, 'frequency_penalty': 0.32, 'presence_penalty': 0.072}}PreviousAnyscaleNextAzure OpenAISet upContent FormatterCustom Content FormatterDolly with LLMChainSerializing an LLM IntegrationsLLMsAzure MLOn this pageAzure MLAzure ML is a platform used to build, train, and deploy machine learning models. Users can explore the types of models to deploy in the Model Catalog, which provides Azure Foundation Models and OpenAI Models. Azure Foundation Models include various open-source models and popular Hugging Face models. Users can also import models of their liking into AzureML.This notebook goes over how to use an LLM hosted on an AzureML online endpointfrom langchain.llms.azureml_endpoint import AzureMLOnlineEndpointAPI Reference:AzureMLOnlineEndpointSet up​To use the wrapper, you must deploy a model on AzureML and obtain the following parameters:endpoint_api_key: Required - The API key provided by the endpointendpoint_url: Required - The REST endpoint url provided by the endpointdeployment_name: Not required - The deployment name of the model using the endpointContent Formatter​The content_formatter parameter is a handler class for transforming the request and response of an AzureML endpoint to match with required schema. Since there are a wide range of models in the model catalog, each of which may process data differently from one another, a ContentFormatterBase class is provided to allow users to transform data to their liking. The following content formatters are provided:GPT2ContentFormatter: Formats request and response data for GPT2DollyContentFormatter: Formats request and response data for the Dolly-v2HFContentFormatter: Formats request and response data for text-generation Hugging Face modelsLLamaContentFormatter: Formats request and response data for LLaMa2Note: OSSContentFormatter is being deprecated and replaced with GPT2ContentFormatter. The logic is the same but GPT2ContentFormatter is a more suitable name. You can still continue to use OSSContentFormatter as the changes are backwards compatibile.Below is an example using a summarization model from Hugging Face.Custom Content Formatter​from typing import Dictfrom langchain.llms.azureml_endpoint import AzureMLOnlineEndpoint, ContentFormatterBaseimport osimport jsonclass CustomFormatter(ContentFormatterBase):    content_type = "application/json"    accepts = "application/json"    def format_request_payload(self, prompt: str, model_kwargs: Dict) -> bytes:        input_str = json.dumps(            {                "inputs": [prompt],                "parameters": model_kwargs,                "options": {"use_cache": False, "wait_for_model": True},            }        )        return str.encode(input_str)    def format_response_payload(self, output: bytes) -> str:        response_json = json.loads(output)        return response_json[0]["summary_text"]content_formatter = CustomFormatter()llm = AzureMLOnlineEndpoint(    endpoint_api_key=os.getenv("BART_ENDPOINT_API_KEY"),    endpoint_url=os.getenv("BART_ENDPOINT_URL"),    model_kwargs={"temperature": 0.8, "max_new_tokens": 400},    content_formatter=content_formatter,)large_text = """On January 7, 2020, Blockberry Creative announced that HaSeul would not participate in the promotion for Loona's next album because of mental health concerns. She was said to be diagnosed with "intermittent anxiety symptoms" and would be taking time to focus on her health.[39] On February 5, 2020, Loona released their second EP titled [#] (read as hash), along with the title track "So What".[40] Although HaSeul did not appear in the title track, her vocals are featured on three other songs on the album, including "365". Once peaked at number 1 on the daily Gaon Retail Album Chart,[41] the EP then debuted at number 2 on the weekly Gaon Album Chart. On March 12, 2020, Loona won their first music show trophy with "So What" on Mnet's M Countdown.[42]On October 19, 2020, Loona released their third EP titled [12:00] (read as midnight),[43] accompanied by its first single "Why Not?". HaSeul was again not involved in the album, out of her own decision to focus on the recovery of her health.[44] The EP then became their first album to enter the Billboard 200, debuting at number 112.[45] On November 18, Loona released the music video for "Star", another song on [12:00].[46] Peaking at number 40, "Star" is Loona's first entry on the Billboard Mainstream Top 40, making them the second K-pop girl group to enter the chart.[47]On June 1, 2021, Loona announced that they would be having a comeback on June 28, with their fourth EP, [&] (read as and).[48] The following day, on June 2, a teaser was posted to Loona's official social media accounts showing twelve sets of eyes, confirming the return of member HaSeul who had been on hiatus since early 2020.[49] On June 12, group members YeoJin, Kim Lip, Choerry, and Go Won released the song "Yum-Yum" as a collaboration with Cocomong.[50] On September 8, they released another collaboration song named "Yummy-Yummy".[51] On June 27, 2021, Loona announced at the end of their special clip that they are making their Japanese debut on September 15 under Universal Music Japan sublabel EMI Records.[52] On August 27, it was announced that Loona will release the double A-side single, "Hula Hoop / Star Seed" on September 15, with a physical CD release on October 20.[53] In December, Chuu filed an injunction to suspend her exclusive contract with Blockberry Creative.[54][55]"""summarized_text = llm(large_text)print(summarized_text)API Reference:AzureMLOnlineEndpointContentFormatterBase    HaSeul won her first music show trophy with "So What" on Mnet's M Countdown. Loona released their second EP titled [#] (read as hash] on February 5, 2020. HaSeul did not take part in the promotion of the album because of mental health issues. On October 19, 2020, they released their third EP called [12:00]. It was their first album to enter the Billboard 200, debuting at number 112. On June 2, 2021, the group released their fourth EP called Yummy-Yummy. On August 27, it was announced that they are making their Japanese debut on September 15 under Universal Music Japan sublabel EMI Records.Dolly with LLMChain​from langchain import PromptTemplatefrom langchain.llms.azureml_endpoint import DollyContentFormatterfrom langchain.chains import LLMChainformatter_template = "Write a {word_count} word essay about {topic}."prompt = PromptTemplate(    input_variables=["word_count", "topic"], template=formatter_template)content_formatter = DollyContentFormatter()llm = AzureMLOnlineEndpoint(    endpoint_api_key=os.getenv("DOLLY_ENDPOINT_API_KEY"),    endpoint_url=os.getenv("DOLLY_ENDPOINT_URL"),    model_kwargs={"temperature": 0.8, "max_tokens": 300},    content_formatter=content_formatter,)chain = LLMChain(llm=llm, prompt=prompt)print(chain.run({"word_count": 100, "topic": "how to make friends"}))API Reference:DollyContentFormatterLLMChain    Many people are willing to talk about themselves; it's others who seem to be stuck up. Try to understand others where they're coming from. Like minded people can build a tribe together.Serializing an LLM​You can also save and load LLM configurationsfrom langchain.llms.loading import load_llmfrom langchain.llms.azureml_endpoint import AzureMLEndpointClientsave_llm = AzureMLOnlineEndpoint(    deployment_name="databricks-dolly-v2-12b-4",    model_kwargs={        "temperature": 0.2,        "max_tokens": 150,        "top_p": 0.8,        "frequency_penalty": 0.32,        "presence_penalty": 72e-3,    },)save_llm.save("azureml.json")loaded_llm = load_llm("azureml.json")print(loaded_llm)API Reference:load_llmAzureMLEndpointClient    AzureMLOnlineEndpoint    Params: {'deployment_name': 'databricks-dolly-v2-12b-4', 'model_kwargs': {'temperature': 0.2, 'max_tokens': 150, 'top_p': 0.8, 'frequency_penalty': 0.32, 'presence_penalty': 0.072}}PreviousAnyscaleNextAzure OpenAISet upContent FormatterCustom Content FormatterDolly with LLMChainSerializing an LLM IntegrationsLLMsAzure MLOn this pageAzure MLAzure ML is a platform used to build, train, and deploy machine learning models. Users can explore the types of models to deploy in the Model Catalog, which provides Azure Foundation Models and OpenAI Models. Azure Foundation Models include various open-source models and popular Hugging Face models. Users can also import models of their liking into AzureML.This notebook goes over how to use an LLM hosted on an AzureML online endpointfrom langchain.llms.azureml_endpoint import AzureMLOnlineEndpointAPI Reference:AzureMLOnlineEndpointSet up​To use the wrapper, you must deploy a model on AzureML and obtain the following parameters:endpoint_api_key: Required - The API key provided by the endpointendpoint_url: Required - The REST endpoint url provided by the endpointdeployment_name: Not required - The deployment name of the model using the endpointContent Formatter​The content_formatter parameter is a handler class for transforming the request and response of an AzureML endpoint to match with required schema. Since there are a wide range of models in the model catalog, each of which may process data differently from one another, a ContentFormatterBase class is provided to allow users to transform data to their liking. The following content formatters are provided:GPT2ContentFormatter: Formats request and response data for GPT2DollyContentFormatter: Formats request and response data for the Dolly-v2HFContentFormatter: Formats request and response data for text-generation Hugging Face modelsLLamaContentFormatter: Formats request and response data for LLaMa2Note: OSSContentFormatter is being deprecated and replaced with GPT2ContentFormatter. The logic is the same but GPT2ContentFormatter is a more suitable name. You can still continue to use OSSContentFormatter as the changes are backwards compatibile.Below is an example using a summarization model from Hugging Face.Custom Content Formatter​from typing import Dictfrom langchain.llms.azureml_endpoint import AzureMLOnlineEndpoint, ContentFormatterBaseimport osimport jsonclass CustomFormatter(ContentFormatterBase):    content_type = "application/json"    accepts = "application/json"    def format_request_payload(self, prompt: str, model_kwargs: Dict) -> bytes:        input_str = json.dumps(            {                "inputs": [prompt],                "parameters": model_kwargs,                "options": {"use_cache": False, "wait_for_model": True},            }        )        return str.encode(input_str)    def format_response_payload(self, output: bytes) -> str:        response_json = json.loads(output)        return response_json[0]["summary_text"]content_formatter = CustomFormatter()llm = AzureMLOnlineEndpoint(    endpoint_api_key=os.getenv("BART_ENDPOINT_API_KEY"),    endpoint_url=os.getenv("BART_ENDPOINT_URL"),    model_kwargs={"temperature": 0.8, "max_new_tokens": 400},    content_formatter=content_formatter,)large_text = """On January 7, 2020, Blockberry Creative announced that HaSeul would not participate in the promotion for Loona's next album because of mental health concerns. She was said to be diagnosed with "intermittent anxiety symptoms" and would be taking time to focus on her health.[39] On February 5, 2020, Loona released their second EP titled [#] (read as hash), along with the title track "So What".[40] Although HaSeul did not appear in the title track, her vocals are featured on three other songs on the album, including "365". Once peaked at number 1 on the daily Gaon Retail Album Chart,[41] the EP then debuted at number 2 on the weekly Gaon Album Chart. On March 12, 2020, Loona won their first music show trophy with "So What" on Mnet's M Countdown.[42]On October 19, 2020, Loona released their third EP titled [12:00] (read as midnight),[43] accompanied by its first single "Why Not?". HaSeul was again not involved in the album, out of her own decision to focus on the recovery of her health.[44] The EP then became their first album to enter the Billboard 200, debuting at number 112.[45] On November 18, Loona released the music video for "Star", another song on [12:00].[46] Peaking at number 40, "Star" is Loona's first entry on the Billboard Mainstream Top 40, making them the second K-pop girl group to enter the chart.[47]On June 1, 2021, Loona announced that they would be having a comeback on June 28, with their fourth EP, [&] (read as and).[48] The following day, on June 2, a teaser was posted to Loona's official social media accounts showing twelve sets of eyes, confirming the return of member HaSeul who had been on hiatus since early 2020.[49] On June 12, group members YeoJin, Kim Lip, Choerry, and Go Won released the song "Yum-Yum" as a collaboration with Cocomong.[50] On September 8, they released another collaboration song named "Yummy-Yummy".[51] On June 27, 2021, Loona announced at the end of their special clip that they are making their Japanese debut on September 15 under Universal Music Japan sublabel EMI Records.[52] On August 27, it was announced that Loona will release the double A-side single, "Hula Hoop / Star Seed" on September 15, with a physical CD release on October 20.[53] In December, Chuu filed an injunction to suspend her exclusive contract with Blockberry Creative.[54][55]"""summarized_text = llm(large_text)print(summarized_text)API Reference:AzureMLOnlineEndpointContentFormatterBase    HaSeul won her first music show trophy with "So What" on Mnet's M Countdown. Loona released their second EP titled [#] (read as hash] on February 5, 2020. HaSeul did not take part in the promotion of the album because of mental health issues. On October 19, 2020, they released their third EP called [12:00]. It was their first album to enter the Billboard 200, debuting at number 112. On June 2, 2021, the group released their fourth EP called Yummy-Yummy. On August 27, it was announced that they are making their Japanese debut on September 15 under Universal Music Japan sublabel EMI Records.Dolly with LLMChain​from langchain import PromptTemplatefrom langchain.llms.azureml_endpoint import DollyContentFormatterfrom langchain.chains import LLMChainformatter_template = "Write a {word_count} word essay about {topic}."prompt = PromptTemplate(    input_variables=["word_count", "topic"], template=formatter_template)content_formatter = DollyContentFormatter()llm = AzureMLOnlineEndpoint(    endpoint_api_key=os.getenv("DOLLY_ENDPOINT_API_KEY"),    endpoint_url=os.getenv("DOLLY_ENDPOINT_URL"),    model_kwargs={"temperature": 0.8, "max_tokens": 300},    content_formatter=content_formatter,)chain = LLMChain(llm=llm, prompt=prompt)print(chain.run({"word_count": 100, "topic": "how to make friends"}))API Reference:DollyContentFormatterLLMChain    Many people are willing to talk about themselves; it's others who seem to be stuck up. Try to understand others where they're coming from. Like minded people can build a tribe together.Serializing an LLM​You can also save and load LLM configurationsfrom langchain.llms.loading import load_llmfrom langchain.llms.azureml_endpoint import AzureMLEndpointClientsave_llm = AzureMLOnlineEndpoint(    deployment_name="databricks-dolly-v2-12b-4",    model_kwargs={        "temperature": 0.2,        "max_tokens": 150,        "top_p": 0.8,        "frequency_penalty": 0.32,        "presence_penalty": 72e-3,    },)save_llm.save("azureml.json")loaded_llm = load_llm("azureml.json")print(loaded_llm)API Reference:load_llmAzureMLEndpointClient    AzureMLOnlineEndpoint    Params: {'deployment_name': 'databricks-dolly-v2-12b-4', 'model_kwargs': {'temperature': 0.2, 'max_tokens': 150, 'top_p': 0.8, 'frequency_penalty': 0.32, 'presence_penalty': 0.072}}PreviousAnyscaleNextAzure OpenAI IntegrationsLLMsAzure MLOn this pageAzure MLAzure ML is a platform used to build, train, and deploy machine learning models. Users can explore the types of models to deploy in the Model Catalog, which provides Azure Foundation Models and OpenAI Models. Azure Foundation Models include various open-source models and popular Hugging Face models. Users can also import models of their liking into AzureML.This notebook goes over how to use an LLM hosted on an AzureML online endpointfrom langchain.llms.azureml_endpoint import AzureMLOnlineEndpointAPI Reference:AzureMLOnlineEndpointSet up​To use the wrapper, you must deploy a model on AzureML and obtain the following parameters:endpoint_api_key: Required - The API key provided by the endpointendpoint_url: Required - The REST endpoint url provided by the endpointdeployment_name: Not required - The deployment name of the model using the endpointContent Formatter​The content_formatter parameter is a handler class for transforming the request and response of an AzureML endpoint to match with required schema. Since there are a wide range of models in the model catalog, each of which may process data differently from one another, a ContentFormatterBase class is provided to allow users to transform data to their liking. The following content formatters are provided:GPT2ContentFormatter: Formats request and response data for GPT2DollyContentFormatter: Formats request and response data for the Dolly-v2HFContentFormatter: Formats request and response data for text-generation Hugging Face modelsLLamaContentFormatter: Formats request and response data for LLaMa2Note: OSSContentFormatter is being deprecated and replaced with GPT2ContentFormatter. The logic is the same but GPT2ContentFormatter is a more suitable name. You can still continue to use OSSContentFormatter as the changes are backwards compatibile.Below is an example using a summarization model from Hugging Face.Custom Content Formatter​from typing import Dictfrom langchain.llms.azureml_endpoint import AzureMLOnlineEndpoint, ContentFormatterBaseimport osimport jsonclass CustomFormatter(ContentFormatterBase):    content_type = "application/json"    accepts = "application/json"    def format_request_payload(self, prompt: str, model_kwargs: Dict) -> bytes:        input_str = json.dumps(            {                "inputs": [prompt],                "parameters": model_kwargs,                "options": {"use_cache": False, "wait_for_model": True},            }        )        return str.encode(input_str)    def format_response_payload(self, output: bytes) -> str:        response_json = json.loads(output)        return response_json[0]["summary_text"]content_formatter = CustomFormatter()llm = AzureMLOnlineEndpoint(    endpoint_api_key=os.getenv("BART_ENDPOINT_API_KEY"),    endpoint_url=os.getenv("BART_ENDPOINT_URL"),    model_kwargs={"temperature": 0.8, "max_new_tokens": 400},    content_formatter=content_formatter,)large_text = """On January 7, 2020, Blockberry Creative announced that HaSeul would not participate in the promotion for Loona's next album because of mental health concerns. She was said to be diagnosed with "intermittent anxiety symptoms" and would be taking time to focus on her health.[39] On February 5, 2020, Loona released their second EP titled [#] (read as hash), along with the title track "So What".[40] Although HaSeul did not appear in the title track, her vocals are featured on three other songs on the album, including "365". Once peaked at number 1 on the daily Gaon Retail Album Chart,[41] the EP then debuted at number 2 on the weekly Gaon Album Chart. On March 12, 2020, Loona won their first music show trophy with "So What" on Mnet's M Countdown.[42]On October 19, 2020, Loona released their third EP titled [12:00] (read as midnight),[43] accompanied by its first single "Why Not?". HaSeul was again not involved in the album, out of her own decision to focus on the recovery of her health.[44] The EP then became their first album to enter the Billboard 200, debuting at number 112.[45] On November 18, Loona released the music video for "Star", another song on [12:00].[46] Peaking at number 40, "Star" is Loona's first entry on the Billboard Mainstream Top 40, making them the second K-pop girl group to enter the chart.[47]On June 1, 2021, Loona announced that they would be having a comeback on June 28, with their fourth EP, [&] (read as and).[48] The following day, on June 2, a teaser was posted to Loona's official social media accounts showing twelve sets of eyes, confirming the return of member HaSeul who had been on hiatus since early 2020.[49] On June 12, group members YeoJin, Kim Lip, Choerry, and Go Won released the song "Yum-Yum" as a collaboration with Cocomong.[50] On September 8, they released another collaboration song named "Yummy-Yummy".[51] On June 27, 2021, Loona announced at the end of their special clip that they are making their Japanese debut on September 15 under Universal Music Japan sublabel EMI Records.[52] On August 27, it was announced that Loona will release the double A-side single, "Hula Hoop / Star Seed" on September 15, with a physical CD release on October 20.[53] In December, Chuu filed an injunction to suspend her exclusive contract with Blockberry Creative.[54][55]"""summarized_text = llm(large_text)print(summarized_text)API Reference:AzureMLOnlineEndpointContentFormatterBase    HaSeul won her first music show trophy with "So What" on Mnet's M Countdown. Loona released their second EP titled [#] (read as hash] on February 5, 2020. HaSeul did not take part in the promotion of the album because of mental health issues. On October 19, 2020, they released their third EP called [12:00]. It was their first album to enter the Billboard 200, debuting at number 112. On June 2, 2021, the group released their fourth EP called Yummy-Yummy. On August 27, it was announced that they are making their Japanese debut on September 15 under Universal Music Japan sublabel EMI Records.Dolly with LLMChain​from langchain import PromptTemplatefrom langchain.llms.azureml_endpoint import DollyContentFormatterfrom langchain.chains import LLMChainformatter_template = "Write a {word_count} word essay about {topic}."prompt = PromptTemplate(    input_variables=["word_count", "topic"], template=formatter_template)content_formatter = DollyContentFormatter()llm = AzureMLOnlineEndpoint(    endpoint_api_key=os.getenv("DOLLY_ENDPOINT_API_KEY"),    endpoint_url=os.getenv("DOLLY_ENDPOINT_URL"),    model_kwargs={"temperature": 0.8, "max_tokens": 300},    content_formatter=content_formatter,)chain = LLMChain(llm=llm, prompt=prompt)print(chain.run({"word_count": 100, "topic": "how to make friends"}))API Reference:DollyContentFormatterLLMChain    Many people are willing to talk about themselves; it's others who seem to be stuck up. Try to understand others where they're coming from. Like minded people can build a tribe together.Serializing an LLM​You can also save and load LLM configurationsfrom langchain.llms.loading import load_llmfrom langchain.llms.azureml_endpoint import AzureMLEndpointClientsave_llm = AzureMLOnlineEndpoint(    deployment_name="databricks-dolly-v2-12b-4",    model_kwargs={        "temperature": 0.2,        "max_tokens": 150,        "top_p": 0.8,        "frequency_penalty": 0.32,        "presence_penalty": 72e-3,    },)save_llm.save("azureml.json")loaded_llm = load_llm("azureml.json")print(loaded_llm)API Reference:load_llmAzureMLEndpointClient    AzureMLOnlineEndpoint    Params: {'deployment_name': 'databricks-dolly-v2-12b-4', 'model_kwargs': {'temperature': 0.2, 'max_tokens': 150, 'top_p': 0.8, 'frequency_penalty': 0.32, 'presence_penalty': 0.072}}PreviousAnyscaleNextAzure OpenAI On this page Azure MLAzure ML is a platform used to build, train, and deploy machine learning models. Users can explore the types of models to deploy in the Model Catalog, which provides Azure Foundation Models and OpenAI Models. Azure Foundation Models include various open-source models and popular Hugging Face models. Users can also import models of their liking into AzureML.This notebook goes over how to use an LLM hosted on an AzureML online endpointfrom langchain.llms.azureml_endpoint import AzureMLOnlineEndpointAPI Reference:AzureMLOnlineEndpointSet up​To use the wrapper, you must deploy a model on AzureML and obtain the following parameters:endpoint_api_key: Required - The API key provided by the endpointendpoint_url: Required - The REST endpoint url provided by the endpointdeployment_name: Not required - The deployment name of the model using the endpointContent Formatter​The content_formatter parameter is a handler class for transforming the request and response of an AzureML endpoint to match with required schema. Since there are a wide range of models in the model catalog, each of which may process data differently from one another, a ContentFormatterBase class is provided to allow users to transform data to their liking. The following content formatters are provided:GPT2ContentFormatter: Formats request and response data for GPT2DollyContentFormatter: Formats request and response data for the Dolly-v2HFContentFormatter: Formats request and response data for text-generation Hugging Face modelsLLamaContentFormatter: Formats request and response data for LLaMa2Note: OSSContentFormatter is being deprecated and replaced with GPT2ContentFormatter. The logic is the same but GPT2ContentFormatter is a more suitable name. You can still continue to use OSSContentFormatter as the changes are backwards compatibile.Below is an example using a summarization model from Hugging Face.Custom Content Formatter​from typing import Dictfrom langchain.llms.azureml_endpoint import AzureMLOnlineEndpoint, ContentFormatterBaseimport osimport jsonclass CustomFormatter(ContentFormatterBase):    content_type = "application/json"    accepts = "application/json"    def format_request_payload(self, prompt: str, model_kwargs: Dict) -> bytes:        input_str = json.dumps(            {                "inputs": [prompt],                "parameters": model_kwargs,                "options": {"use_cache": False, "wait_for_model": True},            }        )        return str.encode(input_str)    def format_response_payload(self, output: bytes) -> str:        response_json = json.loads(output)        return response_json[0]["summary_text"]content_formatter = CustomFormatter()llm = AzureMLOnlineEndpoint(    endpoint_api_key=os.getenv("BART_ENDPOINT_API_KEY"),    endpoint_url=os.getenv("BART_ENDPOINT_URL"),    model_kwargs={"temperature": 0.8, "max_new_tokens": 400},    content_formatter=content_formatter,)large_text = """On January 7, 2020, Blockberry Creative announced that HaSeul would not participate in the promotion for Loona's next album because of mental health concerns. She was said to be diagnosed with "intermittent anxiety symptoms" and would be taking time to focus on her health.[39] On February 5, 2020, Loona released their second EP titled [#] (read as hash), along with the title track "So What".[40] Although HaSeul did not appear in the title track, her vocals are featured on three other songs on the album, including "365". Once peaked at number 1 on the daily Gaon Retail Album Chart,[41] the EP then debuted at number 2 on the weekly Gaon Album Chart. On March 12, 2020, Loona won their first music show trophy with "So What" on Mnet's M Countdown.[42]On October 19, 2020, Loona released their third EP titled [12:00] (read as midnight),[43] accompanied by its first single "Why Not?". HaSeul was again not involved in the album, out of her own decision to focus on the recovery of her health.[44] The EP then became their first album to enter the Billboard 200, debuting at number 112.[45] On November 18, Loona released the music video for "Star", another song on [12:00].[46] Peaking at number 40, "Star" is Loona's first entry on the Billboard Mainstream Top 40, making them the second K-pop girl group to enter the chart.[47]On June 1, 2021, Loona announced that they would be having a comeback on June 28, with their fourth EP, [&] (read as and).[48] The following day, on June 2, a teaser was posted to Loona's official social media accounts showing twelve sets of eyes, confirming the return of member HaSeul who had been on hiatus since early 2020.[49] On June 12, group members YeoJin, Kim Lip, Choerry, and Go Won released the song "Yum-Yum" as a collaboration with Cocomong.[50] On September 8, they released another collaboration song named "Yummy-Yummy".[51] On June 27, 2021, Loona announced at the end of their special clip that they are making their Japanese debut on September 15 under Universal Music Japan sublabel EMI Records.[52] On August 27, it was announced that Loona will release the double A-side single, "Hula Hoop / Star Seed" on September 15, with a physical CD release on October 20.[53] In December, Chuu filed an injunction to suspend her exclusive contract with Blockberry Creative.[54][55]"""summarized_text = llm(large_text)print(summarized_text)API Reference:AzureMLOnlineEndpointContentFormatterBase    HaSeul won her first music show trophy with "So What" on Mnet's M Countdown. Loona released their second EP titled [#] (read as hash] on February 5, 2020. HaSeul did not take part in the promotion of the album because of mental health issues. On October 19, 2020, they released their third EP called [12:00]. It was their first album to enter the Billboard 200, debuting at number 112. On June 2, 2021, the group released their fourth EP called Yummy-Yummy. On August 27, it was announced that they are making their Japanese debut on September 15 under Universal Music Japan sublabel EMI Records.Dolly with LLMChain​from langchain import PromptTemplatefrom langchain.llms.azureml_endpoint import DollyContentFormatterfrom langchain.chains import LLMChainformatter_template = "Write a {word_count} word essay about {topic}."prompt = PromptTemplate(    input_variables=["word_count", "topic"], template=formatter_template)content_formatter = DollyContentFormatter()llm = AzureMLOnlineEndpoint(    endpoint_api_key=os.getenv("DOLLY_ENDPOINT_API_KEY"),    endpoint_url=os.getenv("DOLLY_ENDPOINT_URL"),    model_kwargs={"temperature": 0.8, "max_tokens": 300},    content_formatter=content_formatter,)chain = LLMChain(llm=llm, prompt=prompt)print(chain.run({"word_count": 100, "topic": "how to make friends"}))API Reference:DollyContentFormatterLLMChain    Many people are willing to talk about themselves; it's others who seem to be stuck up. Try to understand others where they're coming from. Like minded people can build a tribe together.Serializing an LLM​You can also save and load LLM configurationsfrom langchain.llms.loading import load_llmfrom langchain.llms.azureml_endpoint import AzureMLEndpointClientsave_llm = AzureMLOnlineEndpoint(    deployment_name="databricks-dolly-v2-12b-4",    model_kwargs={        "temperature": 0.2,        "max_tokens": 150,        "top_p": 0.8,        "frequency_penalty": 0.32,        "presence_penalty": 72e-3,    },)save_llm.save("azureml.json")loaded_llm = load_llm("azureml.json")print(loaded_llm)API Reference:load_llmAzureMLEndpointClient    AzureMLOnlineEndpoint    Params: {'deployment_name': 'databricks-dolly-v2-12b-4', 'model_kwargs': {'temperature': 0.2, 'max_tokens': 150, 'top_p': 0.8, 'frequency_penalty': 0.32, 'presence_penalty': 0.072}} from langchain.llms.azureml_endpoint import AzureMLOnlineEndpoint from langchain.llms.azureml_endpoint import AzureMLOnlineEndpoint  API Reference:AzureMLOnlineEndpoint from typing import Dictfrom langchain.llms.azureml_endpoint import AzureMLOnlineEndpoint, ContentFormatterBaseimport osimport jsonclass CustomFormatter(ContentFormatterBase):    content_type = "application/json"    accepts = "application/json"    def format_request_payload(self, prompt: str, model_kwargs: Dict) -> bytes:        input_str = json.dumps(            {                "inputs": [prompt],                "parameters": model_kwargs,                "options": {"use_cache": False, "wait_for_model": True},            }        )        return str.encode(input_str)    def format_response_payload(self, output: bytes) -> str:        response_json = json.loads(output)        return response_json[0]["summary_text"]content_formatter = CustomFormatter()llm = AzureMLOnlineEndpoint(    endpoint_api_key=os.getenv("BART_ENDPOINT_API_KEY"),    endpoint_url=os.getenv("BART_ENDPOINT_URL"),    model_kwargs={"temperature": 0.8, "max_new_tokens": 400},    content_formatter=content_formatter,)large_text = """On January 7, 2020, Blockberry Creative announced that HaSeul would not participate in the promotion for Loona's next album because of mental health concerns. She was said to be diagnosed with "intermittent anxiety symptoms" and would be taking time to focus on her health.[39] On February 5, 2020, Loona released their second EP titled [#] (read as hash), along with the title track "So What".[40] Although HaSeul did not appear in the title track, her vocals are featured on three other songs on the album, including "365". Once peaked at number 1 on the daily Gaon Retail Album Chart,[41] the EP then debuted at number 2 on the weekly Gaon Album Chart. On March 12, 2020, Loona won their first music show trophy with "So What" on Mnet's M Countdown.[42]On October 19, 2020, Loona released their third EP titled [12:00] (read as midnight),[43] accompanied by its first single "Why Not?". HaSeul was again not involved in the album, out of her own decision to focus on the recovery of her health.[44] The EP then became their first album to enter the Billboard 200, debuting at number 112.[45] On November 18, Loona released the music video for "Star", another song on [12:00].[46] Peaking at number 40, "Star" is Loona's first entry on the Billboard Mainstream Top 40, making them the second K-pop girl group to enter the chart.[47]On June 1, 2021, Loona announced that they would be having a comeback on June 28, with their fourth EP, [&] (read as and).[48] The following day, on June 2, a teaser was posted to Loona's official social media accounts showing twelve sets of eyes, confirming the return of member HaSeul who had been on hiatus since early 2020.[49] On June 12, group members YeoJin, Kim Lip, Choerry, and Go Won released the song "Yum-Yum" as a collaboration with Cocomong.[50] On September 8, they released another collaboration song named "Yummy-Yummy".[51] On June 27, 2021, Loona announced at the end of their special clip that they are making their Japanese debut on September 15 under Universal Music Japan sublabel EMI Records.[52] On August 27, it was announced that Loona will release the double A-side single, "Hula Hoop / Star Seed" on September 15, with a physical CD release on October 20.[53] In December, Chuu filed an injunction to suspend her exclusive contract with Blockberry Creative.[54][55]"""summarized_text = llm(large_text)print(summarized_text) from typing import Dictfrom langchain.llms.azureml_endpoint import AzureMLOnlineEndpoint, ContentFormatterBaseimport osimport jsonclass CustomFormatter(ContentFormatterBase):    content_type = "application/json"    accepts = "application/json"    def format_request_payload(self, prompt: str, model_kwargs: Dict) -> bytes:        input_str = json.dumps(            {                "inputs": [prompt],                "parameters": model_kwargs,                "options": {"use_cache": False, "wait_for_model": True},            }        )        return str.encode(input_str)    def format_response_payload(self, output: bytes) -> str:        response_json = json.loads(output)        return response_json[0]["summary_text"]content_formatter = CustomFormatter()llm = AzureMLOnlineEndpoint(    endpoint_api_key=os.getenv("BART_ENDPOINT_API_KEY"),    endpoint_url=os.getenv("BART_ENDPOINT_URL"),    model_kwargs={"temperature": 0.8, "max_new_tokens": 400},    content_formatter=content_formatter,)large_text = """On January 7, 2020, Blockberry Creative announced that HaSeul would not participate in the promotion for Loona's next album because of mental health concerns. She was said to be diagnosed with "intermittent anxiety symptoms" and would be taking time to focus on her health.[39] On February 5, 2020, Loona released their second EP titled [#] (read as hash), along with the title track "So What".[40] Although HaSeul did not appear in the title track, her vocals are featured on three other songs on the album, including "365". Once peaked at number 1 on the daily Gaon Retail Album Chart,[41] the EP then debuted at number 2 on the weekly Gaon Album Chart. On March 12, 2020, Loona won their first music show trophy with "So What" on Mnet's M Countdown.[42]On October 19, 2020, Loona released their third EP titled [12:00] (read as midnight),[43] accompanied by its first single "Why Not?". HaSeul was again not involved in the album, out of her own decision to focus on the recovery of her health.[44] The EP then became their first album to enter the Billboard 200, debuting at number 112.[45] On November 18, Loona released the music video for "Star", another song on [12:00].[46] Peaking at number 40, "Star" is Loona's first entry on the Billboard Mainstream Top 40, making them the second K-pop girl group to enter the chart.[47]On June 1, 2021, Loona announced that they would be having a comeback on June 28, with their fourth EP, [&] (read as and).[48] The following day, on June 2, a teaser was posted to Loona's official social media accounts showing twelve sets of eyes, confirming the return of member HaSeul who had been on hiatus since early 2020.[49] On June 12, group members YeoJin, Kim Lip, Choerry, and Go Won released the song "Yum-Yum" as a collaboration with Cocomong.[50] On September 8, they released another collaboration song named "Yummy-Yummy".[51] On June 27, 2021, Loona announced at the end of their special clip that they are making their Japanese debut on September 15 under Universal Music Japan sublabel EMI Records.[52] On August 27, it was announced that Loona will release the double A-side single, "Hula Hoop / Star Seed" on September 15, with a physical CD release on October 20.[53] In December, Chuu filed an injunction to suspend her exclusive contract with Blockberry Creative.[54][55]"""summarized_text = llm(large_text)print(summarized_text)  API Reference:AzureMLOnlineEndpointContentFormatterBase     HaSeul won her first music show trophy with "So What" on Mnet's M Countdown. Loona released their second EP titled [#] (read as hash] on February 5, 2020. HaSeul did not take part in the promotion of the album because of mental health issues. On October 19, 2020, they released their third EP called [12:00]. It was their first album to enter the Billboard 200, debuting at number 112. On June 2, 2021, the group released their fourth EP called Yummy-Yummy. On August 27, it was announced that they are making their Japanese debut on September 15 under Universal Music Japan sublabel EMI Records.     HaSeul won her first music show trophy with "So What" on Mnet's M Countdown. Loona released their second EP titled [#] (read as hash] on February 5, 2020. HaSeul did not take part in the promotion of the album because of mental health issues. On October 19, 2020, they released their third EP called [12:00]. It was their first album to enter the Billboard 200, debuting at number 112. On June 2, 2021, the group released their fourth EP called Yummy-Yummy. On August 27, it was announced that they are making their Japanese debut on September 15 under Universal Music Japan sublabel EMI Records.     HaSeul won her first music show trophy with "So What" on Mnet's M Countdown. Loona released their second EP titled [#] (read as hash] on February 5, 2020. HaSeul did not take part in the promotion of the album because of mental health issues. On October 19, 2020, they released their third EP called [12:00]. It was their first album to enter the Billboard 200, debuting at number 112. On June 2, 2021, the group released their fourth EP called Yummy-Yummy. On August 27, it was announced that they are making their Japanese debut on September 15 under Universal Music Japan sublabel EMI Records.  from langchain import PromptTemplatefrom langchain.llms.azureml_endpoint import DollyContentFormatterfrom langchain.chains import LLMChainformatter_template = "Write a {word_count} word essay about {topic}."prompt = PromptTemplate(    input_variables=["word_count", "topic"], template=formatter_template)content_formatter = DollyContentFormatter()llm = AzureMLOnlineEndpoint(    endpoint_api_key=os.getenv("DOLLY_ENDPOINT_API_KEY"),    endpoint_url=os.getenv("DOLLY_ENDPOINT_URL"),    model_kwargs={"temperature": 0.8, "max_tokens": 300},    content_formatter=content_formatter,)chain = LLMChain(llm=llm, prompt=prompt)print(chain.run({"word_count": 100, "topic": "how to make friends"})) from langchain import PromptTemplatefrom langchain.llms.azureml_endpoint import DollyContentFormatterfrom langchain.chains import LLMChainformatter_template = "Write a {word_count} word essay about {topic}."prompt = PromptTemplate(    input_variables=["word_count", "topic"], template=formatter_template)content_formatter = DollyContentFormatter()llm = AzureMLOnlineEndpoint(    endpoint_api_key=os.getenv("DOLLY_ENDPOINT_API_KEY"),    endpoint_url=os.getenv("DOLLY_ENDPOINT_URL"),    model_kwargs={"temperature": 0.8, "max_tokens": 300},    content_formatter=content_formatter,)chain = LLMChain(llm=llm, prompt=prompt)print(chain.run({"word_count": 100, "topic": "how to make friends"}))  API Reference:DollyContentFormatterLLMChain     Many people are willing to talk about themselves; it's others who seem to be stuck up. Try to understand others where they're coming from. Like minded people can build a tribe together.     Many people are willing to talk about themselves; it's others who seem to be stuck up. Try to understand others where they're coming from. Like minded people can build a tribe together.     Many people are willing to talk about themselves; it's others who seem to be stuck up. Try to understand others where they're coming from. Like minded people can build a tribe together.  from langchain.llms.loading import load_llmfrom langchain.llms.azureml_endpoint import AzureMLEndpointClientsave_llm = AzureMLOnlineEndpoint(    deployment_name="databricks-dolly-v2-12b-4",    model_kwargs={        "temperature": 0.2,        "max_tokens": 150,        "top_p": 0.8,        "frequency_penalty": 0.32,        "presence_penalty": 72e-3,    },)save_llm.save("azureml.json")loaded_llm = load_llm("azureml.json")print(loaded_llm) from langchain.llms.loading import load_llmfrom langchain.llms.azureml_endpoint import AzureMLEndpointClientsave_llm = AzureMLOnlineEndpoint(    deployment_name="databricks-dolly-v2-12b-4",    model_kwargs={        "temperature": 0.2,        "max_tokens": 150,        "top_p": 0.8,        "frequency_penalty": 0.32,        "presence_penalty": 72e-3,    },)save_llm.save("azureml.json")loaded_llm = load_llm("azureml.json")print(loaded_llm)  API Reference:load_llmAzureMLEndpointClient     AzureMLOnlineEndpoint    Params: {'deployment_name': 'databricks-dolly-v2-12b-4', 'model_kwargs': {'temperature': 0.2, 'max_tokens': 150, 'top_p': 0.8, 'frequency_penalty': 0.32, 'presence_penalty': 0.072}}     AzureMLOnlineEndpoint    Params: {'deployment_name': 'databricks-dolly-v2-12b-4', 'model_kwargs': {'temperature': 0.2, 'max_tokens': 150, 'top_p': 0.8, 'frequency_penalty': 0.32, 'presence_penalty': 0.072}}     AzureMLOnlineEndpoint    Params: {'deployment_name': 'databricks-dolly-v2-12b-4', 'model_kwargs': {'temperature': 0.2, 'max_tokens': 150, 'top_p': 0.8, 'frequency_penalty': 0.32, 'presence_penalty': 0.072}}  Previous Anyscale Next Azure OpenAI Set upContent FormatterCustom Content FormatterDolly with LLMChainSerializing an LLM Set upContent FormatterCustom Content FormatterDolly with LLMChainSerializing an LLM CommunityDiscordTwitterGitHubPythonJS/TSMoreHomepageBlogCopyright © 2023 LangChain, Inc. CommunityDiscordTwitterGitHubPythonJS/TSMoreHomepageBlog CommunityDiscordTwitter Community GitHubPythonJS/TS GitHub MoreHomepageBlog More Copyright © 2023 LangChain, Inc. Copyright © 2023 LangChain, Inc. Skip to main content (#docusaurus_skipToContent_fallback) 🦜️🔗 LangChain (/) Docs (/docs/get_started/introduction) Use cases (/docs/use_cases) Integrations (/docs/integrations) API (https://api.python.langchain.com) LangSmith (https://smith.langchain.com) JS/TS Docs (https://js.langchain.com/docs)  (https://github.com/hwchase17/langchain) Integrations (/docs/integrations) Callbacks (/docs/integrations/callbacks/) Chat models (/docs/integrations/chat/) Chat loaders (/docs/integrations/chat_loaders/) Document loaders (/docs/integrations/document_loaders/) Document transformers (/docs/integrations/document_transformers/) LLMs (/docs/integrations/llms/) AI21 (/docs/integrations/llms/ai21) Aleph Alpha (/docs/integrations/llms/aleph_alpha) Amazon API Gateway (/docs/integrations/llms/amazon_api_gateway) Anyscale (/docs/integrations/llms/anyscale) Azure ML (/docs/integrations/llms/azure_ml) Azure OpenAI (/docs/integrations/llms/azure_openai) Banana (/docs/integrations/llms/banana) Baseten (/docs/integrations/llms/baseten) Beam (/docs/integrations/llms/beam) Bedrock (/docs/integrations/llms/bedrock) Bittensor (/docs/integrations/llms/bittensor) CerebriumAI (/docs/integrations/llms/cerebriumai) ChatGLM (/docs/integrations/llms/chatglm) Clarifai (/docs/integrations/llms/clarifai) Cohere (/docs/integrations/llms/cohere) C Transformers (/docs/integrations/llms/ctransformers) CTranslate2 (/docs/integrations/llms/ctranslate2) Databricks (/docs/integrations/llms/databricks) DeepInfra (/docs/integrations/llms/deepinfra) DeepSparse (/docs/integrations/llms/deepsparse) Eden AI (/docs/integrations/llms/edenai) Fireworks (/docs/integrations/llms/fireworks) ForefrontAI (/docs/integrations/llms/forefrontai) Google Vertex AI PaLM (/docs/integrations/llms/google_vertex_ai_palm) GooseAI (/docs/integrations/llms/gooseai) GPT4All (/docs/integrations/llms/gpt4all) Hugging Face Hub (/docs/integrations/llms/huggingface_hub) Hugging Face Local Pipelines (/docs/integrations/llms/huggingface_pipelines) Huggingface TextGen Inference (/docs/integrations/llms/huggingface_textgen_inference) JSONFormer (/docs/integrations/llms/jsonformer_experimental) KoboldAI API (/docs/integrations/llms/koboldai) Llama.cpp (/docs/integrations/llms/llamacpp) LLM Caching integrations (/docs/integrations/llms/llm_caching) Manifest (/docs/integrations/llms/manifest) Minimax (/docs/integrations/llms/minimax) Modal (/docs/integrations/llms/modal) MosaicML (/docs/integrations/llms/mosaicml) NLP Cloud (/docs/integrations/llms/nlpcloud) OctoAI (/docs/integrations/llms/octoai) Ollama (/docs/integrations/llms/ollama) OpaquePrompts (/docs/integrations/llms/opaqueprompts) OpenAI (/docs/integrations/llms/openai) OpenLLM (/docs/integrations/llms/openllm) OpenLM (/docs/integrations/llms/openlm) Petals (/docs/integrations/llms/petals) PipelineAI (/docs/integrations/llms/pipelineai) Predibase (/docs/integrations/llms/predibase) Prediction Guard (/docs/integrations/llms/predictionguard) PromptLayer OpenAI (/docs/integrations/llms/promptlayer_openai) RELLM (/docs/integrations/llms/rellm_experimental) Replicate (/docs/integrations/llms/replicate) Runhouse (/docs/integrations/llms/runhouse) SageMakerEndpoint (/docs/integrations/llms/sagemaker) StochasticAI (/docs/integrations/llms/stochasticai) Nebula (Symbl.ai) (/docs/integrations/llms/symblai_nebula) TextGen (/docs/integrations/llms/textgen) Titan Takeoff (/docs/integrations/llms/titan_takeoff) Tongyi Qwen (/docs/integrations/llms/tongyi) vLLM (/docs/integrations/llms/vllm) Writer (/docs/integrations/llms/writer) Xorbits Inference (Xinference) (/docs/integrations/llms/xinference) Memory (/docs/integrations/memory/) Retrievers (/docs/integrations/retrievers/) Text embedding models (/docs/integrations/text_embedding/) Agents & Toolkits (/docs/integrations/toolkits/) Tools (/docs/integrations/tools/) Vector stores (/docs/integrations/vectorstores/) Grouped by provider (/docs/integrations/providers/)  (/) Integrations (/docs/integrations) LLMs (/docs/integrations/llms/) Azure ML (https://azure.microsoft.com/en-us/products/machine-learning/) AzureMLOnlineEndpoint (https://api.python.langchain.com/en/latest/llms/langchain.llms.azureml_endpoint.AzureMLOnlineEndpoint.html) ​ (#set-up) deploy a model on AzureML (https://learn.microsoft.com/en-us/azure/machine-learning/how-to-use-foundation-models?view=azureml-api-2#deploying-foundation-models-to-endpoints-for-inferencing) ​ (#content-formatter) ​ (#custom-content-formatter) AzureMLOnlineEndpoint (https://api.python.langchain.com/en/latest/llms/langchain.llms.azureml_endpoint.AzureMLOnlineEndpoint.html) ContentFormatterBase (https://api.python.langchain.com/en/latest/llms/langchain.llms.azureml_endpoint.ContentFormatterBase.html) ​ (#dolly-with-llmchain) DollyContentFormatter (https://api.python.langchain.com/en/latest/llms/langchain.llms.azureml_endpoint.DollyContentFormatter.html) LLMChain (https://api.python.langchain.com/en/latest/chains/langchain.chains.llm.LLMChain.html) ​ (#serializing-an-llm) load_llm (https://api.python.langchain.com/en/latest/llms/langchain.llms.loading.load_llm.html) AzureMLEndpointClient (https://api.python.langchain.com/en/latest/llms/langchain.llms.azureml_endpoint.AzureMLEndpointClient.html) PreviousAnyscale (/docs/integrations/llms/anyscale) NextAzure OpenAI (/docs/integrations/llms/azure_openai) Set up (#set-up) Content Formatter (#content-formatter) Custom Content Formatter (#custom-content-formatter) Dolly with LLMChain (#dolly-with-llmchain) Serializing an LLM (#serializing-an-llm) Discord (https://discord.gg/cU2adEyC7w) Twitter (https://twitter.com/LangChainAI) Python (https://github.com/hwchase17/langchain) JS/TS (https://github.com/hwchase17/langchainjs) Homepage (https://langchain.com) Blog (https://blog.langchain.dev)