TitanML helps businesses build and deploy better, smaller, cheaper, and faster NLP models through our training, compression, and inference optimization platform.  Our inference server, Titan Takeoff enables deployment of LLMs locally on your hardware in a single command. Most generative model architectures are supported, such as Falcon, Llama 2, GPT2, T5 and many more. To get started with Iris Takeoff, all you need is to have docker and python installed on your local system. If you wish to use the server with gpu suport, then you will need to install docker with cuda support. For Mac and Windows users, make sure you have the docker daemon running! You can check this by running docker ps in your terminal. To start the daemon, open the docker desktop app. Run the following command to install the Iris CLI that will enable you to run the takeoff server: Takeoff supports many of the most powerful generative text models, such as Falcon, MPT, and Llama. See the supported models for more information. For information about using your own models, see the custom models. Going forward in this demo we will be using the falcon 7B instruct model. This is a good open source model that is trained to follow instructions, and is small enough to easily inference even on CPUs. Models are referred to by their model id on HuggingFace. Takeoff uses port 8000 by default, but can be configured to use another port. There is also support to use a Nvidia GPU by specifing cuda for the device flag. To start the takeoff server, run: You will then be directed to a login page, where you will need to create an account to proceed. After logging in, run the command onscreen to check whether the server is ready. When it is ready, you can start using the Takeoff integration. To shutdown the server, run the following command. You will be presented with options on which Takeoff server to shut down, in case you have multiple running servers. To access your LLM, use the TitanTakeoff LLM wrapper: No parameters are needed by default, but a baseURL that points to your desired URL where Takeoff is running can be specified and generation parameters can be supplied. Streaming is also supported via the streaming flag: IntegrationsCallbacksChat modelsChat loadersDocument loadersDocument transformersLLMsAI21Aleph AlphaAmazon API GatewayAnyscaleAzure MLAzure OpenAIBananaBasetenBeamBedrockBittensorCerebriumAIChatGLMClarifaiCohereC TransformersCTranslate2DatabricksDeepInfraDeepSparseEden AIFireworksForefrontAIGoogle Vertex AI PaLMGooseAIGPT4AllHugging Face HubHugging Face Local PipelinesHuggingface TextGen InferenceJSONFormerKoboldAI APILlama.cppLLM Caching integrationsManifestMinimaxModalMosaicMLNLP CloudOctoAIOllamaOpaquePromptsOpenAIOpenLLMOpenLMPetalsPipelineAIPredibasePrediction GuardPromptLayer OpenAIRELLMReplicateRunhouseSageMakerEndpointStochasticAINebula (Symbl.ai)TextGenTitan TakeoffTongyi QwenvLLMWriterXorbits Inference (Xinference)MemoryRetrieversText embedding modelsAgents & ToolkitsToolsVector storesGrouped by provider Callbacks Chat models Chat loaders Document loaders Document transformers LLMsAI21Aleph AlphaAmazon API GatewayAnyscaleAzure MLAzure OpenAIBananaBasetenBeamBedrockBittensorCerebriumAIChatGLMClarifaiCohereC TransformersCTranslate2DatabricksDeepInfraDeepSparseEden AIFireworksForefrontAIGoogle Vertex AI PaLMGooseAIGPT4AllHugging Face HubHugging Face Local PipelinesHuggingface TextGen InferenceJSONFormerKoboldAI APILlama.cppLLM Caching integrationsManifestMinimaxModalMosaicMLNLP CloudOctoAIOllamaOpaquePromptsOpenAIOpenLLMOpenLMPetalsPipelineAIPredibasePrediction GuardPromptLayer OpenAIRELLMReplicateRunhouseSageMakerEndpointStochasticAINebula (Symbl.ai)TextGenTitan TakeoffTongyi QwenvLLMWriterXorbits Inference (Xinference) AI21 Aleph Alpha Amazon API Gateway Anyscale Azure ML Azure OpenAI Banana Baseten Beam Bedrock Bittensor CerebriumAI ChatGLM Clarifai Cohere C Transformers CTranslate2 Databricks DeepInfra DeepSparse Eden AI Fireworks ForefrontAI Google Vertex AI PaLM GooseAI GPT4All Hugging Face Hub Hugging Face Local Pipelines Huggingface TextGen Inference JSONFormer KoboldAI API Llama.cpp LLM Caching integrations Manifest Minimax Modal MosaicML NLP Cloud OctoAI Ollama OpaquePrompts OpenAI OpenLLM OpenLM Petals PipelineAI Predibase Prediction Guard PromptLayer OpenAI RELLM Replicate Runhouse SageMakerEndpoint StochasticAI Nebula (Symbl.ai) TextGen Titan Takeoff Tongyi Qwen vLLM Writer Xorbits Inference (Xinference) Memory Retrievers Text embedding models Agents & Toolkits Tools Vector stores Grouped by provider  Integrations LLMs Titan Takeoff TitanTakeoff StreamingStdOutCallbackHandler CallbackManager Installation Choose a Model Taking off Inferencing your modelStreamingIntegration with LLMChain Streaming Integration with LLMChain Discord Twitter Python JS/TS Homepage Blog Skip to main contentü¶úÔ∏èüîó LangChainDocsUse casesIntegrationsAPILangSmithJS/TS DocsCTRLKIntegrationsCallbacksChat modelsChat loadersDocument loadersDocument transformersLLMsAI21Aleph AlphaAmazon API GatewayAnyscaleAzure MLAzure OpenAIBananaBasetenBeamBedrockBittensorCerebriumAIChatGLMClarifaiCohereC TransformersCTranslate2DatabricksDeepInfraDeepSparseEden AIFireworksForefrontAIGoogle Vertex AI PaLMGooseAIGPT4AllHugging Face HubHugging Face Local PipelinesHuggingface TextGen InferenceJSONFormerKoboldAI APILlama.cppLLM Caching integrationsManifestMinimaxModalMosaicMLNLP CloudOctoAIOllamaOpaquePromptsOpenAIOpenLLMOpenLMPetalsPipelineAIPredibasePrediction GuardPromptLayer OpenAIRELLMReplicateRunhouseSageMakerEndpointStochasticAINebula (Symbl.ai)TextGenTitan TakeoffTongyi QwenvLLMWriterXorbits Inference (Xinference)MemoryRetrieversText embedding modelsAgents & ToolkitsToolsVector storesGrouped by providerIntegrationsLLMsTitan TakeoffOn this pageTitan TakeoffTitanML helps businesses build and deploy better, smaller, cheaper, and faster NLP models through our training, compression, and inference optimization platform. Our inference server, Titan Takeoff enables deployment of LLMs locally on your hardware in a single command. Most generative model architectures are supported, such as Falcon, Llama 2, GPT2, T5 and many more.Installation‚ÄãTo get started with Iris Takeoff, all you need is to have docker and python installed on your local system. If you wish to use the server with gpu suport, then you will need to install docker with cuda support.For Mac and Windows users, make sure you have the docker daemon running! You can check this by running docker ps in your terminal. To start the daemon, open the docker desktop app.Run the following command to install the Iris CLI that will enable you to run the takeoff server:pip install titan-irisChoose a Model‚ÄãTakeoff supports many of the most powerful generative text models, such as Falcon, MPT, and Llama. See the supported models for more information. For information about using your own models, see the custom models.Going forward in this demo we will be using the falcon 7B instruct model. This is a good open source model that is trained to follow instructions, and is small enough to easily inference even on CPUs.Taking off‚ÄãModels are referred to by their model id on HuggingFace. Takeoff uses port 8000 by default, but can be configured to use another port. There is also support to use a Nvidia GPU by specifing cuda for the device flag.To start the takeoff server, run:iris takeoff --model tiiuae/falcon-7b-instruct --device cpuiris takeoff --model tiiuae/falcon-7b-instruct --device cuda # Nvidia GPU requirediris takeoff --model tiiuae/falcon-7b-instruct --device cpu --port 5000 # run on port 5000 (default: 8000)You will then be directed to a login page, where you will need to create an account to proceed. After logging in, run the command onscreen to check whether the server is ready. When it is ready, you can start using the Takeoff integration.To shutdown the server, run the following command. You will be presented with options on which Takeoff server to shut down, in case you have multiple running servers.iris takeoff --shutdown # shutdown the serverInferencing your model‚ÄãTo access your LLM, use the TitanTakeoff LLM wrapper:from langchain.llms import TitanTakeoffllm = TitanTakeoff(    baseURL="http://localhost:8000",    generate_max_length=128,    temperature=1.0)prompt = "What is the largest planet in the solar system?"llm(prompt)API Reference:TitanTakeoffNo parameters are needed by default, but a baseURL that points to your desired URL where Takeoff is running can be specified and generation parameters can be supplied.Streaming‚ÄãStreaming is also supported via the streaming flag:from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandlerfrom langchain.callbacks.manager import CallbackManagerllm = TitanTakeoff(callback_manager=CallbackManager([StreamingStdOutCallbackHandler()]), streaming=True)prompt = "What is the capital of France?"llm(prompt)API Reference:StreamingStdOutCallbackHandlerCallbackManagerIntegration with LLMChain‚Äãfrom langchain import PromptTemplate, LLMChainllm = TitanTakeoff()template = "What is the capital of {country}"prompt = PromptTemplate(template=template, input_variables=["country"])llm_chain = LLMChain(llm=llm, prompt=prompt)generated = llm_chain.run(country="Belgium")print(generated)PreviousTextGenNextTongyi QwenInstallationChoose a ModelTaking offInferencing your modelStreamingIntegration with LLMChainCommunityDiscordTwitterGitHubPythonJS/TSMoreHomepageBlogCopyright ¬© 2023 LangChain, Inc. Skip to main content ü¶úÔ∏èüîó LangChainDocsUse casesIntegrationsAPILangSmithJS/TS DocsCTRLK ü¶úÔ∏èüîó LangChainDocsUse casesIntegrationsAPI LangSmithJS/TS DocsCTRLK  CTRLK CTRLK  CTRLK   IntegrationsCallbacksChat modelsChat loadersDocument loadersDocument transformersLLMsAI21Aleph AlphaAmazon API GatewayAnyscaleAzure MLAzure OpenAIBananaBasetenBeamBedrockBittensorCerebriumAIChatGLMClarifaiCohereC TransformersCTranslate2DatabricksDeepInfraDeepSparseEden AIFireworksForefrontAIGoogle Vertex AI PaLMGooseAIGPT4AllHugging Face HubHugging Face Local PipelinesHuggingface TextGen InferenceJSONFormerKoboldAI APILlama.cppLLM Caching integrationsManifestMinimaxModalMosaicMLNLP CloudOctoAIOllamaOpaquePromptsOpenAIOpenLLMOpenLMPetalsPipelineAIPredibasePrediction GuardPromptLayer OpenAIRELLMReplicateRunhouseSageMakerEndpointStochasticAINebula (Symbl.ai)TextGenTitan TakeoffTongyi QwenvLLMWriterXorbits Inference (Xinference)MemoryRetrieversText embedding modelsAgents & ToolkitsToolsVector storesGrouped by providerIntegrationsLLMsTitan TakeoffOn this pageTitan TakeoffTitanML helps businesses build and deploy better, smaller, cheaper, and faster NLP models through our training, compression, and inference optimization platform. Our inference server, Titan Takeoff enables deployment of LLMs locally on your hardware in a single command. Most generative model architectures are supported, such as Falcon, Llama 2, GPT2, T5 and many more.Installation‚ÄãTo get started with Iris Takeoff, all you need is to have docker and python installed on your local system. If you wish to use the server with gpu suport, then you will need to install docker with cuda support.For Mac and Windows users, make sure you have the docker daemon running! You can check this by running docker ps in your terminal. To start the daemon, open the docker desktop app.Run the following command to install the Iris CLI that will enable you to run the takeoff server:pip install titan-irisChoose a Model‚ÄãTakeoff supports many of the most powerful generative text models, such as Falcon, MPT, and Llama. See the supported models for more information. For information about using your own models, see the custom models.Going forward in this demo we will be using the falcon 7B instruct model. This is a good open source model that is trained to follow instructions, and is small enough to easily inference even on CPUs.Taking off‚ÄãModels are referred to by their model id on HuggingFace. Takeoff uses port 8000 by default, but can be configured to use another port. There is also support to use a Nvidia GPU by specifing cuda for the device flag.To start the takeoff server, run:iris takeoff --model tiiuae/falcon-7b-instruct --device cpuiris takeoff --model tiiuae/falcon-7b-instruct --device cuda # Nvidia GPU requirediris takeoff --model tiiuae/falcon-7b-instruct --device cpu --port 5000 # run on port 5000 (default: 8000)You will then be directed to a login page, where you will need to create an account to proceed. After logging in, run the command onscreen to check whether the server is ready. When it is ready, you can start using the Takeoff integration.To shutdown the server, run the following command. You will be presented with options on which Takeoff server to shut down, in case you have multiple running servers.iris takeoff --shutdown # shutdown the serverInferencing your model‚ÄãTo access your LLM, use the TitanTakeoff LLM wrapper:from langchain.llms import TitanTakeoffllm = TitanTakeoff(    baseURL="http://localhost:8000",    generate_max_length=128,    temperature=1.0)prompt = "What is the largest planet in the solar system?"llm(prompt)API Reference:TitanTakeoffNo parameters are needed by default, but a baseURL that points to your desired URL where Takeoff is running can be specified and generation parameters can be supplied.Streaming‚ÄãStreaming is also supported via the streaming flag:from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandlerfrom langchain.callbacks.manager import CallbackManagerllm = TitanTakeoff(callback_manager=CallbackManager([StreamingStdOutCallbackHandler()]), streaming=True)prompt = "What is the capital of France?"llm(prompt)API Reference:StreamingStdOutCallbackHandlerCallbackManagerIntegration with LLMChain‚Äãfrom langchain import PromptTemplate, LLMChainllm = TitanTakeoff()template = "What is the capital of {country}"prompt = PromptTemplate(template=template, input_variables=["country"])llm_chain = LLMChain(llm=llm, prompt=prompt)generated = llm_chain.run(country="Belgium")print(generated)PreviousTextGenNextTongyi QwenInstallationChoose a ModelTaking offInferencing your modelStreamingIntegration with LLMChain IntegrationsCallbacksChat modelsChat loadersDocument loadersDocument transformersLLMsAI21Aleph AlphaAmazon API GatewayAnyscaleAzure MLAzure OpenAIBananaBasetenBeamBedrockBittensorCerebriumAIChatGLMClarifaiCohereC TransformersCTranslate2DatabricksDeepInfraDeepSparseEden AIFireworksForefrontAIGoogle Vertex AI PaLMGooseAIGPT4AllHugging Face HubHugging Face Local PipelinesHuggingface TextGen InferenceJSONFormerKoboldAI APILlama.cppLLM Caching integrationsManifestMinimaxModalMosaicMLNLP CloudOctoAIOllamaOpaquePromptsOpenAIOpenLLMOpenLMPetalsPipelineAIPredibasePrediction GuardPromptLayer OpenAIRELLMReplicateRunhouseSageMakerEndpointStochasticAINebula (Symbl.ai)TextGenTitan TakeoffTongyi QwenvLLMWriterXorbits Inference (Xinference)MemoryRetrieversText embedding modelsAgents & ToolkitsToolsVector storesGrouped by providerIntegrationsLLMsTitan TakeoffOn this pageTitan TakeoffTitanML helps businesses build and deploy better, smaller, cheaper, and faster NLP models through our training, compression, and inference optimization platform. Our inference server, Titan Takeoff enables deployment of LLMs locally on your hardware in a single command. Most generative model architectures are supported, such as Falcon, Llama 2, GPT2, T5 and many more.Installation‚ÄãTo get started with Iris Takeoff, all you need is to have docker and python installed on your local system. If you wish to use the server with gpu suport, then you will need to install docker with cuda support.For Mac and Windows users, make sure you have the docker daemon running! You can check this by running docker ps in your terminal. To start the daemon, open the docker desktop app.Run the following command to install the Iris CLI that will enable you to run the takeoff server:pip install titan-irisChoose a Model‚ÄãTakeoff supports many of the most powerful generative text models, such as Falcon, MPT, and Llama. See the supported models for more information. For information about using your own models, see the custom models.Going forward in this demo we will be using the falcon 7B instruct model. This is a good open source model that is trained to follow instructions, and is small enough to easily inference even on CPUs.Taking off‚ÄãModels are referred to by their model id on HuggingFace. Takeoff uses port 8000 by default, but can be configured to use another port. There is also support to use a Nvidia GPU by specifing cuda for the device flag.To start the takeoff server, run:iris takeoff --model tiiuae/falcon-7b-instruct --device cpuiris takeoff --model tiiuae/falcon-7b-instruct --device cuda # Nvidia GPU requirediris takeoff --model tiiuae/falcon-7b-instruct --device cpu --port 5000 # run on port 5000 (default: 8000)You will then be directed to a login page, where you will need to create an account to proceed. After logging in, run the command onscreen to check whether the server is ready. When it is ready, you can start using the Takeoff integration.To shutdown the server, run the following command. You will be presented with options on which Takeoff server to shut down, in case you have multiple running servers.iris takeoff --shutdown # shutdown the serverInferencing your model‚ÄãTo access your LLM, use the TitanTakeoff LLM wrapper:from langchain.llms import TitanTakeoffllm = TitanTakeoff(    baseURL="http://localhost:8000",    generate_max_length=128,    temperature=1.0)prompt = "What is the largest planet in the solar system?"llm(prompt)API Reference:TitanTakeoffNo parameters are needed by default, but a baseURL that points to your desired URL where Takeoff is running can be specified and generation parameters can be supplied.Streaming‚ÄãStreaming is also supported via the streaming flag:from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandlerfrom langchain.callbacks.manager import CallbackManagerllm = TitanTakeoff(callback_manager=CallbackManager([StreamingStdOutCallbackHandler()]), streaming=True)prompt = "What is the capital of France?"llm(prompt)API Reference:StreamingStdOutCallbackHandlerCallbackManagerIntegration with LLMChain‚Äãfrom langchain import PromptTemplate, LLMChainllm = TitanTakeoff()template = "What is the capital of {country}"prompt = PromptTemplate(template=template, input_variables=["country"])llm_chain = LLMChain(llm=llm, prompt=prompt)generated = llm_chain.run(country="Belgium")print(generated)PreviousTextGenNextTongyi QwenInstallationChoose a ModelTaking offInferencing your modelStreamingIntegration with LLMChain IntegrationsCallbacksChat modelsChat loadersDocument loadersDocument transformersLLMsAI21Aleph AlphaAmazon API GatewayAnyscaleAzure MLAzure OpenAIBananaBasetenBeamBedrockBittensorCerebriumAIChatGLMClarifaiCohereC TransformersCTranslate2DatabricksDeepInfraDeepSparseEden AIFireworksForefrontAIGoogle Vertex AI PaLMGooseAIGPT4AllHugging Face HubHugging Face Local PipelinesHuggingface TextGen InferenceJSONFormerKoboldAI APILlama.cppLLM Caching integrationsManifestMinimaxModalMosaicMLNLP CloudOctoAIOllamaOpaquePromptsOpenAIOpenLLMOpenLMPetalsPipelineAIPredibasePrediction GuardPromptLayer OpenAIRELLMReplicateRunhouseSageMakerEndpointStochasticAINebula (Symbl.ai)TextGenTitan TakeoffTongyi QwenvLLMWriterXorbits Inference (Xinference)MemoryRetrieversText embedding modelsAgents & ToolkitsToolsVector storesGrouped by provider IntegrationsCallbacksChat modelsChat loadersDocument loadersDocument transformersLLMsAI21Aleph AlphaAmazon API GatewayAnyscaleAzure MLAzure OpenAIBananaBasetenBeamBedrockBittensorCerebriumAIChatGLMClarifaiCohereC TransformersCTranslate2DatabricksDeepInfraDeepSparseEden AIFireworksForefrontAIGoogle Vertex AI PaLMGooseAIGPT4AllHugging Face HubHugging Face Local PipelinesHuggingface TextGen InferenceJSONFormerKoboldAI APILlama.cppLLM Caching integrationsManifestMinimaxModalMosaicMLNLP CloudOctoAIOllamaOpaquePromptsOpenAIOpenLLMOpenLMPetalsPipelineAIPredibasePrediction GuardPromptLayer OpenAIRELLMReplicateRunhouseSageMakerEndpointStochasticAINebula (Symbl.ai)TextGenTitan TakeoffTongyi QwenvLLMWriterXorbits Inference (Xinference)MemoryRetrieversText embedding modelsAgents & ToolkitsToolsVector storesGrouped by provider Integrations Callbacks Chat models Chat loaders Document loaders Document transformers LLMs Memory Retrievers Text embedding models Agents & Toolkits Tools Vector stores Grouped by provider IntegrationsLLMsTitan TakeoffOn this pageTitan TakeoffTitanML helps businesses build and deploy better, smaller, cheaper, and faster NLP models through our training, compression, and inference optimization platform. Our inference server, Titan Takeoff enables deployment of LLMs locally on your hardware in a single command. Most generative model architectures are supported, such as Falcon, Llama 2, GPT2, T5 and many more.Installation‚ÄãTo get started with Iris Takeoff, all you need is to have docker and python installed on your local system. If you wish to use the server with gpu suport, then you will need to install docker with cuda support.For Mac and Windows users, make sure you have the docker daemon running! You can check this by running docker ps in your terminal. To start the daemon, open the docker desktop app.Run the following command to install the Iris CLI that will enable you to run the takeoff server:pip install titan-irisChoose a Model‚ÄãTakeoff supports many of the most powerful generative text models, such as Falcon, MPT, and Llama. See the supported models for more information. For information about using your own models, see the custom models.Going forward in this demo we will be using the falcon 7B instruct model. This is a good open source model that is trained to follow instructions, and is small enough to easily inference even on CPUs.Taking off‚ÄãModels are referred to by their model id on HuggingFace. Takeoff uses port 8000 by default, but can be configured to use another port. There is also support to use a Nvidia GPU by specifing cuda for the device flag.To start the takeoff server, run:iris takeoff --model tiiuae/falcon-7b-instruct --device cpuiris takeoff --model tiiuae/falcon-7b-instruct --device cuda # Nvidia GPU requirediris takeoff --model tiiuae/falcon-7b-instruct --device cpu --port 5000 # run on port 5000 (default: 8000)You will then be directed to a login page, where you will need to create an account to proceed. After logging in, run the command onscreen to check whether the server is ready. When it is ready, you can start using the Takeoff integration.To shutdown the server, run the following command. You will be presented with options on which Takeoff server to shut down, in case you have multiple running servers.iris takeoff --shutdown # shutdown the serverInferencing your model‚ÄãTo access your LLM, use the TitanTakeoff LLM wrapper:from langchain.llms import TitanTakeoffllm = TitanTakeoff(    baseURL="http://localhost:8000",    generate_max_length=128,    temperature=1.0)prompt = "What is the largest planet in the solar system?"llm(prompt)API Reference:TitanTakeoffNo parameters are needed by default, but a baseURL that points to your desired URL where Takeoff is running can be specified and generation parameters can be supplied.Streaming‚ÄãStreaming is also supported via the streaming flag:from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandlerfrom langchain.callbacks.manager import CallbackManagerllm = TitanTakeoff(callback_manager=CallbackManager([StreamingStdOutCallbackHandler()]), streaming=True)prompt = "What is the capital of France?"llm(prompt)API Reference:StreamingStdOutCallbackHandlerCallbackManagerIntegration with LLMChain‚Äãfrom langchain import PromptTemplate, LLMChainllm = TitanTakeoff()template = "What is the capital of {country}"prompt = PromptTemplate(template=template, input_variables=["country"])llm_chain = LLMChain(llm=llm, prompt=prompt)generated = llm_chain.run(country="Belgium")print(generated)PreviousTextGenNextTongyi QwenInstallationChoose a ModelTaking offInferencing your modelStreamingIntegration with LLMChain IntegrationsLLMsTitan TakeoffOn this pageTitan TakeoffTitanML helps businesses build and deploy better, smaller, cheaper, and faster NLP models through our training, compression, and inference optimization platform. Our inference server, Titan Takeoff enables deployment of LLMs locally on your hardware in a single command. Most generative model architectures are supported, such as Falcon, Llama 2, GPT2, T5 and many more.Installation‚ÄãTo get started with Iris Takeoff, all you need is to have docker and python installed on your local system. If you wish to use the server with gpu suport, then you will need to install docker with cuda support.For Mac and Windows users, make sure you have the docker daemon running! You can check this by running docker ps in your terminal. To start the daemon, open the docker desktop app.Run the following command to install the Iris CLI that will enable you to run the takeoff server:pip install titan-irisChoose a Model‚ÄãTakeoff supports many of the most powerful generative text models, such as Falcon, MPT, and Llama. See the supported models for more information. For information about using your own models, see the custom models.Going forward in this demo we will be using the falcon 7B instruct model. This is a good open source model that is trained to follow instructions, and is small enough to easily inference even on CPUs.Taking off‚ÄãModels are referred to by their model id on HuggingFace. Takeoff uses port 8000 by default, but can be configured to use another port. There is also support to use a Nvidia GPU by specifing cuda for the device flag.To start the takeoff server, run:iris takeoff --model tiiuae/falcon-7b-instruct --device cpuiris takeoff --model tiiuae/falcon-7b-instruct --device cuda # Nvidia GPU requirediris takeoff --model tiiuae/falcon-7b-instruct --device cpu --port 5000 # run on port 5000 (default: 8000)You will then be directed to a login page, where you will need to create an account to proceed. After logging in, run the command onscreen to check whether the server is ready. When it is ready, you can start using the Takeoff integration.To shutdown the server, run the following command. You will be presented with options on which Takeoff server to shut down, in case you have multiple running servers.iris takeoff --shutdown # shutdown the serverInferencing your model‚ÄãTo access your LLM, use the TitanTakeoff LLM wrapper:from langchain.llms import TitanTakeoffllm = TitanTakeoff(    baseURL="http://localhost:8000",    generate_max_length=128,    temperature=1.0)prompt = "What is the largest planet in the solar system?"llm(prompt)API Reference:TitanTakeoffNo parameters are needed by default, but a baseURL that points to your desired URL where Takeoff is running can be specified and generation parameters can be supplied.Streaming‚ÄãStreaming is also supported via the streaming flag:from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandlerfrom langchain.callbacks.manager import CallbackManagerllm = TitanTakeoff(callback_manager=CallbackManager([StreamingStdOutCallbackHandler()]), streaming=True)prompt = "What is the capital of France?"llm(prompt)API Reference:StreamingStdOutCallbackHandlerCallbackManagerIntegration with LLMChain‚Äãfrom langchain import PromptTemplate, LLMChainllm = TitanTakeoff()template = "What is the capital of {country}"prompt = PromptTemplate(template=template, input_variables=["country"])llm_chain = LLMChain(llm=llm, prompt=prompt)generated = llm_chain.run(country="Belgium")print(generated)PreviousTextGenNextTongyi Qwen IntegrationsLLMsTitan TakeoffOn this pageTitan TakeoffTitanML helps businesses build and deploy better, smaller, cheaper, and faster NLP models through our training, compression, and inference optimization platform. Our inference server, Titan Takeoff enables deployment of LLMs locally on your hardware in a single command. Most generative model architectures are supported, such as Falcon, Llama 2, GPT2, T5 and many more.Installation‚ÄãTo get started with Iris Takeoff, all you need is to have docker and python installed on your local system. If you wish to use the server with gpu suport, then you will need to install docker with cuda support.For Mac and Windows users, make sure you have the docker daemon running! You can check this by running docker ps in your terminal. To start the daemon, open the docker desktop app.Run the following command to install the Iris CLI that will enable you to run the takeoff server:pip install titan-irisChoose a Model‚ÄãTakeoff supports many of the most powerful generative text models, such as Falcon, MPT, and Llama. See the supported models for more information. For information about using your own models, see the custom models.Going forward in this demo we will be using the falcon 7B instruct model. This is a good open source model that is trained to follow instructions, and is small enough to easily inference even on CPUs.Taking off‚ÄãModels are referred to by their model id on HuggingFace. Takeoff uses port 8000 by default, but can be configured to use another port. There is also support to use a Nvidia GPU by specifing cuda for the device flag.To start the takeoff server, run:iris takeoff --model tiiuae/falcon-7b-instruct --device cpuiris takeoff --model tiiuae/falcon-7b-instruct --device cuda # Nvidia GPU requirediris takeoff --model tiiuae/falcon-7b-instruct --device cpu --port 5000 # run on port 5000 (default: 8000)You will then be directed to a login page, where you will need to create an account to proceed. After logging in, run the command onscreen to check whether the server is ready. When it is ready, you can start using the Takeoff integration.To shutdown the server, run the following command. You will be presented with options on which Takeoff server to shut down, in case you have multiple running servers.iris takeoff --shutdown # shutdown the serverInferencing your model‚ÄãTo access your LLM, use the TitanTakeoff LLM wrapper:from langchain.llms import TitanTakeoffllm = TitanTakeoff(    baseURL="http://localhost:8000",    generate_max_length=128,    temperature=1.0)prompt = "What is the largest planet in the solar system?"llm(prompt)API Reference:TitanTakeoffNo parameters are needed by default, but a baseURL that points to your desired URL where Takeoff is running can be specified and generation parameters can be supplied.Streaming‚ÄãStreaming is also supported via the streaming flag:from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandlerfrom langchain.callbacks.manager import CallbackManagerllm = TitanTakeoff(callback_manager=CallbackManager([StreamingStdOutCallbackHandler()]), streaming=True)prompt = "What is the capital of France?"llm(prompt)API Reference:StreamingStdOutCallbackHandlerCallbackManagerIntegration with LLMChain‚Äãfrom langchain import PromptTemplate, LLMChainllm = TitanTakeoff()template = "What is the capital of {country}"prompt = PromptTemplate(template=template, input_variables=["country"])llm_chain = LLMChain(llm=llm, prompt=prompt)generated = llm_chain.run(country="Belgium")print(generated)PreviousTextGenNextTongyi Qwen On this page Titan TakeoffTitanML helps businesses build and deploy better, smaller, cheaper, and faster NLP models through our training, compression, and inference optimization platform. Our inference server, Titan Takeoff enables deployment of LLMs locally on your hardware in a single command. Most generative model architectures are supported, such as Falcon, Llama 2, GPT2, T5 and many more.Installation‚ÄãTo get started with Iris Takeoff, all you need is to have docker and python installed on your local system. If you wish to use the server with gpu suport, then you will need to install docker with cuda support.For Mac and Windows users, make sure you have the docker daemon running! You can check this by running docker ps in your terminal. To start the daemon, open the docker desktop app.Run the following command to install the Iris CLI that will enable you to run the takeoff server:pip install titan-irisChoose a Model‚ÄãTakeoff supports many of the most powerful generative text models, such as Falcon, MPT, and Llama. See the supported models for more information. For information about using your own models, see the custom models.Going forward in this demo we will be using the falcon 7B instruct model. This is a good open source model that is trained to follow instructions, and is small enough to easily inference even on CPUs.Taking off‚ÄãModels are referred to by their model id on HuggingFace. Takeoff uses port 8000 by default, but can be configured to use another port. There is also support to use a Nvidia GPU by specifing cuda for the device flag.To start the takeoff server, run:iris takeoff --model tiiuae/falcon-7b-instruct --device cpuiris takeoff --model tiiuae/falcon-7b-instruct --device cuda # Nvidia GPU requirediris takeoff --model tiiuae/falcon-7b-instruct --device cpu --port 5000 # run on port 5000 (default: 8000)You will then be directed to a login page, where you will need to create an account to proceed. After logging in, run the command onscreen to check whether the server is ready. When it is ready, you can start using the Takeoff integration.To shutdown the server, run the following command. You will be presented with options on which Takeoff server to shut down, in case you have multiple running servers.iris takeoff --shutdown # shutdown the serverInferencing your model‚ÄãTo access your LLM, use the TitanTakeoff LLM wrapper:from langchain.llms import TitanTakeoffllm = TitanTakeoff(    baseURL="http://localhost:8000",    generate_max_length=128,    temperature=1.0)prompt = "What is the largest planet in the solar system?"llm(prompt)API Reference:TitanTakeoffNo parameters are needed by default, but a baseURL that points to your desired URL where Takeoff is running can be specified and generation parameters can be supplied.Streaming‚ÄãStreaming is also supported via the streaming flag:from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandlerfrom langchain.callbacks.manager import CallbackManagerllm = TitanTakeoff(callback_manager=CallbackManager([StreamingStdOutCallbackHandler()]), streaming=True)prompt = "What is the capital of France?"llm(prompt)API Reference:StreamingStdOutCallbackHandlerCallbackManagerIntegration with LLMChain‚Äãfrom langchain import PromptTemplate, LLMChainllm = TitanTakeoff()template = "What is the capital of {country}"prompt = PromptTemplate(template=template, input_variables=["country"])llm_chain = LLMChain(llm=llm, prompt=prompt)generated = llm_chain.run(country="Belgium")print(generated) pip install titan-iris pip install titan-iris  iris takeoff --model tiiuae/falcon-7b-instruct --device cpuiris takeoff --model tiiuae/falcon-7b-instruct --device cuda # Nvidia GPU requirediris takeoff --model tiiuae/falcon-7b-instruct --device cpu --port 5000 # run on port 5000 (default: 8000) iris takeoff --model tiiuae/falcon-7b-instruct --device cpuiris takeoff --model tiiuae/falcon-7b-instruct --device cuda # Nvidia GPU requirediris takeoff --model tiiuae/falcon-7b-instruct --device cpu --port 5000 # run on port 5000 (default: 8000)  iris takeoff --shutdown # shutdown the server iris takeoff --shutdown # shutdown the server  from langchain.llms import TitanTakeoffllm = TitanTakeoff(    baseURL="http://localhost:8000",    generate_max_length=128,    temperature=1.0)prompt = "What is the largest planet in the solar system?"llm(prompt) from langchain.llms import TitanTakeoffllm = TitanTakeoff(    baseURL="http://localhost:8000",    generate_max_length=128,    temperature=1.0)prompt = "What is the largest planet in the solar system?"llm(prompt)  API Reference:TitanTakeoff from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandlerfrom langchain.callbacks.manager import CallbackManagerllm = TitanTakeoff(callback_manager=CallbackManager([StreamingStdOutCallbackHandler()]), streaming=True)prompt = "What is the capital of France?"llm(prompt) from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandlerfrom langchain.callbacks.manager import CallbackManagerllm = TitanTakeoff(callback_manager=CallbackManager([StreamingStdOutCallbackHandler()]), streaming=True)prompt = "What is the capital of France?"llm(prompt)  API Reference:StreamingStdOutCallbackHandlerCallbackManager from langchain import PromptTemplate, LLMChainllm = TitanTakeoff()template = "What is the capital of {country}"prompt = PromptTemplate(template=template, input_variables=["country"])llm_chain = LLMChain(llm=llm, prompt=prompt)generated = llm_chain.run(country="Belgium")print(generated) from langchain import PromptTemplate, LLMChainllm = TitanTakeoff()template = "What is the capital of {country}"prompt = PromptTemplate(template=template, input_variables=["country"])llm_chain = LLMChain(llm=llm, prompt=prompt)generated = llm_chain.run(country="Belgium")print(generated)  Previous TextGen Next Tongyi Qwen InstallationChoose a ModelTaking offInferencing your modelStreamingIntegration with LLMChain InstallationChoose a ModelTaking offInferencing your modelStreamingIntegration with LLMChain CommunityDiscordTwitterGitHubPythonJS/TSMoreHomepageBlogCopyright ¬© 2023 LangChain, Inc. CommunityDiscordTwitterGitHubPythonJS/TSMoreHomepageBlog CommunityDiscordTwitter Community GitHubPythonJS/TS GitHub MoreHomepageBlog More Copyright ¬© 2023 LangChain, Inc. Copyright ¬© 2023 LangChain, Inc. Skip to main content (#docusaurus_skipToContent_fallback) ü¶úÔ∏èüîó LangChain (/) Docs (/docs/get_started/introduction) Use cases (/docs/use_cases) Integrations (/docs/integrations) API (https://api.python.langchain.com) LangSmith (https://smith.langchain.com) JS/TS Docs (https://js.langchain.com/docs)  (https://github.com/hwchase17/langchain) Integrations (/docs/integrations) Callbacks (/docs/integrations/callbacks/) Chat models (/docs/integrations/chat/) Chat loaders (/docs/integrations/chat_loaders/) Document loaders (/docs/integrations/document_loaders/) Document transformers (/docs/integrations/document_transformers/) LLMs (/docs/integrations/llms/) AI21 (/docs/integrations/llms/ai21) Aleph Alpha (/docs/integrations/llms/aleph_alpha) Amazon API Gateway (/docs/integrations/llms/amazon_api_gateway) Anyscale (/docs/integrations/llms/anyscale) Azure ML (/docs/integrations/llms/azure_ml) Azure OpenAI (/docs/integrations/llms/azure_openai) Banana (/docs/integrations/llms/banana) Baseten (/docs/integrations/llms/baseten) Beam (/docs/integrations/llms/beam) Bedrock (/docs/integrations/llms/bedrock) Bittensor (/docs/integrations/llms/bittensor) CerebriumAI (/docs/integrations/llms/cerebriumai) ChatGLM (/docs/integrations/llms/chatglm) Clarifai (/docs/integrations/llms/clarifai) Cohere (/docs/integrations/llms/cohere) C Transformers (/docs/integrations/llms/ctransformers) CTranslate2 (/docs/integrations/llms/ctranslate2) Databricks (/docs/integrations/llms/databricks) DeepInfra (/docs/integrations/llms/deepinfra) DeepSparse (/docs/integrations/llms/deepsparse) Eden AI (/docs/integrations/llms/edenai) Fireworks (/docs/integrations/llms/fireworks) ForefrontAI (/docs/integrations/llms/forefrontai) Google Vertex AI PaLM (/docs/integrations/llms/google_vertex_ai_palm) GooseAI (/docs/integrations/llms/gooseai) GPT4All (/docs/integrations/llms/gpt4all) Hugging Face Hub (/docs/integrations/llms/huggingface_hub) Hugging Face Local Pipelines (/docs/integrations/llms/huggingface_pipelines) Huggingface TextGen Inference (/docs/integrations/llms/huggingface_textgen_inference) JSONFormer (/docs/integrations/llms/jsonformer_experimental) KoboldAI API (/docs/integrations/llms/koboldai) Llama.cpp (/docs/integrations/llms/llamacpp) LLM Caching integrations (/docs/integrations/llms/llm_caching) Manifest (/docs/integrations/llms/manifest) Minimax (/docs/integrations/llms/minimax) Modal (/docs/integrations/llms/modal) MosaicML (/docs/integrations/llms/mosaicml) NLP Cloud (/docs/integrations/llms/nlpcloud) OctoAI (/docs/integrations/llms/octoai) Ollama (/docs/integrations/llms/ollama) OpaquePrompts (/docs/integrations/llms/opaqueprompts) OpenAI (/docs/integrations/llms/openai) OpenLLM (/docs/integrations/llms/openllm) OpenLM (/docs/integrations/llms/openlm) Petals (/docs/integrations/llms/petals) PipelineAI (/docs/integrations/llms/pipelineai) Predibase (/docs/integrations/llms/predibase) Prediction Guard (/docs/integrations/llms/predictionguard) PromptLayer OpenAI (/docs/integrations/llms/promptlayer_openai) RELLM (/docs/integrations/llms/rellm_experimental) Replicate (/docs/integrations/llms/replicate) Runhouse (/docs/integrations/llms/runhouse) SageMakerEndpoint (/docs/integrations/llms/sagemaker) StochasticAI (/docs/integrations/llms/stochasticai) Nebula (Symbl.ai) (/docs/integrations/llms/symblai_nebula) TextGen (/docs/integrations/llms/textgen) Titan Takeoff (/docs/integrations/llms/titan_takeoff) Tongyi Qwen (/docs/integrations/llms/tongyi) vLLM (/docs/integrations/llms/vllm) Writer (/docs/integrations/llms/writer) Xorbits Inference (Xinference) (/docs/integrations/llms/xinference) Memory (/docs/integrations/memory/) Retrievers (/docs/integrations/retrievers/) Text embedding models (/docs/integrations/text_embedding/) Agents & Toolkits (/docs/integrations/toolkits/) Tools (/docs/integrations/tools/) Vector stores (/docs/integrations/vectorstores/) Grouped by provider (/docs/integrations/providers/)  (/) Integrations (/docs/integrations) LLMs (/docs/integrations/llms/) Titan Takeoff (https://docs.titanml.co/docs/titan-takeoff/getting-started) ‚Äã (#installation) ‚Äã (#choose-a-model) supported models (https://docs.titanml.co/docs/titan-takeoff/supported-models) custom models (https://docs.titanml.co/docs/titan-takeoff/Advanced/custom-models) ‚Äã (#taking-off) ‚Äã (#inferencing-your-model) TitanTakeoff (https://api.python.langchain.com/en/latest/llms/langchain.llms.titan_takeoff.TitanTakeoff.html) generation parameters (https://docs.titanml.co/docs/titan-takeoff/Advanced/generation-parameters) ‚Äã (#streaming) StreamingStdOutCallbackHandler (https://api.python.langchain.com/en/latest/callbacks/langchain.callbacks.streaming_stdout.StreamingStdOutCallbackHandler.html) CallbackManager (https://api.python.langchain.com/en/latest/callbacks/langchain.callbacks.manager.CallbackManager.html) ‚Äã (#integration-with-llmchain) PreviousTextGen (/docs/integrations/llms/textgen) NextTongyi Qwen (/docs/integrations/llms/tongyi) Installation (#installation) Choose a Model (#choose-a-model) Taking off (#taking-off) Inferencing your model (#inferencing-your-model) Streaming (#streaming) Integration with LLMChain (#integration-with-llmchain) Discord (https://discord.gg/cU2adEyC7w) Twitter (https://twitter.com/LangChainAI) Python (https://github.com/hwchase17/langchain) JS/TS (https://github.com/hwchase17/langchainjs) Homepage (https://langchain.com) Blog (https://blog.langchain.dev)