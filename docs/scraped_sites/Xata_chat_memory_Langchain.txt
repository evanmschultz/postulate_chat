Xata is a serverless data platform, based on PostgreSQL and Elasticsearch. It provides a Python SDK for interacting with your database, and a UI for managing your data. With the XataChatMessageHistory class, you can use Xata databases for longer-term persistence of chat sessions. This notebook covers: In the Xata UI create a new database. You can name it whatever you want, in this notepad we'll use langchain. The Langchain integration can auto-create the table used for storying the memory, and this is what we'll use in this example. If you want to pre-create the table, ensure it has the right schema and set create_table to False when creating the class. Pre-creating the table saves one round-trip to the database during each session initialization. Let's first install our dependencies: Next, we need to get the environment variables for Xata. You can create a new API key by visiting your account settings. To find the database URL, go to the Settings page of the database that you have created. The database URL should look something like this: https://demo-uni3q8.eu-west-1.xata.sh/db/langchain. To test the memory store functionality in isolation, let's use the following code snippet: The above code creates a session with the ID session-1 and stores two messages in it. After running the above, if you visit the Xata UI, you should see a table named memory and the two messages added to it. You can retrieve the message history for a particular session with the following code: Let's now see a more complex example in which we combine OpenAI, the Xata Vector Store integration, and the Xata memory store integration to create a Q&A chat bot on your data, with follow-up questions and history. We're going to need to access the OpenAI API, so let's configure the API key: To store the documents that the chatbot will search for answers, add a table named docs to your langchain database using the Xata UI, and add the following columns: Let's create the vector store and add some sample docs to it: After running the above command, if you go to the Xata UI, you should see the documents loaded together with their embeddings in the docs table. Let's now create a ConversationBufferMemory to store the chat messages from both the user and the AI. Now it's time to create an Agent to use both the vector store and the chat memory together. To test, let's tell the agent our name: Now, let's now ask the agent some questions about Xata: Notice that it answers based on the data stored in the document store. And now, let's ask a follow up question: And now let's test its memory: IntegrationsCallbacksChat modelsChat loadersDocument loadersDocument transformersLLMsMemoryCassandra Chat Message HistoryDynamodb Chat Message HistoryEntity Memory with SQLite storageMomento Chat Message HistoryMongodb Chat Message HistoryMot√∂rhead MemoryMot√∂rhead Memory (Managed)Postgres Chat Message HistoryRedis Chat Message HistoryRockset Chat Message HistorySQL Chat Message HistoryStreamlit Chat Message HistoryXata chat memoryZep MemoryRetrieversText embedding modelsAgents & ToolkitsToolsVector storesGrouped by provider Callbacks Chat models Chat loaders Document loaders Document transformers LLMs MemoryCassandra Chat Message HistoryDynamodb Chat Message HistoryEntity Memory with SQLite storageMomento Chat Message HistoryMongodb Chat Message HistoryMot√∂rhead MemoryMot√∂rhead Memory (Managed)Postgres Chat Message HistoryRedis Chat Message HistoryRockset Chat Message HistorySQL Chat Message HistoryStreamlit Chat Message HistoryXata chat memoryZep Memory Cassandra Chat Message History Dynamodb Chat Message History Entity Memory with SQLite storage Momento Chat Message History Mongodb Chat Message History Mot√∂rhead Memory Mot√∂rhead Memory (Managed) Postgres Chat Message History Redis Chat Message History Rockset Chat Message History SQL Chat Message History Streamlit Chat Message History Xata chat memory Zep Memory Retrievers Text embedding models Agents & Toolkits Tools Vector stores Grouped by provider  Integrations Memory Xata chat memory A simple example showing what XataChatMessageHistory does. A more complex example using a REACT agent that answer questions based on a knowledge based or documentation (stored in Xata as a vector store) and also having a long-term searchable history of its past messages (stored in Xata as a memory store) XataChatMessageHistory content of type "Text". This is used to store the Document.pageContent values. embedding of type "Vector". Use the dimension used by the model you plan to use. In this notebook we use OpenAI embeddings, which have 1536 dimensions. OpenAIEmbeddings XataVectorStore ConversationBufferMemory initialize_agent AgentType create_retriever_tool ChatOpenAI SetupCreate a database Create a database Create a simple memory store Conversational Q&A chain on your data with memory Discord Twitter Python JS/TS Homepage Blog Skip to main contentü¶úÔ∏èüîó LangChainDocsUse casesIntegrationsAPILangSmithJS/TS DocsCTRLKIntegrationsCallbacksChat modelsChat loadersDocument loadersDocument transformersLLMsMemoryCassandra Chat Message HistoryDynamodb Chat Message HistoryEntity Memory with SQLite storageMomento Chat Message HistoryMongodb Chat Message HistoryMot√∂rhead MemoryMot√∂rhead Memory (Managed)Postgres Chat Message HistoryRedis Chat Message HistoryRockset Chat Message HistorySQL Chat Message HistoryStreamlit Chat Message HistoryXata chat memoryZep MemoryRetrieversText embedding modelsAgents & ToolkitsToolsVector storesGrouped by providerIntegrationsMemoryXata chat memoryOn this pageXata chat memoryXata is a serverless data platform, based on PostgreSQL and Elasticsearch. It provides a Python SDK for interacting with your database, and a UI for managing your data. With the XataChatMessageHistory class, you can use Xata databases for longer-term persistence of chat sessions.This notebook covers:A simple example showing what XataChatMessageHistory does.A more complex example using a REACT agent that answer questions based on a knowledge based or documentation (stored in Xata as a vector store) and also having a long-term searchable history of its past messages (stored in Xata as a memory store)Setup‚ÄãCreate a database‚ÄãIn the Xata UI create a new database. You can name it whatever you want, in this notepad we'll use langchain. The Langchain integration can auto-create the table used for storying the memory, and this is what we'll use in this example. If you want to pre-create the table, ensure it has the right schema and set create_table to False when creating the class. Pre-creating the table saves one round-trip to the database during each session initialization.Let's first install our dependencies:pip install xata openai langchainNext, we need to get the environment variables for Xata. You can create a new API key by visiting your account settings. To find the database URL, go to the Settings page of the database that you have created. The database URL should look something like this: https://demo-uni3q8.eu-west-1.xata.sh/db/langchain.import getpassapi_key = getpass.getpass("Xata API key: ")db_url = input("Xata database URL (copy it from your DB settings):")Create a simple memory store‚ÄãTo test the memory store functionality in isolation, let's use the following code snippet:from langchain.memory import XataChatMessageHistoryhistory = XataChatMessageHistory(    session_id="session-1",    api_key=api_key,    db_url=db_url,    table_name="memory")history.add_user_message("hi!")history.add_ai_message("whats up?")API Reference:XataChatMessageHistoryThe above code creates a session with the ID session-1 and stores two messages in it. After running the above, if you visit the Xata UI, you should see a table named memory and the two messages added to it.You can retrieve the message history for a particular session with the following code:history.messagesConversational Q&A chain on your data with memory‚ÄãLet's now see a more complex example in which we combine OpenAI, the Xata Vector Store integration, and the Xata memory store integration to create a Q&A chat bot on your data, with follow-up questions and history.We're going to need to access the OpenAI API, so let's configure the API key:import osos.environ["OPENAI_API_KEY"] = getpass.getpass("OpenAI API Key:")To store the documents that the chatbot will search for answers, add a table named docs to your langchain database using the Xata UI, and add the following columns:content of type "Text". This is used to store the Document.pageContent values.embedding of type "Vector". Use the dimension used by the model you plan to use. In this notebook we use OpenAI embeddings, which have 1536 dimensions.Let's create the vector store and add some sample docs to it:from langchain.embeddings.openai import OpenAIEmbeddingsfrom langchain.vectorstores.xata import XataVectorStoreembeddings = OpenAIEmbeddings()texts = [    "Xata is a Serverless Data platform based on PostgreSQL",    "Xata offers a built-in vector type that can be used to store and query vectors",    "Xata includes similarity search"]vector_store = XataVectorStore.from_texts(texts, embeddings, api_key=api_key, db_url=db_url, table_name="docs")API Reference:OpenAIEmbeddingsXataVectorStoreAfter running the above command, if you go to the Xata UI, you should see the documents loaded together with their embeddings in the docs table.Let's now create a ConversationBufferMemory to store the chat messages from both the user and the AI.from langchain.memory import ConversationBufferMemoryfrom uuid import uuid4chat_memory = XataChatMessageHistory(    session_id=str(uuid4()),   # needs to be unique per user session    api_key=api_key,    db_url=db_url,    table_name="memory")memory = ConversationBufferMemory(memory_key="chat_history", chat_memory=chat_memory, return_messages=True)API Reference:ConversationBufferMemoryNow it's time to create an Agent to use both the vector store and the chat memory together.from langchain.agents import initialize_agent, AgentTypefrom langchain.agents.agent_toolkits import create_retriever_toolfrom langchain.chat_models import ChatOpenAItool = create_retriever_tool(    vector_store.as_retriever(),     "search_docs",    "Searches and returns documents from the Xata manual. Useful when you need to answer questions about Xata.")tools = [tool]llm = ChatOpenAI(temperature=0)agent = initialize_agent(    tools,    llm,    agent=AgentType.CHAT_CONVERSATIONAL_REACT_DESCRIPTION,    verbose=True,    memory=memory)API Reference:initialize_agentAgentTypecreate_retriever_toolChatOpenAITo test, let's tell the agent our name:agent.run(input="My name is bob")Now, let's now ask the agent some questions about Xata:agent.run(input="What is xata?")Notice that it answers based on the data stored in the document store. And now, let's ask a follow up question:agent.run(input="Does it support similarity search?")And now let's test its memory:agent.run(input="Did I tell you my name? What is it?")PreviousStreamlit Chat Message HistoryNextZep MemorySetupCreate a databaseCreate a simple memory storeConversational Q&A chain on your data with memoryCommunityDiscordTwitterGitHubPythonJS/TSMoreHomepageBlogCopyright ¬© 2023 LangChain, Inc. Skip to main content ü¶úÔ∏èüîó LangChainDocsUse casesIntegrationsAPILangSmithJS/TS DocsCTRLK ü¶úÔ∏èüîó LangChainDocsUse casesIntegrationsAPI LangSmithJS/TS DocsCTRLK  CTRLK CTRLK  CTRLK   IntegrationsCallbacksChat modelsChat loadersDocument loadersDocument transformersLLMsMemoryCassandra Chat Message HistoryDynamodb Chat Message HistoryEntity Memory with SQLite storageMomento Chat Message HistoryMongodb Chat Message HistoryMot√∂rhead MemoryMot√∂rhead Memory (Managed)Postgres Chat Message HistoryRedis Chat Message HistoryRockset Chat Message HistorySQL Chat Message HistoryStreamlit Chat Message HistoryXata chat memoryZep MemoryRetrieversText embedding modelsAgents & ToolkitsToolsVector storesGrouped by providerIntegrationsMemoryXata chat memoryOn this pageXata chat memoryXata is a serverless data platform, based on PostgreSQL and Elasticsearch. It provides a Python SDK for interacting with your database, and a UI for managing your data. With the XataChatMessageHistory class, you can use Xata databases for longer-term persistence of chat sessions.This notebook covers:A simple example showing what XataChatMessageHistory does.A more complex example using a REACT agent that answer questions based on a knowledge based or documentation (stored in Xata as a vector store) and also having a long-term searchable history of its past messages (stored in Xata as a memory store)Setup‚ÄãCreate a database‚ÄãIn the Xata UI create a new database. You can name it whatever you want, in this notepad we'll use langchain. The Langchain integration can auto-create the table used for storying the memory, and this is what we'll use in this example. If you want to pre-create the table, ensure it has the right schema and set create_table to False when creating the class. Pre-creating the table saves one round-trip to the database during each session initialization.Let's first install our dependencies:pip install xata openai langchainNext, we need to get the environment variables for Xata. You can create a new API key by visiting your account settings. To find the database URL, go to the Settings page of the database that you have created. The database URL should look something like this: https://demo-uni3q8.eu-west-1.xata.sh/db/langchain.import getpassapi_key = getpass.getpass("Xata API key: ")db_url = input("Xata database URL (copy it from your DB settings):")Create a simple memory store‚ÄãTo test the memory store functionality in isolation, let's use the following code snippet:from langchain.memory import XataChatMessageHistoryhistory = XataChatMessageHistory(    session_id="session-1",    api_key=api_key,    db_url=db_url,    table_name="memory")history.add_user_message("hi!")history.add_ai_message("whats up?")API Reference:XataChatMessageHistoryThe above code creates a session with the ID session-1 and stores two messages in it. After running the above, if you visit the Xata UI, you should see a table named memory and the two messages added to it.You can retrieve the message history for a particular session with the following code:history.messagesConversational Q&A chain on your data with memory‚ÄãLet's now see a more complex example in which we combine OpenAI, the Xata Vector Store integration, and the Xata memory store integration to create a Q&A chat bot on your data, with follow-up questions and history.We're going to need to access the OpenAI API, so let's configure the API key:import osos.environ["OPENAI_API_KEY"] = getpass.getpass("OpenAI API Key:")To store the documents that the chatbot will search for answers, add a table named docs to your langchain database using the Xata UI, and add the following columns:content of type "Text". This is used to store the Document.pageContent values.embedding of type "Vector". Use the dimension used by the model you plan to use. In this notebook we use OpenAI embeddings, which have 1536 dimensions.Let's create the vector store and add some sample docs to it:from langchain.embeddings.openai import OpenAIEmbeddingsfrom langchain.vectorstores.xata import XataVectorStoreembeddings = OpenAIEmbeddings()texts = [    "Xata is a Serverless Data platform based on PostgreSQL",    "Xata offers a built-in vector type that can be used to store and query vectors",    "Xata includes similarity search"]vector_store = XataVectorStore.from_texts(texts, embeddings, api_key=api_key, db_url=db_url, table_name="docs")API Reference:OpenAIEmbeddingsXataVectorStoreAfter running the above command, if you go to the Xata UI, you should see the documents loaded together with their embeddings in the docs table.Let's now create a ConversationBufferMemory to store the chat messages from both the user and the AI.from langchain.memory import ConversationBufferMemoryfrom uuid import uuid4chat_memory = XataChatMessageHistory(    session_id=str(uuid4()),   # needs to be unique per user session    api_key=api_key,    db_url=db_url,    table_name="memory")memory = ConversationBufferMemory(memory_key="chat_history", chat_memory=chat_memory, return_messages=True)API Reference:ConversationBufferMemoryNow it's time to create an Agent to use both the vector store and the chat memory together.from langchain.agents import initialize_agent, AgentTypefrom langchain.agents.agent_toolkits import create_retriever_toolfrom langchain.chat_models import ChatOpenAItool = create_retriever_tool(    vector_store.as_retriever(),     "search_docs",    "Searches and returns documents from the Xata manual. Useful when you need to answer questions about Xata.")tools = [tool]llm = ChatOpenAI(temperature=0)agent = initialize_agent(    tools,    llm,    agent=AgentType.CHAT_CONVERSATIONAL_REACT_DESCRIPTION,    verbose=True,    memory=memory)API Reference:initialize_agentAgentTypecreate_retriever_toolChatOpenAITo test, let's tell the agent our name:agent.run(input="My name is bob")Now, let's now ask the agent some questions about Xata:agent.run(input="What is xata?")Notice that it answers based on the data stored in the document store. And now, let's ask a follow up question:agent.run(input="Does it support similarity search?")And now let's test its memory:agent.run(input="Did I tell you my name? What is it?")PreviousStreamlit Chat Message HistoryNextZep MemorySetupCreate a databaseCreate a simple memory storeConversational Q&A chain on your data with memory IntegrationsCallbacksChat modelsChat loadersDocument loadersDocument transformersLLMsMemoryCassandra Chat Message HistoryDynamodb Chat Message HistoryEntity Memory with SQLite storageMomento Chat Message HistoryMongodb Chat Message HistoryMot√∂rhead MemoryMot√∂rhead Memory (Managed)Postgres Chat Message HistoryRedis Chat Message HistoryRockset Chat Message HistorySQL Chat Message HistoryStreamlit Chat Message HistoryXata chat memoryZep MemoryRetrieversText embedding modelsAgents & ToolkitsToolsVector storesGrouped by providerIntegrationsMemoryXata chat memoryOn this pageXata chat memoryXata is a serverless data platform, based on PostgreSQL and Elasticsearch. It provides a Python SDK for interacting with your database, and a UI for managing your data. With the XataChatMessageHistory class, you can use Xata databases for longer-term persistence of chat sessions.This notebook covers:A simple example showing what XataChatMessageHistory does.A more complex example using a REACT agent that answer questions based on a knowledge based or documentation (stored in Xata as a vector store) and also having a long-term searchable history of its past messages (stored in Xata as a memory store)Setup‚ÄãCreate a database‚ÄãIn the Xata UI create a new database. You can name it whatever you want, in this notepad we'll use langchain. The Langchain integration can auto-create the table used for storying the memory, and this is what we'll use in this example. If you want to pre-create the table, ensure it has the right schema and set create_table to False when creating the class. Pre-creating the table saves one round-trip to the database during each session initialization.Let's first install our dependencies:pip install xata openai langchainNext, we need to get the environment variables for Xata. You can create a new API key by visiting your account settings. To find the database URL, go to the Settings page of the database that you have created. The database URL should look something like this: https://demo-uni3q8.eu-west-1.xata.sh/db/langchain.import getpassapi_key = getpass.getpass("Xata API key: ")db_url = input("Xata database URL (copy it from your DB settings):")Create a simple memory store‚ÄãTo test the memory store functionality in isolation, let's use the following code snippet:from langchain.memory import XataChatMessageHistoryhistory = XataChatMessageHistory(    session_id="session-1",    api_key=api_key,    db_url=db_url,    table_name="memory")history.add_user_message("hi!")history.add_ai_message("whats up?")API Reference:XataChatMessageHistoryThe above code creates a session with the ID session-1 and stores two messages in it. After running the above, if you visit the Xata UI, you should see a table named memory and the two messages added to it.You can retrieve the message history for a particular session with the following code:history.messagesConversational Q&A chain on your data with memory‚ÄãLet's now see a more complex example in which we combine OpenAI, the Xata Vector Store integration, and the Xata memory store integration to create a Q&A chat bot on your data, with follow-up questions and history.We're going to need to access the OpenAI API, so let's configure the API key:import osos.environ["OPENAI_API_KEY"] = getpass.getpass("OpenAI API Key:")To store the documents that the chatbot will search for answers, add a table named docs to your langchain database using the Xata UI, and add the following columns:content of type "Text". This is used to store the Document.pageContent values.embedding of type "Vector". Use the dimension used by the model you plan to use. In this notebook we use OpenAI embeddings, which have 1536 dimensions.Let's create the vector store and add some sample docs to it:from langchain.embeddings.openai import OpenAIEmbeddingsfrom langchain.vectorstores.xata import XataVectorStoreembeddings = OpenAIEmbeddings()texts = [    "Xata is a Serverless Data platform based on PostgreSQL",    "Xata offers a built-in vector type that can be used to store and query vectors",    "Xata includes similarity search"]vector_store = XataVectorStore.from_texts(texts, embeddings, api_key=api_key, db_url=db_url, table_name="docs")API Reference:OpenAIEmbeddingsXataVectorStoreAfter running the above command, if you go to the Xata UI, you should see the documents loaded together with their embeddings in the docs table.Let's now create a ConversationBufferMemory to store the chat messages from both the user and the AI.from langchain.memory import ConversationBufferMemoryfrom uuid import uuid4chat_memory = XataChatMessageHistory(    session_id=str(uuid4()),   # needs to be unique per user session    api_key=api_key,    db_url=db_url,    table_name="memory")memory = ConversationBufferMemory(memory_key="chat_history", chat_memory=chat_memory, return_messages=True)API Reference:ConversationBufferMemoryNow it's time to create an Agent to use both the vector store and the chat memory together.from langchain.agents import initialize_agent, AgentTypefrom langchain.agents.agent_toolkits import create_retriever_toolfrom langchain.chat_models import ChatOpenAItool = create_retriever_tool(    vector_store.as_retriever(),     "search_docs",    "Searches and returns documents from the Xata manual. Useful when you need to answer questions about Xata.")tools = [tool]llm = ChatOpenAI(temperature=0)agent = initialize_agent(    tools,    llm,    agent=AgentType.CHAT_CONVERSATIONAL_REACT_DESCRIPTION,    verbose=True,    memory=memory)API Reference:initialize_agentAgentTypecreate_retriever_toolChatOpenAITo test, let's tell the agent our name:agent.run(input="My name is bob")Now, let's now ask the agent some questions about Xata:agent.run(input="What is xata?")Notice that it answers based on the data stored in the document store. And now, let's ask a follow up question:agent.run(input="Does it support similarity search?")And now let's test its memory:agent.run(input="Did I tell you my name? What is it?")PreviousStreamlit Chat Message HistoryNextZep MemorySetupCreate a databaseCreate a simple memory storeConversational Q&A chain on your data with memory IntegrationsCallbacksChat modelsChat loadersDocument loadersDocument transformersLLMsMemoryCassandra Chat Message HistoryDynamodb Chat Message HistoryEntity Memory with SQLite storageMomento Chat Message HistoryMongodb Chat Message HistoryMot√∂rhead MemoryMot√∂rhead Memory (Managed)Postgres Chat Message HistoryRedis Chat Message HistoryRockset Chat Message HistorySQL Chat Message HistoryStreamlit Chat Message HistoryXata chat memoryZep MemoryRetrieversText embedding modelsAgents & ToolkitsToolsVector storesGrouped by provider IntegrationsCallbacksChat modelsChat loadersDocument loadersDocument transformersLLMsMemoryCassandra Chat Message HistoryDynamodb Chat Message HistoryEntity Memory with SQLite storageMomento Chat Message HistoryMongodb Chat Message HistoryMot√∂rhead MemoryMot√∂rhead Memory (Managed)Postgres Chat Message HistoryRedis Chat Message HistoryRockset Chat Message HistorySQL Chat Message HistoryStreamlit Chat Message HistoryXata chat memoryZep MemoryRetrieversText embedding modelsAgents & ToolkitsToolsVector storesGrouped by provider Integrations Callbacks Chat models Chat loaders Document loaders Document transformers LLMs Memory Retrievers Text embedding models Agents & Toolkits Tools Vector stores Grouped by provider IntegrationsMemoryXata chat memoryOn this pageXata chat memoryXata is a serverless data platform, based on PostgreSQL and Elasticsearch. It provides a Python SDK for interacting with your database, and a UI for managing your data. With the XataChatMessageHistory class, you can use Xata databases for longer-term persistence of chat sessions.This notebook covers:A simple example showing what XataChatMessageHistory does.A more complex example using a REACT agent that answer questions based on a knowledge based or documentation (stored in Xata as a vector store) and also having a long-term searchable history of its past messages (stored in Xata as a memory store)Setup‚ÄãCreate a database‚ÄãIn the Xata UI create a new database. You can name it whatever you want, in this notepad we'll use langchain. The Langchain integration can auto-create the table used for storying the memory, and this is what we'll use in this example. If you want to pre-create the table, ensure it has the right schema and set create_table to False when creating the class. Pre-creating the table saves one round-trip to the database during each session initialization.Let's first install our dependencies:pip install xata openai langchainNext, we need to get the environment variables for Xata. You can create a new API key by visiting your account settings. To find the database URL, go to the Settings page of the database that you have created. The database URL should look something like this: https://demo-uni3q8.eu-west-1.xata.sh/db/langchain.import getpassapi_key = getpass.getpass("Xata API key: ")db_url = input("Xata database URL (copy it from your DB settings):")Create a simple memory store‚ÄãTo test the memory store functionality in isolation, let's use the following code snippet:from langchain.memory import XataChatMessageHistoryhistory = XataChatMessageHistory(    session_id="session-1",    api_key=api_key,    db_url=db_url,    table_name="memory")history.add_user_message("hi!")history.add_ai_message("whats up?")API Reference:XataChatMessageHistoryThe above code creates a session with the ID session-1 and stores two messages in it. After running the above, if you visit the Xata UI, you should see a table named memory and the two messages added to it.You can retrieve the message history for a particular session with the following code:history.messagesConversational Q&A chain on your data with memory‚ÄãLet's now see a more complex example in which we combine OpenAI, the Xata Vector Store integration, and the Xata memory store integration to create a Q&A chat bot on your data, with follow-up questions and history.We're going to need to access the OpenAI API, so let's configure the API key:import osos.environ["OPENAI_API_KEY"] = getpass.getpass("OpenAI API Key:")To store the documents that the chatbot will search for answers, add a table named docs to your langchain database using the Xata UI, and add the following columns:content of type "Text". This is used to store the Document.pageContent values.embedding of type "Vector". Use the dimension used by the model you plan to use. In this notebook we use OpenAI embeddings, which have 1536 dimensions.Let's create the vector store and add some sample docs to it:from langchain.embeddings.openai import OpenAIEmbeddingsfrom langchain.vectorstores.xata import XataVectorStoreembeddings = OpenAIEmbeddings()texts = [    "Xata is a Serverless Data platform based on PostgreSQL",    "Xata offers a built-in vector type that can be used to store and query vectors",    "Xata includes similarity search"]vector_store = XataVectorStore.from_texts(texts, embeddings, api_key=api_key, db_url=db_url, table_name="docs")API Reference:OpenAIEmbeddingsXataVectorStoreAfter running the above command, if you go to the Xata UI, you should see the documents loaded together with their embeddings in the docs table.Let's now create a ConversationBufferMemory to store the chat messages from both the user and the AI.from langchain.memory import ConversationBufferMemoryfrom uuid import uuid4chat_memory = XataChatMessageHistory(    session_id=str(uuid4()),   # needs to be unique per user session    api_key=api_key,    db_url=db_url,    table_name="memory")memory = ConversationBufferMemory(memory_key="chat_history", chat_memory=chat_memory, return_messages=True)API Reference:ConversationBufferMemoryNow it's time to create an Agent to use both the vector store and the chat memory together.from langchain.agents import initialize_agent, AgentTypefrom langchain.agents.agent_toolkits import create_retriever_toolfrom langchain.chat_models import ChatOpenAItool = create_retriever_tool(    vector_store.as_retriever(),     "search_docs",    "Searches and returns documents from the Xata manual. Useful when you need to answer questions about Xata.")tools = [tool]llm = ChatOpenAI(temperature=0)agent = initialize_agent(    tools,    llm,    agent=AgentType.CHAT_CONVERSATIONAL_REACT_DESCRIPTION,    verbose=True,    memory=memory)API Reference:initialize_agentAgentTypecreate_retriever_toolChatOpenAITo test, let's tell the agent our name:agent.run(input="My name is bob")Now, let's now ask the agent some questions about Xata:agent.run(input="What is xata?")Notice that it answers based on the data stored in the document store. And now, let's ask a follow up question:agent.run(input="Does it support similarity search?")And now let's test its memory:agent.run(input="Did I tell you my name? What is it?")PreviousStreamlit Chat Message HistoryNextZep MemorySetupCreate a databaseCreate a simple memory storeConversational Q&A chain on your data with memory IntegrationsMemoryXata chat memoryOn this pageXata chat memoryXata is a serverless data platform, based on PostgreSQL and Elasticsearch. It provides a Python SDK for interacting with your database, and a UI for managing your data. With the XataChatMessageHistory class, you can use Xata databases for longer-term persistence of chat sessions.This notebook covers:A simple example showing what XataChatMessageHistory does.A more complex example using a REACT agent that answer questions based on a knowledge based or documentation (stored in Xata as a vector store) and also having a long-term searchable history of its past messages (stored in Xata as a memory store)Setup‚ÄãCreate a database‚ÄãIn the Xata UI create a new database. You can name it whatever you want, in this notepad we'll use langchain. The Langchain integration can auto-create the table used for storying the memory, and this is what we'll use in this example. If you want to pre-create the table, ensure it has the right schema and set create_table to False when creating the class. Pre-creating the table saves one round-trip to the database during each session initialization.Let's first install our dependencies:pip install xata openai langchainNext, we need to get the environment variables for Xata. You can create a new API key by visiting your account settings. To find the database URL, go to the Settings page of the database that you have created. The database URL should look something like this: https://demo-uni3q8.eu-west-1.xata.sh/db/langchain.import getpassapi_key = getpass.getpass("Xata API key: ")db_url = input("Xata database URL (copy it from your DB settings):")Create a simple memory store‚ÄãTo test the memory store functionality in isolation, let's use the following code snippet:from langchain.memory import XataChatMessageHistoryhistory = XataChatMessageHistory(    session_id="session-1",    api_key=api_key,    db_url=db_url,    table_name="memory")history.add_user_message("hi!")history.add_ai_message("whats up?")API Reference:XataChatMessageHistoryThe above code creates a session with the ID session-1 and stores two messages in it. After running the above, if you visit the Xata UI, you should see a table named memory and the two messages added to it.You can retrieve the message history for a particular session with the following code:history.messagesConversational Q&A chain on your data with memory‚ÄãLet's now see a more complex example in which we combine OpenAI, the Xata Vector Store integration, and the Xata memory store integration to create a Q&A chat bot on your data, with follow-up questions and history.We're going to need to access the OpenAI API, so let's configure the API key:import osos.environ["OPENAI_API_KEY"] = getpass.getpass("OpenAI API Key:")To store the documents that the chatbot will search for answers, add a table named docs to your langchain database using the Xata UI, and add the following columns:content of type "Text". This is used to store the Document.pageContent values.embedding of type "Vector". Use the dimension used by the model you plan to use. In this notebook we use OpenAI embeddings, which have 1536 dimensions.Let's create the vector store and add some sample docs to it:from langchain.embeddings.openai import OpenAIEmbeddingsfrom langchain.vectorstores.xata import XataVectorStoreembeddings = OpenAIEmbeddings()texts = [    "Xata is a Serverless Data platform based on PostgreSQL",    "Xata offers a built-in vector type that can be used to store and query vectors",    "Xata includes similarity search"]vector_store = XataVectorStore.from_texts(texts, embeddings, api_key=api_key, db_url=db_url, table_name="docs")API Reference:OpenAIEmbeddingsXataVectorStoreAfter running the above command, if you go to the Xata UI, you should see the documents loaded together with their embeddings in the docs table.Let's now create a ConversationBufferMemory to store the chat messages from both the user and the AI.from langchain.memory import ConversationBufferMemoryfrom uuid import uuid4chat_memory = XataChatMessageHistory(    session_id=str(uuid4()),   # needs to be unique per user session    api_key=api_key,    db_url=db_url,    table_name="memory")memory = ConversationBufferMemory(memory_key="chat_history", chat_memory=chat_memory, return_messages=True)API Reference:ConversationBufferMemoryNow it's time to create an Agent to use both the vector store and the chat memory together.from langchain.agents import initialize_agent, AgentTypefrom langchain.agents.agent_toolkits import create_retriever_toolfrom langchain.chat_models import ChatOpenAItool = create_retriever_tool(    vector_store.as_retriever(),     "search_docs",    "Searches and returns documents from the Xata manual. Useful when you need to answer questions about Xata.")tools = [tool]llm = ChatOpenAI(temperature=0)agent = initialize_agent(    tools,    llm,    agent=AgentType.CHAT_CONVERSATIONAL_REACT_DESCRIPTION,    verbose=True,    memory=memory)API Reference:initialize_agentAgentTypecreate_retriever_toolChatOpenAITo test, let's tell the agent our name:agent.run(input="My name is bob")Now, let's now ask the agent some questions about Xata:agent.run(input="What is xata?")Notice that it answers based on the data stored in the document store. And now, let's ask a follow up question:agent.run(input="Does it support similarity search?")And now let's test its memory:agent.run(input="Did I tell you my name? What is it?")PreviousStreamlit Chat Message HistoryNextZep MemorySetupCreate a databaseCreate a simple memory storeConversational Q&A chain on your data with memory IntegrationsMemoryXata chat memoryOn this pageXata chat memoryXata is a serverless data platform, based on PostgreSQL and Elasticsearch. It provides a Python SDK for interacting with your database, and a UI for managing your data. With the XataChatMessageHistory class, you can use Xata databases for longer-term persistence of chat sessions.This notebook covers:A simple example showing what XataChatMessageHistory does.A more complex example using a REACT agent that answer questions based on a knowledge based or documentation (stored in Xata as a vector store) and also having a long-term searchable history of its past messages (stored in Xata as a memory store)Setup‚ÄãCreate a database‚ÄãIn the Xata UI create a new database. You can name it whatever you want, in this notepad we'll use langchain. The Langchain integration can auto-create the table used for storying the memory, and this is what we'll use in this example. If you want to pre-create the table, ensure it has the right schema and set create_table to False when creating the class. Pre-creating the table saves one round-trip to the database during each session initialization.Let's first install our dependencies:pip install xata openai langchainNext, we need to get the environment variables for Xata. You can create a new API key by visiting your account settings. To find the database URL, go to the Settings page of the database that you have created. The database URL should look something like this: https://demo-uni3q8.eu-west-1.xata.sh/db/langchain.import getpassapi_key = getpass.getpass("Xata API key: ")db_url = input("Xata database URL (copy it from your DB settings):")Create a simple memory store‚ÄãTo test the memory store functionality in isolation, let's use the following code snippet:from langchain.memory import XataChatMessageHistoryhistory = XataChatMessageHistory(    session_id="session-1",    api_key=api_key,    db_url=db_url,    table_name="memory")history.add_user_message("hi!")history.add_ai_message("whats up?")API Reference:XataChatMessageHistoryThe above code creates a session with the ID session-1 and stores two messages in it. After running the above, if you visit the Xata UI, you should see a table named memory and the two messages added to it.You can retrieve the message history for a particular session with the following code:history.messagesConversational Q&A chain on your data with memory‚ÄãLet's now see a more complex example in which we combine OpenAI, the Xata Vector Store integration, and the Xata memory store integration to create a Q&A chat bot on your data, with follow-up questions and history.We're going to need to access the OpenAI API, so let's configure the API key:import osos.environ["OPENAI_API_KEY"] = getpass.getpass("OpenAI API Key:")To store the documents that the chatbot will search for answers, add a table named docs to your langchain database using the Xata UI, and add the following columns:content of type "Text". This is used to store the Document.pageContent values.embedding of type "Vector". Use the dimension used by the model you plan to use. In this notebook we use OpenAI embeddings, which have 1536 dimensions.Let's create the vector store and add some sample docs to it:from langchain.embeddings.openai import OpenAIEmbeddingsfrom langchain.vectorstores.xata import XataVectorStoreembeddings = OpenAIEmbeddings()texts = [    "Xata is a Serverless Data platform based on PostgreSQL",    "Xata offers a built-in vector type that can be used to store and query vectors",    "Xata includes similarity search"]vector_store = XataVectorStore.from_texts(texts, embeddings, api_key=api_key, db_url=db_url, table_name="docs")API Reference:OpenAIEmbeddingsXataVectorStoreAfter running the above command, if you go to the Xata UI, you should see the documents loaded together with their embeddings in the docs table.Let's now create a ConversationBufferMemory to store the chat messages from both the user and the AI.from langchain.memory import ConversationBufferMemoryfrom uuid import uuid4chat_memory = XataChatMessageHistory(    session_id=str(uuid4()),   # needs to be unique per user session    api_key=api_key,    db_url=db_url,    table_name="memory")memory = ConversationBufferMemory(memory_key="chat_history", chat_memory=chat_memory, return_messages=True)API Reference:ConversationBufferMemoryNow it's time to create an Agent to use both the vector store and the chat memory together.from langchain.agents import initialize_agent, AgentTypefrom langchain.agents.agent_toolkits import create_retriever_toolfrom langchain.chat_models import ChatOpenAItool = create_retriever_tool(    vector_store.as_retriever(),     "search_docs",    "Searches and returns documents from the Xata manual. Useful when you need to answer questions about Xata.")tools = [tool]llm = ChatOpenAI(temperature=0)agent = initialize_agent(    tools,    llm,    agent=AgentType.CHAT_CONVERSATIONAL_REACT_DESCRIPTION,    verbose=True,    memory=memory)API Reference:initialize_agentAgentTypecreate_retriever_toolChatOpenAITo test, let's tell the agent our name:agent.run(input="My name is bob")Now, let's now ask the agent some questions about Xata:agent.run(input="What is xata?")Notice that it answers based on the data stored in the document store. And now, let's ask a follow up question:agent.run(input="Does it support similarity search?")And now let's test its memory:agent.run(input="Did I tell you my name? What is it?")PreviousStreamlit Chat Message HistoryNextZep Memory IntegrationsMemoryXata chat memoryOn this pageXata chat memoryXata is a serverless data platform, based on PostgreSQL and Elasticsearch. It provides a Python SDK for interacting with your database, and a UI for managing your data. With the XataChatMessageHistory class, you can use Xata databases for longer-term persistence of chat sessions.This notebook covers:A simple example showing what XataChatMessageHistory does.A more complex example using a REACT agent that answer questions based on a knowledge based or documentation (stored in Xata as a vector store) and also having a long-term searchable history of its past messages (stored in Xata as a memory store)Setup‚ÄãCreate a database‚ÄãIn the Xata UI create a new database. You can name it whatever you want, in this notepad we'll use langchain. The Langchain integration can auto-create the table used for storying the memory, and this is what we'll use in this example. If you want to pre-create the table, ensure it has the right schema and set create_table to False when creating the class. Pre-creating the table saves one round-trip to the database during each session initialization.Let's first install our dependencies:pip install xata openai langchainNext, we need to get the environment variables for Xata. You can create a new API key by visiting your account settings. To find the database URL, go to the Settings page of the database that you have created. The database URL should look something like this: https://demo-uni3q8.eu-west-1.xata.sh/db/langchain.import getpassapi_key = getpass.getpass("Xata API key: ")db_url = input("Xata database URL (copy it from your DB settings):")Create a simple memory store‚ÄãTo test the memory store functionality in isolation, let's use the following code snippet:from langchain.memory import XataChatMessageHistoryhistory = XataChatMessageHistory(    session_id="session-1",    api_key=api_key,    db_url=db_url,    table_name="memory")history.add_user_message("hi!")history.add_ai_message("whats up?")API Reference:XataChatMessageHistoryThe above code creates a session with the ID session-1 and stores two messages in it. After running the above, if you visit the Xata UI, you should see a table named memory and the two messages added to it.You can retrieve the message history for a particular session with the following code:history.messagesConversational Q&A chain on your data with memory‚ÄãLet's now see a more complex example in which we combine OpenAI, the Xata Vector Store integration, and the Xata memory store integration to create a Q&A chat bot on your data, with follow-up questions and history.We're going to need to access the OpenAI API, so let's configure the API key:import osos.environ["OPENAI_API_KEY"] = getpass.getpass("OpenAI API Key:")To store the documents that the chatbot will search for answers, add a table named docs to your langchain database using the Xata UI, and add the following columns:content of type "Text". This is used to store the Document.pageContent values.embedding of type "Vector". Use the dimension used by the model you plan to use. In this notebook we use OpenAI embeddings, which have 1536 dimensions.Let's create the vector store and add some sample docs to it:from langchain.embeddings.openai import OpenAIEmbeddingsfrom langchain.vectorstores.xata import XataVectorStoreembeddings = OpenAIEmbeddings()texts = [    "Xata is a Serverless Data platform based on PostgreSQL",    "Xata offers a built-in vector type that can be used to store and query vectors",    "Xata includes similarity search"]vector_store = XataVectorStore.from_texts(texts, embeddings, api_key=api_key, db_url=db_url, table_name="docs")API Reference:OpenAIEmbeddingsXataVectorStoreAfter running the above command, if you go to the Xata UI, you should see the documents loaded together with their embeddings in the docs table.Let's now create a ConversationBufferMemory to store the chat messages from both the user and the AI.from langchain.memory import ConversationBufferMemoryfrom uuid import uuid4chat_memory = XataChatMessageHistory(    session_id=str(uuid4()),   # needs to be unique per user session    api_key=api_key,    db_url=db_url,    table_name="memory")memory = ConversationBufferMemory(memory_key="chat_history", chat_memory=chat_memory, return_messages=True)API Reference:ConversationBufferMemoryNow it's time to create an Agent to use both the vector store and the chat memory together.from langchain.agents import initialize_agent, AgentTypefrom langchain.agents.agent_toolkits import create_retriever_toolfrom langchain.chat_models import ChatOpenAItool = create_retriever_tool(    vector_store.as_retriever(),     "search_docs",    "Searches and returns documents from the Xata manual. Useful when you need to answer questions about Xata.")tools = [tool]llm = ChatOpenAI(temperature=0)agent = initialize_agent(    tools,    llm,    agent=AgentType.CHAT_CONVERSATIONAL_REACT_DESCRIPTION,    verbose=True,    memory=memory)API Reference:initialize_agentAgentTypecreate_retriever_toolChatOpenAITo test, let's tell the agent our name:agent.run(input="My name is bob")Now, let's now ask the agent some questions about Xata:agent.run(input="What is xata?")Notice that it answers based on the data stored in the document store. And now, let's ask a follow up question:agent.run(input="Does it support similarity search?")And now let's test its memory:agent.run(input="Did I tell you my name? What is it?")PreviousStreamlit Chat Message HistoryNextZep Memory On this page Xata chat memoryXata is a serverless data platform, based on PostgreSQL and Elasticsearch. It provides a Python SDK for interacting with your database, and a UI for managing your data. With the XataChatMessageHistory class, you can use Xata databases for longer-term persistence of chat sessions.This notebook covers:A simple example showing what XataChatMessageHistory does.A more complex example using a REACT agent that answer questions based on a knowledge based or documentation (stored in Xata as a vector store) and also having a long-term searchable history of its past messages (stored in Xata as a memory store)Setup‚ÄãCreate a database‚ÄãIn the Xata UI create a new database. You can name it whatever you want, in this notepad we'll use langchain. The Langchain integration can auto-create the table used for storying the memory, and this is what we'll use in this example. If you want to pre-create the table, ensure it has the right schema and set create_table to False when creating the class. Pre-creating the table saves one round-trip to the database during each session initialization.Let's first install our dependencies:pip install xata openai langchainNext, we need to get the environment variables for Xata. You can create a new API key by visiting your account settings. To find the database URL, go to the Settings page of the database that you have created. The database URL should look something like this: https://demo-uni3q8.eu-west-1.xata.sh/db/langchain.import getpassapi_key = getpass.getpass("Xata API key: ")db_url = input("Xata database URL (copy it from your DB settings):")Create a simple memory store‚ÄãTo test the memory store functionality in isolation, let's use the following code snippet:from langchain.memory import XataChatMessageHistoryhistory = XataChatMessageHistory(    session_id="session-1",    api_key=api_key,    db_url=db_url,    table_name="memory")history.add_user_message("hi!")history.add_ai_message("whats up?")API Reference:XataChatMessageHistoryThe above code creates a session with the ID session-1 and stores two messages in it. After running the above, if you visit the Xata UI, you should see a table named memory and the two messages added to it.You can retrieve the message history for a particular session with the following code:history.messagesConversational Q&A chain on your data with memory‚ÄãLet's now see a more complex example in which we combine OpenAI, the Xata Vector Store integration, and the Xata memory store integration to create a Q&A chat bot on your data, with follow-up questions and history.We're going to need to access the OpenAI API, so let's configure the API key:import osos.environ["OPENAI_API_KEY"] = getpass.getpass("OpenAI API Key:")To store the documents that the chatbot will search for answers, add a table named docs to your langchain database using the Xata UI, and add the following columns:content of type "Text". This is used to store the Document.pageContent values.embedding of type "Vector". Use the dimension used by the model you plan to use. In this notebook we use OpenAI embeddings, which have 1536 dimensions.Let's create the vector store and add some sample docs to it:from langchain.embeddings.openai import OpenAIEmbeddingsfrom langchain.vectorstores.xata import XataVectorStoreembeddings = OpenAIEmbeddings()texts = [    "Xata is a Serverless Data platform based on PostgreSQL",    "Xata offers a built-in vector type that can be used to store and query vectors",    "Xata includes similarity search"]vector_store = XataVectorStore.from_texts(texts, embeddings, api_key=api_key, db_url=db_url, table_name="docs")API Reference:OpenAIEmbeddingsXataVectorStoreAfter running the above command, if you go to the Xata UI, you should see the documents loaded together with their embeddings in the docs table.Let's now create a ConversationBufferMemory to store the chat messages from both the user and the AI.from langchain.memory import ConversationBufferMemoryfrom uuid import uuid4chat_memory = XataChatMessageHistory(    session_id=str(uuid4()),   # needs to be unique per user session    api_key=api_key,    db_url=db_url,    table_name="memory")memory = ConversationBufferMemory(memory_key="chat_history", chat_memory=chat_memory, return_messages=True)API Reference:ConversationBufferMemoryNow it's time to create an Agent to use both the vector store and the chat memory together.from langchain.agents import initialize_agent, AgentTypefrom langchain.agents.agent_toolkits import create_retriever_toolfrom langchain.chat_models import ChatOpenAItool = create_retriever_tool(    vector_store.as_retriever(),     "search_docs",    "Searches and returns documents from the Xata manual. Useful when you need to answer questions about Xata.")tools = [tool]llm = ChatOpenAI(temperature=0)agent = initialize_agent(    tools,    llm,    agent=AgentType.CHAT_CONVERSATIONAL_REACT_DESCRIPTION,    verbose=True,    memory=memory)API Reference:initialize_agentAgentTypecreate_retriever_toolChatOpenAITo test, let's tell the agent our name:agent.run(input="My name is bob")Now, let's now ask the agent some questions about Xata:agent.run(input="What is xata?")Notice that it answers based on the data stored in the document store. And now, let's ask a follow up question:agent.run(input="Does it support similarity search?")And now let's test its memory:agent.run(input="Did I tell you my name? What is it?") pip install xata openai langchain pip install xata openai langchain  import getpassapi_key = getpass.getpass("Xata API key: ")db_url = input("Xata database URL (copy it from your DB settings):") import getpassapi_key = getpass.getpass("Xata API key: ")db_url = input("Xata database URL (copy it from your DB settings):")  from langchain.memory import XataChatMessageHistoryhistory = XataChatMessageHistory(    session_id="session-1",    api_key=api_key,    db_url=db_url,    table_name="memory")history.add_user_message("hi!")history.add_ai_message("whats up?") from langchain.memory import XataChatMessageHistoryhistory = XataChatMessageHistory(    session_id="session-1",    api_key=api_key,    db_url=db_url,    table_name="memory")history.add_user_message("hi!")history.add_ai_message("whats up?")  API Reference:XataChatMessageHistory history.messages history.messages  import osos.environ["OPENAI_API_KEY"] = getpass.getpass("OpenAI API Key:") import osos.environ["OPENAI_API_KEY"] = getpass.getpass("OpenAI API Key:")  from langchain.embeddings.openai import OpenAIEmbeddingsfrom langchain.vectorstores.xata import XataVectorStoreembeddings = OpenAIEmbeddings()texts = [    "Xata is a Serverless Data platform based on PostgreSQL",    "Xata offers a built-in vector type that can be used to store and query vectors",    "Xata includes similarity search"]vector_store = XataVectorStore.from_texts(texts, embeddings, api_key=api_key, db_url=db_url, table_name="docs") from langchain.embeddings.openai import OpenAIEmbeddingsfrom langchain.vectorstores.xata import XataVectorStoreembeddings = OpenAIEmbeddings()texts = [    "Xata is a Serverless Data platform based on PostgreSQL",    "Xata offers a built-in vector type that can be used to store and query vectors",    "Xata includes similarity search"]vector_store = XataVectorStore.from_texts(texts, embeddings, api_key=api_key, db_url=db_url, table_name="docs")  API Reference:OpenAIEmbeddingsXataVectorStore from langchain.memory import ConversationBufferMemoryfrom uuid import uuid4chat_memory = XataChatMessageHistory(    session_id=str(uuid4()),   # needs to be unique per user session    api_key=api_key,    db_url=db_url,    table_name="memory")memory = ConversationBufferMemory(memory_key="chat_history", chat_memory=chat_memory, return_messages=True) from langchain.memory import ConversationBufferMemoryfrom uuid import uuid4chat_memory = XataChatMessageHistory(    session_id=str(uuid4()),   # needs to be unique per user session    api_key=api_key,    db_url=db_url,    table_name="memory")memory = ConversationBufferMemory(memory_key="chat_history", chat_memory=chat_memory, return_messages=True)  API Reference:ConversationBufferMemory from langchain.agents import initialize_agent, AgentTypefrom langchain.agents.agent_toolkits import create_retriever_toolfrom langchain.chat_models import ChatOpenAItool = create_retriever_tool(    vector_store.as_retriever(),     "search_docs",    "Searches and returns documents from the Xata manual. Useful when you need to answer questions about Xata.")tools = [tool]llm = ChatOpenAI(temperature=0)agent = initialize_agent(    tools,    llm,    agent=AgentType.CHAT_CONVERSATIONAL_REACT_DESCRIPTION,    verbose=True,    memory=memory) from langchain.agents import initialize_agent, AgentTypefrom langchain.agents.agent_toolkits import create_retriever_toolfrom langchain.chat_models import ChatOpenAItool = create_retriever_tool(    vector_store.as_retriever(),     "search_docs",    "Searches and returns documents from the Xata manual. Useful when you need to answer questions about Xata.")tools = [tool]llm = ChatOpenAI(temperature=0)agent = initialize_agent(    tools,    llm,    agent=AgentType.CHAT_CONVERSATIONAL_REACT_DESCRIPTION,    verbose=True,    memory=memory)  API Reference:initialize_agentAgentTypecreate_retriever_toolChatOpenAI agent.run(input="My name is bob") agent.run(input="My name is bob")  agent.run(input="What is xata?") agent.run(input="What is xata?")  agent.run(input="Does it support similarity search?") agent.run(input="Does it support similarity search?")  agent.run(input="Did I tell you my name? What is it?") agent.run(input="Did I tell you my name? What is it?")  Previous Streamlit Chat Message History Next Zep Memory SetupCreate a databaseCreate a simple memory storeConversational Q&A chain on your data with memory SetupCreate a databaseCreate a simple memory storeConversational Q&A chain on your data with memory CommunityDiscordTwitterGitHubPythonJS/TSMoreHomepageBlogCopyright ¬© 2023 LangChain, Inc. CommunityDiscordTwitterGitHubPythonJS/TSMoreHomepageBlog CommunityDiscordTwitter Community GitHubPythonJS/TS GitHub MoreHomepageBlog More Copyright ¬© 2023 LangChain, Inc. Copyright ¬© 2023 LangChain, Inc. Skip to main content (#docusaurus_skipToContent_fallback) ü¶úÔ∏èüîó LangChain (/) Docs (/docs/get_started/introduction) Use cases (/docs/use_cases) Integrations (/docs/integrations) API (https://api.python.langchain.com) LangSmith (https://smith.langchain.com) JS/TS Docs (https://js.langchain.com/docs)  (https://github.com/hwchase17/langchain) Integrations (/docs/integrations) Callbacks (/docs/integrations/callbacks/) Chat models (/docs/integrations/chat/) Chat loaders (/docs/integrations/chat_loaders/) Document loaders (/docs/integrations/document_loaders/) Document transformers (/docs/integrations/document_transformers/) LLMs (/docs/integrations/llms/) Memory (/docs/integrations/memory/) Cassandra Chat Message History (/docs/integrations/memory/cassandra_chat_message_history) Dynamodb Chat Message History (/docs/integrations/memory/dynamodb_chat_message_history) Entity Memory with SQLite storage (/docs/integrations/memory/entity_memory_with_sqlite) Momento Chat Message History (/docs/integrations/memory/momento_chat_message_history) Mongodb Chat Message History (/docs/integrations/memory/mongodb_chat_message_history) Mot√∂rhead Memory (/docs/integrations/memory/motorhead_memory) Mot√∂rhead Memory (Managed) (/docs/integrations/memory/motorhead_memory_managed) Postgres Chat Message History (/docs/integrations/memory/postgres_chat_message_history) Redis Chat Message History (/docs/integrations/memory/redis_chat_message_history) Rockset Chat Message History (/docs/integrations/memory/rockset_chat_message_history) SQL Chat Message History (/docs/integrations/memory/sql_chat_message_history) Streamlit Chat Message History (/docs/integrations/memory/streamlit_chat_message_history) Xata chat memory (/docs/integrations/memory/xata_chat_message_history) Zep Memory (/docs/integrations/memory/zep_memory) Retrievers (/docs/integrations/retrievers/) Text embedding models (/docs/integrations/text_embedding/) Agents & Toolkits (/docs/integrations/toolkits/) Tools (/docs/integrations/tools/) Vector stores (/docs/integrations/vectorstores/) Grouped by provider (/docs/integrations/providers/)  (/) Integrations (/docs/integrations) Memory (/docs/integrations/memory/) Xata (https://xata.io) ‚Äã (#setup) ‚Äã (#create-a-database) Xata UI (https://app.xata.io) account settings (https://app.xata.io/settings) ‚Äã (#create-a-simple-memory-store) XataChatMessageHistory (https://api.python.langchain.com/en/latest/memory/langchain.memory.chat_message_histories.xata.XataChatMessageHistory.html) ‚Äã (#conversational-qa-chain-on-your-data-with-memory) OpenAIEmbeddings (https://api.python.langchain.com/en/latest/embeddings/langchain.embeddings.openai.OpenAIEmbeddings.html) XataVectorStore (https://api.python.langchain.com/en/latest/vectorstores/langchain.vectorstores.xata.XataVectorStore.html) ConversationBufferMemory (https://api.python.langchain.com/en/latest/memory/langchain.memory.buffer.ConversationBufferMemory.html) initialize_agent (https://api.python.langchain.com/en/latest/agents/langchain.agents.initialize.initialize_agent.html) AgentType (https://api.python.langchain.com/en/latest/agents/langchain.agents.agent_types.AgentType.html) create_retriever_tool (https://api.python.langchain.com/en/latest/agents/langchain.agents.agent_toolkits.conversational_retrieval.tool.create_retriever_tool.html) ChatOpenAI (https://api.python.langchain.com/en/latest/chat_models/langchain.chat_models.openai.ChatOpenAI.html) PreviousStreamlit Chat Message History (/docs/integrations/memory/streamlit_chat_message_history) NextZep Memory (/docs/integrations/memory/zep_memory) Setup (#setup) Create a database (#create-a-database) Create a simple memory store (#create-a-simple-memory-store) Conversational Q&A chain on your data with memory (#conversational-qa-chain-on-your-data-with-memory) Discord (https://discord.gg/cU2adEyC7w) Twitter (https://twitter.com/LangChainAI) Python (https://github.com/hwchase17/langchain) JS/TS (https://github.com/hwchase17/langchainjs) Homepage (https://langchain.com) Blog (https://blog.langchain.dev)