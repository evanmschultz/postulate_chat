PipelineAI allows you to run your ML models at scale in the cloud. It also provides API access to several LLM models. This notebook goes over how to use Langchain with PipelineAI. This example shows how PipelineAI integrated with LangChain and it is created by PipelineAI. The pipeline-ai library is required to use the PipelineAI API, AKA Pipeline Cloud. Install pipeline-ai using pip install pipeline-ai. Make sure to get your API key from PipelineAI. Check out the cloud quickstart guide. You'll be given a 30 day free trial with 10 hours of serverless GPU compute to test different models. When instantiating PipelineAI, you need to specify the id or tag of the pipeline you want to use, e.g. pipeline_key = "public/gpt-j:base". You then have the option of passing additional pipeline-specific keyword arguments: We will create a prompt template for Question and Answer. Provide a question and run the LLMChain. IntegrationsCallbacksChat modelsChat loadersDocument loadersDocument transformersLLMsAI21Aleph AlphaAmazon API GatewayAnyscaleAzure MLAzure OpenAIBananaBasetenBeamBedrockBittensorCerebriumAIChatGLMClarifaiCohereC TransformersCTranslate2DatabricksDeepInfraDeepSparseEden AIFireworksForefrontAIGoogle Vertex AI PaLMGooseAIGPT4AllHugging Face HubHugging Face Local PipelinesHuggingface TextGen InferenceJSONFormerKoboldAI APILlama.cppLLM Caching integrationsManifestMinimaxModalMosaicMLNLP CloudOctoAIOllamaOpaquePromptsOpenAIOpenLLMOpenLMPetalsPipelineAIPredibasePrediction GuardPromptLayer OpenAIRELLMReplicateRunhouseSageMakerEndpointStochasticAINebula (Symbl.ai)TextGenTitan TakeoffTongyi QwenvLLMWriterXorbits Inference (Xinference)MemoryRetrieversText embedding modelsAgents & ToolkitsToolsVector storesGrouped by provider Callbacks Chat models Chat loaders Document loaders Document transformers LLMsAI21Aleph AlphaAmazon API GatewayAnyscaleAzure MLAzure OpenAIBananaBasetenBeamBedrockBittensorCerebriumAIChatGLMClarifaiCohereC TransformersCTranslate2DatabricksDeepInfraDeepSparseEden AIFireworksForefrontAIGoogle Vertex AI PaLMGooseAIGPT4AllHugging Face HubHugging Face Local PipelinesHuggingface TextGen InferenceJSONFormerKoboldAI APILlama.cppLLM Caching integrationsManifestMinimaxModalMosaicMLNLP CloudOctoAIOllamaOpaquePromptsOpenAIOpenLLMOpenLMPetalsPipelineAIPredibasePrediction GuardPromptLayer OpenAIRELLMReplicateRunhouseSageMakerEndpointStochasticAINebula (Symbl.ai)TextGenTitan TakeoffTongyi QwenvLLMWriterXorbits Inference (Xinference) AI21 Aleph Alpha Amazon API Gateway Anyscale Azure ML Azure OpenAI Banana Baseten Beam Bedrock Bittensor CerebriumAI ChatGLM Clarifai Cohere C Transformers CTranslate2 Databricks DeepInfra DeepSparse Eden AI Fireworks ForefrontAI Google Vertex AI PaLM GooseAI GPT4All Hugging Face Hub Hugging Face Local Pipelines Huggingface TextGen Inference JSONFormer KoboldAI API Llama.cpp LLM Caching integrations Manifest Minimax Modal MosaicML NLP Cloud OctoAI Ollama OpaquePrompts OpenAI OpenLLM OpenLM Petals PipelineAI Predibase Prediction Guard PromptLayer OpenAI RELLM Replicate Runhouse SageMakerEndpoint StochasticAI Nebula (Symbl.ai) TextGen Titan Takeoff Tongyi Qwen vLLM Writer Xorbits Inference (Xinference) Memory Retrievers Text embedding models Agents & Toolkits Tools Vector stores Grouped by provider  Integrations LLMs PipelineAI PipelineAI PipelineAI example Setup ExampleImportsSet the Environment API Key Imports Set the Environment API Key Create the PipelineAI instanceCreate a Prompt TemplateInitiate the LLMChainRun the LLMChain Create a Prompt Template Initiate the LLMChain Run the LLMChain Discord Twitter Python JS/TS Homepage Blog Skip to main contentü¶úÔ∏èüîó LangChainDocsUse casesIntegrationsAPILangSmithJS/TS DocsCTRLKIntegrationsCallbacksChat modelsChat loadersDocument loadersDocument transformersLLMsAI21Aleph AlphaAmazon API GatewayAnyscaleAzure MLAzure OpenAIBananaBasetenBeamBedrockBittensorCerebriumAIChatGLMClarifaiCohereC TransformersCTranslate2DatabricksDeepInfraDeepSparseEden AIFireworksForefrontAIGoogle Vertex AI PaLMGooseAIGPT4AllHugging Face HubHugging Face Local PipelinesHuggingface TextGen InferenceJSONFormerKoboldAI APILlama.cppLLM Caching integrationsManifestMinimaxModalMosaicMLNLP CloudOctoAIOllamaOpaquePromptsOpenAIOpenLLMOpenLMPetalsPipelineAIPredibasePrediction GuardPromptLayer OpenAIRELLMReplicateRunhouseSageMakerEndpointStochasticAINebula (Symbl.ai)TextGenTitan TakeoffTongyi QwenvLLMWriterXorbits Inference (Xinference)MemoryRetrieversText embedding modelsAgents & ToolkitsToolsVector storesGrouped by providerIntegrationsLLMsPipelineAIOn this pagePipelineAIPipelineAI allows you to run your ML models at scale in the cloud. It also provides API access to several LLM models.This notebook goes over how to use Langchain with PipelineAI.PipelineAI example‚ÄãThis example shows how PipelineAI integrated with LangChain and it is created by PipelineAI.Setup‚ÄãThe pipeline-ai library is required to use the PipelineAI API, AKA Pipeline Cloud. Install pipeline-ai using pip install pipeline-ai.# Install the packagepip install pipeline-aiExample‚ÄãImports‚Äãimport osfrom langchain.llms import PipelineAIfrom langchain import PromptTemplate, LLMChainAPI Reference:PipelineAISet the Environment API Key‚ÄãMake sure to get your API key from PipelineAI. Check out the cloud quickstart guide. You'll be given a 30 day free trial with 10 hours of serverless GPU compute to test different models.os.environ["PIPELINE_API_KEY"] = "YOUR_API_KEY_HERE"Create the PipelineAI instance‚ÄãWhen instantiating PipelineAI, you need to specify the id or tag of the pipeline you want to use, e.g. pipeline_key = "public/gpt-j:base". You then have the option of passing additional pipeline-specific keyword arguments:llm = PipelineAI(pipeline_key="YOUR_PIPELINE_KEY", pipeline_kwargs={...})Create a Prompt Template‚ÄãWe will create a prompt template for Question and Answer.template = """Question: {question}Answer: Let's think step by step."""prompt = PromptTemplate(template=template, input_variables=["question"])Initiate the LLMChain‚Äãllm_chain = LLMChain(prompt=prompt, llm=llm)Run the LLMChain‚ÄãProvide a question and run the LLMChain.question = "What NFL team won the Super Bowl in the year Justin Beiber was born?"llm_chain.run(question)PreviousPetalsNextPredibasePipelineAI exampleSetupExampleImportsSet the Environment API KeyCreate the PipelineAI instanceCreate a Prompt TemplateInitiate the LLMChainRun the LLMChainCommunityDiscordTwitterGitHubPythonJS/TSMoreHomepageBlogCopyright ¬© 2023 LangChain, Inc. Skip to main content ü¶úÔ∏èüîó LangChainDocsUse casesIntegrationsAPILangSmithJS/TS DocsCTRLK ü¶úÔ∏èüîó LangChainDocsUse casesIntegrationsAPI LangSmithJS/TS DocsCTRLK  CTRLK CTRLK  CTRLK   IntegrationsCallbacksChat modelsChat loadersDocument loadersDocument transformersLLMsAI21Aleph AlphaAmazon API GatewayAnyscaleAzure MLAzure OpenAIBananaBasetenBeamBedrockBittensorCerebriumAIChatGLMClarifaiCohereC TransformersCTranslate2DatabricksDeepInfraDeepSparseEden AIFireworksForefrontAIGoogle Vertex AI PaLMGooseAIGPT4AllHugging Face HubHugging Face Local PipelinesHuggingface TextGen InferenceJSONFormerKoboldAI APILlama.cppLLM Caching integrationsManifestMinimaxModalMosaicMLNLP CloudOctoAIOllamaOpaquePromptsOpenAIOpenLLMOpenLMPetalsPipelineAIPredibasePrediction GuardPromptLayer OpenAIRELLMReplicateRunhouseSageMakerEndpointStochasticAINebula (Symbl.ai)TextGenTitan TakeoffTongyi QwenvLLMWriterXorbits Inference (Xinference)MemoryRetrieversText embedding modelsAgents & ToolkitsToolsVector storesGrouped by providerIntegrationsLLMsPipelineAIOn this pagePipelineAIPipelineAI allows you to run your ML models at scale in the cloud. It also provides API access to several LLM models.This notebook goes over how to use Langchain with PipelineAI.PipelineAI example‚ÄãThis example shows how PipelineAI integrated with LangChain and it is created by PipelineAI.Setup‚ÄãThe pipeline-ai library is required to use the PipelineAI API, AKA Pipeline Cloud. Install pipeline-ai using pip install pipeline-ai.# Install the packagepip install pipeline-aiExample‚ÄãImports‚Äãimport osfrom langchain.llms import PipelineAIfrom langchain import PromptTemplate, LLMChainAPI Reference:PipelineAISet the Environment API Key‚ÄãMake sure to get your API key from PipelineAI. Check out the cloud quickstart guide. You'll be given a 30 day free trial with 10 hours of serverless GPU compute to test different models.os.environ["PIPELINE_API_KEY"] = "YOUR_API_KEY_HERE"Create the PipelineAI instance‚ÄãWhen instantiating PipelineAI, you need to specify the id or tag of the pipeline you want to use, e.g. pipeline_key = "public/gpt-j:base". You then have the option of passing additional pipeline-specific keyword arguments:llm = PipelineAI(pipeline_key="YOUR_PIPELINE_KEY", pipeline_kwargs={...})Create a Prompt Template‚ÄãWe will create a prompt template for Question and Answer.template = """Question: {question}Answer: Let's think step by step."""prompt = PromptTemplate(template=template, input_variables=["question"])Initiate the LLMChain‚Äãllm_chain = LLMChain(prompt=prompt, llm=llm)Run the LLMChain‚ÄãProvide a question and run the LLMChain.question = "What NFL team won the Super Bowl in the year Justin Beiber was born?"llm_chain.run(question)PreviousPetalsNextPredibasePipelineAI exampleSetupExampleImportsSet the Environment API KeyCreate the PipelineAI instanceCreate a Prompt TemplateInitiate the LLMChainRun the LLMChain IntegrationsCallbacksChat modelsChat loadersDocument loadersDocument transformersLLMsAI21Aleph AlphaAmazon API GatewayAnyscaleAzure MLAzure OpenAIBananaBasetenBeamBedrockBittensorCerebriumAIChatGLMClarifaiCohereC TransformersCTranslate2DatabricksDeepInfraDeepSparseEden AIFireworksForefrontAIGoogle Vertex AI PaLMGooseAIGPT4AllHugging Face HubHugging Face Local PipelinesHuggingface TextGen InferenceJSONFormerKoboldAI APILlama.cppLLM Caching integrationsManifestMinimaxModalMosaicMLNLP CloudOctoAIOllamaOpaquePromptsOpenAIOpenLLMOpenLMPetalsPipelineAIPredibasePrediction GuardPromptLayer OpenAIRELLMReplicateRunhouseSageMakerEndpointStochasticAINebula (Symbl.ai)TextGenTitan TakeoffTongyi QwenvLLMWriterXorbits Inference (Xinference)MemoryRetrieversText embedding modelsAgents & ToolkitsToolsVector storesGrouped by providerIntegrationsLLMsPipelineAIOn this pagePipelineAIPipelineAI allows you to run your ML models at scale in the cloud. It also provides API access to several LLM models.This notebook goes over how to use Langchain with PipelineAI.PipelineAI example‚ÄãThis example shows how PipelineAI integrated with LangChain and it is created by PipelineAI.Setup‚ÄãThe pipeline-ai library is required to use the PipelineAI API, AKA Pipeline Cloud. Install pipeline-ai using pip install pipeline-ai.# Install the packagepip install pipeline-aiExample‚ÄãImports‚Äãimport osfrom langchain.llms import PipelineAIfrom langchain import PromptTemplate, LLMChainAPI Reference:PipelineAISet the Environment API Key‚ÄãMake sure to get your API key from PipelineAI. Check out the cloud quickstart guide. You'll be given a 30 day free trial with 10 hours of serverless GPU compute to test different models.os.environ["PIPELINE_API_KEY"] = "YOUR_API_KEY_HERE"Create the PipelineAI instance‚ÄãWhen instantiating PipelineAI, you need to specify the id or tag of the pipeline you want to use, e.g. pipeline_key = "public/gpt-j:base". You then have the option of passing additional pipeline-specific keyword arguments:llm = PipelineAI(pipeline_key="YOUR_PIPELINE_KEY", pipeline_kwargs={...})Create a Prompt Template‚ÄãWe will create a prompt template for Question and Answer.template = """Question: {question}Answer: Let's think step by step."""prompt = PromptTemplate(template=template, input_variables=["question"])Initiate the LLMChain‚Äãllm_chain = LLMChain(prompt=prompt, llm=llm)Run the LLMChain‚ÄãProvide a question and run the LLMChain.question = "What NFL team won the Super Bowl in the year Justin Beiber was born?"llm_chain.run(question)PreviousPetalsNextPredibasePipelineAI exampleSetupExampleImportsSet the Environment API KeyCreate the PipelineAI instanceCreate a Prompt TemplateInitiate the LLMChainRun the LLMChain IntegrationsCallbacksChat modelsChat loadersDocument loadersDocument transformersLLMsAI21Aleph AlphaAmazon API GatewayAnyscaleAzure MLAzure OpenAIBananaBasetenBeamBedrockBittensorCerebriumAIChatGLMClarifaiCohereC TransformersCTranslate2DatabricksDeepInfraDeepSparseEden AIFireworksForefrontAIGoogle Vertex AI PaLMGooseAIGPT4AllHugging Face HubHugging Face Local PipelinesHuggingface TextGen InferenceJSONFormerKoboldAI APILlama.cppLLM Caching integrationsManifestMinimaxModalMosaicMLNLP CloudOctoAIOllamaOpaquePromptsOpenAIOpenLLMOpenLMPetalsPipelineAIPredibasePrediction GuardPromptLayer OpenAIRELLMReplicateRunhouseSageMakerEndpointStochasticAINebula (Symbl.ai)TextGenTitan TakeoffTongyi QwenvLLMWriterXorbits Inference (Xinference)MemoryRetrieversText embedding modelsAgents & ToolkitsToolsVector storesGrouped by provider IntegrationsCallbacksChat modelsChat loadersDocument loadersDocument transformersLLMsAI21Aleph AlphaAmazon API GatewayAnyscaleAzure MLAzure OpenAIBananaBasetenBeamBedrockBittensorCerebriumAIChatGLMClarifaiCohereC TransformersCTranslate2DatabricksDeepInfraDeepSparseEden AIFireworksForefrontAIGoogle Vertex AI PaLMGooseAIGPT4AllHugging Face HubHugging Face Local PipelinesHuggingface TextGen InferenceJSONFormerKoboldAI APILlama.cppLLM Caching integrationsManifestMinimaxModalMosaicMLNLP CloudOctoAIOllamaOpaquePromptsOpenAIOpenLLMOpenLMPetalsPipelineAIPredibasePrediction GuardPromptLayer OpenAIRELLMReplicateRunhouseSageMakerEndpointStochasticAINebula (Symbl.ai)TextGenTitan TakeoffTongyi QwenvLLMWriterXorbits Inference (Xinference)MemoryRetrieversText embedding modelsAgents & ToolkitsToolsVector storesGrouped by provider Integrations Callbacks Chat models Chat loaders Document loaders Document transformers LLMs Memory Retrievers Text embedding models Agents & Toolkits Tools Vector stores Grouped by provider IntegrationsLLMsPipelineAIOn this pagePipelineAIPipelineAI allows you to run your ML models at scale in the cloud. It also provides API access to several LLM models.This notebook goes over how to use Langchain with PipelineAI.PipelineAI example‚ÄãThis example shows how PipelineAI integrated with LangChain and it is created by PipelineAI.Setup‚ÄãThe pipeline-ai library is required to use the PipelineAI API, AKA Pipeline Cloud. Install pipeline-ai using pip install pipeline-ai.# Install the packagepip install pipeline-aiExample‚ÄãImports‚Äãimport osfrom langchain.llms import PipelineAIfrom langchain import PromptTemplate, LLMChainAPI Reference:PipelineAISet the Environment API Key‚ÄãMake sure to get your API key from PipelineAI. Check out the cloud quickstart guide. You'll be given a 30 day free trial with 10 hours of serverless GPU compute to test different models.os.environ["PIPELINE_API_KEY"] = "YOUR_API_KEY_HERE"Create the PipelineAI instance‚ÄãWhen instantiating PipelineAI, you need to specify the id or tag of the pipeline you want to use, e.g. pipeline_key = "public/gpt-j:base". You then have the option of passing additional pipeline-specific keyword arguments:llm = PipelineAI(pipeline_key="YOUR_PIPELINE_KEY", pipeline_kwargs={...})Create a Prompt Template‚ÄãWe will create a prompt template for Question and Answer.template = """Question: {question}Answer: Let's think step by step."""prompt = PromptTemplate(template=template, input_variables=["question"])Initiate the LLMChain‚Äãllm_chain = LLMChain(prompt=prompt, llm=llm)Run the LLMChain‚ÄãProvide a question and run the LLMChain.question = "What NFL team won the Super Bowl in the year Justin Beiber was born?"llm_chain.run(question)PreviousPetalsNextPredibasePipelineAI exampleSetupExampleImportsSet the Environment API KeyCreate the PipelineAI instanceCreate a Prompt TemplateInitiate the LLMChainRun the LLMChain IntegrationsLLMsPipelineAIOn this pagePipelineAIPipelineAI allows you to run your ML models at scale in the cloud. It also provides API access to several LLM models.This notebook goes over how to use Langchain with PipelineAI.PipelineAI example‚ÄãThis example shows how PipelineAI integrated with LangChain and it is created by PipelineAI.Setup‚ÄãThe pipeline-ai library is required to use the PipelineAI API, AKA Pipeline Cloud. Install pipeline-ai using pip install pipeline-ai.# Install the packagepip install pipeline-aiExample‚ÄãImports‚Äãimport osfrom langchain.llms import PipelineAIfrom langchain import PromptTemplate, LLMChainAPI Reference:PipelineAISet the Environment API Key‚ÄãMake sure to get your API key from PipelineAI. Check out the cloud quickstart guide. You'll be given a 30 day free trial with 10 hours of serverless GPU compute to test different models.os.environ["PIPELINE_API_KEY"] = "YOUR_API_KEY_HERE"Create the PipelineAI instance‚ÄãWhen instantiating PipelineAI, you need to specify the id or tag of the pipeline you want to use, e.g. pipeline_key = "public/gpt-j:base". You then have the option of passing additional pipeline-specific keyword arguments:llm = PipelineAI(pipeline_key="YOUR_PIPELINE_KEY", pipeline_kwargs={...})Create a Prompt Template‚ÄãWe will create a prompt template for Question and Answer.template = """Question: {question}Answer: Let's think step by step."""prompt = PromptTemplate(template=template, input_variables=["question"])Initiate the LLMChain‚Äãllm_chain = LLMChain(prompt=prompt, llm=llm)Run the LLMChain‚ÄãProvide a question and run the LLMChain.question = "What NFL team won the Super Bowl in the year Justin Beiber was born?"llm_chain.run(question)PreviousPetalsNextPredibasePipelineAI exampleSetupExampleImportsSet the Environment API KeyCreate the PipelineAI instanceCreate a Prompt TemplateInitiate the LLMChainRun the LLMChain IntegrationsLLMsPipelineAIOn this pagePipelineAIPipelineAI allows you to run your ML models at scale in the cloud. It also provides API access to several LLM models.This notebook goes over how to use Langchain with PipelineAI.PipelineAI example‚ÄãThis example shows how PipelineAI integrated with LangChain and it is created by PipelineAI.Setup‚ÄãThe pipeline-ai library is required to use the PipelineAI API, AKA Pipeline Cloud. Install pipeline-ai using pip install pipeline-ai.# Install the packagepip install pipeline-aiExample‚ÄãImports‚Äãimport osfrom langchain.llms import PipelineAIfrom langchain import PromptTemplate, LLMChainAPI Reference:PipelineAISet the Environment API Key‚ÄãMake sure to get your API key from PipelineAI. Check out the cloud quickstart guide. You'll be given a 30 day free trial with 10 hours of serverless GPU compute to test different models.os.environ["PIPELINE_API_KEY"] = "YOUR_API_KEY_HERE"Create the PipelineAI instance‚ÄãWhen instantiating PipelineAI, you need to specify the id or tag of the pipeline you want to use, e.g. pipeline_key = "public/gpt-j:base". You then have the option of passing additional pipeline-specific keyword arguments:llm = PipelineAI(pipeline_key="YOUR_PIPELINE_KEY", pipeline_kwargs={...})Create a Prompt Template‚ÄãWe will create a prompt template for Question and Answer.template = """Question: {question}Answer: Let's think step by step."""prompt = PromptTemplate(template=template, input_variables=["question"])Initiate the LLMChain‚Äãllm_chain = LLMChain(prompt=prompt, llm=llm)Run the LLMChain‚ÄãProvide a question and run the LLMChain.question = "What NFL team won the Super Bowl in the year Justin Beiber was born?"llm_chain.run(question)PreviousPetalsNextPredibase IntegrationsLLMsPipelineAIOn this pagePipelineAIPipelineAI allows you to run your ML models at scale in the cloud. It also provides API access to several LLM models.This notebook goes over how to use Langchain with PipelineAI.PipelineAI example‚ÄãThis example shows how PipelineAI integrated with LangChain and it is created by PipelineAI.Setup‚ÄãThe pipeline-ai library is required to use the PipelineAI API, AKA Pipeline Cloud. Install pipeline-ai using pip install pipeline-ai.# Install the packagepip install pipeline-aiExample‚ÄãImports‚Äãimport osfrom langchain.llms import PipelineAIfrom langchain import PromptTemplate, LLMChainAPI Reference:PipelineAISet the Environment API Key‚ÄãMake sure to get your API key from PipelineAI. Check out the cloud quickstart guide. You'll be given a 30 day free trial with 10 hours of serverless GPU compute to test different models.os.environ["PIPELINE_API_KEY"] = "YOUR_API_KEY_HERE"Create the PipelineAI instance‚ÄãWhen instantiating PipelineAI, you need to specify the id or tag of the pipeline you want to use, e.g. pipeline_key = "public/gpt-j:base". You then have the option of passing additional pipeline-specific keyword arguments:llm = PipelineAI(pipeline_key="YOUR_PIPELINE_KEY", pipeline_kwargs={...})Create a Prompt Template‚ÄãWe will create a prompt template for Question and Answer.template = """Question: {question}Answer: Let's think step by step."""prompt = PromptTemplate(template=template, input_variables=["question"])Initiate the LLMChain‚Äãllm_chain = LLMChain(prompt=prompt, llm=llm)Run the LLMChain‚ÄãProvide a question and run the LLMChain.question = "What NFL team won the Super Bowl in the year Justin Beiber was born?"llm_chain.run(question)PreviousPetalsNextPredibase On this page PipelineAIPipelineAI allows you to run your ML models at scale in the cloud. It also provides API access to several LLM models.This notebook goes over how to use Langchain with PipelineAI.PipelineAI example‚ÄãThis example shows how PipelineAI integrated with LangChain and it is created by PipelineAI.Setup‚ÄãThe pipeline-ai library is required to use the PipelineAI API, AKA Pipeline Cloud. Install pipeline-ai using pip install pipeline-ai.# Install the packagepip install pipeline-aiExample‚ÄãImports‚Äãimport osfrom langchain.llms import PipelineAIfrom langchain import PromptTemplate, LLMChainAPI Reference:PipelineAISet the Environment API Key‚ÄãMake sure to get your API key from PipelineAI. Check out the cloud quickstart guide. You'll be given a 30 day free trial with 10 hours of serverless GPU compute to test different models.os.environ["PIPELINE_API_KEY"] = "YOUR_API_KEY_HERE"Create the PipelineAI instance‚ÄãWhen instantiating PipelineAI, you need to specify the id or tag of the pipeline you want to use, e.g. pipeline_key = "public/gpt-j:base". You then have the option of passing additional pipeline-specific keyword arguments:llm = PipelineAI(pipeline_key="YOUR_PIPELINE_KEY", pipeline_kwargs={...})Create a Prompt Template‚ÄãWe will create a prompt template for Question and Answer.template = """Question: {question}Answer: Let's think step by step."""prompt = PromptTemplate(template=template, input_variables=["question"])Initiate the LLMChain‚Äãllm_chain = LLMChain(prompt=prompt, llm=llm)Run the LLMChain‚ÄãProvide a question and run the LLMChain.question = "What NFL team won the Super Bowl in the year Justin Beiber was born?"llm_chain.run(question) # Install the packagepip install pipeline-ai # Install the packagepip install pipeline-ai  import osfrom langchain.llms import PipelineAIfrom langchain import PromptTemplate, LLMChain import osfrom langchain.llms import PipelineAIfrom langchain import PromptTemplate, LLMChain  API Reference:PipelineAI os.environ["PIPELINE_API_KEY"] = "YOUR_API_KEY_HERE" os.environ["PIPELINE_API_KEY"] = "YOUR_API_KEY_HERE"  llm = PipelineAI(pipeline_key="YOUR_PIPELINE_KEY", pipeline_kwargs={...}) llm = PipelineAI(pipeline_key="YOUR_PIPELINE_KEY", pipeline_kwargs={...})  template = """Question: {question}Answer: Let's think step by step."""prompt = PromptTemplate(template=template, input_variables=["question"]) template = """Question: {question}Answer: Let's think step by step."""prompt = PromptTemplate(template=template, input_variables=["question"])  llm_chain = LLMChain(prompt=prompt, llm=llm) llm_chain = LLMChain(prompt=prompt, llm=llm)  question = "What NFL team won the Super Bowl in the year Justin Beiber was born?"llm_chain.run(question) question = "What NFL team won the Super Bowl in the year Justin Beiber was born?"llm_chain.run(question)  Previous Petals Next Predibase PipelineAI exampleSetupExampleImportsSet the Environment API KeyCreate the PipelineAI instanceCreate a Prompt TemplateInitiate the LLMChainRun the LLMChain PipelineAI exampleSetupExampleImportsSet the Environment API KeyCreate the PipelineAI instanceCreate a Prompt TemplateInitiate the LLMChainRun the LLMChain CommunityDiscordTwitterGitHubPythonJS/TSMoreHomepageBlogCopyright ¬© 2023 LangChain, Inc. CommunityDiscordTwitterGitHubPythonJS/TSMoreHomepageBlog CommunityDiscordTwitter Community GitHubPythonJS/TS GitHub MoreHomepageBlog More Copyright ¬© 2023 LangChain, Inc. Copyright ¬© 2023 LangChain, Inc. Skip to main content (#docusaurus_skipToContent_fallback) ü¶úÔ∏èüîó LangChain (/) Docs (/docs/get_started/introduction) Use cases (/docs/use_cases) Integrations (/docs/integrations) API (https://api.python.langchain.com) LangSmith (https://smith.langchain.com) JS/TS Docs (https://js.langchain.com/docs)  (https://github.com/hwchase17/langchain) Integrations (/docs/integrations) Callbacks (/docs/integrations/callbacks/) Chat models (/docs/integrations/chat/) Chat loaders (/docs/integrations/chat_loaders/) Document loaders (/docs/integrations/document_loaders/) Document transformers (/docs/integrations/document_transformers/) LLMs (/docs/integrations/llms/) AI21 (/docs/integrations/llms/ai21) Aleph Alpha (/docs/integrations/llms/aleph_alpha) Amazon API Gateway (/docs/integrations/llms/amazon_api_gateway) Anyscale (/docs/integrations/llms/anyscale) Azure ML (/docs/integrations/llms/azure_ml) Azure OpenAI (/docs/integrations/llms/azure_openai) Banana (/docs/integrations/llms/banana) Baseten (/docs/integrations/llms/baseten) Beam (/docs/integrations/llms/beam) Bedrock (/docs/integrations/llms/bedrock) Bittensor (/docs/integrations/llms/bittensor) CerebriumAI (/docs/integrations/llms/cerebriumai) ChatGLM (/docs/integrations/llms/chatglm) Clarifai (/docs/integrations/llms/clarifai) Cohere (/docs/integrations/llms/cohere) C Transformers (/docs/integrations/llms/ctransformers) CTranslate2 (/docs/integrations/llms/ctranslate2) Databricks (/docs/integrations/llms/databricks) DeepInfra (/docs/integrations/llms/deepinfra) DeepSparse (/docs/integrations/llms/deepsparse) Eden AI (/docs/integrations/llms/edenai) Fireworks (/docs/integrations/llms/fireworks) ForefrontAI (/docs/integrations/llms/forefrontai) Google Vertex AI PaLM (/docs/integrations/llms/google_vertex_ai_palm) GooseAI (/docs/integrations/llms/gooseai) GPT4All (/docs/integrations/llms/gpt4all) Hugging Face Hub (/docs/integrations/llms/huggingface_hub) Hugging Face Local Pipelines (/docs/integrations/llms/huggingface_pipelines) Huggingface TextGen Inference (/docs/integrations/llms/huggingface_textgen_inference) JSONFormer (/docs/integrations/llms/jsonformer_experimental) KoboldAI API (/docs/integrations/llms/koboldai) Llama.cpp (/docs/integrations/llms/llamacpp) LLM Caching integrations (/docs/integrations/llms/llm_caching) Manifest (/docs/integrations/llms/manifest) Minimax (/docs/integrations/llms/minimax) Modal (/docs/integrations/llms/modal) MosaicML (/docs/integrations/llms/mosaicml) NLP Cloud (/docs/integrations/llms/nlpcloud) OctoAI (/docs/integrations/llms/octoai) Ollama (/docs/integrations/llms/ollama) OpaquePrompts (/docs/integrations/llms/opaqueprompts) OpenAI (/docs/integrations/llms/openai) OpenLLM (/docs/integrations/llms/openllm) OpenLM (/docs/integrations/llms/openlm) Petals (/docs/integrations/llms/petals) PipelineAI (/docs/integrations/llms/pipelineai) Predibase (/docs/integrations/llms/predibase) Prediction Guard (/docs/integrations/llms/predictionguard) PromptLayer OpenAI (/docs/integrations/llms/promptlayer_openai) RELLM (/docs/integrations/llms/rellm_experimental) Replicate (/docs/integrations/llms/replicate) Runhouse (/docs/integrations/llms/runhouse) SageMakerEndpoint (/docs/integrations/llms/sagemaker) StochasticAI (/docs/integrations/llms/stochasticai) Nebula (Symbl.ai) (/docs/integrations/llms/symblai_nebula) TextGen (/docs/integrations/llms/textgen) Titan Takeoff (/docs/integrations/llms/titan_takeoff) Tongyi Qwen (/docs/integrations/llms/tongyi) vLLM (/docs/integrations/llms/vllm) Writer (/docs/integrations/llms/writer) Xorbits Inference (Xinference) (/docs/integrations/llms/xinference) Memory (/docs/integrations/memory/) Retrievers (/docs/integrations/retrievers/) Text embedding models (/docs/integrations/text_embedding/) Agents & Toolkits (/docs/integrations/toolkits/) Tools (/docs/integrations/tools/) Vector stores (/docs/integrations/vectorstores/) Grouped by provider (/docs/integrations/providers/)  (/) Integrations (/docs/integrations) LLMs (/docs/integrations/llms/) PipelineAI (https://pipeline.ai) several LLM models (https://pipeline.ai) PipelineAI (https://docs.pipeline.ai/docs) ‚Äã (#pipelineai-example) This example shows how PipelineAI integrated with LangChain (https://docs.pipeline.ai/docs/langchain) ‚Äã (#setup) ‚Äã (#example) ‚Äã (#imports) PipelineAI (https://api.python.langchain.com/en/latest/llms/langchain.llms.pipelineai.PipelineAI.html) ‚Äã (#set-the-environment-api-key) cloud quickstart guide (https://docs.pipeline.ai/docs/cloud-quickstart) ‚Äã (#create-the-pipelineai-instance) ‚Äã (#create-a-prompt-template) ‚Äã (#initiate-the-llmchain) ‚Äã (#run-the-llmchain) PreviousPetals (/docs/integrations/llms/petals) NextPredibase (/docs/integrations/llms/predibase) PipelineAI example (#pipelineai-example) Setup (#setup) Example (#example) Imports (#imports) Set the Environment API Key (#set-the-environment-api-key) Create the PipelineAI instance (#create-the-pipelineai-instance) Create a Prompt Template (#create-a-prompt-template) Initiate the LLMChain (#initiate-the-llmchain) Run the LLMChain (#run-the-llmchain) Discord (https://discord.gg/cU2adEyC7w) Twitter (https://twitter.com/LangChainAI) Python (https://github.com/hwchase17/langchain) JS/TS (https://github.com/hwchase17/langchainjs) Homepage (https://langchain.com) Blog (https://blog.langchain.dev)