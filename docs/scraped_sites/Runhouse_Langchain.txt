The Runhouse allows remote compute and data across environments and users. See the Runhouse docs. This example goes over how to use LangChain and Runhouse to interact with models hosted on your own GPU, or on-demand GPUs on AWS, GCP, AWS, or Lambda. Note: Code uses SelfHosted name instead of the Runhouse. You can also load more custom models through the SelfHostedHuggingFaceLLM interface: Using a custom load function, we can load a custom pipeline directly on the remote hardware: You can send your pipeline directly over the wire to your model, but this will only work for small models (<2 Gb), and will be pretty slow: Instead, we can also send it to the hardware's filesystem, which will be much faster. IntegrationsCallbacksChat modelsChat loadersDocument loadersDocument transformersLLMsAI21Aleph AlphaAmazon API GatewayAnyscaleAzure MLAzure OpenAIBananaBasetenBeamBedrockBittensorCerebriumAIChatGLMClarifaiCohereC TransformersCTranslate2DatabricksDeepInfraDeepSparseEden AIFireworksForefrontAIGoogle Vertex AI PaLMGooseAIGPT4AllHugging Face HubHugging Face Local PipelinesHuggingface TextGen InferenceJSONFormerKoboldAI APILlama.cppLLM Caching integrationsManifestMinimaxModalMosaicMLNLP CloudOctoAIOllamaOpaquePromptsOpenAIOpenLLMOpenLMPetalsPipelineAIPredibasePrediction GuardPromptLayer OpenAIRELLMReplicateRunhouseSageMakerEndpointStochasticAINebula (Symbl.ai)TextGenTitan TakeoffTongyi QwenvLLMWriterXorbits Inference (Xinference)MemoryRetrieversText embedding modelsAgents & ToolkitsToolsVector storesGrouped by provider Callbacks Chat models Chat loaders Document loaders Document transformers LLMsAI21Aleph AlphaAmazon API GatewayAnyscaleAzure MLAzure OpenAIBananaBasetenBeamBedrockBittensorCerebriumAIChatGLMClarifaiCohereC TransformersCTranslate2DatabricksDeepInfraDeepSparseEden AIFireworksForefrontAIGoogle Vertex AI PaLMGooseAIGPT4AllHugging Face HubHugging Face Local PipelinesHuggingface TextGen InferenceJSONFormerKoboldAI APILlama.cppLLM Caching integrationsManifestMinimaxModalMosaicMLNLP CloudOctoAIOllamaOpaquePromptsOpenAIOpenLLMOpenLMPetalsPipelineAIPredibasePrediction GuardPromptLayer OpenAIRELLMReplicateRunhouseSageMakerEndpointStochasticAINebula (Symbl.ai)TextGenTitan TakeoffTongyi QwenvLLMWriterXorbits Inference (Xinference) AI21 Aleph Alpha Amazon API Gateway Anyscale Azure ML Azure OpenAI Banana Baseten Beam Bedrock Bittensor CerebriumAI ChatGLM Clarifai Cohere C Transformers CTranslate2 Databricks DeepInfra DeepSparse Eden AI Fireworks ForefrontAI Google Vertex AI PaLM GooseAI GPT4All Hugging Face Hub Hugging Face Local Pipelines Huggingface TextGen Inference JSONFormer KoboldAI API Llama.cpp LLM Caching integrations Manifest Minimax Modal MosaicML NLP Cloud OctoAI Ollama OpaquePrompts OpenAI OpenLLM OpenLM Petals PipelineAI Predibase Prediction Guard PromptLayer OpenAI RELLM Replicate Runhouse SageMakerEndpoint StochasticAI Nebula (Symbl.ai) TextGen Titan Takeoff Tongyi Qwen vLLM Writer Xorbits Inference (Xinference) Memory Retrievers Text embedding models Agents & Toolkits Tools Vector stores Grouped by provider  Integrations LLMs Runhouse SelfHostedPipeline SelfHostedHuggingFaceLLM Discord Twitter Python JS/TS Homepage Blog Skip to main content🦜️🔗 LangChainDocsUse casesIntegrationsAPILangSmithJS/TS DocsCTRLKIntegrationsCallbacksChat modelsChat loadersDocument loadersDocument transformersLLMsAI21Aleph AlphaAmazon API GatewayAnyscaleAzure MLAzure OpenAIBananaBasetenBeamBedrockBittensorCerebriumAIChatGLMClarifaiCohereC TransformersCTranslate2DatabricksDeepInfraDeepSparseEden AIFireworksForefrontAIGoogle Vertex AI PaLMGooseAIGPT4AllHugging Face HubHugging Face Local PipelinesHuggingface TextGen InferenceJSONFormerKoboldAI APILlama.cppLLM Caching integrationsManifestMinimaxModalMosaicMLNLP CloudOctoAIOllamaOpaquePromptsOpenAIOpenLLMOpenLMPetalsPipelineAIPredibasePrediction GuardPromptLayer OpenAIRELLMReplicateRunhouseSageMakerEndpointStochasticAINebula (Symbl.ai)TextGenTitan TakeoffTongyi QwenvLLMWriterXorbits Inference (Xinference)MemoryRetrieversText embedding modelsAgents & ToolkitsToolsVector storesGrouped by providerIntegrationsLLMsRunhouseRunhouseThe Runhouse allows remote compute and data across environments and users. See the Runhouse docs.This example goes over how to use LangChain and Runhouse to interact with models hosted on your own GPU, or on-demand GPUs on AWS, GCP, AWS, or Lambda.Note: Code uses SelfHosted name instead of the Runhouse.pip install runhousefrom langchain.llms import SelfHostedPipeline, SelfHostedHuggingFaceLLMfrom langchain import PromptTemplate, LLMChainimport runhouse as rhAPI Reference:SelfHostedPipelineSelfHostedHuggingFaceLLM    INFO | 2023-04-17 16:47:36,173 | No auth token provided, so not using RNS API to save and load configs# For an on-demand A100 with GCP, Azure, or Lambdagpu = rh.cluster(name="rh-a10x", instance_type="A100:1", use_spot=False)# For an on-demand A10G with AWS (no single A100s on AWS)# gpu = rh.cluster(name='rh-a10x', instance_type='g5.2xlarge', provider='aws')# For an existing cluster# gpu = rh.cluster(ips=['<ip of the cluster>'],#                  ssh_creds={'ssh_user': '...', 'ssh_private_key':'<path_to_key>'},#                  name='rh-a10x')template = """Question: {question}Answer: Let's think step by step."""prompt = PromptTemplate(template=template, input_variables=["question"])llm = SelfHostedHuggingFaceLLM(    model_id="gpt2", hardware=gpu, model_reqs=["pip:./", "transformers", "torch"])llm_chain = LLMChain(prompt=prompt, llm=llm)question = "What NFL team won the Super Bowl in the year Justin Beiber was born?"llm_chain.run(question)    INFO | 2023-02-17 05:42:23,537 | Running _generate_text via gRPC    INFO | 2023-02-17 05:42:24,016 | Time to send message: 0.48 seconds    "\n\nLet's say we're talking sports teams who won the Super Bowl in the year Justin Beiber"You can also load more custom models through the SelfHostedHuggingFaceLLM interface:llm = SelfHostedHuggingFaceLLM(    model_id="google/flan-t5-small",    task="text2text-generation",    hardware=gpu,)llm("What is the capital of Germany?")    INFO | 2023-02-17 05:54:21,681 | Running _generate_text via gRPC    INFO | 2023-02-17 05:54:21,937 | Time to send message: 0.25 seconds    'berlin'Using a custom load function, we can load a custom pipeline directly on the remote hardware:def load_pipeline():    from transformers import (        AutoModelForCausalLM,        AutoTokenizer,        pipeline,    )  # Need to be inside the fn in notebooks    model_id = "gpt2"    tokenizer = AutoTokenizer.from_pretrained(model_id)    model = AutoModelForCausalLM.from_pretrained(model_id)    pipe = pipeline(        "text-generation", model=model, tokenizer=tokenizer, max_new_tokens=10    )    return pipedef inference_fn(pipeline, prompt, stop=None):    return pipeline(prompt)[0]["generated_text"][len(prompt) :]llm = SelfHostedHuggingFaceLLM(    model_load_fn=load_pipeline, hardware=gpu, inference_fn=inference_fn)llm("Who is the current US president?")    INFO | 2023-02-17 05:42:59,219 | Running _generate_text via gRPC    INFO | 2023-02-17 05:42:59,522 | Time to send message: 0.3 seconds    'john w. bush'You can send your pipeline directly over the wire to your model, but this will only work for small models (<2 Gb), and will be pretty slow:pipeline = load_pipeline()llm = SelfHostedPipeline.from_pipeline(    pipeline=pipeline, hardware=gpu, model_reqs=model_reqs)Instead, we can also send it to the hardware's filesystem, which will be much faster.rh.blob(pickle.dumps(pipeline), path="models/pipeline.pkl").save().to(    gpu, path="models")llm = SelfHostedPipeline.from_pipeline(pipeline="models/pipeline.pkl", hardware=gpu)PreviousReplicateNextSageMakerEndpointCommunityDiscordTwitterGitHubPythonJS/TSMoreHomepageBlogCopyright © 2023 LangChain, Inc. Skip to main content 🦜️🔗 LangChainDocsUse casesIntegrationsAPILangSmithJS/TS DocsCTRLK 🦜️🔗 LangChainDocsUse casesIntegrationsAPI LangSmithJS/TS DocsCTRLK  CTRLK CTRLK  CTRLK   IntegrationsCallbacksChat modelsChat loadersDocument loadersDocument transformersLLMsAI21Aleph AlphaAmazon API GatewayAnyscaleAzure MLAzure OpenAIBananaBasetenBeamBedrockBittensorCerebriumAIChatGLMClarifaiCohereC TransformersCTranslate2DatabricksDeepInfraDeepSparseEden AIFireworksForefrontAIGoogle Vertex AI PaLMGooseAIGPT4AllHugging Face HubHugging Face Local PipelinesHuggingface TextGen InferenceJSONFormerKoboldAI APILlama.cppLLM Caching integrationsManifestMinimaxModalMosaicMLNLP CloudOctoAIOllamaOpaquePromptsOpenAIOpenLLMOpenLMPetalsPipelineAIPredibasePrediction GuardPromptLayer OpenAIRELLMReplicateRunhouseSageMakerEndpointStochasticAINebula (Symbl.ai)TextGenTitan TakeoffTongyi QwenvLLMWriterXorbits Inference (Xinference)MemoryRetrieversText embedding modelsAgents & ToolkitsToolsVector storesGrouped by providerIntegrationsLLMsRunhouseRunhouseThe Runhouse allows remote compute and data across environments and users. See the Runhouse docs.This example goes over how to use LangChain and Runhouse to interact with models hosted on your own GPU, or on-demand GPUs on AWS, GCP, AWS, or Lambda.Note: Code uses SelfHosted name instead of the Runhouse.pip install runhousefrom langchain.llms import SelfHostedPipeline, SelfHostedHuggingFaceLLMfrom langchain import PromptTemplate, LLMChainimport runhouse as rhAPI Reference:SelfHostedPipelineSelfHostedHuggingFaceLLM    INFO | 2023-04-17 16:47:36,173 | No auth token provided, so not using RNS API to save and load configs# For an on-demand A100 with GCP, Azure, or Lambdagpu = rh.cluster(name="rh-a10x", instance_type="A100:1", use_spot=False)# For an on-demand A10G with AWS (no single A100s on AWS)# gpu = rh.cluster(name='rh-a10x', instance_type='g5.2xlarge', provider='aws')# For an existing cluster# gpu = rh.cluster(ips=['<ip of the cluster>'],#                  ssh_creds={'ssh_user': '...', 'ssh_private_key':'<path_to_key>'},#                  name='rh-a10x')template = """Question: {question}Answer: Let's think step by step."""prompt = PromptTemplate(template=template, input_variables=["question"])llm = SelfHostedHuggingFaceLLM(    model_id="gpt2", hardware=gpu, model_reqs=["pip:./", "transformers", "torch"])llm_chain = LLMChain(prompt=prompt, llm=llm)question = "What NFL team won the Super Bowl in the year Justin Beiber was born?"llm_chain.run(question)    INFO | 2023-02-17 05:42:23,537 | Running _generate_text via gRPC    INFO | 2023-02-17 05:42:24,016 | Time to send message: 0.48 seconds    "\n\nLet's say we're talking sports teams who won the Super Bowl in the year Justin Beiber"You can also load more custom models through the SelfHostedHuggingFaceLLM interface:llm = SelfHostedHuggingFaceLLM(    model_id="google/flan-t5-small",    task="text2text-generation",    hardware=gpu,)llm("What is the capital of Germany?")    INFO | 2023-02-17 05:54:21,681 | Running _generate_text via gRPC    INFO | 2023-02-17 05:54:21,937 | Time to send message: 0.25 seconds    'berlin'Using a custom load function, we can load a custom pipeline directly on the remote hardware:def load_pipeline():    from transformers import (        AutoModelForCausalLM,        AutoTokenizer,        pipeline,    )  # Need to be inside the fn in notebooks    model_id = "gpt2"    tokenizer = AutoTokenizer.from_pretrained(model_id)    model = AutoModelForCausalLM.from_pretrained(model_id)    pipe = pipeline(        "text-generation", model=model, tokenizer=tokenizer, max_new_tokens=10    )    return pipedef inference_fn(pipeline, prompt, stop=None):    return pipeline(prompt)[0]["generated_text"][len(prompt) :]llm = SelfHostedHuggingFaceLLM(    model_load_fn=load_pipeline, hardware=gpu, inference_fn=inference_fn)llm("Who is the current US president?")    INFO | 2023-02-17 05:42:59,219 | Running _generate_text via gRPC    INFO | 2023-02-17 05:42:59,522 | Time to send message: 0.3 seconds    'john w. bush'You can send your pipeline directly over the wire to your model, but this will only work for small models (<2 Gb), and will be pretty slow:pipeline = load_pipeline()llm = SelfHostedPipeline.from_pipeline(    pipeline=pipeline, hardware=gpu, model_reqs=model_reqs)Instead, we can also send it to the hardware's filesystem, which will be much faster.rh.blob(pickle.dumps(pipeline), path="models/pipeline.pkl").save().to(    gpu, path="models")llm = SelfHostedPipeline.from_pipeline(pipeline="models/pipeline.pkl", hardware=gpu)PreviousReplicateNextSageMakerEndpoint IntegrationsCallbacksChat modelsChat loadersDocument loadersDocument transformersLLMsAI21Aleph AlphaAmazon API GatewayAnyscaleAzure MLAzure OpenAIBananaBasetenBeamBedrockBittensorCerebriumAIChatGLMClarifaiCohereC TransformersCTranslate2DatabricksDeepInfraDeepSparseEden AIFireworksForefrontAIGoogle Vertex AI PaLMGooseAIGPT4AllHugging Face HubHugging Face Local PipelinesHuggingface TextGen InferenceJSONFormerKoboldAI APILlama.cppLLM Caching integrationsManifestMinimaxModalMosaicMLNLP CloudOctoAIOllamaOpaquePromptsOpenAIOpenLLMOpenLMPetalsPipelineAIPredibasePrediction GuardPromptLayer OpenAIRELLMReplicateRunhouseSageMakerEndpointStochasticAINebula (Symbl.ai)TextGenTitan TakeoffTongyi QwenvLLMWriterXorbits Inference (Xinference)MemoryRetrieversText embedding modelsAgents & ToolkitsToolsVector storesGrouped by providerIntegrationsLLMsRunhouseRunhouseThe Runhouse allows remote compute and data across environments and users. See the Runhouse docs.This example goes over how to use LangChain and Runhouse to interact with models hosted on your own GPU, or on-demand GPUs on AWS, GCP, AWS, or Lambda.Note: Code uses SelfHosted name instead of the Runhouse.pip install runhousefrom langchain.llms import SelfHostedPipeline, SelfHostedHuggingFaceLLMfrom langchain import PromptTemplate, LLMChainimport runhouse as rhAPI Reference:SelfHostedPipelineSelfHostedHuggingFaceLLM    INFO | 2023-04-17 16:47:36,173 | No auth token provided, so not using RNS API to save and load configs# For an on-demand A100 with GCP, Azure, or Lambdagpu = rh.cluster(name="rh-a10x", instance_type="A100:1", use_spot=False)# For an on-demand A10G with AWS (no single A100s on AWS)# gpu = rh.cluster(name='rh-a10x', instance_type='g5.2xlarge', provider='aws')# For an existing cluster# gpu = rh.cluster(ips=['<ip of the cluster>'],#                  ssh_creds={'ssh_user': '...', 'ssh_private_key':'<path_to_key>'},#                  name='rh-a10x')template = """Question: {question}Answer: Let's think step by step."""prompt = PromptTemplate(template=template, input_variables=["question"])llm = SelfHostedHuggingFaceLLM(    model_id="gpt2", hardware=gpu, model_reqs=["pip:./", "transformers", "torch"])llm_chain = LLMChain(prompt=prompt, llm=llm)question = "What NFL team won the Super Bowl in the year Justin Beiber was born?"llm_chain.run(question)    INFO | 2023-02-17 05:42:23,537 | Running _generate_text via gRPC    INFO | 2023-02-17 05:42:24,016 | Time to send message: 0.48 seconds    "\n\nLet's say we're talking sports teams who won the Super Bowl in the year Justin Beiber"You can also load more custom models through the SelfHostedHuggingFaceLLM interface:llm = SelfHostedHuggingFaceLLM(    model_id="google/flan-t5-small",    task="text2text-generation",    hardware=gpu,)llm("What is the capital of Germany?")    INFO | 2023-02-17 05:54:21,681 | Running _generate_text via gRPC    INFO | 2023-02-17 05:54:21,937 | Time to send message: 0.25 seconds    'berlin'Using a custom load function, we can load a custom pipeline directly on the remote hardware:def load_pipeline():    from transformers import (        AutoModelForCausalLM,        AutoTokenizer,        pipeline,    )  # Need to be inside the fn in notebooks    model_id = "gpt2"    tokenizer = AutoTokenizer.from_pretrained(model_id)    model = AutoModelForCausalLM.from_pretrained(model_id)    pipe = pipeline(        "text-generation", model=model, tokenizer=tokenizer, max_new_tokens=10    )    return pipedef inference_fn(pipeline, prompt, stop=None):    return pipeline(prompt)[0]["generated_text"][len(prompt) :]llm = SelfHostedHuggingFaceLLM(    model_load_fn=load_pipeline, hardware=gpu, inference_fn=inference_fn)llm("Who is the current US president?")    INFO | 2023-02-17 05:42:59,219 | Running _generate_text via gRPC    INFO | 2023-02-17 05:42:59,522 | Time to send message: 0.3 seconds    'john w. bush'You can send your pipeline directly over the wire to your model, but this will only work for small models (<2 Gb), and will be pretty slow:pipeline = load_pipeline()llm = SelfHostedPipeline.from_pipeline(    pipeline=pipeline, hardware=gpu, model_reqs=model_reqs)Instead, we can also send it to the hardware's filesystem, which will be much faster.rh.blob(pickle.dumps(pipeline), path="models/pipeline.pkl").save().to(    gpu, path="models")llm = SelfHostedPipeline.from_pipeline(pipeline="models/pipeline.pkl", hardware=gpu)PreviousReplicateNextSageMakerEndpoint IntegrationsCallbacksChat modelsChat loadersDocument loadersDocument transformersLLMsAI21Aleph AlphaAmazon API GatewayAnyscaleAzure MLAzure OpenAIBananaBasetenBeamBedrockBittensorCerebriumAIChatGLMClarifaiCohereC TransformersCTranslate2DatabricksDeepInfraDeepSparseEden AIFireworksForefrontAIGoogle Vertex AI PaLMGooseAIGPT4AllHugging Face HubHugging Face Local PipelinesHuggingface TextGen InferenceJSONFormerKoboldAI APILlama.cppLLM Caching integrationsManifestMinimaxModalMosaicMLNLP CloudOctoAIOllamaOpaquePromptsOpenAIOpenLLMOpenLMPetalsPipelineAIPredibasePrediction GuardPromptLayer OpenAIRELLMReplicateRunhouseSageMakerEndpointStochasticAINebula (Symbl.ai)TextGenTitan TakeoffTongyi QwenvLLMWriterXorbits Inference (Xinference)MemoryRetrieversText embedding modelsAgents & ToolkitsToolsVector storesGrouped by provider IntegrationsCallbacksChat modelsChat loadersDocument loadersDocument transformersLLMsAI21Aleph AlphaAmazon API GatewayAnyscaleAzure MLAzure OpenAIBananaBasetenBeamBedrockBittensorCerebriumAIChatGLMClarifaiCohereC TransformersCTranslate2DatabricksDeepInfraDeepSparseEden AIFireworksForefrontAIGoogle Vertex AI PaLMGooseAIGPT4AllHugging Face HubHugging Face Local PipelinesHuggingface TextGen InferenceJSONFormerKoboldAI APILlama.cppLLM Caching integrationsManifestMinimaxModalMosaicMLNLP CloudOctoAIOllamaOpaquePromptsOpenAIOpenLLMOpenLMPetalsPipelineAIPredibasePrediction GuardPromptLayer OpenAIRELLMReplicateRunhouseSageMakerEndpointStochasticAINebula (Symbl.ai)TextGenTitan TakeoffTongyi QwenvLLMWriterXorbits Inference (Xinference)MemoryRetrieversText embedding modelsAgents & ToolkitsToolsVector storesGrouped by provider Integrations Callbacks Chat models Chat loaders Document loaders Document transformers LLMs Memory Retrievers Text embedding models Agents & Toolkits Tools Vector stores Grouped by provider IntegrationsLLMsRunhouseRunhouseThe Runhouse allows remote compute and data across environments and users. See the Runhouse docs.This example goes over how to use LangChain and Runhouse to interact with models hosted on your own GPU, or on-demand GPUs on AWS, GCP, AWS, or Lambda.Note: Code uses SelfHosted name instead of the Runhouse.pip install runhousefrom langchain.llms import SelfHostedPipeline, SelfHostedHuggingFaceLLMfrom langchain import PromptTemplate, LLMChainimport runhouse as rhAPI Reference:SelfHostedPipelineSelfHostedHuggingFaceLLM    INFO | 2023-04-17 16:47:36,173 | No auth token provided, so not using RNS API to save and load configs# For an on-demand A100 with GCP, Azure, or Lambdagpu = rh.cluster(name="rh-a10x", instance_type="A100:1", use_spot=False)# For an on-demand A10G with AWS (no single A100s on AWS)# gpu = rh.cluster(name='rh-a10x', instance_type='g5.2xlarge', provider='aws')# For an existing cluster# gpu = rh.cluster(ips=['<ip of the cluster>'],#                  ssh_creds={'ssh_user': '...', 'ssh_private_key':'<path_to_key>'},#                  name='rh-a10x')template = """Question: {question}Answer: Let's think step by step."""prompt = PromptTemplate(template=template, input_variables=["question"])llm = SelfHostedHuggingFaceLLM(    model_id="gpt2", hardware=gpu, model_reqs=["pip:./", "transformers", "torch"])llm_chain = LLMChain(prompt=prompt, llm=llm)question = "What NFL team won the Super Bowl in the year Justin Beiber was born?"llm_chain.run(question)    INFO | 2023-02-17 05:42:23,537 | Running _generate_text via gRPC    INFO | 2023-02-17 05:42:24,016 | Time to send message: 0.48 seconds    "\n\nLet's say we're talking sports teams who won the Super Bowl in the year Justin Beiber"You can also load more custom models through the SelfHostedHuggingFaceLLM interface:llm = SelfHostedHuggingFaceLLM(    model_id="google/flan-t5-small",    task="text2text-generation",    hardware=gpu,)llm("What is the capital of Germany?")    INFO | 2023-02-17 05:54:21,681 | Running _generate_text via gRPC    INFO | 2023-02-17 05:54:21,937 | Time to send message: 0.25 seconds    'berlin'Using a custom load function, we can load a custom pipeline directly on the remote hardware:def load_pipeline():    from transformers import (        AutoModelForCausalLM,        AutoTokenizer,        pipeline,    )  # Need to be inside the fn in notebooks    model_id = "gpt2"    tokenizer = AutoTokenizer.from_pretrained(model_id)    model = AutoModelForCausalLM.from_pretrained(model_id)    pipe = pipeline(        "text-generation", model=model, tokenizer=tokenizer, max_new_tokens=10    )    return pipedef inference_fn(pipeline, prompt, stop=None):    return pipeline(prompt)[0]["generated_text"][len(prompt) :]llm = SelfHostedHuggingFaceLLM(    model_load_fn=load_pipeline, hardware=gpu, inference_fn=inference_fn)llm("Who is the current US president?")    INFO | 2023-02-17 05:42:59,219 | Running _generate_text via gRPC    INFO | 2023-02-17 05:42:59,522 | Time to send message: 0.3 seconds    'john w. bush'You can send your pipeline directly over the wire to your model, but this will only work for small models (<2 Gb), and will be pretty slow:pipeline = load_pipeline()llm = SelfHostedPipeline.from_pipeline(    pipeline=pipeline, hardware=gpu, model_reqs=model_reqs)Instead, we can also send it to the hardware's filesystem, which will be much faster.rh.blob(pickle.dumps(pipeline), path="models/pipeline.pkl").save().to(    gpu, path="models")llm = SelfHostedPipeline.from_pipeline(pipeline="models/pipeline.pkl", hardware=gpu)PreviousReplicateNextSageMakerEndpoint IntegrationsLLMsRunhouseRunhouseThe Runhouse allows remote compute and data across environments and users. See the Runhouse docs.This example goes over how to use LangChain and Runhouse to interact with models hosted on your own GPU, or on-demand GPUs on AWS, GCP, AWS, or Lambda.Note: Code uses SelfHosted name instead of the Runhouse.pip install runhousefrom langchain.llms import SelfHostedPipeline, SelfHostedHuggingFaceLLMfrom langchain import PromptTemplate, LLMChainimport runhouse as rhAPI Reference:SelfHostedPipelineSelfHostedHuggingFaceLLM    INFO | 2023-04-17 16:47:36,173 | No auth token provided, so not using RNS API to save and load configs# For an on-demand A100 with GCP, Azure, or Lambdagpu = rh.cluster(name="rh-a10x", instance_type="A100:1", use_spot=False)# For an on-demand A10G with AWS (no single A100s on AWS)# gpu = rh.cluster(name='rh-a10x', instance_type='g5.2xlarge', provider='aws')# For an existing cluster# gpu = rh.cluster(ips=['<ip of the cluster>'],#                  ssh_creds={'ssh_user': '...', 'ssh_private_key':'<path_to_key>'},#                  name='rh-a10x')template = """Question: {question}Answer: Let's think step by step."""prompt = PromptTemplate(template=template, input_variables=["question"])llm = SelfHostedHuggingFaceLLM(    model_id="gpt2", hardware=gpu, model_reqs=["pip:./", "transformers", "torch"])llm_chain = LLMChain(prompt=prompt, llm=llm)question = "What NFL team won the Super Bowl in the year Justin Beiber was born?"llm_chain.run(question)    INFO | 2023-02-17 05:42:23,537 | Running _generate_text via gRPC    INFO | 2023-02-17 05:42:24,016 | Time to send message: 0.48 seconds    "\n\nLet's say we're talking sports teams who won the Super Bowl in the year Justin Beiber"You can also load more custom models through the SelfHostedHuggingFaceLLM interface:llm = SelfHostedHuggingFaceLLM(    model_id="google/flan-t5-small",    task="text2text-generation",    hardware=gpu,)llm("What is the capital of Germany?")    INFO | 2023-02-17 05:54:21,681 | Running _generate_text via gRPC    INFO | 2023-02-17 05:54:21,937 | Time to send message: 0.25 seconds    'berlin'Using a custom load function, we can load a custom pipeline directly on the remote hardware:def load_pipeline():    from transformers import (        AutoModelForCausalLM,        AutoTokenizer,        pipeline,    )  # Need to be inside the fn in notebooks    model_id = "gpt2"    tokenizer = AutoTokenizer.from_pretrained(model_id)    model = AutoModelForCausalLM.from_pretrained(model_id)    pipe = pipeline(        "text-generation", model=model, tokenizer=tokenizer, max_new_tokens=10    )    return pipedef inference_fn(pipeline, prompt, stop=None):    return pipeline(prompt)[0]["generated_text"][len(prompt) :]llm = SelfHostedHuggingFaceLLM(    model_load_fn=load_pipeline, hardware=gpu, inference_fn=inference_fn)llm("Who is the current US president?")    INFO | 2023-02-17 05:42:59,219 | Running _generate_text via gRPC    INFO | 2023-02-17 05:42:59,522 | Time to send message: 0.3 seconds    'john w. bush'You can send your pipeline directly over the wire to your model, but this will only work for small models (<2 Gb), and will be pretty slow:pipeline = load_pipeline()llm = SelfHostedPipeline.from_pipeline(    pipeline=pipeline, hardware=gpu, model_reqs=model_reqs)Instead, we can also send it to the hardware's filesystem, which will be much faster.rh.blob(pickle.dumps(pipeline), path="models/pipeline.pkl").save().to(    gpu, path="models")llm = SelfHostedPipeline.from_pipeline(pipeline="models/pipeline.pkl", hardware=gpu)PreviousReplicateNextSageMakerEndpoint IntegrationsLLMsRunhouseRunhouseThe Runhouse allows remote compute and data across environments and users. See the Runhouse docs.This example goes over how to use LangChain and Runhouse to interact with models hosted on your own GPU, or on-demand GPUs on AWS, GCP, AWS, or Lambda.Note: Code uses SelfHosted name instead of the Runhouse.pip install runhousefrom langchain.llms import SelfHostedPipeline, SelfHostedHuggingFaceLLMfrom langchain import PromptTemplate, LLMChainimport runhouse as rhAPI Reference:SelfHostedPipelineSelfHostedHuggingFaceLLM    INFO | 2023-04-17 16:47:36,173 | No auth token provided, so not using RNS API to save and load configs# For an on-demand A100 with GCP, Azure, or Lambdagpu = rh.cluster(name="rh-a10x", instance_type="A100:1", use_spot=False)# For an on-demand A10G with AWS (no single A100s on AWS)# gpu = rh.cluster(name='rh-a10x', instance_type='g5.2xlarge', provider='aws')# For an existing cluster# gpu = rh.cluster(ips=['<ip of the cluster>'],#                  ssh_creds={'ssh_user': '...', 'ssh_private_key':'<path_to_key>'},#                  name='rh-a10x')template = """Question: {question}Answer: Let's think step by step."""prompt = PromptTemplate(template=template, input_variables=["question"])llm = SelfHostedHuggingFaceLLM(    model_id="gpt2", hardware=gpu, model_reqs=["pip:./", "transformers", "torch"])llm_chain = LLMChain(prompt=prompt, llm=llm)question = "What NFL team won the Super Bowl in the year Justin Beiber was born?"llm_chain.run(question)    INFO | 2023-02-17 05:42:23,537 | Running _generate_text via gRPC    INFO | 2023-02-17 05:42:24,016 | Time to send message: 0.48 seconds    "\n\nLet's say we're talking sports teams who won the Super Bowl in the year Justin Beiber"You can also load more custom models through the SelfHostedHuggingFaceLLM interface:llm = SelfHostedHuggingFaceLLM(    model_id="google/flan-t5-small",    task="text2text-generation",    hardware=gpu,)llm("What is the capital of Germany?")    INFO | 2023-02-17 05:54:21,681 | Running _generate_text via gRPC    INFO | 2023-02-17 05:54:21,937 | Time to send message: 0.25 seconds    'berlin'Using a custom load function, we can load a custom pipeline directly on the remote hardware:def load_pipeline():    from transformers import (        AutoModelForCausalLM,        AutoTokenizer,        pipeline,    )  # Need to be inside the fn in notebooks    model_id = "gpt2"    tokenizer = AutoTokenizer.from_pretrained(model_id)    model = AutoModelForCausalLM.from_pretrained(model_id)    pipe = pipeline(        "text-generation", model=model, tokenizer=tokenizer, max_new_tokens=10    )    return pipedef inference_fn(pipeline, prompt, stop=None):    return pipeline(prompt)[0]["generated_text"][len(prompt) :]llm = SelfHostedHuggingFaceLLM(    model_load_fn=load_pipeline, hardware=gpu, inference_fn=inference_fn)llm("Who is the current US president?")    INFO | 2023-02-17 05:42:59,219 | Running _generate_text via gRPC    INFO | 2023-02-17 05:42:59,522 | Time to send message: 0.3 seconds    'john w. bush'You can send your pipeline directly over the wire to your model, but this will only work for small models (<2 Gb), and will be pretty slow:pipeline = load_pipeline()llm = SelfHostedPipeline.from_pipeline(    pipeline=pipeline, hardware=gpu, model_reqs=model_reqs)Instead, we can also send it to the hardware's filesystem, which will be much faster.rh.blob(pickle.dumps(pipeline), path="models/pipeline.pkl").save().to(    gpu, path="models")llm = SelfHostedPipeline.from_pipeline(pipeline="models/pipeline.pkl", hardware=gpu)PreviousReplicateNextSageMakerEndpoint IntegrationsLLMsRunhouseRunhouseThe Runhouse allows remote compute and data across environments and users. See the Runhouse docs.This example goes over how to use LangChain and Runhouse to interact with models hosted on your own GPU, or on-demand GPUs on AWS, GCP, AWS, or Lambda.Note: Code uses SelfHosted name instead of the Runhouse.pip install runhousefrom langchain.llms import SelfHostedPipeline, SelfHostedHuggingFaceLLMfrom langchain import PromptTemplate, LLMChainimport runhouse as rhAPI Reference:SelfHostedPipelineSelfHostedHuggingFaceLLM    INFO | 2023-04-17 16:47:36,173 | No auth token provided, so not using RNS API to save and load configs# For an on-demand A100 with GCP, Azure, or Lambdagpu = rh.cluster(name="rh-a10x", instance_type="A100:1", use_spot=False)# For an on-demand A10G with AWS (no single A100s on AWS)# gpu = rh.cluster(name='rh-a10x', instance_type='g5.2xlarge', provider='aws')# For an existing cluster# gpu = rh.cluster(ips=['<ip of the cluster>'],#                  ssh_creds={'ssh_user': '...', 'ssh_private_key':'<path_to_key>'},#                  name='rh-a10x')template = """Question: {question}Answer: Let's think step by step."""prompt = PromptTemplate(template=template, input_variables=["question"])llm = SelfHostedHuggingFaceLLM(    model_id="gpt2", hardware=gpu, model_reqs=["pip:./", "transformers", "torch"])llm_chain = LLMChain(prompt=prompt, llm=llm)question = "What NFL team won the Super Bowl in the year Justin Beiber was born?"llm_chain.run(question)    INFO | 2023-02-17 05:42:23,537 | Running _generate_text via gRPC    INFO | 2023-02-17 05:42:24,016 | Time to send message: 0.48 seconds    "\n\nLet's say we're talking sports teams who won the Super Bowl in the year Justin Beiber"You can also load more custom models through the SelfHostedHuggingFaceLLM interface:llm = SelfHostedHuggingFaceLLM(    model_id="google/flan-t5-small",    task="text2text-generation",    hardware=gpu,)llm("What is the capital of Germany?")    INFO | 2023-02-17 05:54:21,681 | Running _generate_text via gRPC    INFO | 2023-02-17 05:54:21,937 | Time to send message: 0.25 seconds    'berlin'Using a custom load function, we can load a custom pipeline directly on the remote hardware:def load_pipeline():    from transformers import (        AutoModelForCausalLM,        AutoTokenizer,        pipeline,    )  # Need to be inside the fn in notebooks    model_id = "gpt2"    tokenizer = AutoTokenizer.from_pretrained(model_id)    model = AutoModelForCausalLM.from_pretrained(model_id)    pipe = pipeline(        "text-generation", model=model, tokenizer=tokenizer, max_new_tokens=10    )    return pipedef inference_fn(pipeline, prompt, stop=None):    return pipeline(prompt)[0]["generated_text"][len(prompt) :]llm = SelfHostedHuggingFaceLLM(    model_load_fn=load_pipeline, hardware=gpu, inference_fn=inference_fn)llm("Who is the current US president?")    INFO | 2023-02-17 05:42:59,219 | Running _generate_text via gRPC    INFO | 2023-02-17 05:42:59,522 | Time to send message: 0.3 seconds    'john w. bush'You can send your pipeline directly over the wire to your model, but this will only work for small models (<2 Gb), and will be pretty slow:pipeline = load_pipeline()llm = SelfHostedPipeline.from_pipeline(    pipeline=pipeline, hardware=gpu, model_reqs=model_reqs)Instead, we can also send it to the hardware's filesystem, which will be much faster.rh.blob(pickle.dumps(pipeline), path="models/pipeline.pkl").save().to(    gpu, path="models")llm = SelfHostedPipeline.from_pipeline(pipeline="models/pipeline.pkl", hardware=gpu)PreviousReplicateNextSageMakerEndpoint RunhouseThe Runhouse allows remote compute and data across environments and users. See the Runhouse docs.This example goes over how to use LangChain and Runhouse to interact with models hosted on your own GPU, or on-demand GPUs on AWS, GCP, AWS, or Lambda.Note: Code uses SelfHosted name instead of the Runhouse.pip install runhousefrom langchain.llms import SelfHostedPipeline, SelfHostedHuggingFaceLLMfrom langchain import PromptTemplate, LLMChainimport runhouse as rhAPI Reference:SelfHostedPipelineSelfHostedHuggingFaceLLM    INFO | 2023-04-17 16:47:36,173 | No auth token provided, so not using RNS API to save and load configs# For an on-demand A100 with GCP, Azure, or Lambdagpu = rh.cluster(name="rh-a10x", instance_type="A100:1", use_spot=False)# For an on-demand A10G with AWS (no single A100s on AWS)# gpu = rh.cluster(name='rh-a10x', instance_type='g5.2xlarge', provider='aws')# For an existing cluster# gpu = rh.cluster(ips=['<ip of the cluster>'],#                  ssh_creds={'ssh_user': '...', 'ssh_private_key':'<path_to_key>'},#                  name='rh-a10x')template = """Question: {question}Answer: Let's think step by step."""prompt = PromptTemplate(template=template, input_variables=["question"])llm = SelfHostedHuggingFaceLLM(    model_id="gpt2", hardware=gpu, model_reqs=["pip:./", "transformers", "torch"])llm_chain = LLMChain(prompt=prompt, llm=llm)question = "What NFL team won the Super Bowl in the year Justin Beiber was born?"llm_chain.run(question)    INFO | 2023-02-17 05:42:23,537 | Running _generate_text via gRPC    INFO | 2023-02-17 05:42:24,016 | Time to send message: 0.48 seconds    "\n\nLet's say we're talking sports teams who won the Super Bowl in the year Justin Beiber"You can also load more custom models through the SelfHostedHuggingFaceLLM interface:llm = SelfHostedHuggingFaceLLM(    model_id="google/flan-t5-small",    task="text2text-generation",    hardware=gpu,)llm("What is the capital of Germany?")    INFO | 2023-02-17 05:54:21,681 | Running _generate_text via gRPC    INFO | 2023-02-17 05:54:21,937 | Time to send message: 0.25 seconds    'berlin'Using a custom load function, we can load a custom pipeline directly on the remote hardware:def load_pipeline():    from transformers import (        AutoModelForCausalLM,        AutoTokenizer,        pipeline,    )  # Need to be inside the fn in notebooks    model_id = "gpt2"    tokenizer = AutoTokenizer.from_pretrained(model_id)    model = AutoModelForCausalLM.from_pretrained(model_id)    pipe = pipeline(        "text-generation", model=model, tokenizer=tokenizer, max_new_tokens=10    )    return pipedef inference_fn(pipeline, prompt, stop=None):    return pipeline(prompt)[0]["generated_text"][len(prompt) :]llm = SelfHostedHuggingFaceLLM(    model_load_fn=load_pipeline, hardware=gpu, inference_fn=inference_fn)llm("Who is the current US president?")    INFO | 2023-02-17 05:42:59,219 | Running _generate_text via gRPC    INFO | 2023-02-17 05:42:59,522 | Time to send message: 0.3 seconds    'john w. bush'You can send your pipeline directly over the wire to your model, but this will only work for small models (<2 Gb), and will be pretty slow:pipeline = load_pipeline()llm = SelfHostedPipeline.from_pipeline(    pipeline=pipeline, hardware=gpu, model_reqs=model_reqs)Instead, we can also send it to the hardware's filesystem, which will be much faster.rh.blob(pickle.dumps(pipeline), path="models/pipeline.pkl").save().to(    gpu, path="models")llm = SelfHostedPipeline.from_pipeline(pipeline="models/pipeline.pkl", hardware=gpu) pip install runhouse pip install runhouse  from langchain.llms import SelfHostedPipeline, SelfHostedHuggingFaceLLMfrom langchain import PromptTemplate, LLMChainimport runhouse as rh from langchain.llms import SelfHostedPipeline, SelfHostedHuggingFaceLLMfrom langchain import PromptTemplate, LLMChainimport runhouse as rh  API Reference:SelfHostedPipelineSelfHostedHuggingFaceLLM     INFO | 2023-04-17 16:47:36,173 | No auth token provided, so not using RNS API to save and load configs     INFO | 2023-04-17 16:47:36,173 | No auth token provided, so not using RNS API to save and load configs     INFO | 2023-04-17 16:47:36,173 | No auth token provided, so not using RNS API to save and load configs  # For an on-demand A100 with GCP, Azure, or Lambdagpu = rh.cluster(name="rh-a10x", instance_type="A100:1", use_spot=False)# For an on-demand A10G with AWS (no single A100s on AWS)# gpu = rh.cluster(name='rh-a10x', instance_type='g5.2xlarge', provider='aws')# For an existing cluster# gpu = rh.cluster(ips=['<ip of the cluster>'],#                  ssh_creds={'ssh_user': '...', 'ssh_private_key':'<path_to_key>'},#                  name='rh-a10x') # For an on-demand A100 with GCP, Azure, or Lambdagpu = rh.cluster(name="rh-a10x", instance_type="A100:1", use_spot=False)# For an on-demand A10G with AWS (no single A100s on AWS)# gpu = rh.cluster(name='rh-a10x', instance_type='g5.2xlarge', provider='aws')# For an existing cluster# gpu = rh.cluster(ips=['<ip of the cluster>'],#                  ssh_creds={'ssh_user': '...', 'ssh_private_key':'<path_to_key>'},#                  name='rh-a10x')  template = """Question: {question}Answer: Let's think step by step."""prompt = PromptTemplate(template=template, input_variables=["question"]) template = """Question: {question}Answer: Let's think step by step."""prompt = PromptTemplate(template=template, input_variables=["question"])  llm = SelfHostedHuggingFaceLLM(    model_id="gpt2", hardware=gpu, model_reqs=["pip:./", "transformers", "torch"]) llm = SelfHostedHuggingFaceLLM(    model_id="gpt2", hardware=gpu, model_reqs=["pip:./", "transformers", "torch"])  llm_chain = LLMChain(prompt=prompt, llm=llm) llm_chain = LLMChain(prompt=prompt, llm=llm)  question = "What NFL team won the Super Bowl in the year Justin Beiber was born?"llm_chain.run(question) question = "What NFL team won the Super Bowl in the year Justin Beiber was born?"llm_chain.run(question)      INFO | 2023-02-17 05:42:23,537 | Running _generate_text via gRPC    INFO | 2023-02-17 05:42:24,016 | Time to send message: 0.48 seconds    "\n\nLet's say we're talking sports teams who won the Super Bowl in the year Justin Beiber"     INFO | 2023-02-17 05:42:23,537 | Running _generate_text via gRPC    INFO | 2023-02-17 05:42:24,016 | Time to send message: 0.48 seconds    "\n\nLet's say we're talking sports teams who won the Super Bowl in the year Justin Beiber"     INFO | 2023-02-17 05:42:23,537 | Running _generate_text via gRPC    INFO | 2023-02-17 05:42:24,016 | Time to send message: 0.48 seconds    "\n\nLet's say we're talking sports teams who won the Super Bowl in the year Justin Beiber"  llm = SelfHostedHuggingFaceLLM(    model_id="google/flan-t5-small",    task="text2text-generation",    hardware=gpu,) llm = SelfHostedHuggingFaceLLM(    model_id="google/flan-t5-small",    task="text2text-generation",    hardware=gpu,)  llm("What is the capital of Germany?") llm("What is the capital of Germany?")      INFO | 2023-02-17 05:54:21,681 | Running _generate_text via gRPC    INFO | 2023-02-17 05:54:21,937 | Time to send message: 0.25 seconds    'berlin'     INFO | 2023-02-17 05:54:21,681 | Running _generate_text via gRPC    INFO | 2023-02-17 05:54:21,937 | Time to send message: 0.25 seconds    'berlin'     INFO | 2023-02-17 05:54:21,681 | Running _generate_text via gRPC    INFO | 2023-02-17 05:54:21,937 | Time to send message: 0.25 seconds    'berlin'  def load_pipeline():    from transformers import (        AutoModelForCausalLM,        AutoTokenizer,        pipeline,    )  # Need to be inside the fn in notebooks    model_id = "gpt2"    tokenizer = AutoTokenizer.from_pretrained(model_id)    model = AutoModelForCausalLM.from_pretrained(model_id)    pipe = pipeline(        "text-generation", model=model, tokenizer=tokenizer, max_new_tokens=10    )    return pipedef inference_fn(pipeline, prompt, stop=None):    return pipeline(prompt)[0]["generated_text"][len(prompt) :] def load_pipeline():    from transformers import (        AutoModelForCausalLM,        AutoTokenizer,        pipeline,    )  # Need to be inside the fn in notebooks    model_id = "gpt2"    tokenizer = AutoTokenizer.from_pretrained(model_id)    model = AutoModelForCausalLM.from_pretrained(model_id)    pipe = pipeline(        "text-generation", model=model, tokenizer=tokenizer, max_new_tokens=10    )    return pipedef inference_fn(pipeline, prompt, stop=None):    return pipeline(prompt)[0]["generated_text"][len(prompt) :]  llm = SelfHostedHuggingFaceLLM(    model_load_fn=load_pipeline, hardware=gpu, inference_fn=inference_fn) llm = SelfHostedHuggingFaceLLM(    model_load_fn=load_pipeline, hardware=gpu, inference_fn=inference_fn)  llm("Who is the current US president?") llm("Who is the current US president?")      INFO | 2023-02-17 05:42:59,219 | Running _generate_text via gRPC    INFO | 2023-02-17 05:42:59,522 | Time to send message: 0.3 seconds    'john w. bush'     INFO | 2023-02-17 05:42:59,219 | Running _generate_text via gRPC    INFO | 2023-02-17 05:42:59,522 | Time to send message: 0.3 seconds    'john w. bush'     INFO | 2023-02-17 05:42:59,219 | Running _generate_text via gRPC    INFO | 2023-02-17 05:42:59,522 | Time to send message: 0.3 seconds    'john w. bush'  pipeline = load_pipeline()llm = SelfHostedPipeline.from_pipeline(    pipeline=pipeline, hardware=gpu, model_reqs=model_reqs) pipeline = load_pipeline()llm = SelfHostedPipeline.from_pipeline(    pipeline=pipeline, hardware=gpu, model_reqs=model_reqs)  rh.blob(pickle.dumps(pipeline), path="models/pipeline.pkl").save().to(    gpu, path="models")llm = SelfHostedPipeline.from_pipeline(pipeline="models/pipeline.pkl", hardware=gpu) rh.blob(pickle.dumps(pipeline), path="models/pipeline.pkl").save().to(    gpu, path="models")llm = SelfHostedPipeline.from_pipeline(pipeline="models/pipeline.pkl", hardware=gpu)  Previous Replicate Next SageMakerEndpoint CommunityDiscordTwitterGitHubPythonJS/TSMoreHomepageBlogCopyright © 2023 LangChain, Inc. CommunityDiscordTwitterGitHubPythonJS/TSMoreHomepageBlog CommunityDiscordTwitter Community GitHubPythonJS/TS GitHub MoreHomepageBlog More Copyright © 2023 LangChain, Inc. Copyright © 2023 LangChain, Inc. Skip to main content (#docusaurus_skipToContent_fallback) 🦜️🔗 LangChain (/) Docs (/docs/get_started/introduction) Use cases (/docs/use_cases) Integrations (/docs/integrations) API (https://api.python.langchain.com) LangSmith (https://smith.langchain.com) JS/TS Docs (https://js.langchain.com/docs)  (https://github.com/hwchase17/langchain) Integrations (/docs/integrations) Callbacks (/docs/integrations/callbacks/) Chat models (/docs/integrations/chat/) Chat loaders (/docs/integrations/chat_loaders/) Document loaders (/docs/integrations/document_loaders/) Document transformers (/docs/integrations/document_transformers/) LLMs (/docs/integrations/llms/) AI21 (/docs/integrations/llms/ai21) Aleph Alpha (/docs/integrations/llms/aleph_alpha) Amazon API Gateway (/docs/integrations/llms/amazon_api_gateway) Anyscale (/docs/integrations/llms/anyscale) Azure ML (/docs/integrations/llms/azure_ml) Azure OpenAI (/docs/integrations/llms/azure_openai) Banana (/docs/integrations/llms/banana) Baseten (/docs/integrations/llms/baseten) Beam (/docs/integrations/llms/beam) Bedrock (/docs/integrations/llms/bedrock) Bittensor (/docs/integrations/llms/bittensor) CerebriumAI (/docs/integrations/llms/cerebriumai) ChatGLM (/docs/integrations/llms/chatglm) Clarifai (/docs/integrations/llms/clarifai) Cohere (/docs/integrations/llms/cohere) C Transformers (/docs/integrations/llms/ctransformers) CTranslate2 (/docs/integrations/llms/ctranslate2) Databricks (/docs/integrations/llms/databricks) DeepInfra (/docs/integrations/llms/deepinfra) DeepSparse (/docs/integrations/llms/deepsparse) Eden AI (/docs/integrations/llms/edenai) Fireworks (/docs/integrations/llms/fireworks) ForefrontAI (/docs/integrations/llms/forefrontai) Google Vertex AI PaLM (/docs/integrations/llms/google_vertex_ai_palm) GooseAI (/docs/integrations/llms/gooseai) GPT4All (/docs/integrations/llms/gpt4all) Hugging Face Hub (/docs/integrations/llms/huggingface_hub) Hugging Face Local Pipelines (/docs/integrations/llms/huggingface_pipelines) Huggingface TextGen Inference (/docs/integrations/llms/huggingface_textgen_inference) JSONFormer (/docs/integrations/llms/jsonformer_experimental) KoboldAI API (/docs/integrations/llms/koboldai) Llama.cpp (/docs/integrations/llms/llamacpp) LLM Caching integrations (/docs/integrations/llms/llm_caching) Manifest (/docs/integrations/llms/manifest) Minimax (/docs/integrations/llms/minimax) Modal (/docs/integrations/llms/modal) MosaicML (/docs/integrations/llms/mosaicml) NLP Cloud (/docs/integrations/llms/nlpcloud) OctoAI (/docs/integrations/llms/octoai) Ollama (/docs/integrations/llms/ollama) OpaquePrompts (/docs/integrations/llms/opaqueprompts) OpenAI (/docs/integrations/llms/openai) OpenLLM (/docs/integrations/llms/openllm) OpenLM (/docs/integrations/llms/openlm) Petals (/docs/integrations/llms/petals) PipelineAI (/docs/integrations/llms/pipelineai) Predibase (/docs/integrations/llms/predibase) Prediction Guard (/docs/integrations/llms/predictionguard) PromptLayer OpenAI (/docs/integrations/llms/promptlayer_openai) RELLM (/docs/integrations/llms/rellm_experimental) Replicate (/docs/integrations/llms/replicate) Runhouse (/docs/integrations/llms/runhouse) SageMakerEndpoint (/docs/integrations/llms/sagemaker) StochasticAI (/docs/integrations/llms/stochasticai) Nebula (Symbl.ai) (/docs/integrations/llms/symblai_nebula) TextGen (/docs/integrations/llms/textgen) Titan Takeoff (/docs/integrations/llms/titan_takeoff) Tongyi Qwen (/docs/integrations/llms/tongyi) vLLM (/docs/integrations/llms/vllm) Writer (/docs/integrations/llms/writer) Xorbits Inference (Xinference) (/docs/integrations/llms/xinference) Memory (/docs/integrations/memory/) Retrievers (/docs/integrations/retrievers/) Text embedding models (/docs/integrations/text_embedding/) Agents & Toolkits (/docs/integrations/toolkits/) Tools (/docs/integrations/tools/) Vector stores (/docs/integrations/vectorstores/) Grouped by provider (/docs/integrations/providers/)  (/) Integrations (/docs/integrations) LLMs (/docs/integrations/llms/) Runhouse (https://github.com/run-house/runhouse) Runhouse docs (https://runhouse-docs.readthedocs-hosted.com/en/latest/) Runhouse (https://github.com/run-house/runhouse) SelfHostedPipeline (https://api.python.langchain.com/en/latest/llms/langchain.llms.self_hosted.SelfHostedPipeline.html) SelfHostedHuggingFaceLLM (https://api.python.langchain.com/en/latest/llms/langchain.llms.self_hosted_hugging_face.SelfHostedHuggingFaceLLM.html) PreviousReplicate (/docs/integrations/llms/replicate) NextSageMakerEndpoint (/docs/integrations/llms/sagemaker) Discord (https://discord.gg/cU2adEyC7w) Twitter (https://twitter.com/LangChainAI) Python (https://github.com/hwchase17/langchain) JS/TS (https://github.com/hwchase17/langchainjs) Homepage (https://langchain.com) Blog (https://blog.langchain.dev)