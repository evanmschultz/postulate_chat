Xinference is a powerful and versatile library designed to serve LLMs, speech recognition models, and multimodal models, even on your laptop. It supports a variety of models compatible with GGML, such as chatglm, baichuan, whisper, vicuna, orca, and many others. This notebook demonstrates how to use Xinference with LangChain. Install Xinference through PyPI: For local deployment, run xinference.  To deploy Xinference in a cluster, first start an Xinference supervisor using the xinference-supervisor. You can also use the option -p to specify the port and -H to specify the host. The default port is 9997. Then, start the Xinference workers using xinference-worker on each server you want to run them on.  You can consult the README file from Xinference for more information. To use Xinference with LangChain, you need to first launch a model. You can use command line interface (CLI) to do so: A model UID is returned for you to use. Now you can use Xinference with LangChain: Lastly, terminate the model when you do not need to use it: IntegrationsCallbacksChat modelsChat loadersDocument loadersDocument transformersLLMsAI21Aleph AlphaAmazon API GatewayAnyscaleAzure MLAzure OpenAIBananaBasetenBeamBedrockBittensorCerebriumAIChatGLMClarifaiCohereC TransformersCTranslate2DatabricksDeepInfraDeepSparseEden AIFireworksForefrontAIGoogle Vertex AI PaLMGooseAIGPT4AllHugging Face HubHugging Face Local PipelinesHuggingface TextGen InferenceJSONFormerKoboldAI APILlama.cppLLM Caching integrationsManifestMinimaxModalMosaicMLNLP CloudOctoAIOllamaOpaquePromptsOpenAIOpenLLMOpenLMPetalsPipelineAIPredibasePrediction GuardPromptLayer OpenAIRELLMReplicateRunhouseSageMakerEndpointStochasticAINebula (Symbl.ai)TextGenTitan TakeoffTongyi QwenvLLMWriterXorbits Inference (Xinference)MemoryRetrieversText embedding modelsAgents & ToolkitsToolsVector storesGrouped by provider Callbacks Chat models Chat loaders Document loaders Document transformers LLMsAI21Aleph AlphaAmazon API GatewayAnyscaleAzure MLAzure OpenAIBananaBasetenBeamBedrockBittensorCerebriumAIChatGLMClarifaiCohereC TransformersCTranslate2DatabricksDeepInfraDeepSparseEden AIFireworksForefrontAIGoogle Vertex AI PaLMGooseAIGPT4AllHugging Face HubHugging Face Local PipelinesHuggingface TextGen InferenceJSONFormerKoboldAI APILlama.cppLLM Caching integrationsManifestMinimaxModalMosaicMLNLP CloudOctoAIOllamaOpaquePromptsOpenAIOpenLLMOpenLMPetalsPipelineAIPredibasePrediction GuardPromptLayer OpenAIRELLMReplicateRunhouseSageMakerEndpointStochasticAINebula (Symbl.ai)TextGenTitan TakeoffTongyi QwenvLLMWriterXorbits Inference (Xinference) AI21 Aleph Alpha Amazon API Gateway Anyscale Azure ML Azure OpenAI Banana Baseten Beam Bedrock Bittensor CerebriumAI ChatGLM Clarifai Cohere C Transformers CTranslate2 Databricks DeepInfra DeepSparse Eden AI Fireworks ForefrontAI Google Vertex AI PaLM GooseAI GPT4All Hugging Face Hub Hugging Face Local Pipelines Huggingface TextGen Inference JSONFormer KoboldAI API Llama.cpp LLM Caching integrations Manifest Minimax Modal MosaicML NLP Cloud OctoAI Ollama OpaquePrompts OpenAI OpenLLM OpenLM Petals PipelineAI Predibase Prediction Guard PromptLayer OpenAI RELLM Replicate Runhouse SageMakerEndpoint StochasticAI Nebula (Symbl.ai) TextGen Titan Takeoff Tongyi Qwen vLLM Writer Xorbits Inference (Xinference) Memory Retrievers Text embedding models Agents & Toolkits Tools Vector stores Grouped by provider  Integrations LLMs Xorbits Inference (Xinference) Xinference Installation Deploy Xinference Locally or in a Distributed Cluster. WrapperIntegrate with a LLMChain Integrate with a LLMChain Discord Twitter Python JS/TS Homepage Blog Skip to main contentü¶úÔ∏èüîó LangChainDocsUse casesIntegrationsAPILangSmithJS/TS DocsCTRLKIntegrationsCallbacksChat modelsChat loadersDocument loadersDocument transformersLLMsAI21Aleph AlphaAmazon API GatewayAnyscaleAzure MLAzure OpenAIBananaBasetenBeamBedrockBittensorCerebriumAIChatGLMClarifaiCohereC TransformersCTranslate2DatabricksDeepInfraDeepSparseEden AIFireworksForefrontAIGoogle Vertex AI PaLMGooseAIGPT4AllHugging Face HubHugging Face Local PipelinesHuggingface TextGen InferenceJSONFormerKoboldAI APILlama.cppLLM Caching integrationsManifestMinimaxModalMosaicMLNLP CloudOctoAIOllamaOpaquePromptsOpenAIOpenLLMOpenLMPetalsPipelineAIPredibasePrediction GuardPromptLayer OpenAIRELLMReplicateRunhouseSageMakerEndpointStochasticAINebula (Symbl.ai)TextGenTitan TakeoffTongyi QwenvLLMWriterXorbits Inference (Xinference)MemoryRetrieversText embedding modelsAgents & ToolkitsToolsVector storesGrouped by providerIntegrationsLLMsXorbits Inference (Xinference)On this pageXorbits Inference (Xinference)Xinference is a powerful and versatile library designed to serve LLMs, speech recognition models, and multimodal models, even on your laptop. It supports a variety of models compatible with GGML, such as chatglm, baichuan, whisper, vicuna, orca, and many others. This notebook demonstrates how to use Xinference with LangChain.Installation‚ÄãInstall Xinference through PyPI:%pip install "xinference[all]"Deploy Xinference Locally or in a Distributed Cluster.‚ÄãFor local deployment, run xinference. To deploy Xinference in a cluster, first start an Xinference supervisor using the xinference-supervisor. You can also use the option -p to specify the port and -H to specify the host. The default port is 9997.Then, start the Xinference workers using xinference-worker on each server you want to run them on. You can consult the README file from Xinference for more information.Wrapper‚ÄãTo use Xinference with LangChain, you need to first launch a model. You can use command line interface (CLI) to do so:xinference launch -n vicuna-v1.3 -f ggmlv3 -q q4_0    Model uid: 7167b2b0-2a04-11ee-83f0-d29396a3f064A model UID is returned for you to use. Now you can use Xinference with LangChain:from langchain.llms import Xinferencellm = Xinference(    server_url="http://0.0.0.0:9997",    model_uid = "7167b2b0-2a04-11ee-83f0-d29396a3f064")llm(    prompt="Q: where can we visit in the capital of France? A:",    generate_config={"max_tokens": 1024, "stream": True},)API Reference:Xinference    ' You can visit the Eiffel Tower, Notre-Dame Cathedral, the Louvre Museum, and many other historical sites in Paris, the capital of France.'Integrate with a LLMChain‚Äãfrom langchain import PromptTemplate, LLMChaintemplate = "Where can we visit in the capital of {country}?"prompt = PromptTemplate(template=template, input_variables=["country"])llm_chain = LLMChain(prompt=prompt, llm=llm)generated = llm_chain.run(country="France")print(generated)        A: You can visit many places in Paris, such as the Eiffel Tower, the Louvre Museum, Notre-Dame Cathedral, the Champs-Elys√©es, Montmartre, Sacr√©-C≈ìur, and the Palace of Versailles.Lastly, terminate the model when you do not need to use it:xinference terminate --model-uid "7167b2b0-2a04-11ee-83f0-d29396a3f064"PreviousWriterNextMemoryInstallationDeploy Xinference Locally or in a Distributed Cluster.WrapperIntegrate with a LLMChainCommunityDiscordTwitterGitHubPythonJS/TSMoreHomepageBlogCopyright ¬© 2023 LangChain, Inc. Skip to main content ü¶úÔ∏èüîó LangChainDocsUse casesIntegrationsAPILangSmithJS/TS DocsCTRLK ü¶úÔ∏èüîó LangChainDocsUse casesIntegrationsAPI LangSmithJS/TS DocsCTRLK  CTRLK CTRLK  CTRLK   IntegrationsCallbacksChat modelsChat loadersDocument loadersDocument transformersLLMsAI21Aleph AlphaAmazon API GatewayAnyscaleAzure MLAzure OpenAIBananaBasetenBeamBedrockBittensorCerebriumAIChatGLMClarifaiCohereC TransformersCTranslate2DatabricksDeepInfraDeepSparseEden AIFireworksForefrontAIGoogle Vertex AI PaLMGooseAIGPT4AllHugging Face HubHugging Face Local PipelinesHuggingface TextGen InferenceJSONFormerKoboldAI APILlama.cppLLM Caching integrationsManifestMinimaxModalMosaicMLNLP CloudOctoAIOllamaOpaquePromptsOpenAIOpenLLMOpenLMPetalsPipelineAIPredibasePrediction GuardPromptLayer OpenAIRELLMReplicateRunhouseSageMakerEndpointStochasticAINebula (Symbl.ai)TextGenTitan TakeoffTongyi QwenvLLMWriterXorbits Inference (Xinference)MemoryRetrieversText embedding modelsAgents & ToolkitsToolsVector storesGrouped by providerIntegrationsLLMsXorbits Inference (Xinference)On this pageXorbits Inference (Xinference)Xinference is a powerful and versatile library designed to serve LLMs, speech recognition models, and multimodal models, even on your laptop. It supports a variety of models compatible with GGML, such as chatglm, baichuan, whisper, vicuna, orca, and many others. This notebook demonstrates how to use Xinference with LangChain.Installation‚ÄãInstall Xinference through PyPI:%pip install "xinference[all]"Deploy Xinference Locally or in a Distributed Cluster.‚ÄãFor local deployment, run xinference. To deploy Xinference in a cluster, first start an Xinference supervisor using the xinference-supervisor. You can also use the option -p to specify the port and -H to specify the host. The default port is 9997.Then, start the Xinference workers using xinference-worker on each server you want to run them on. You can consult the README file from Xinference for more information.Wrapper‚ÄãTo use Xinference with LangChain, you need to first launch a model. You can use command line interface (CLI) to do so:xinference launch -n vicuna-v1.3 -f ggmlv3 -q q4_0    Model uid: 7167b2b0-2a04-11ee-83f0-d29396a3f064A model UID is returned for you to use. Now you can use Xinference with LangChain:from langchain.llms import Xinferencellm = Xinference(    server_url="http://0.0.0.0:9997",    model_uid = "7167b2b0-2a04-11ee-83f0-d29396a3f064")llm(    prompt="Q: where can we visit in the capital of France? A:",    generate_config={"max_tokens": 1024, "stream": True},)API Reference:Xinference    ' You can visit the Eiffel Tower, Notre-Dame Cathedral, the Louvre Museum, and many other historical sites in Paris, the capital of France.'Integrate with a LLMChain‚Äãfrom langchain import PromptTemplate, LLMChaintemplate = "Where can we visit in the capital of {country}?"prompt = PromptTemplate(template=template, input_variables=["country"])llm_chain = LLMChain(prompt=prompt, llm=llm)generated = llm_chain.run(country="France")print(generated)        A: You can visit many places in Paris, such as the Eiffel Tower, the Louvre Museum, Notre-Dame Cathedral, the Champs-Elys√©es, Montmartre, Sacr√©-C≈ìur, and the Palace of Versailles.Lastly, terminate the model when you do not need to use it:xinference terminate --model-uid "7167b2b0-2a04-11ee-83f0-d29396a3f064"PreviousWriterNextMemoryInstallationDeploy Xinference Locally or in a Distributed Cluster.WrapperIntegrate with a LLMChain IntegrationsCallbacksChat modelsChat loadersDocument loadersDocument transformersLLMsAI21Aleph AlphaAmazon API GatewayAnyscaleAzure MLAzure OpenAIBananaBasetenBeamBedrockBittensorCerebriumAIChatGLMClarifaiCohereC TransformersCTranslate2DatabricksDeepInfraDeepSparseEden AIFireworksForefrontAIGoogle Vertex AI PaLMGooseAIGPT4AllHugging Face HubHugging Face Local PipelinesHuggingface TextGen InferenceJSONFormerKoboldAI APILlama.cppLLM Caching integrationsManifestMinimaxModalMosaicMLNLP CloudOctoAIOllamaOpaquePromptsOpenAIOpenLLMOpenLMPetalsPipelineAIPredibasePrediction GuardPromptLayer OpenAIRELLMReplicateRunhouseSageMakerEndpointStochasticAINebula (Symbl.ai)TextGenTitan TakeoffTongyi QwenvLLMWriterXorbits Inference (Xinference)MemoryRetrieversText embedding modelsAgents & ToolkitsToolsVector storesGrouped by providerIntegrationsLLMsXorbits Inference (Xinference)On this pageXorbits Inference (Xinference)Xinference is a powerful and versatile library designed to serve LLMs, speech recognition models, and multimodal models, even on your laptop. It supports a variety of models compatible with GGML, such as chatglm, baichuan, whisper, vicuna, orca, and many others. This notebook demonstrates how to use Xinference with LangChain.Installation‚ÄãInstall Xinference through PyPI:%pip install "xinference[all]"Deploy Xinference Locally or in a Distributed Cluster.‚ÄãFor local deployment, run xinference. To deploy Xinference in a cluster, first start an Xinference supervisor using the xinference-supervisor. You can also use the option -p to specify the port and -H to specify the host. The default port is 9997.Then, start the Xinference workers using xinference-worker on each server you want to run them on. You can consult the README file from Xinference for more information.Wrapper‚ÄãTo use Xinference with LangChain, you need to first launch a model. You can use command line interface (CLI) to do so:xinference launch -n vicuna-v1.3 -f ggmlv3 -q q4_0    Model uid: 7167b2b0-2a04-11ee-83f0-d29396a3f064A model UID is returned for you to use. Now you can use Xinference with LangChain:from langchain.llms import Xinferencellm = Xinference(    server_url="http://0.0.0.0:9997",    model_uid = "7167b2b0-2a04-11ee-83f0-d29396a3f064")llm(    prompt="Q: where can we visit in the capital of France? A:",    generate_config={"max_tokens": 1024, "stream": True},)API Reference:Xinference    ' You can visit the Eiffel Tower, Notre-Dame Cathedral, the Louvre Museum, and many other historical sites in Paris, the capital of France.'Integrate with a LLMChain‚Äãfrom langchain import PromptTemplate, LLMChaintemplate = "Where can we visit in the capital of {country}?"prompt = PromptTemplate(template=template, input_variables=["country"])llm_chain = LLMChain(prompt=prompt, llm=llm)generated = llm_chain.run(country="France")print(generated)        A: You can visit many places in Paris, such as the Eiffel Tower, the Louvre Museum, Notre-Dame Cathedral, the Champs-Elys√©es, Montmartre, Sacr√©-C≈ìur, and the Palace of Versailles.Lastly, terminate the model when you do not need to use it:xinference terminate --model-uid "7167b2b0-2a04-11ee-83f0-d29396a3f064"PreviousWriterNextMemoryInstallationDeploy Xinference Locally or in a Distributed Cluster.WrapperIntegrate with a LLMChain IntegrationsCallbacksChat modelsChat loadersDocument loadersDocument transformersLLMsAI21Aleph AlphaAmazon API GatewayAnyscaleAzure MLAzure OpenAIBananaBasetenBeamBedrockBittensorCerebriumAIChatGLMClarifaiCohereC TransformersCTranslate2DatabricksDeepInfraDeepSparseEden AIFireworksForefrontAIGoogle Vertex AI PaLMGooseAIGPT4AllHugging Face HubHugging Face Local PipelinesHuggingface TextGen InferenceJSONFormerKoboldAI APILlama.cppLLM Caching integrationsManifestMinimaxModalMosaicMLNLP CloudOctoAIOllamaOpaquePromptsOpenAIOpenLLMOpenLMPetalsPipelineAIPredibasePrediction GuardPromptLayer OpenAIRELLMReplicateRunhouseSageMakerEndpointStochasticAINebula (Symbl.ai)TextGenTitan TakeoffTongyi QwenvLLMWriterXorbits Inference (Xinference)MemoryRetrieversText embedding modelsAgents & ToolkitsToolsVector storesGrouped by provider IntegrationsCallbacksChat modelsChat loadersDocument loadersDocument transformersLLMsAI21Aleph AlphaAmazon API GatewayAnyscaleAzure MLAzure OpenAIBananaBasetenBeamBedrockBittensorCerebriumAIChatGLMClarifaiCohereC TransformersCTranslate2DatabricksDeepInfraDeepSparseEden AIFireworksForefrontAIGoogle Vertex AI PaLMGooseAIGPT4AllHugging Face HubHugging Face Local PipelinesHuggingface TextGen InferenceJSONFormerKoboldAI APILlama.cppLLM Caching integrationsManifestMinimaxModalMosaicMLNLP CloudOctoAIOllamaOpaquePromptsOpenAIOpenLLMOpenLMPetalsPipelineAIPredibasePrediction GuardPromptLayer OpenAIRELLMReplicateRunhouseSageMakerEndpointStochasticAINebula (Symbl.ai)TextGenTitan TakeoffTongyi QwenvLLMWriterXorbits Inference (Xinference)MemoryRetrieversText embedding modelsAgents & ToolkitsToolsVector storesGrouped by provider Integrations Callbacks Chat models Chat loaders Document loaders Document transformers LLMs Memory Retrievers Text embedding models Agents & Toolkits Tools Vector stores Grouped by provider IntegrationsLLMsXorbits Inference (Xinference)On this pageXorbits Inference (Xinference)Xinference is a powerful and versatile library designed to serve LLMs, speech recognition models, and multimodal models, even on your laptop. It supports a variety of models compatible with GGML, such as chatglm, baichuan, whisper, vicuna, orca, and many others. This notebook demonstrates how to use Xinference with LangChain.Installation‚ÄãInstall Xinference through PyPI:%pip install "xinference[all]"Deploy Xinference Locally or in a Distributed Cluster.‚ÄãFor local deployment, run xinference. To deploy Xinference in a cluster, first start an Xinference supervisor using the xinference-supervisor. You can also use the option -p to specify the port and -H to specify the host. The default port is 9997.Then, start the Xinference workers using xinference-worker on each server you want to run them on. You can consult the README file from Xinference for more information.Wrapper‚ÄãTo use Xinference with LangChain, you need to first launch a model. You can use command line interface (CLI) to do so:xinference launch -n vicuna-v1.3 -f ggmlv3 -q q4_0    Model uid: 7167b2b0-2a04-11ee-83f0-d29396a3f064A model UID is returned for you to use. Now you can use Xinference with LangChain:from langchain.llms import Xinferencellm = Xinference(    server_url="http://0.0.0.0:9997",    model_uid = "7167b2b0-2a04-11ee-83f0-d29396a3f064")llm(    prompt="Q: where can we visit in the capital of France? A:",    generate_config={"max_tokens": 1024, "stream": True},)API Reference:Xinference    ' You can visit the Eiffel Tower, Notre-Dame Cathedral, the Louvre Museum, and many other historical sites in Paris, the capital of France.'Integrate with a LLMChain‚Äãfrom langchain import PromptTemplate, LLMChaintemplate = "Where can we visit in the capital of {country}?"prompt = PromptTemplate(template=template, input_variables=["country"])llm_chain = LLMChain(prompt=prompt, llm=llm)generated = llm_chain.run(country="France")print(generated)        A: You can visit many places in Paris, such as the Eiffel Tower, the Louvre Museum, Notre-Dame Cathedral, the Champs-Elys√©es, Montmartre, Sacr√©-C≈ìur, and the Palace of Versailles.Lastly, terminate the model when you do not need to use it:xinference terminate --model-uid "7167b2b0-2a04-11ee-83f0-d29396a3f064"PreviousWriterNextMemoryInstallationDeploy Xinference Locally or in a Distributed Cluster.WrapperIntegrate with a LLMChain IntegrationsLLMsXorbits Inference (Xinference)On this pageXorbits Inference (Xinference)Xinference is a powerful and versatile library designed to serve LLMs, speech recognition models, and multimodal models, even on your laptop. It supports a variety of models compatible with GGML, such as chatglm, baichuan, whisper, vicuna, orca, and many others. This notebook demonstrates how to use Xinference with LangChain.Installation‚ÄãInstall Xinference through PyPI:%pip install "xinference[all]"Deploy Xinference Locally or in a Distributed Cluster.‚ÄãFor local deployment, run xinference. To deploy Xinference in a cluster, first start an Xinference supervisor using the xinference-supervisor. You can also use the option -p to specify the port and -H to specify the host. The default port is 9997.Then, start the Xinference workers using xinference-worker on each server you want to run them on. You can consult the README file from Xinference for more information.Wrapper‚ÄãTo use Xinference with LangChain, you need to first launch a model. You can use command line interface (CLI) to do so:xinference launch -n vicuna-v1.3 -f ggmlv3 -q q4_0    Model uid: 7167b2b0-2a04-11ee-83f0-d29396a3f064A model UID is returned for you to use. Now you can use Xinference with LangChain:from langchain.llms import Xinferencellm = Xinference(    server_url="http://0.0.0.0:9997",    model_uid = "7167b2b0-2a04-11ee-83f0-d29396a3f064")llm(    prompt="Q: where can we visit in the capital of France? A:",    generate_config={"max_tokens": 1024, "stream": True},)API Reference:Xinference    ' You can visit the Eiffel Tower, Notre-Dame Cathedral, the Louvre Museum, and many other historical sites in Paris, the capital of France.'Integrate with a LLMChain‚Äãfrom langchain import PromptTemplate, LLMChaintemplate = "Where can we visit in the capital of {country}?"prompt = PromptTemplate(template=template, input_variables=["country"])llm_chain = LLMChain(prompt=prompt, llm=llm)generated = llm_chain.run(country="France")print(generated)        A: You can visit many places in Paris, such as the Eiffel Tower, the Louvre Museum, Notre-Dame Cathedral, the Champs-Elys√©es, Montmartre, Sacr√©-C≈ìur, and the Palace of Versailles.Lastly, terminate the model when you do not need to use it:xinference terminate --model-uid "7167b2b0-2a04-11ee-83f0-d29396a3f064"PreviousWriterNextMemory IntegrationsLLMsXorbits Inference (Xinference)On this pageXorbits Inference (Xinference)Xinference is a powerful and versatile library designed to serve LLMs, speech recognition models, and multimodal models, even on your laptop. It supports a variety of models compatible with GGML, such as chatglm, baichuan, whisper, vicuna, orca, and many others. This notebook demonstrates how to use Xinference with LangChain.Installation‚ÄãInstall Xinference through PyPI:%pip install "xinference[all]"Deploy Xinference Locally or in a Distributed Cluster.‚ÄãFor local deployment, run xinference. To deploy Xinference in a cluster, first start an Xinference supervisor using the xinference-supervisor. You can also use the option -p to specify the port and -H to specify the host. The default port is 9997.Then, start the Xinference workers using xinference-worker on each server you want to run them on. You can consult the README file from Xinference for more information.Wrapper‚ÄãTo use Xinference with LangChain, you need to first launch a model. You can use command line interface (CLI) to do so:xinference launch -n vicuna-v1.3 -f ggmlv3 -q q4_0    Model uid: 7167b2b0-2a04-11ee-83f0-d29396a3f064A model UID is returned for you to use. Now you can use Xinference with LangChain:from langchain.llms import Xinferencellm = Xinference(    server_url="http://0.0.0.0:9997",    model_uid = "7167b2b0-2a04-11ee-83f0-d29396a3f064")llm(    prompt="Q: where can we visit in the capital of France? A:",    generate_config={"max_tokens": 1024, "stream": True},)API Reference:Xinference    ' You can visit the Eiffel Tower, Notre-Dame Cathedral, the Louvre Museum, and many other historical sites in Paris, the capital of France.'Integrate with a LLMChain‚Äãfrom langchain import PromptTemplate, LLMChaintemplate = "Where can we visit in the capital of {country}?"prompt = PromptTemplate(template=template, input_variables=["country"])llm_chain = LLMChain(prompt=prompt, llm=llm)generated = llm_chain.run(country="France")print(generated)        A: You can visit many places in Paris, such as the Eiffel Tower, the Louvre Museum, Notre-Dame Cathedral, the Champs-Elys√©es, Montmartre, Sacr√©-C≈ìur, and the Palace of Versailles.Lastly, terminate the model when you do not need to use it:xinference terminate --model-uid "7167b2b0-2a04-11ee-83f0-d29396a3f064"PreviousWriterNextMemory On this page Xorbits Inference (Xinference)Xinference is a powerful and versatile library designed to serve LLMs, speech recognition models, and multimodal models, even on your laptop. It supports a variety of models compatible with GGML, such as chatglm, baichuan, whisper, vicuna, orca, and many others. This notebook demonstrates how to use Xinference with LangChain.Installation‚ÄãInstall Xinference through PyPI:%pip install "xinference[all]"Deploy Xinference Locally or in a Distributed Cluster.‚ÄãFor local deployment, run xinference. To deploy Xinference in a cluster, first start an Xinference supervisor using the xinference-supervisor. You can also use the option -p to specify the port and -H to specify the host. The default port is 9997.Then, start the Xinference workers using xinference-worker on each server you want to run them on. You can consult the README file from Xinference for more information.Wrapper‚ÄãTo use Xinference with LangChain, you need to first launch a model. You can use command line interface (CLI) to do so:xinference launch -n vicuna-v1.3 -f ggmlv3 -q q4_0    Model uid: 7167b2b0-2a04-11ee-83f0-d29396a3f064A model UID is returned for you to use. Now you can use Xinference with LangChain:from langchain.llms import Xinferencellm = Xinference(    server_url="http://0.0.0.0:9997",    model_uid = "7167b2b0-2a04-11ee-83f0-d29396a3f064")llm(    prompt="Q: where can we visit in the capital of France? A:",    generate_config={"max_tokens": 1024, "stream": True},)API Reference:Xinference    ' You can visit the Eiffel Tower, Notre-Dame Cathedral, the Louvre Museum, and many other historical sites in Paris, the capital of France.'Integrate with a LLMChain‚Äãfrom langchain import PromptTemplate, LLMChaintemplate = "Where can we visit in the capital of {country}?"prompt = PromptTemplate(template=template, input_variables=["country"])llm_chain = LLMChain(prompt=prompt, llm=llm)generated = llm_chain.run(country="France")print(generated)        A: You can visit many places in Paris, such as the Eiffel Tower, the Louvre Museum, Notre-Dame Cathedral, the Champs-Elys√©es, Montmartre, Sacr√©-C≈ìur, and the Palace of Versailles.Lastly, terminate the model when you do not need to use it:xinference terminate --model-uid "7167b2b0-2a04-11ee-83f0-d29396a3f064" %pip install "xinference[all]" %pip install "xinference[all]"  xinference launch -n vicuna-v1.3 -f ggmlv3 -q q4_0 xinference launch -n vicuna-v1.3 -f ggmlv3 -q q4_0      Model uid: 7167b2b0-2a04-11ee-83f0-d29396a3f064     Model uid: 7167b2b0-2a04-11ee-83f0-d29396a3f064     Model uid: 7167b2b0-2a04-11ee-83f0-d29396a3f064  from langchain.llms import Xinferencellm = Xinference(    server_url="http://0.0.0.0:9997",    model_uid = "7167b2b0-2a04-11ee-83f0-d29396a3f064")llm(    prompt="Q: where can we visit in the capital of France? A:",    generate_config={"max_tokens": 1024, "stream": True},) from langchain.llms import Xinferencellm = Xinference(    server_url="http://0.0.0.0:9997",    model_uid = "7167b2b0-2a04-11ee-83f0-d29396a3f064")llm(    prompt="Q: where can we visit in the capital of France? A:",    generate_config={"max_tokens": 1024, "stream": True},)  API Reference:Xinference     ' You can visit the Eiffel Tower, Notre-Dame Cathedral, the Louvre Museum, and many other historical sites in Paris, the capital of France.'     ' You can visit the Eiffel Tower, Notre-Dame Cathedral, the Louvre Museum, and many other historical sites in Paris, the capital of France.'     ' You can visit the Eiffel Tower, Notre-Dame Cathedral, the Louvre Museum, and many other historical sites in Paris, the capital of France.'  from langchain import PromptTemplate, LLMChaintemplate = "Where can we visit in the capital of {country}?"prompt = PromptTemplate(template=template, input_variables=["country"])llm_chain = LLMChain(prompt=prompt, llm=llm)generated = llm_chain.run(country="France")print(generated) from langchain import PromptTemplate, LLMChaintemplate = "Where can we visit in the capital of {country}?"prompt = PromptTemplate(template=template, input_variables=["country"])llm_chain = LLMChain(prompt=prompt, llm=llm)generated = llm_chain.run(country="France")print(generated)          A: You can visit many places in Paris, such as the Eiffel Tower, the Louvre Museum, Notre-Dame Cathedral, the Champs-Elys√©es, Montmartre, Sacr√©-C≈ìur, and the Palace of Versailles.         A: You can visit many places in Paris, such as the Eiffel Tower, the Louvre Museum, Notre-Dame Cathedral, the Champs-Elys√©es, Montmartre, Sacr√©-C≈ìur, and the Palace of Versailles.         A: You can visit many places in Paris, such as the Eiffel Tower, the Louvre Museum, Notre-Dame Cathedral, the Champs-Elys√©es, Montmartre, Sacr√©-C≈ìur, and the Palace of Versailles.  xinference terminate --model-uid "7167b2b0-2a04-11ee-83f0-d29396a3f064" xinference terminate --model-uid "7167b2b0-2a04-11ee-83f0-d29396a3f064"  Previous Writer Next Memory InstallationDeploy Xinference Locally or in a Distributed Cluster.WrapperIntegrate with a LLMChain InstallationDeploy Xinference Locally or in a Distributed Cluster.WrapperIntegrate with a LLMChain CommunityDiscordTwitterGitHubPythonJS/TSMoreHomepageBlogCopyright ¬© 2023 LangChain, Inc. CommunityDiscordTwitterGitHubPythonJS/TSMoreHomepageBlog CommunityDiscordTwitter Community GitHubPythonJS/TS GitHub MoreHomepageBlog More Copyright ¬© 2023 LangChain, Inc. Copyright ¬© 2023 LangChain, Inc. Skip to main content (#docusaurus_skipToContent_fallback) ü¶úÔ∏èüîó LangChain (/) Docs (/docs/get_started/introduction) Use cases (/docs/use_cases) Integrations (/docs/integrations) API (https://api.python.langchain.com) LangSmith (https://smith.langchain.com) JS/TS Docs (https://js.langchain.com/docs)  (https://github.com/hwchase17/langchain) Integrations (/docs/integrations) Callbacks (/docs/integrations/callbacks/) Chat models (/docs/integrations/chat/) Chat loaders (/docs/integrations/chat_loaders/) Document loaders (/docs/integrations/document_loaders/) Document transformers (/docs/integrations/document_transformers/) LLMs (/docs/integrations/llms/) AI21 (/docs/integrations/llms/ai21) Aleph Alpha (/docs/integrations/llms/aleph_alpha) Amazon API Gateway (/docs/integrations/llms/amazon_api_gateway) Anyscale (/docs/integrations/llms/anyscale) Azure ML (/docs/integrations/llms/azure_ml) Azure OpenAI (/docs/integrations/llms/azure_openai) Banana (/docs/integrations/llms/banana) Baseten (/docs/integrations/llms/baseten) Beam (/docs/integrations/llms/beam) Bedrock (/docs/integrations/llms/bedrock) Bittensor (/docs/integrations/llms/bittensor) CerebriumAI (/docs/integrations/llms/cerebriumai) ChatGLM (/docs/integrations/llms/chatglm) Clarifai (/docs/integrations/llms/clarifai) Cohere (/docs/integrations/llms/cohere) C Transformers (/docs/integrations/llms/ctransformers) CTranslate2 (/docs/integrations/llms/ctranslate2) Databricks (/docs/integrations/llms/databricks) DeepInfra (/docs/integrations/llms/deepinfra) DeepSparse (/docs/integrations/llms/deepsparse) Eden AI (/docs/integrations/llms/edenai) Fireworks (/docs/integrations/llms/fireworks) ForefrontAI (/docs/integrations/llms/forefrontai) Google Vertex AI PaLM (/docs/integrations/llms/google_vertex_ai_palm) GooseAI (/docs/integrations/llms/gooseai) GPT4All (/docs/integrations/llms/gpt4all) Hugging Face Hub (/docs/integrations/llms/huggingface_hub) Hugging Face Local Pipelines (/docs/integrations/llms/huggingface_pipelines) Huggingface TextGen Inference (/docs/integrations/llms/huggingface_textgen_inference) JSONFormer (/docs/integrations/llms/jsonformer_experimental) KoboldAI API (/docs/integrations/llms/koboldai) Llama.cpp (/docs/integrations/llms/llamacpp) LLM Caching integrations (/docs/integrations/llms/llm_caching) Manifest (/docs/integrations/llms/manifest) Minimax (/docs/integrations/llms/minimax) Modal (/docs/integrations/llms/modal) MosaicML (/docs/integrations/llms/mosaicml) NLP Cloud (/docs/integrations/llms/nlpcloud) OctoAI (/docs/integrations/llms/octoai) Ollama (/docs/integrations/llms/ollama) OpaquePrompts (/docs/integrations/llms/opaqueprompts) OpenAI (/docs/integrations/llms/openai) OpenLLM (/docs/integrations/llms/openllm) OpenLM (/docs/integrations/llms/openlm) Petals (/docs/integrations/llms/petals) PipelineAI (/docs/integrations/llms/pipelineai) Predibase (/docs/integrations/llms/predibase) Prediction Guard (/docs/integrations/llms/predictionguard) PromptLayer OpenAI (/docs/integrations/llms/promptlayer_openai) RELLM (/docs/integrations/llms/rellm_experimental) Replicate (/docs/integrations/llms/replicate) Runhouse (/docs/integrations/llms/runhouse) SageMakerEndpoint (/docs/integrations/llms/sagemaker) StochasticAI (/docs/integrations/llms/stochasticai) Nebula (Symbl.ai) (/docs/integrations/llms/symblai_nebula) TextGen (/docs/integrations/llms/textgen) Titan Takeoff (/docs/integrations/llms/titan_takeoff) Tongyi Qwen (/docs/integrations/llms/tongyi) vLLM (/docs/integrations/llms/vllm) Writer (/docs/integrations/llms/writer) Xorbits Inference (Xinference) (/docs/integrations/llms/xinference) Memory (/docs/integrations/memory/) Retrievers (/docs/integrations/retrievers/) Text embedding models (/docs/integrations/text_embedding/) Agents & Toolkits (/docs/integrations/toolkits/) Tools (/docs/integrations/tools/) Vector stores (/docs/integrations/vectorstores/) Grouped by provider (/docs/integrations/providers/)  (/) Integrations (/docs/integrations) LLMs (/docs/integrations/llms/) Xinference (https://github.com/xorbitsai/inference) ‚Äã (#installation) ‚Äã (#deploy-xinference-locally-or-in-a-distributed-cluster) Xinference (https://github.com/xorbitsai/inference) ‚Äã (#wrapper) Xinference (https://api.python.langchain.com/en/latest/llms/langchain.llms.xinference.Xinference.html) ‚Äã (#integrate-with-a-llmchain) PreviousWriter (/docs/integrations/llms/writer) NextMemory (/docs/integrations/memory/) Installation (#installation) Deploy Xinference Locally or in a Distributed Cluster. (#deploy-xinference-locally-or-in-a-distributed-cluster) Wrapper (#wrapper) Integrate with a LLMChain (#integrate-with-a-llmchain) Discord (https://discord.gg/cU2adEyC7w) Twitter (https://twitter.com/LangChainAI) Python (https://github.com/hwchase17/langchain) JS/TS (https://github.com/hwchase17/langchainjs) Homepage (https://langchain.com) Blog (https://blog.langchain.dev)