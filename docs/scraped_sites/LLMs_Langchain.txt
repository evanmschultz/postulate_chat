AI21 Studio provides API access to Jurassic-2 large language models. The Luminous series is a family of large language models. Amazon API Gateway is a fully managed service that makes it easy for developers to create, publish, maintain, monitor, and secure APIs at any scale. APIs act as the "front door" for applications to access data, business logic, or functionality from your backend services. Using API Gateway, you can create RESTful APIs and WebSocket APIs that enable real-time two-way communication applications. API Gateway supports containerized and serverless workloads, as well as web applications. Anyscale is a fully-managed Ray platform, on which you can build, deploy, and manage scalable AI and Python applications Azure ML is a platform used to build, train, and deploy machine learning models. Users can explore the types of models to deploy in the Model Catalog, which provides Azure Foundation Models and OpenAI Models. Azure Foundation Models include various open-source models and popular Hugging Face models. Users can also import models of their liking into AzureML. This notebook goes over how to use Langchain with Azure OpenAI. Banana is focused on building the machine learning infrastructure. Baseten provides all the infrastructure you need to deploy and serve ML models performantly, scalably, and cost-efficiently. Calls the Beam API wrapper to deploy and make subsequent calls to an instance of the gpt2 LLM in a cloud deployment. Requires installation of the Beam library and registration of Beam Client ID and Client Secret. By calling the wrapper an instance of the model is created and run, with returned text relating to the prompt. Additional calls can then be made by directly calling the Beam API. Amazon Bedrock is a fully managed service that makes FMs from leading AI startups and Amazon available via an API, so you can choose from a wide range of FMs to find the model that is best suited for your use case Bittensor is a mining network, similar to Bitcoin, that includes built-in incentives designed to encourage miners to contribute compute + knowledge. Cerebrium is an AWS Sagemaker alternative. It also provides API access to several LLM models. ChatGLM-6B is an open bilingual language model based on General Language Model (GLM) framework, with 6.2 billion parameters. With the quantization technique, users can deploy locally on consumer-grade graphics cards (only 6GB of GPU memory is required at the INT4 quantization level). Clarifai is an AI Platform that provides the full AI lifecycle ranging from data exploration, data labeling, model training, evaluation, and inference. Cohere is a Canadian startup that provides natural language processing models that help companies improve human-machine interactions. The C Transformers library provides Python bindings for GGML models. CTranslate2 is a C++ and Python library for efficient inference with Transformer models. The Databricks Lakehouse Platform unifies data, analytics, and AI on one platform. DeepInfra provides several LLMs. This page covers how to use the DeepSparse inference runtime within LangChain. Eden AI is revolutionizing the AI landscape by uniting the best AI providers, empowering users to unlock limitless possibilities and tap into the true potential of artificial intelligence. With an all-in-one comprehensive and hassle-free platform, it allows users to deploy AI features to production lightning fast, enabling effortless access to the full breadth of AI capabilities via a single API. (website//edenai.co/) Fireworks accelerates product development on generative AI by creating an innovative AI experiment and production platform. The Forefront platform gives you the ability to fine-tune and use open source large language models. Note: This is seperate from the Google PaLM integration, it exposes Vertex AI PaLM API on Google Cloud. GooseAI is a fully managed NLP-as-a-Service, delivered via API. GooseAI provides access to these models. GitHub:nomic-ai/gpt4all an ecosystem of open-source chatbots trained on a massive collections of clean assistant data including code, stories and dialogue. The Hugging Face Hub is a platform with over 120k models, 20k datasets, and 50k demo apps (Spaces), all open source and publicly available, in an online platform where people can easily collaborate and build ML together. Hugging Face models can be run locally through the HuggingFacePipeline class. Text Generation Inference is a Rust, Python and gRPC server for text generation inference. Used in production at HuggingFace to power LLMs api-inference widgets. JSONFormer is a library that wraps local HuggingFace pipeline models for structured decoding of a subset of the JSON Schema. KoboldAI is a "a browser-based front-end for AI-assisted writing with multiple local & remote AI models...". It has a public and local API that is able to be used in langchain. llama-cpp-python is a Python binding for llama.cpp. This notebook covers how to cache results of individual LLM calls using different caches. This notebook goes over how to use Manifest and LangChain. Minimax is a Chinese startup that provides natural language processing models for companies and individuals. The Modal cloud platform provides convenient, on-demand access to serverless cloud compute from Python scripts on your local computer. MosaicML offers a managed inference service. You can either use a variety of open source models, or deploy your own. The NLP Cloud serves high performance pre-trained or custom models for NER, sentiment-analysis, classification, summarization, paraphrasing, grammar and spelling correction, keywords and keyphrases extraction, chatbot, product description and ad generation, intent classification, text generation, image generation, blog post generation, code generation, question answering, automatic speech recognition, machine translation, language detection, semantic search, semantic similarity, tokenization, POS tagging, embeddings, and dependency parsing. It is ready for production, served through a REST API. OctoML is a service with efficient compute. It enables users to integrate their choice of AI models into applications. The OctoAI compute service helps you run, tune, and scale AI applications. Ollama allows you to run open-source large language models, such as Llama 2, locally. OpaquePrompts is a service that enables applications to leverage the power of language models without compromising user privacy. Designed for composability and ease of integration into existing applications and services, OpaquePrompts is consumable via a simple Python library as well as through LangChain. Perhaps more importantly, OpaquePrompts leverages the power of confidential computing to ensure that even the OpaquePrompts service itself cannot access the data it is protecting. OpenAI offers a spectrum of models with different levels of power suitable for different tasks. 🦾 OpenLLM is an open platform for operating large language models (LLMs) in production. It enables developers to easily run inference with any open-source LLMs, deploy to the cloud or on-premises, and build powerful AI apps. OpenLM is a zero-dependency OpenAI-compatible LLM provider that can call different inference endpoints directly via HTTP. Petals runs 100B+ language models at home, BitTorrent-style. PipelineAI allows you to run your ML models at scale in the cloud. It also provides API access to several LLM models. Predibase allows you to train, finetune, and deploy any ML model—from linear regression to large language model. Basic LLM usage PromptLayer is the first platform that allows you to track, manage, and share your GPT prompt engineering. PromptLayer acts a middleware between your code and OpenAI’s python library. RELLM is a library that wraps local Hugging Face pipeline models for structured decoding. Replicate runs machine learning models in the cloud. We have a library of open-source models that you can run with a few lines of code. If you're building your own machine learning models, Replicate makes it easy to deploy them at scale. The Runhouse allows remote compute and data across environments and users. See the Runhouse docs. Amazon SageMaker is a system that can build, train, and deploy machine learning (ML) models for any use case with fully managed infrastructure, tools, and workflows. Stochastic Acceleration Platform aims to simplify the life cycle of a Deep Learning model. From uploading and versioning the model, through training, compression and acceleration to putting it into production. Nebula is a large language model (LLM) built by Symbl.ai. It is trained to perform generative tasks on human conversations. Nebula excels at modeling the nuanced details of a conversation and performing tasks on the conversation. GitHub:oobabooga/text-generation-webui A gradio web UI for running Large Language Models like LLaMA, llama.cpp, GPT-J, Pythia, OPT, and GALACTICA. TitanML helps businesses build and deploy better, smaller, cheaper, and faster NLP models through our training, compression, and inference optimization platform. Tongyi Qwen is a large-scale language model developed by Alibaba's Damo Academy. It is capable of understanding user intent through natural language understanding and semantic analysis, based on user input in natural language. It provides services and assistance to users in different domains and tasks. By providing clear and detailed instructions, you can obtain results that better align with your expectations. vLLM is a fast and easy-to-use library for LLM inference and serving, offering: Writer is a platform to generate different language content. Xinference is a powerful and versatile library designed to serve LLMs, IntegrationsCallbacksChat modelsChat loadersDocument loadersDocument transformersLLMsAI21Aleph AlphaAmazon API GatewayAnyscaleAzure MLAzure OpenAIBananaBasetenBeamBedrockBittensorCerebriumAIChatGLMClarifaiCohereC TransformersCTranslate2DatabricksDeepInfraDeepSparseEden AIFireworksForefrontAIGoogle Vertex AI PaLMGooseAIGPT4AllHugging Face HubHugging Face Local PipelinesHuggingface TextGen InferenceJSONFormerKoboldAI APILlama.cppLLM Caching integrationsManifestMinimaxModalMosaicMLNLP CloudOctoAIOllamaOpaquePromptsOpenAIOpenLLMOpenLMPetalsPipelineAIPredibasePrediction GuardPromptLayer OpenAIRELLMReplicateRunhouseSageMakerEndpointStochasticAINebula (Symbl.ai)TextGenTitan TakeoffTongyi QwenvLLMWriterXorbits Inference (Xinference)MemoryRetrieversText embedding modelsAgents & ToolkitsToolsVector storesGrouped by provider Callbacks Chat models Chat loaders Document loaders Document transformers LLMsAI21Aleph AlphaAmazon API GatewayAnyscaleAzure MLAzure OpenAIBananaBasetenBeamBedrockBittensorCerebriumAIChatGLMClarifaiCohereC TransformersCTranslate2DatabricksDeepInfraDeepSparseEden AIFireworksForefrontAIGoogle Vertex AI PaLMGooseAIGPT4AllHugging Face HubHugging Face Local PipelinesHuggingface TextGen InferenceJSONFormerKoboldAI APILlama.cppLLM Caching integrationsManifestMinimaxModalMosaicMLNLP CloudOctoAIOllamaOpaquePromptsOpenAIOpenLLMOpenLMPetalsPipelineAIPredibasePrediction GuardPromptLayer OpenAIRELLMReplicateRunhouseSageMakerEndpointStochasticAINebula (Symbl.ai)TextGenTitan TakeoffTongyi QwenvLLMWriterXorbits Inference (Xinference) AI21 Aleph Alpha Amazon API Gateway Anyscale Azure ML Azure OpenAI Banana Baseten Beam Bedrock Bittensor CerebriumAI ChatGLM Clarifai Cohere C Transformers CTranslate2 Databricks DeepInfra DeepSparse Eden AI Fireworks ForefrontAI Google Vertex AI PaLM GooseAI GPT4All Hugging Face Hub Hugging Face Local Pipelines Huggingface TextGen Inference JSONFormer KoboldAI API Llama.cpp LLM Caching integrations Manifest Minimax Modal MosaicML NLP Cloud OctoAI Ollama OpaquePrompts OpenAI OpenLLM OpenLM Petals PipelineAI Predibase Prediction Guard PromptLayer OpenAI RELLM Replicate Runhouse SageMakerEndpoint StochasticAI Nebula (Symbl.ai) TextGen Titan Takeoff Tongyi Qwen vLLM Writer Xorbits Inference (Xinference) Memory Retrievers Text embedding models Agents & Toolkits Tools Vector stores Grouped by provider  Integrations LLMs Discord Twitter Python JS/TS Homepage Blog Skip to main content🦜️🔗 LangChainDocsUse casesIntegrationsAPILangSmithJS/TS DocsCTRLKIntegrationsCallbacksChat modelsChat loadersDocument loadersDocument transformersLLMsAI21Aleph AlphaAmazon API GatewayAnyscaleAzure MLAzure OpenAIBananaBasetenBeamBedrockBittensorCerebriumAIChatGLMClarifaiCohereC TransformersCTranslate2DatabricksDeepInfraDeepSparseEden AIFireworksForefrontAIGoogle Vertex AI PaLMGooseAIGPT4AllHugging Face HubHugging Face Local PipelinesHuggingface TextGen InferenceJSONFormerKoboldAI APILlama.cppLLM Caching integrationsManifestMinimaxModalMosaicMLNLP CloudOctoAIOllamaOpaquePromptsOpenAIOpenLLMOpenLMPetalsPipelineAIPredibasePrediction GuardPromptLayer OpenAIRELLMReplicateRunhouseSageMakerEndpointStochasticAINebula (Symbl.ai)TextGenTitan TakeoffTongyi QwenvLLMWriterXorbits Inference (Xinference)MemoryRetrieversText embedding modelsAgents & ToolkitsToolsVector storesGrouped by providerIntegrationsLLMsLLMs📄️ AI21AI21 Studio provides API access to Jurassic-2 large language models.📄️ Aleph AlphaThe Luminous series is a family of large language models.📄️ Amazon API GatewayAmazon API Gateway is a fully managed service that makes it easy for developers to create, publish, maintain, monitor, and secure APIs at any scale. APIs act as the "front door" for applications to access data, business logic, or functionality from your backend services. Using API Gateway, you can create RESTful APIs and WebSocket APIs that enable real-time two-way communication applications. API Gateway supports containerized and serverless workloads, as well as web applications.📄️ AnyscaleAnyscale is a fully-managed Ray platform, on which you can build, deploy, and manage scalable AI and Python applications📄️ Azure MLAzure ML is a platform used to build, train, and deploy machine learning models. Users can explore the types of models to deploy in the Model Catalog, which provides Azure Foundation Models and OpenAI Models. Azure Foundation Models include various open-source models and popular Hugging Face models. Users can also import models of their liking into AzureML.📄️ Azure OpenAIThis notebook goes over how to use Langchain with Azure OpenAI.📄️ BananaBanana is focused on building the machine learning infrastructure.📄️ BasetenBaseten provides all the infrastructure you need to deploy and serve ML models performantly, scalably, and cost-efficiently.📄️ BeamCalls the Beam API wrapper to deploy and make subsequent calls to an instance of the gpt2 LLM in a cloud deployment. Requires installation of the Beam library and registration of Beam Client ID and Client Secret. By calling the wrapper an instance of the model is created and run, with returned text relating to the prompt. Additional calls can then be made by directly calling the Beam API.📄️ BedrockAmazon Bedrock is a fully managed service that makes FMs from leading AI startups and Amazon available via an API, so you can choose from a wide range of FMs to find the model that is best suited for your use case📄️ BittensorBittensor is a mining network, similar to Bitcoin, that includes built-in incentives designed to encourage miners to contribute compute + knowledge.📄️ CerebriumAICerebrium is an AWS Sagemaker alternative. It also provides API access to several LLM models.📄️ ChatGLMChatGLM-6B is an open bilingual language model based on General Language Model (GLM) framework, with 6.2 billion parameters. With the quantization technique, users can deploy locally on consumer-grade graphics cards (only 6GB of GPU memory is required at the INT4 quantization level).📄️ ClarifaiClarifai is an AI Platform that provides the full AI lifecycle ranging from data exploration, data labeling, model training, evaluation, and inference.📄️ CohereCohere is a Canadian startup that provides natural language processing models that help companies improve human-machine interactions.📄️ C TransformersThe C Transformers library provides Python bindings for GGML models.📄️ CTranslate2CTranslate2 is a C++ and Python library for efficient inference with Transformer models.📄️ DatabricksThe Databricks Lakehouse Platform unifies data, analytics, and AI on one platform.📄️ DeepInfraDeepInfra provides several LLMs.📄️ DeepSparseThis page covers how to use the DeepSparse inference runtime within LangChain.📄️ Eden AIEden AI is revolutionizing the AI landscape by uniting the best AI providers, empowering users to unlock limitless possibilities and tap into the true potential of artificial intelligence. With an all-in-one comprehensive and hassle-free platform, it allows users to deploy AI features to production lightning fast, enabling effortless access to the full breadth of AI capabilities via a single API. (website//edenai.co/)📄️ FireworksFireworks accelerates product development on generative AI by creating an innovative AI experiment and production platform.📄️ ForefrontAIThe Forefront platform gives you the ability to fine-tune and use open source large language models.📄️ Google Vertex AI PaLMNote: This is seperate from the Google PaLM integration, it exposes Vertex AI PaLM API on Google Cloud.📄️ GooseAIGooseAI is a fully managed NLP-as-a-Service, delivered via API. GooseAI provides access to these models.📄️ GPT4AllGitHub:nomic-ai/gpt4all an ecosystem of open-source chatbots trained on a massive collections of clean assistant data including code, stories and dialogue.📄️ Hugging Face HubThe Hugging Face Hub is a platform with over 120k models, 20k datasets, and 50k demo apps (Spaces), all open source and publicly available, in an online platform where people can easily collaborate and build ML together.📄️ Hugging Face Local PipelinesHugging Face models can be run locally through the HuggingFacePipeline class.📄️ Huggingface TextGen InferenceText Generation Inference is a Rust, Python and gRPC server for text generation inference. Used in production at HuggingFace to power LLMs api-inference widgets.📄️ JSONFormerJSONFormer is a library that wraps local HuggingFace pipeline models for structured decoding of a subset of the JSON Schema.📄️ KoboldAI APIKoboldAI is a "a browser-based front-end for AI-assisted writing with multiple local & remote AI models...". It has a public and local API that is able to be used in langchain.📄️ Llama.cppllama-cpp-python is a Python binding for llama.cpp.📄️ LLM Caching integrationsThis notebook covers how to cache results of individual LLM calls using different caches.📄️ ManifestThis notebook goes over how to use Manifest and LangChain.📄️ MinimaxMinimax is a Chinese startup that provides natural language processing models for companies and individuals.📄️ ModalThe Modal cloud platform provides convenient, on-demand access to serverless cloud compute from Python scripts on your local computer.📄️ MosaicMLMosaicML offers a managed inference service. You can either use a variety of open source models, or deploy your own.📄️ NLP CloudThe NLP Cloud serves high performance pre-trained or custom models for NER, sentiment-analysis, classification, summarization, paraphrasing, grammar and spelling correction, keywords and keyphrases extraction, chatbot, product description and ad generation, intent classification, text generation, image generation, blog post generation, code generation, question answering, automatic speech recognition, machine translation, language detection, semantic search, semantic similarity, tokenization, POS tagging, embeddings, and dependency parsing. It is ready for production, served through a REST API.📄️ OctoAIOctoML is a service with efficient compute. It enables users to integrate their choice of AI models into applications. The OctoAI compute service helps you run, tune, and scale AI applications.📄️ OllamaOllama allows you to run open-source large language models, such as Llama 2, locally.📄️ OpaquePromptsOpaquePrompts is a service that enables applications to leverage the power of language models without compromising user privacy. Designed for composability and ease of integration into existing applications and services, OpaquePrompts is consumable via a simple Python library as well as through LangChain. Perhaps more importantly, OpaquePrompts leverages the power of confidential computing to ensure that even the OpaquePrompts service itself cannot access the data it is protecting.📄️ OpenAIOpenAI offers a spectrum of models with different levels of power suitable for different tasks.📄️ OpenLLM🦾 OpenLLM is an open platform for operating large language models (LLMs) in production. It enables developers to easily run inference with any open-source LLMs, deploy to the cloud or on-premises, and build powerful AI apps.📄️ OpenLMOpenLM is a zero-dependency OpenAI-compatible LLM provider that can call different inference endpoints directly via HTTP.📄️ PetalsPetals runs 100B+ language models at home, BitTorrent-style.📄️ PipelineAIPipelineAI allows you to run your ML models at scale in the cloud. It also provides API access to several LLM models.📄️ PredibasePredibase allows you to train, finetune, and deploy any ML model—from linear regression to large language model.📄️ Prediction GuardBasic LLM usage📄️ PromptLayer OpenAIPromptLayer is the first platform that allows you to track, manage, and share your GPT prompt engineering. PromptLayer acts a middleware between your code and OpenAI’s python library.📄️ RELLMRELLM is a library that wraps local Hugging Face pipeline models for structured decoding.📄️ ReplicateReplicate runs machine learning models in the cloud. We have a library of open-source models that you can run with a few lines of code. If you're building your own machine learning models, Replicate makes it easy to deploy them at scale.📄️ RunhouseThe Runhouse allows remote compute and data across environments and users. See the Runhouse docs.📄️ SageMakerEndpointAmazon SageMaker is a system that can build, train, and deploy machine learning (ML) models for any use case with fully managed infrastructure, tools, and workflows.📄️ StochasticAIStochastic Acceleration Platform aims to simplify the life cycle of a Deep Learning model. From uploading and versioning the model, through training, compression and acceleration to putting it into production.📄️ Nebula (Symbl.ai)Nebula is a large language model (LLM) built by Symbl.ai. It is trained to perform generative tasks on human conversations. Nebula excels at modeling the nuanced details of a conversation and performing tasks on the conversation.📄️ TextGenGitHub:oobabooga/text-generation-webui A gradio web UI for running Large Language Models like LLaMA, llama.cpp, GPT-J, Pythia, OPT, and GALACTICA.📄️ Titan TakeoffTitanML helps businesses build and deploy better, smaller, cheaper, and faster NLP models through our training, compression, and inference optimization platform.📄️ Tongyi QwenTongyi Qwen is a large-scale language model developed by Alibaba's Damo Academy. It is capable of understanding user intent through natural language understanding and semantic analysis, based on user input in natural language. It provides services and assistance to users in different domains and tasks. By providing clear and detailed instructions, you can obtain results that better align with your expectations.📄️ vLLMvLLM is a fast and easy-to-use library for LLM inference and serving, offering:📄️ WriterWriter is a platform to generate different language content.📄️ Xorbits Inference (Xinference)Xinference is a powerful and versatile library designed to serve LLMs,PreviousOpenAI Functions Metadata TaggerNextAI21CommunityDiscordTwitterGitHubPythonJS/TSMoreHomepageBlogCopyright © 2023 LangChain, Inc. Skip to main content 🦜️🔗 LangChainDocsUse casesIntegrationsAPILangSmithJS/TS DocsCTRLK 🦜️🔗 LangChainDocsUse casesIntegrationsAPI LangSmithJS/TS DocsCTRLK  CTRLK CTRLK  CTRLK   IntegrationsCallbacksChat modelsChat loadersDocument loadersDocument transformersLLMsAI21Aleph AlphaAmazon API GatewayAnyscaleAzure MLAzure OpenAIBananaBasetenBeamBedrockBittensorCerebriumAIChatGLMClarifaiCohereC TransformersCTranslate2DatabricksDeepInfraDeepSparseEden AIFireworksForefrontAIGoogle Vertex AI PaLMGooseAIGPT4AllHugging Face HubHugging Face Local PipelinesHuggingface TextGen InferenceJSONFormerKoboldAI APILlama.cppLLM Caching integrationsManifestMinimaxModalMosaicMLNLP CloudOctoAIOllamaOpaquePromptsOpenAIOpenLLMOpenLMPetalsPipelineAIPredibasePrediction GuardPromptLayer OpenAIRELLMReplicateRunhouseSageMakerEndpointStochasticAINebula (Symbl.ai)TextGenTitan TakeoffTongyi QwenvLLMWriterXorbits Inference (Xinference)MemoryRetrieversText embedding modelsAgents & ToolkitsToolsVector storesGrouped by providerIntegrationsLLMsLLMs📄️ AI21AI21 Studio provides API access to Jurassic-2 large language models.📄️ Aleph AlphaThe Luminous series is a family of large language models.📄️ Amazon API GatewayAmazon API Gateway is a fully managed service that makes it easy for developers to create, publish, maintain, monitor, and secure APIs at any scale. APIs act as the "front door" for applications to access data, business logic, or functionality from your backend services. Using API Gateway, you can create RESTful APIs and WebSocket APIs that enable real-time two-way communication applications. API Gateway supports containerized and serverless workloads, as well as web applications.📄️ AnyscaleAnyscale is a fully-managed Ray platform, on which you can build, deploy, and manage scalable AI and Python applications📄️ Azure MLAzure ML is a platform used to build, train, and deploy machine learning models. Users can explore the types of models to deploy in the Model Catalog, which provides Azure Foundation Models and OpenAI Models. Azure Foundation Models include various open-source models and popular Hugging Face models. Users can also import models of their liking into AzureML.📄️ Azure OpenAIThis notebook goes over how to use Langchain with Azure OpenAI.📄️ BananaBanana is focused on building the machine learning infrastructure.📄️ BasetenBaseten provides all the infrastructure you need to deploy and serve ML models performantly, scalably, and cost-efficiently.📄️ BeamCalls the Beam API wrapper to deploy and make subsequent calls to an instance of the gpt2 LLM in a cloud deployment. Requires installation of the Beam library and registration of Beam Client ID and Client Secret. By calling the wrapper an instance of the model is created and run, with returned text relating to the prompt. Additional calls can then be made by directly calling the Beam API.📄️ BedrockAmazon Bedrock is a fully managed service that makes FMs from leading AI startups and Amazon available via an API, so you can choose from a wide range of FMs to find the model that is best suited for your use case📄️ BittensorBittensor is a mining network, similar to Bitcoin, that includes built-in incentives designed to encourage miners to contribute compute + knowledge.📄️ CerebriumAICerebrium is an AWS Sagemaker alternative. It also provides API access to several LLM models.📄️ ChatGLMChatGLM-6B is an open bilingual language model based on General Language Model (GLM) framework, with 6.2 billion parameters. With the quantization technique, users can deploy locally on consumer-grade graphics cards (only 6GB of GPU memory is required at the INT4 quantization level).📄️ ClarifaiClarifai is an AI Platform that provides the full AI lifecycle ranging from data exploration, data labeling, model training, evaluation, and inference.📄️ CohereCohere is a Canadian startup that provides natural language processing models that help companies improve human-machine interactions.📄️ C TransformersThe C Transformers library provides Python bindings for GGML models.📄️ CTranslate2CTranslate2 is a C++ and Python library for efficient inference with Transformer models.📄️ DatabricksThe Databricks Lakehouse Platform unifies data, analytics, and AI on one platform.📄️ DeepInfraDeepInfra provides several LLMs.📄️ DeepSparseThis page covers how to use the DeepSparse inference runtime within LangChain.📄️ Eden AIEden AI is revolutionizing the AI landscape by uniting the best AI providers, empowering users to unlock limitless possibilities and tap into the true potential of artificial intelligence. With an all-in-one comprehensive and hassle-free platform, it allows users to deploy AI features to production lightning fast, enabling effortless access to the full breadth of AI capabilities via a single API. (website//edenai.co/)📄️ FireworksFireworks accelerates product development on generative AI by creating an innovative AI experiment and production platform.📄️ ForefrontAIThe Forefront platform gives you the ability to fine-tune and use open source large language models.📄️ Google Vertex AI PaLMNote: This is seperate from the Google PaLM integration, it exposes Vertex AI PaLM API on Google Cloud.📄️ GooseAIGooseAI is a fully managed NLP-as-a-Service, delivered via API. GooseAI provides access to these models.📄️ GPT4AllGitHub:nomic-ai/gpt4all an ecosystem of open-source chatbots trained on a massive collections of clean assistant data including code, stories and dialogue.📄️ Hugging Face HubThe Hugging Face Hub is a platform with over 120k models, 20k datasets, and 50k demo apps (Spaces), all open source and publicly available, in an online platform where people can easily collaborate and build ML together.📄️ Hugging Face Local PipelinesHugging Face models can be run locally through the HuggingFacePipeline class.📄️ Huggingface TextGen InferenceText Generation Inference is a Rust, Python and gRPC server for text generation inference. Used in production at HuggingFace to power LLMs api-inference widgets.📄️ JSONFormerJSONFormer is a library that wraps local HuggingFace pipeline models for structured decoding of a subset of the JSON Schema.📄️ KoboldAI APIKoboldAI is a "a browser-based front-end for AI-assisted writing with multiple local & remote AI models...". It has a public and local API that is able to be used in langchain.📄️ Llama.cppllama-cpp-python is a Python binding for llama.cpp.📄️ LLM Caching integrationsThis notebook covers how to cache results of individual LLM calls using different caches.📄️ ManifestThis notebook goes over how to use Manifest and LangChain.📄️ MinimaxMinimax is a Chinese startup that provides natural language processing models for companies and individuals.📄️ ModalThe Modal cloud platform provides convenient, on-demand access to serverless cloud compute from Python scripts on your local computer.📄️ MosaicMLMosaicML offers a managed inference service. You can either use a variety of open source models, or deploy your own.📄️ NLP CloudThe NLP Cloud serves high performance pre-trained or custom models for NER, sentiment-analysis, classification, summarization, paraphrasing, grammar and spelling correction, keywords and keyphrases extraction, chatbot, product description and ad generation, intent classification, text generation, image generation, blog post generation, code generation, question answering, automatic speech recognition, machine translation, language detection, semantic search, semantic similarity, tokenization, POS tagging, embeddings, and dependency parsing. It is ready for production, served through a REST API.📄️ OctoAIOctoML is a service with efficient compute. It enables users to integrate their choice of AI models into applications. The OctoAI compute service helps you run, tune, and scale AI applications.📄️ OllamaOllama allows you to run open-source large language models, such as Llama 2, locally.📄️ OpaquePromptsOpaquePrompts is a service that enables applications to leverage the power of language models without compromising user privacy. Designed for composability and ease of integration into existing applications and services, OpaquePrompts is consumable via a simple Python library as well as through LangChain. Perhaps more importantly, OpaquePrompts leverages the power of confidential computing to ensure that even the OpaquePrompts service itself cannot access the data it is protecting.📄️ OpenAIOpenAI offers a spectrum of models with different levels of power suitable for different tasks.📄️ OpenLLM🦾 OpenLLM is an open platform for operating large language models (LLMs) in production. It enables developers to easily run inference with any open-source LLMs, deploy to the cloud or on-premises, and build powerful AI apps.📄️ OpenLMOpenLM is a zero-dependency OpenAI-compatible LLM provider that can call different inference endpoints directly via HTTP.📄️ PetalsPetals runs 100B+ language models at home, BitTorrent-style.📄️ PipelineAIPipelineAI allows you to run your ML models at scale in the cloud. It also provides API access to several LLM models.📄️ PredibasePredibase allows you to train, finetune, and deploy any ML model—from linear regression to large language model.📄️ Prediction GuardBasic LLM usage📄️ PromptLayer OpenAIPromptLayer is the first platform that allows you to track, manage, and share your GPT prompt engineering. PromptLayer acts a middleware between your code and OpenAI’s python library.📄️ RELLMRELLM is a library that wraps local Hugging Face pipeline models for structured decoding.📄️ ReplicateReplicate runs machine learning models in the cloud. We have a library of open-source models that you can run with a few lines of code. If you're building your own machine learning models, Replicate makes it easy to deploy them at scale.📄️ RunhouseThe Runhouse allows remote compute and data across environments and users. See the Runhouse docs.📄️ SageMakerEndpointAmazon SageMaker is a system that can build, train, and deploy machine learning (ML) models for any use case with fully managed infrastructure, tools, and workflows.📄️ StochasticAIStochastic Acceleration Platform aims to simplify the life cycle of a Deep Learning model. From uploading and versioning the model, through training, compression and acceleration to putting it into production.📄️ Nebula (Symbl.ai)Nebula is a large language model (LLM) built by Symbl.ai. It is trained to perform generative tasks on human conversations. Nebula excels at modeling the nuanced details of a conversation and performing tasks on the conversation.📄️ TextGenGitHub:oobabooga/text-generation-webui A gradio web UI for running Large Language Models like LLaMA, llama.cpp, GPT-J, Pythia, OPT, and GALACTICA.📄️ Titan TakeoffTitanML helps businesses build and deploy better, smaller, cheaper, and faster NLP models through our training, compression, and inference optimization platform.📄️ Tongyi QwenTongyi Qwen is a large-scale language model developed by Alibaba's Damo Academy. It is capable of understanding user intent through natural language understanding and semantic analysis, based on user input in natural language. It provides services and assistance to users in different domains and tasks. By providing clear and detailed instructions, you can obtain results that better align with your expectations.📄️ vLLMvLLM is a fast and easy-to-use library for LLM inference and serving, offering:📄️ WriterWriter is a platform to generate different language content.📄️ Xorbits Inference (Xinference)Xinference is a powerful and versatile library designed to serve LLMs,PreviousOpenAI Functions Metadata TaggerNextAI21 IntegrationsCallbacksChat modelsChat loadersDocument loadersDocument transformersLLMsAI21Aleph AlphaAmazon API GatewayAnyscaleAzure MLAzure OpenAIBananaBasetenBeamBedrockBittensorCerebriumAIChatGLMClarifaiCohereC TransformersCTranslate2DatabricksDeepInfraDeepSparseEden AIFireworksForefrontAIGoogle Vertex AI PaLMGooseAIGPT4AllHugging Face HubHugging Face Local PipelinesHuggingface TextGen InferenceJSONFormerKoboldAI APILlama.cppLLM Caching integrationsManifestMinimaxModalMosaicMLNLP CloudOctoAIOllamaOpaquePromptsOpenAIOpenLLMOpenLMPetalsPipelineAIPredibasePrediction GuardPromptLayer OpenAIRELLMReplicateRunhouseSageMakerEndpointStochasticAINebula (Symbl.ai)TextGenTitan TakeoffTongyi QwenvLLMWriterXorbits Inference (Xinference)MemoryRetrieversText embedding modelsAgents & ToolkitsToolsVector storesGrouped by providerIntegrationsLLMsLLMs📄️ AI21AI21 Studio provides API access to Jurassic-2 large language models.📄️ Aleph AlphaThe Luminous series is a family of large language models.📄️ Amazon API GatewayAmazon API Gateway is a fully managed service that makes it easy for developers to create, publish, maintain, monitor, and secure APIs at any scale. APIs act as the "front door" for applications to access data, business logic, or functionality from your backend services. Using API Gateway, you can create RESTful APIs and WebSocket APIs that enable real-time two-way communication applications. API Gateway supports containerized and serverless workloads, as well as web applications.📄️ AnyscaleAnyscale is a fully-managed Ray platform, on which you can build, deploy, and manage scalable AI and Python applications📄️ Azure MLAzure ML is a platform used to build, train, and deploy machine learning models. Users can explore the types of models to deploy in the Model Catalog, which provides Azure Foundation Models and OpenAI Models. Azure Foundation Models include various open-source models and popular Hugging Face models. Users can also import models of their liking into AzureML.📄️ Azure OpenAIThis notebook goes over how to use Langchain with Azure OpenAI.📄️ BananaBanana is focused on building the machine learning infrastructure.📄️ BasetenBaseten provides all the infrastructure you need to deploy and serve ML models performantly, scalably, and cost-efficiently.📄️ BeamCalls the Beam API wrapper to deploy and make subsequent calls to an instance of the gpt2 LLM in a cloud deployment. Requires installation of the Beam library and registration of Beam Client ID and Client Secret. By calling the wrapper an instance of the model is created and run, with returned text relating to the prompt. Additional calls can then be made by directly calling the Beam API.📄️ BedrockAmazon Bedrock is a fully managed service that makes FMs from leading AI startups and Amazon available via an API, so you can choose from a wide range of FMs to find the model that is best suited for your use case📄️ BittensorBittensor is a mining network, similar to Bitcoin, that includes built-in incentives designed to encourage miners to contribute compute + knowledge.📄️ CerebriumAICerebrium is an AWS Sagemaker alternative. It also provides API access to several LLM models.📄️ ChatGLMChatGLM-6B is an open bilingual language model based on General Language Model (GLM) framework, with 6.2 billion parameters. With the quantization technique, users can deploy locally on consumer-grade graphics cards (only 6GB of GPU memory is required at the INT4 quantization level).📄️ ClarifaiClarifai is an AI Platform that provides the full AI lifecycle ranging from data exploration, data labeling, model training, evaluation, and inference.📄️ CohereCohere is a Canadian startup that provides natural language processing models that help companies improve human-machine interactions.📄️ C TransformersThe C Transformers library provides Python bindings for GGML models.📄️ CTranslate2CTranslate2 is a C++ and Python library for efficient inference with Transformer models.📄️ DatabricksThe Databricks Lakehouse Platform unifies data, analytics, and AI on one platform.📄️ DeepInfraDeepInfra provides several LLMs.📄️ DeepSparseThis page covers how to use the DeepSparse inference runtime within LangChain.📄️ Eden AIEden AI is revolutionizing the AI landscape by uniting the best AI providers, empowering users to unlock limitless possibilities and tap into the true potential of artificial intelligence. With an all-in-one comprehensive and hassle-free platform, it allows users to deploy AI features to production lightning fast, enabling effortless access to the full breadth of AI capabilities via a single API. (website//edenai.co/)📄️ FireworksFireworks accelerates product development on generative AI by creating an innovative AI experiment and production platform.📄️ ForefrontAIThe Forefront platform gives you the ability to fine-tune and use open source large language models.📄️ Google Vertex AI PaLMNote: This is seperate from the Google PaLM integration, it exposes Vertex AI PaLM API on Google Cloud.📄️ GooseAIGooseAI is a fully managed NLP-as-a-Service, delivered via API. GooseAI provides access to these models.📄️ GPT4AllGitHub:nomic-ai/gpt4all an ecosystem of open-source chatbots trained on a massive collections of clean assistant data including code, stories and dialogue.📄️ Hugging Face HubThe Hugging Face Hub is a platform with over 120k models, 20k datasets, and 50k demo apps (Spaces), all open source and publicly available, in an online platform where people can easily collaborate and build ML together.📄️ Hugging Face Local PipelinesHugging Face models can be run locally through the HuggingFacePipeline class.📄️ Huggingface TextGen InferenceText Generation Inference is a Rust, Python and gRPC server for text generation inference. Used in production at HuggingFace to power LLMs api-inference widgets.📄️ JSONFormerJSONFormer is a library that wraps local HuggingFace pipeline models for structured decoding of a subset of the JSON Schema.📄️ KoboldAI APIKoboldAI is a "a browser-based front-end for AI-assisted writing with multiple local & remote AI models...". It has a public and local API that is able to be used in langchain.📄️ Llama.cppllama-cpp-python is a Python binding for llama.cpp.📄️ LLM Caching integrationsThis notebook covers how to cache results of individual LLM calls using different caches.📄️ ManifestThis notebook goes over how to use Manifest and LangChain.📄️ MinimaxMinimax is a Chinese startup that provides natural language processing models for companies and individuals.📄️ ModalThe Modal cloud platform provides convenient, on-demand access to serverless cloud compute from Python scripts on your local computer.📄️ MosaicMLMosaicML offers a managed inference service. You can either use a variety of open source models, or deploy your own.📄️ NLP CloudThe NLP Cloud serves high performance pre-trained or custom models for NER, sentiment-analysis, classification, summarization, paraphrasing, grammar and spelling correction, keywords and keyphrases extraction, chatbot, product description and ad generation, intent classification, text generation, image generation, blog post generation, code generation, question answering, automatic speech recognition, machine translation, language detection, semantic search, semantic similarity, tokenization, POS tagging, embeddings, and dependency parsing. It is ready for production, served through a REST API.📄️ OctoAIOctoML is a service with efficient compute. It enables users to integrate their choice of AI models into applications. The OctoAI compute service helps you run, tune, and scale AI applications.📄️ OllamaOllama allows you to run open-source large language models, such as Llama 2, locally.📄️ OpaquePromptsOpaquePrompts is a service that enables applications to leverage the power of language models without compromising user privacy. Designed for composability and ease of integration into existing applications and services, OpaquePrompts is consumable via a simple Python library as well as through LangChain. Perhaps more importantly, OpaquePrompts leverages the power of confidential computing to ensure that even the OpaquePrompts service itself cannot access the data it is protecting.📄️ OpenAIOpenAI offers a spectrum of models with different levels of power suitable for different tasks.📄️ OpenLLM🦾 OpenLLM is an open platform for operating large language models (LLMs) in production. It enables developers to easily run inference with any open-source LLMs, deploy to the cloud or on-premises, and build powerful AI apps.📄️ OpenLMOpenLM is a zero-dependency OpenAI-compatible LLM provider that can call different inference endpoints directly via HTTP.📄️ PetalsPetals runs 100B+ language models at home, BitTorrent-style.📄️ PipelineAIPipelineAI allows you to run your ML models at scale in the cloud. It also provides API access to several LLM models.📄️ PredibasePredibase allows you to train, finetune, and deploy any ML model—from linear regression to large language model.📄️ Prediction GuardBasic LLM usage📄️ PromptLayer OpenAIPromptLayer is the first platform that allows you to track, manage, and share your GPT prompt engineering. PromptLayer acts a middleware between your code and OpenAI’s python library.📄️ RELLMRELLM is a library that wraps local Hugging Face pipeline models for structured decoding.📄️ ReplicateReplicate runs machine learning models in the cloud. We have a library of open-source models that you can run with a few lines of code. If you're building your own machine learning models, Replicate makes it easy to deploy them at scale.📄️ RunhouseThe Runhouse allows remote compute and data across environments and users. See the Runhouse docs.📄️ SageMakerEndpointAmazon SageMaker is a system that can build, train, and deploy machine learning (ML) models for any use case with fully managed infrastructure, tools, and workflows.📄️ StochasticAIStochastic Acceleration Platform aims to simplify the life cycle of a Deep Learning model. From uploading and versioning the model, through training, compression and acceleration to putting it into production.📄️ Nebula (Symbl.ai)Nebula is a large language model (LLM) built by Symbl.ai. It is trained to perform generative tasks on human conversations. Nebula excels at modeling the nuanced details of a conversation and performing tasks on the conversation.📄️ TextGenGitHub:oobabooga/text-generation-webui A gradio web UI for running Large Language Models like LLaMA, llama.cpp, GPT-J, Pythia, OPT, and GALACTICA.📄️ Titan TakeoffTitanML helps businesses build and deploy better, smaller, cheaper, and faster NLP models through our training, compression, and inference optimization platform.📄️ Tongyi QwenTongyi Qwen is a large-scale language model developed by Alibaba's Damo Academy. It is capable of understanding user intent through natural language understanding and semantic analysis, based on user input in natural language. It provides services and assistance to users in different domains and tasks. By providing clear and detailed instructions, you can obtain results that better align with your expectations.📄️ vLLMvLLM is a fast and easy-to-use library for LLM inference and serving, offering:📄️ WriterWriter is a platform to generate different language content.📄️ Xorbits Inference (Xinference)Xinference is a powerful and versatile library designed to serve LLMs,PreviousOpenAI Functions Metadata TaggerNextAI21 IntegrationsCallbacksChat modelsChat loadersDocument loadersDocument transformersLLMsAI21Aleph AlphaAmazon API GatewayAnyscaleAzure MLAzure OpenAIBananaBasetenBeamBedrockBittensorCerebriumAIChatGLMClarifaiCohereC TransformersCTranslate2DatabricksDeepInfraDeepSparseEden AIFireworksForefrontAIGoogle Vertex AI PaLMGooseAIGPT4AllHugging Face HubHugging Face Local PipelinesHuggingface TextGen InferenceJSONFormerKoboldAI APILlama.cppLLM Caching integrationsManifestMinimaxModalMosaicMLNLP CloudOctoAIOllamaOpaquePromptsOpenAIOpenLLMOpenLMPetalsPipelineAIPredibasePrediction GuardPromptLayer OpenAIRELLMReplicateRunhouseSageMakerEndpointStochasticAINebula (Symbl.ai)TextGenTitan TakeoffTongyi QwenvLLMWriterXorbits Inference (Xinference)MemoryRetrieversText embedding modelsAgents & ToolkitsToolsVector storesGrouped by provider IntegrationsCallbacksChat modelsChat loadersDocument loadersDocument transformersLLMsAI21Aleph AlphaAmazon API GatewayAnyscaleAzure MLAzure OpenAIBananaBasetenBeamBedrockBittensorCerebriumAIChatGLMClarifaiCohereC TransformersCTranslate2DatabricksDeepInfraDeepSparseEden AIFireworksForefrontAIGoogle Vertex AI PaLMGooseAIGPT4AllHugging Face HubHugging Face Local PipelinesHuggingface TextGen InferenceJSONFormerKoboldAI APILlama.cppLLM Caching integrationsManifestMinimaxModalMosaicMLNLP CloudOctoAIOllamaOpaquePromptsOpenAIOpenLLMOpenLMPetalsPipelineAIPredibasePrediction GuardPromptLayer OpenAIRELLMReplicateRunhouseSageMakerEndpointStochasticAINebula (Symbl.ai)TextGenTitan TakeoffTongyi QwenvLLMWriterXorbits Inference (Xinference)MemoryRetrieversText embedding modelsAgents & ToolkitsToolsVector storesGrouped by provider Integrations Callbacks Chat models Chat loaders Document loaders Document transformers LLMs Memory Retrievers Text embedding models Agents & Toolkits Tools Vector stores Grouped by provider IntegrationsLLMsLLMs📄️ AI21AI21 Studio provides API access to Jurassic-2 large language models.📄️ Aleph AlphaThe Luminous series is a family of large language models.📄️ Amazon API GatewayAmazon API Gateway is a fully managed service that makes it easy for developers to create, publish, maintain, monitor, and secure APIs at any scale. APIs act as the "front door" for applications to access data, business logic, or functionality from your backend services. Using API Gateway, you can create RESTful APIs and WebSocket APIs that enable real-time two-way communication applications. API Gateway supports containerized and serverless workloads, as well as web applications.📄️ AnyscaleAnyscale is a fully-managed Ray platform, on which you can build, deploy, and manage scalable AI and Python applications📄️ Azure MLAzure ML is a platform used to build, train, and deploy machine learning models. Users can explore the types of models to deploy in the Model Catalog, which provides Azure Foundation Models and OpenAI Models. Azure Foundation Models include various open-source models and popular Hugging Face models. Users can also import models of their liking into AzureML.📄️ Azure OpenAIThis notebook goes over how to use Langchain with Azure OpenAI.📄️ BananaBanana is focused on building the machine learning infrastructure.📄️ BasetenBaseten provides all the infrastructure you need to deploy and serve ML models performantly, scalably, and cost-efficiently.📄️ BeamCalls the Beam API wrapper to deploy and make subsequent calls to an instance of the gpt2 LLM in a cloud deployment. Requires installation of the Beam library and registration of Beam Client ID and Client Secret. By calling the wrapper an instance of the model is created and run, with returned text relating to the prompt. Additional calls can then be made by directly calling the Beam API.📄️ BedrockAmazon Bedrock is a fully managed service that makes FMs from leading AI startups and Amazon available via an API, so you can choose from a wide range of FMs to find the model that is best suited for your use case📄️ BittensorBittensor is a mining network, similar to Bitcoin, that includes built-in incentives designed to encourage miners to contribute compute + knowledge.📄️ CerebriumAICerebrium is an AWS Sagemaker alternative. It also provides API access to several LLM models.📄️ ChatGLMChatGLM-6B is an open bilingual language model based on General Language Model (GLM) framework, with 6.2 billion parameters. With the quantization technique, users can deploy locally on consumer-grade graphics cards (only 6GB of GPU memory is required at the INT4 quantization level).📄️ ClarifaiClarifai is an AI Platform that provides the full AI lifecycle ranging from data exploration, data labeling, model training, evaluation, and inference.📄️ CohereCohere is a Canadian startup that provides natural language processing models that help companies improve human-machine interactions.📄️ C TransformersThe C Transformers library provides Python bindings for GGML models.📄️ CTranslate2CTranslate2 is a C++ and Python library for efficient inference with Transformer models.📄️ DatabricksThe Databricks Lakehouse Platform unifies data, analytics, and AI on one platform.📄️ DeepInfraDeepInfra provides several LLMs.📄️ DeepSparseThis page covers how to use the DeepSparse inference runtime within LangChain.📄️ Eden AIEden AI is revolutionizing the AI landscape by uniting the best AI providers, empowering users to unlock limitless possibilities and tap into the true potential of artificial intelligence. With an all-in-one comprehensive and hassle-free platform, it allows users to deploy AI features to production lightning fast, enabling effortless access to the full breadth of AI capabilities via a single API. (website//edenai.co/)📄️ FireworksFireworks accelerates product development on generative AI by creating an innovative AI experiment and production platform.📄️ ForefrontAIThe Forefront platform gives you the ability to fine-tune and use open source large language models.📄️ Google Vertex AI PaLMNote: This is seperate from the Google PaLM integration, it exposes Vertex AI PaLM API on Google Cloud.📄️ GooseAIGooseAI is a fully managed NLP-as-a-Service, delivered via API. GooseAI provides access to these models.📄️ GPT4AllGitHub:nomic-ai/gpt4all an ecosystem of open-source chatbots trained on a massive collections of clean assistant data including code, stories and dialogue.📄️ Hugging Face HubThe Hugging Face Hub is a platform with over 120k models, 20k datasets, and 50k demo apps (Spaces), all open source and publicly available, in an online platform where people can easily collaborate and build ML together.📄️ Hugging Face Local PipelinesHugging Face models can be run locally through the HuggingFacePipeline class.📄️ Huggingface TextGen InferenceText Generation Inference is a Rust, Python and gRPC server for text generation inference. Used in production at HuggingFace to power LLMs api-inference widgets.📄️ JSONFormerJSONFormer is a library that wraps local HuggingFace pipeline models for structured decoding of a subset of the JSON Schema.📄️ KoboldAI APIKoboldAI is a "a browser-based front-end for AI-assisted writing with multiple local & remote AI models...". It has a public and local API that is able to be used in langchain.📄️ Llama.cppllama-cpp-python is a Python binding for llama.cpp.📄️ LLM Caching integrationsThis notebook covers how to cache results of individual LLM calls using different caches.📄️ ManifestThis notebook goes over how to use Manifest and LangChain.📄️ MinimaxMinimax is a Chinese startup that provides natural language processing models for companies and individuals.📄️ ModalThe Modal cloud platform provides convenient, on-demand access to serverless cloud compute from Python scripts on your local computer.📄️ MosaicMLMosaicML offers a managed inference service. You can either use a variety of open source models, or deploy your own.📄️ NLP CloudThe NLP Cloud serves high performance pre-trained or custom models for NER, sentiment-analysis, classification, summarization, paraphrasing, grammar and spelling correction, keywords and keyphrases extraction, chatbot, product description and ad generation, intent classification, text generation, image generation, blog post generation, code generation, question answering, automatic speech recognition, machine translation, language detection, semantic search, semantic similarity, tokenization, POS tagging, embeddings, and dependency parsing. It is ready for production, served through a REST API.📄️ OctoAIOctoML is a service with efficient compute. It enables users to integrate their choice of AI models into applications. The OctoAI compute service helps you run, tune, and scale AI applications.📄️ OllamaOllama allows you to run open-source large language models, such as Llama 2, locally.📄️ OpaquePromptsOpaquePrompts is a service that enables applications to leverage the power of language models without compromising user privacy. Designed for composability and ease of integration into existing applications and services, OpaquePrompts is consumable via a simple Python library as well as through LangChain. Perhaps more importantly, OpaquePrompts leverages the power of confidential computing to ensure that even the OpaquePrompts service itself cannot access the data it is protecting.📄️ OpenAIOpenAI offers a spectrum of models with different levels of power suitable for different tasks.📄️ OpenLLM🦾 OpenLLM is an open platform for operating large language models (LLMs) in production. It enables developers to easily run inference with any open-source LLMs, deploy to the cloud or on-premises, and build powerful AI apps.📄️ OpenLMOpenLM is a zero-dependency OpenAI-compatible LLM provider that can call different inference endpoints directly via HTTP.📄️ PetalsPetals runs 100B+ language models at home, BitTorrent-style.📄️ PipelineAIPipelineAI allows you to run your ML models at scale in the cloud. It also provides API access to several LLM models.📄️ PredibasePredibase allows you to train, finetune, and deploy any ML model—from linear regression to large language model.📄️ Prediction GuardBasic LLM usage📄️ PromptLayer OpenAIPromptLayer is the first platform that allows you to track, manage, and share your GPT prompt engineering. PromptLayer acts a middleware between your code and OpenAI’s python library.📄️ RELLMRELLM is a library that wraps local Hugging Face pipeline models for structured decoding.📄️ ReplicateReplicate runs machine learning models in the cloud. We have a library of open-source models that you can run with a few lines of code. If you're building your own machine learning models, Replicate makes it easy to deploy them at scale.📄️ RunhouseThe Runhouse allows remote compute and data across environments and users. See the Runhouse docs.📄️ SageMakerEndpointAmazon SageMaker is a system that can build, train, and deploy machine learning (ML) models for any use case with fully managed infrastructure, tools, and workflows.📄️ StochasticAIStochastic Acceleration Platform aims to simplify the life cycle of a Deep Learning model. From uploading and versioning the model, through training, compression and acceleration to putting it into production.📄️ Nebula (Symbl.ai)Nebula is a large language model (LLM) built by Symbl.ai. It is trained to perform generative tasks on human conversations. Nebula excels at modeling the nuanced details of a conversation and performing tasks on the conversation.📄️ TextGenGitHub:oobabooga/text-generation-webui A gradio web UI for running Large Language Models like LLaMA, llama.cpp, GPT-J, Pythia, OPT, and GALACTICA.📄️ Titan TakeoffTitanML helps businesses build and deploy better, smaller, cheaper, and faster NLP models through our training, compression, and inference optimization platform.📄️ Tongyi QwenTongyi Qwen is a large-scale language model developed by Alibaba's Damo Academy. It is capable of understanding user intent through natural language understanding and semantic analysis, based on user input in natural language. It provides services and assistance to users in different domains and tasks. By providing clear and detailed instructions, you can obtain results that better align with your expectations.📄️ vLLMvLLM is a fast and easy-to-use library for LLM inference and serving, offering:📄️ WriterWriter is a platform to generate different language content.📄️ Xorbits Inference (Xinference)Xinference is a powerful and versatile library designed to serve LLMs,PreviousOpenAI Functions Metadata TaggerNextAI21 IntegrationsLLMsLLMs📄️ AI21AI21 Studio provides API access to Jurassic-2 large language models.📄️ Aleph AlphaThe Luminous series is a family of large language models.📄️ Amazon API GatewayAmazon API Gateway is a fully managed service that makes it easy for developers to create, publish, maintain, monitor, and secure APIs at any scale. APIs act as the "front door" for applications to access data, business logic, or functionality from your backend services. Using API Gateway, you can create RESTful APIs and WebSocket APIs that enable real-time two-way communication applications. API Gateway supports containerized and serverless workloads, as well as web applications.📄️ AnyscaleAnyscale is a fully-managed Ray platform, on which you can build, deploy, and manage scalable AI and Python applications📄️ Azure MLAzure ML is a platform used to build, train, and deploy machine learning models. Users can explore the types of models to deploy in the Model Catalog, which provides Azure Foundation Models and OpenAI Models. Azure Foundation Models include various open-source models and popular Hugging Face models. Users can also import models of their liking into AzureML.📄️ Azure OpenAIThis notebook goes over how to use Langchain with Azure OpenAI.📄️ BananaBanana is focused on building the machine learning infrastructure.📄️ BasetenBaseten provides all the infrastructure you need to deploy and serve ML models performantly, scalably, and cost-efficiently.📄️ BeamCalls the Beam API wrapper to deploy and make subsequent calls to an instance of the gpt2 LLM in a cloud deployment. Requires installation of the Beam library and registration of Beam Client ID and Client Secret. By calling the wrapper an instance of the model is created and run, with returned text relating to the prompt. Additional calls can then be made by directly calling the Beam API.📄️ BedrockAmazon Bedrock is a fully managed service that makes FMs from leading AI startups and Amazon available via an API, so you can choose from a wide range of FMs to find the model that is best suited for your use case📄️ BittensorBittensor is a mining network, similar to Bitcoin, that includes built-in incentives designed to encourage miners to contribute compute + knowledge.📄️ CerebriumAICerebrium is an AWS Sagemaker alternative. It also provides API access to several LLM models.📄️ ChatGLMChatGLM-6B is an open bilingual language model based on General Language Model (GLM) framework, with 6.2 billion parameters. With the quantization technique, users can deploy locally on consumer-grade graphics cards (only 6GB of GPU memory is required at the INT4 quantization level).📄️ ClarifaiClarifai is an AI Platform that provides the full AI lifecycle ranging from data exploration, data labeling, model training, evaluation, and inference.📄️ CohereCohere is a Canadian startup that provides natural language processing models that help companies improve human-machine interactions.📄️ C TransformersThe C Transformers library provides Python bindings for GGML models.📄️ CTranslate2CTranslate2 is a C++ and Python library for efficient inference with Transformer models.📄️ DatabricksThe Databricks Lakehouse Platform unifies data, analytics, and AI on one platform.📄️ DeepInfraDeepInfra provides several LLMs.📄️ DeepSparseThis page covers how to use the DeepSparse inference runtime within LangChain.📄️ Eden AIEden AI is revolutionizing the AI landscape by uniting the best AI providers, empowering users to unlock limitless possibilities and tap into the true potential of artificial intelligence. With an all-in-one comprehensive and hassle-free platform, it allows users to deploy AI features to production lightning fast, enabling effortless access to the full breadth of AI capabilities via a single API. (website//edenai.co/)📄️ FireworksFireworks accelerates product development on generative AI by creating an innovative AI experiment and production platform.📄️ ForefrontAIThe Forefront platform gives you the ability to fine-tune and use open source large language models.📄️ Google Vertex AI PaLMNote: This is seperate from the Google PaLM integration, it exposes Vertex AI PaLM API on Google Cloud.📄️ GooseAIGooseAI is a fully managed NLP-as-a-Service, delivered via API. GooseAI provides access to these models.📄️ GPT4AllGitHub:nomic-ai/gpt4all an ecosystem of open-source chatbots trained on a massive collections of clean assistant data including code, stories and dialogue.📄️ Hugging Face HubThe Hugging Face Hub is a platform with over 120k models, 20k datasets, and 50k demo apps (Spaces), all open source and publicly available, in an online platform where people can easily collaborate and build ML together.📄️ Hugging Face Local PipelinesHugging Face models can be run locally through the HuggingFacePipeline class.📄️ Huggingface TextGen InferenceText Generation Inference is a Rust, Python and gRPC server for text generation inference. Used in production at HuggingFace to power LLMs api-inference widgets.📄️ JSONFormerJSONFormer is a library that wraps local HuggingFace pipeline models for structured decoding of a subset of the JSON Schema.📄️ KoboldAI APIKoboldAI is a "a browser-based front-end for AI-assisted writing with multiple local & remote AI models...". It has a public and local API that is able to be used in langchain.📄️ Llama.cppllama-cpp-python is a Python binding for llama.cpp.📄️ LLM Caching integrationsThis notebook covers how to cache results of individual LLM calls using different caches.📄️ ManifestThis notebook goes over how to use Manifest and LangChain.📄️ MinimaxMinimax is a Chinese startup that provides natural language processing models for companies and individuals.📄️ ModalThe Modal cloud platform provides convenient, on-demand access to serverless cloud compute from Python scripts on your local computer.📄️ MosaicMLMosaicML offers a managed inference service. You can either use a variety of open source models, or deploy your own.📄️ NLP CloudThe NLP Cloud serves high performance pre-trained or custom models for NER, sentiment-analysis, classification, summarization, paraphrasing, grammar and spelling correction, keywords and keyphrases extraction, chatbot, product description and ad generation, intent classification, text generation, image generation, blog post generation, code generation, question answering, automatic speech recognition, machine translation, language detection, semantic search, semantic similarity, tokenization, POS tagging, embeddings, and dependency parsing. It is ready for production, served through a REST API.📄️ OctoAIOctoML is a service with efficient compute. It enables users to integrate their choice of AI models into applications. The OctoAI compute service helps you run, tune, and scale AI applications.📄️ OllamaOllama allows you to run open-source large language models, such as Llama 2, locally.📄️ OpaquePromptsOpaquePrompts is a service that enables applications to leverage the power of language models without compromising user privacy. Designed for composability and ease of integration into existing applications and services, OpaquePrompts is consumable via a simple Python library as well as through LangChain. Perhaps more importantly, OpaquePrompts leverages the power of confidential computing to ensure that even the OpaquePrompts service itself cannot access the data it is protecting.📄️ OpenAIOpenAI offers a spectrum of models with different levels of power suitable for different tasks.📄️ OpenLLM🦾 OpenLLM is an open platform for operating large language models (LLMs) in production. It enables developers to easily run inference with any open-source LLMs, deploy to the cloud or on-premises, and build powerful AI apps.📄️ OpenLMOpenLM is a zero-dependency OpenAI-compatible LLM provider that can call different inference endpoints directly via HTTP.📄️ PetalsPetals runs 100B+ language models at home, BitTorrent-style.📄️ PipelineAIPipelineAI allows you to run your ML models at scale in the cloud. It also provides API access to several LLM models.📄️ PredibasePredibase allows you to train, finetune, and deploy any ML model—from linear regression to large language model.📄️ Prediction GuardBasic LLM usage📄️ PromptLayer OpenAIPromptLayer is the first platform that allows you to track, manage, and share your GPT prompt engineering. PromptLayer acts a middleware between your code and OpenAI’s python library.📄️ RELLMRELLM is a library that wraps local Hugging Face pipeline models for structured decoding.📄️ ReplicateReplicate runs machine learning models in the cloud. We have a library of open-source models that you can run with a few lines of code. If you're building your own machine learning models, Replicate makes it easy to deploy them at scale.📄️ RunhouseThe Runhouse allows remote compute and data across environments and users. See the Runhouse docs.📄️ SageMakerEndpointAmazon SageMaker is a system that can build, train, and deploy machine learning (ML) models for any use case with fully managed infrastructure, tools, and workflows.📄️ StochasticAIStochastic Acceleration Platform aims to simplify the life cycle of a Deep Learning model. From uploading and versioning the model, through training, compression and acceleration to putting it into production.📄️ Nebula (Symbl.ai)Nebula is a large language model (LLM) built by Symbl.ai. It is trained to perform generative tasks on human conversations. Nebula excels at modeling the nuanced details of a conversation and performing tasks on the conversation.📄️ TextGenGitHub:oobabooga/text-generation-webui A gradio web UI for running Large Language Models like LLaMA, llama.cpp, GPT-J, Pythia, OPT, and GALACTICA.📄️ Titan TakeoffTitanML helps businesses build and deploy better, smaller, cheaper, and faster NLP models through our training, compression, and inference optimization platform.📄️ Tongyi QwenTongyi Qwen is a large-scale language model developed by Alibaba's Damo Academy. It is capable of understanding user intent through natural language understanding and semantic analysis, based on user input in natural language. It provides services and assistance to users in different domains and tasks. By providing clear and detailed instructions, you can obtain results that better align with your expectations.📄️ vLLMvLLM is a fast and easy-to-use library for LLM inference and serving, offering:📄️ WriterWriter is a platform to generate different language content.📄️ Xorbits Inference (Xinference)Xinference is a powerful and versatile library designed to serve LLMs,PreviousOpenAI Functions Metadata TaggerNextAI21 IntegrationsLLMsLLMs📄️ AI21AI21 Studio provides API access to Jurassic-2 large language models.📄️ Aleph AlphaThe Luminous series is a family of large language models.📄️ Amazon API GatewayAmazon API Gateway is a fully managed service that makes it easy for developers to create, publish, maintain, monitor, and secure APIs at any scale. APIs act as the "front door" for applications to access data, business logic, or functionality from your backend services. Using API Gateway, you can create RESTful APIs and WebSocket APIs that enable real-time two-way communication applications. API Gateway supports containerized and serverless workloads, as well as web applications.📄️ AnyscaleAnyscale is a fully-managed Ray platform, on which you can build, deploy, and manage scalable AI and Python applications📄️ Azure MLAzure ML is a platform used to build, train, and deploy machine learning models. Users can explore the types of models to deploy in the Model Catalog, which provides Azure Foundation Models and OpenAI Models. Azure Foundation Models include various open-source models and popular Hugging Face models. Users can also import models of their liking into AzureML.📄️ Azure OpenAIThis notebook goes over how to use Langchain with Azure OpenAI.📄️ BananaBanana is focused on building the machine learning infrastructure.📄️ BasetenBaseten provides all the infrastructure you need to deploy and serve ML models performantly, scalably, and cost-efficiently.📄️ BeamCalls the Beam API wrapper to deploy and make subsequent calls to an instance of the gpt2 LLM in a cloud deployment. Requires installation of the Beam library and registration of Beam Client ID and Client Secret. By calling the wrapper an instance of the model is created and run, with returned text relating to the prompt. Additional calls can then be made by directly calling the Beam API.📄️ BedrockAmazon Bedrock is a fully managed service that makes FMs from leading AI startups and Amazon available via an API, so you can choose from a wide range of FMs to find the model that is best suited for your use case📄️ BittensorBittensor is a mining network, similar to Bitcoin, that includes built-in incentives designed to encourage miners to contribute compute + knowledge.📄️ CerebriumAICerebrium is an AWS Sagemaker alternative. It also provides API access to several LLM models.📄️ ChatGLMChatGLM-6B is an open bilingual language model based on General Language Model (GLM) framework, with 6.2 billion parameters. With the quantization technique, users can deploy locally on consumer-grade graphics cards (only 6GB of GPU memory is required at the INT4 quantization level).📄️ ClarifaiClarifai is an AI Platform that provides the full AI lifecycle ranging from data exploration, data labeling, model training, evaluation, and inference.📄️ CohereCohere is a Canadian startup that provides natural language processing models that help companies improve human-machine interactions.📄️ C TransformersThe C Transformers library provides Python bindings for GGML models.📄️ CTranslate2CTranslate2 is a C++ and Python library for efficient inference with Transformer models.📄️ DatabricksThe Databricks Lakehouse Platform unifies data, analytics, and AI on one platform.📄️ DeepInfraDeepInfra provides several LLMs.📄️ DeepSparseThis page covers how to use the DeepSparse inference runtime within LangChain.📄️ Eden AIEden AI is revolutionizing the AI landscape by uniting the best AI providers, empowering users to unlock limitless possibilities and tap into the true potential of artificial intelligence. With an all-in-one comprehensive and hassle-free platform, it allows users to deploy AI features to production lightning fast, enabling effortless access to the full breadth of AI capabilities via a single API. (website//edenai.co/)📄️ FireworksFireworks accelerates product development on generative AI by creating an innovative AI experiment and production platform.📄️ ForefrontAIThe Forefront platform gives you the ability to fine-tune and use open source large language models.📄️ Google Vertex AI PaLMNote: This is seperate from the Google PaLM integration, it exposes Vertex AI PaLM API on Google Cloud.📄️ GooseAIGooseAI is a fully managed NLP-as-a-Service, delivered via API. GooseAI provides access to these models.📄️ GPT4AllGitHub:nomic-ai/gpt4all an ecosystem of open-source chatbots trained on a massive collections of clean assistant data including code, stories and dialogue.📄️ Hugging Face HubThe Hugging Face Hub is a platform with over 120k models, 20k datasets, and 50k demo apps (Spaces), all open source and publicly available, in an online platform where people can easily collaborate and build ML together.📄️ Hugging Face Local PipelinesHugging Face models can be run locally through the HuggingFacePipeline class.📄️ Huggingface TextGen InferenceText Generation Inference is a Rust, Python and gRPC server for text generation inference. Used in production at HuggingFace to power LLMs api-inference widgets.📄️ JSONFormerJSONFormer is a library that wraps local HuggingFace pipeline models for structured decoding of a subset of the JSON Schema.📄️ KoboldAI APIKoboldAI is a "a browser-based front-end for AI-assisted writing with multiple local & remote AI models...". It has a public and local API that is able to be used in langchain.📄️ Llama.cppllama-cpp-python is a Python binding for llama.cpp.📄️ LLM Caching integrationsThis notebook covers how to cache results of individual LLM calls using different caches.📄️ ManifestThis notebook goes over how to use Manifest and LangChain.📄️ MinimaxMinimax is a Chinese startup that provides natural language processing models for companies and individuals.📄️ ModalThe Modal cloud platform provides convenient, on-demand access to serverless cloud compute from Python scripts on your local computer.📄️ MosaicMLMosaicML offers a managed inference service. You can either use a variety of open source models, or deploy your own.📄️ NLP CloudThe NLP Cloud serves high performance pre-trained or custom models for NER, sentiment-analysis, classification, summarization, paraphrasing, grammar and spelling correction, keywords and keyphrases extraction, chatbot, product description and ad generation, intent classification, text generation, image generation, blog post generation, code generation, question answering, automatic speech recognition, machine translation, language detection, semantic search, semantic similarity, tokenization, POS tagging, embeddings, and dependency parsing. It is ready for production, served through a REST API.📄️ OctoAIOctoML is a service with efficient compute. It enables users to integrate their choice of AI models into applications. The OctoAI compute service helps you run, tune, and scale AI applications.📄️ OllamaOllama allows you to run open-source large language models, such as Llama 2, locally.📄️ OpaquePromptsOpaquePrompts is a service that enables applications to leverage the power of language models without compromising user privacy. Designed for composability and ease of integration into existing applications and services, OpaquePrompts is consumable via a simple Python library as well as through LangChain. Perhaps more importantly, OpaquePrompts leverages the power of confidential computing to ensure that even the OpaquePrompts service itself cannot access the data it is protecting.📄️ OpenAIOpenAI offers a spectrum of models with different levels of power suitable for different tasks.📄️ OpenLLM🦾 OpenLLM is an open platform for operating large language models (LLMs) in production. It enables developers to easily run inference with any open-source LLMs, deploy to the cloud or on-premises, and build powerful AI apps.📄️ OpenLMOpenLM is a zero-dependency OpenAI-compatible LLM provider that can call different inference endpoints directly via HTTP.📄️ PetalsPetals runs 100B+ language models at home, BitTorrent-style.📄️ PipelineAIPipelineAI allows you to run your ML models at scale in the cloud. It also provides API access to several LLM models.📄️ PredibasePredibase allows you to train, finetune, and deploy any ML model—from linear regression to large language model.📄️ Prediction GuardBasic LLM usage📄️ PromptLayer OpenAIPromptLayer is the first platform that allows you to track, manage, and share your GPT prompt engineering. PromptLayer acts a middleware between your code and OpenAI’s python library.📄️ RELLMRELLM is a library that wraps local Hugging Face pipeline models for structured decoding.📄️ ReplicateReplicate runs machine learning models in the cloud. We have a library of open-source models that you can run with a few lines of code. If you're building your own machine learning models, Replicate makes it easy to deploy them at scale.📄️ RunhouseThe Runhouse allows remote compute and data across environments and users. See the Runhouse docs.📄️ SageMakerEndpointAmazon SageMaker is a system that can build, train, and deploy machine learning (ML) models for any use case with fully managed infrastructure, tools, and workflows.📄️ StochasticAIStochastic Acceleration Platform aims to simplify the life cycle of a Deep Learning model. From uploading and versioning the model, through training, compression and acceleration to putting it into production.📄️ Nebula (Symbl.ai)Nebula is a large language model (LLM) built by Symbl.ai. It is trained to perform generative tasks on human conversations. Nebula excels at modeling the nuanced details of a conversation and performing tasks on the conversation.📄️ TextGenGitHub:oobabooga/text-generation-webui A gradio web UI for running Large Language Models like LLaMA, llama.cpp, GPT-J, Pythia, OPT, and GALACTICA.📄️ Titan TakeoffTitanML helps businesses build and deploy better, smaller, cheaper, and faster NLP models through our training, compression, and inference optimization platform.📄️ Tongyi QwenTongyi Qwen is a large-scale language model developed by Alibaba's Damo Academy. It is capable of understanding user intent through natural language understanding and semantic analysis, based on user input in natural language. It provides services and assistance to users in different domains and tasks. By providing clear and detailed instructions, you can obtain results that better align with your expectations.📄️ vLLMvLLM is a fast and easy-to-use library for LLM inference and serving, offering:📄️ WriterWriter is a platform to generate different language content.📄️ Xorbits Inference (Xinference)Xinference is a powerful and versatile library designed to serve LLMs,PreviousOpenAI Functions Metadata TaggerNextAI21 IntegrationsLLMsLLMs📄️ AI21AI21 Studio provides API access to Jurassic-2 large language models.📄️ Aleph AlphaThe Luminous series is a family of large language models.📄️ Amazon API GatewayAmazon API Gateway is a fully managed service that makes it easy for developers to create, publish, maintain, monitor, and secure APIs at any scale. APIs act as the "front door" for applications to access data, business logic, or functionality from your backend services. Using API Gateway, you can create RESTful APIs and WebSocket APIs that enable real-time two-way communication applications. API Gateway supports containerized and serverless workloads, as well as web applications.📄️ AnyscaleAnyscale is a fully-managed Ray platform, on which you can build, deploy, and manage scalable AI and Python applications📄️ Azure MLAzure ML is a platform used to build, train, and deploy machine learning models. Users can explore the types of models to deploy in the Model Catalog, which provides Azure Foundation Models and OpenAI Models. Azure Foundation Models include various open-source models and popular Hugging Face models. Users can also import models of their liking into AzureML.📄️ Azure OpenAIThis notebook goes over how to use Langchain with Azure OpenAI.📄️ BananaBanana is focused on building the machine learning infrastructure.📄️ BasetenBaseten provides all the infrastructure you need to deploy and serve ML models performantly, scalably, and cost-efficiently.📄️ BeamCalls the Beam API wrapper to deploy and make subsequent calls to an instance of the gpt2 LLM in a cloud deployment. Requires installation of the Beam library and registration of Beam Client ID and Client Secret. By calling the wrapper an instance of the model is created and run, with returned text relating to the prompt. Additional calls can then be made by directly calling the Beam API.📄️ BedrockAmazon Bedrock is a fully managed service that makes FMs from leading AI startups and Amazon available via an API, so you can choose from a wide range of FMs to find the model that is best suited for your use case📄️ BittensorBittensor is a mining network, similar to Bitcoin, that includes built-in incentives designed to encourage miners to contribute compute + knowledge.📄️ CerebriumAICerebrium is an AWS Sagemaker alternative. It also provides API access to several LLM models.📄️ ChatGLMChatGLM-6B is an open bilingual language model based on General Language Model (GLM) framework, with 6.2 billion parameters. With the quantization technique, users can deploy locally on consumer-grade graphics cards (only 6GB of GPU memory is required at the INT4 quantization level).📄️ ClarifaiClarifai is an AI Platform that provides the full AI lifecycle ranging from data exploration, data labeling, model training, evaluation, and inference.📄️ CohereCohere is a Canadian startup that provides natural language processing models that help companies improve human-machine interactions.📄️ C TransformersThe C Transformers library provides Python bindings for GGML models.📄️ CTranslate2CTranslate2 is a C++ and Python library for efficient inference with Transformer models.📄️ DatabricksThe Databricks Lakehouse Platform unifies data, analytics, and AI on one platform.📄️ DeepInfraDeepInfra provides several LLMs.📄️ DeepSparseThis page covers how to use the DeepSparse inference runtime within LangChain.📄️ Eden AIEden AI is revolutionizing the AI landscape by uniting the best AI providers, empowering users to unlock limitless possibilities and tap into the true potential of artificial intelligence. With an all-in-one comprehensive and hassle-free platform, it allows users to deploy AI features to production lightning fast, enabling effortless access to the full breadth of AI capabilities via a single API. (website//edenai.co/)📄️ FireworksFireworks accelerates product development on generative AI by creating an innovative AI experiment and production platform.📄️ ForefrontAIThe Forefront platform gives you the ability to fine-tune and use open source large language models.📄️ Google Vertex AI PaLMNote: This is seperate from the Google PaLM integration, it exposes Vertex AI PaLM API on Google Cloud.📄️ GooseAIGooseAI is a fully managed NLP-as-a-Service, delivered via API. GooseAI provides access to these models.📄️ GPT4AllGitHub:nomic-ai/gpt4all an ecosystem of open-source chatbots trained on a massive collections of clean assistant data including code, stories and dialogue.📄️ Hugging Face HubThe Hugging Face Hub is a platform with over 120k models, 20k datasets, and 50k demo apps (Spaces), all open source and publicly available, in an online platform where people can easily collaborate and build ML together.📄️ Hugging Face Local PipelinesHugging Face models can be run locally through the HuggingFacePipeline class.📄️ Huggingface TextGen InferenceText Generation Inference is a Rust, Python and gRPC server for text generation inference. Used in production at HuggingFace to power LLMs api-inference widgets.📄️ JSONFormerJSONFormer is a library that wraps local HuggingFace pipeline models for structured decoding of a subset of the JSON Schema.📄️ KoboldAI APIKoboldAI is a "a browser-based front-end for AI-assisted writing with multiple local & remote AI models...". It has a public and local API that is able to be used in langchain.📄️ Llama.cppllama-cpp-python is a Python binding for llama.cpp.📄️ LLM Caching integrationsThis notebook covers how to cache results of individual LLM calls using different caches.📄️ ManifestThis notebook goes over how to use Manifest and LangChain.📄️ MinimaxMinimax is a Chinese startup that provides natural language processing models for companies and individuals.📄️ ModalThe Modal cloud platform provides convenient, on-demand access to serverless cloud compute from Python scripts on your local computer.📄️ MosaicMLMosaicML offers a managed inference service. You can either use a variety of open source models, or deploy your own.📄️ NLP CloudThe NLP Cloud serves high performance pre-trained or custom models for NER, sentiment-analysis, classification, summarization, paraphrasing, grammar and spelling correction, keywords and keyphrases extraction, chatbot, product description and ad generation, intent classification, text generation, image generation, blog post generation, code generation, question answering, automatic speech recognition, machine translation, language detection, semantic search, semantic similarity, tokenization, POS tagging, embeddings, and dependency parsing. It is ready for production, served through a REST API.📄️ OctoAIOctoML is a service with efficient compute. It enables users to integrate their choice of AI models into applications. The OctoAI compute service helps you run, tune, and scale AI applications.📄️ OllamaOllama allows you to run open-source large language models, such as Llama 2, locally.📄️ OpaquePromptsOpaquePrompts is a service that enables applications to leverage the power of language models without compromising user privacy. Designed for composability and ease of integration into existing applications and services, OpaquePrompts is consumable via a simple Python library as well as through LangChain. Perhaps more importantly, OpaquePrompts leverages the power of confidential computing to ensure that even the OpaquePrompts service itself cannot access the data it is protecting.📄️ OpenAIOpenAI offers a spectrum of models with different levels of power suitable for different tasks.📄️ OpenLLM🦾 OpenLLM is an open platform for operating large language models (LLMs) in production. It enables developers to easily run inference with any open-source LLMs, deploy to the cloud or on-premises, and build powerful AI apps.📄️ OpenLMOpenLM is a zero-dependency OpenAI-compatible LLM provider that can call different inference endpoints directly via HTTP.📄️ PetalsPetals runs 100B+ language models at home, BitTorrent-style.📄️ PipelineAIPipelineAI allows you to run your ML models at scale in the cloud. It also provides API access to several LLM models.📄️ PredibasePredibase allows you to train, finetune, and deploy any ML model—from linear regression to large language model.📄️ Prediction GuardBasic LLM usage📄️ PromptLayer OpenAIPromptLayer is the first platform that allows you to track, manage, and share your GPT prompt engineering. PromptLayer acts a middleware between your code and OpenAI’s python library.📄️ RELLMRELLM is a library that wraps local Hugging Face pipeline models for structured decoding.📄️ ReplicateReplicate runs machine learning models in the cloud. We have a library of open-source models that you can run with a few lines of code. If you're building your own machine learning models, Replicate makes it easy to deploy them at scale.📄️ RunhouseThe Runhouse allows remote compute and data across environments and users. See the Runhouse docs.📄️ SageMakerEndpointAmazon SageMaker is a system that can build, train, and deploy machine learning (ML) models for any use case with fully managed infrastructure, tools, and workflows.📄️ StochasticAIStochastic Acceleration Platform aims to simplify the life cycle of a Deep Learning model. From uploading and versioning the model, through training, compression and acceleration to putting it into production.📄️ Nebula (Symbl.ai)Nebula is a large language model (LLM) built by Symbl.ai. It is trained to perform generative tasks on human conversations. Nebula excels at modeling the nuanced details of a conversation and performing tasks on the conversation.📄️ TextGenGitHub:oobabooga/text-generation-webui A gradio web UI for running Large Language Models like LLaMA, llama.cpp, GPT-J, Pythia, OPT, and GALACTICA.📄️ Titan TakeoffTitanML helps businesses build and deploy better, smaller, cheaper, and faster NLP models through our training, compression, and inference optimization platform.📄️ Tongyi QwenTongyi Qwen is a large-scale language model developed by Alibaba's Damo Academy. It is capable of understanding user intent through natural language understanding and semantic analysis, based on user input in natural language. It provides services and assistance to users in different domains and tasks. By providing clear and detailed instructions, you can obtain results that better align with your expectations.📄️ vLLMvLLM is a fast and easy-to-use library for LLM inference and serving, offering:📄️ WriterWriter is a platform to generate different language content.📄️ Xorbits Inference (Xinference)Xinference is a powerful and versatile library designed to serve LLMs,PreviousOpenAI Functions Metadata TaggerNextAI21 LLMs📄️ AI21AI21 Studio provides API access to Jurassic-2 large language models.📄️ Aleph AlphaThe Luminous series is a family of large language models.📄️ Amazon API GatewayAmazon API Gateway is a fully managed service that makes it easy for developers to create, publish, maintain, monitor, and secure APIs at any scale. APIs act as the "front door" for applications to access data, business logic, or functionality from your backend services. Using API Gateway, you can create RESTful APIs and WebSocket APIs that enable real-time two-way communication applications. API Gateway supports containerized and serverless workloads, as well as web applications.📄️ AnyscaleAnyscale is a fully-managed Ray platform, on which you can build, deploy, and manage scalable AI and Python applications📄️ Azure MLAzure ML is a platform used to build, train, and deploy machine learning models. Users can explore the types of models to deploy in the Model Catalog, which provides Azure Foundation Models and OpenAI Models. Azure Foundation Models include various open-source models and popular Hugging Face models. Users can also import models of their liking into AzureML.📄️ Azure OpenAIThis notebook goes over how to use Langchain with Azure OpenAI.📄️ BananaBanana is focused on building the machine learning infrastructure.📄️ BasetenBaseten provides all the infrastructure you need to deploy and serve ML models performantly, scalably, and cost-efficiently.📄️ BeamCalls the Beam API wrapper to deploy and make subsequent calls to an instance of the gpt2 LLM in a cloud deployment. Requires installation of the Beam library and registration of Beam Client ID and Client Secret. By calling the wrapper an instance of the model is created and run, with returned text relating to the prompt. Additional calls can then be made by directly calling the Beam API.📄️ BedrockAmazon Bedrock is a fully managed service that makes FMs from leading AI startups and Amazon available via an API, so you can choose from a wide range of FMs to find the model that is best suited for your use case📄️ BittensorBittensor is a mining network, similar to Bitcoin, that includes built-in incentives designed to encourage miners to contribute compute + knowledge.📄️ CerebriumAICerebrium is an AWS Sagemaker alternative. It also provides API access to several LLM models.📄️ ChatGLMChatGLM-6B is an open bilingual language model based on General Language Model (GLM) framework, with 6.2 billion parameters. With the quantization technique, users can deploy locally on consumer-grade graphics cards (only 6GB of GPU memory is required at the INT4 quantization level).📄️ ClarifaiClarifai is an AI Platform that provides the full AI lifecycle ranging from data exploration, data labeling, model training, evaluation, and inference.📄️ CohereCohere is a Canadian startup that provides natural language processing models that help companies improve human-machine interactions.📄️ C TransformersThe C Transformers library provides Python bindings for GGML models.📄️ CTranslate2CTranslate2 is a C++ and Python library for efficient inference with Transformer models.📄️ DatabricksThe Databricks Lakehouse Platform unifies data, analytics, and AI on one platform.📄️ DeepInfraDeepInfra provides several LLMs.📄️ DeepSparseThis page covers how to use the DeepSparse inference runtime within LangChain.📄️ Eden AIEden AI is revolutionizing the AI landscape by uniting the best AI providers, empowering users to unlock limitless possibilities and tap into the true potential of artificial intelligence. With an all-in-one comprehensive and hassle-free platform, it allows users to deploy AI features to production lightning fast, enabling effortless access to the full breadth of AI capabilities via a single API. (website//edenai.co/)📄️ FireworksFireworks accelerates product development on generative AI by creating an innovative AI experiment and production platform.📄️ ForefrontAIThe Forefront platform gives you the ability to fine-tune and use open source large language models.📄️ Google Vertex AI PaLMNote: This is seperate from the Google PaLM integration, it exposes Vertex AI PaLM API on Google Cloud.📄️ GooseAIGooseAI is a fully managed NLP-as-a-Service, delivered via API. GooseAI provides access to these models.📄️ GPT4AllGitHub:nomic-ai/gpt4all an ecosystem of open-source chatbots trained on a massive collections of clean assistant data including code, stories and dialogue.📄️ Hugging Face HubThe Hugging Face Hub is a platform with over 120k models, 20k datasets, and 50k demo apps (Spaces), all open source and publicly available, in an online platform where people can easily collaborate and build ML together.📄️ Hugging Face Local PipelinesHugging Face models can be run locally through the HuggingFacePipeline class.📄️ Huggingface TextGen InferenceText Generation Inference is a Rust, Python and gRPC server for text generation inference. Used in production at HuggingFace to power LLMs api-inference widgets.📄️ JSONFormerJSONFormer is a library that wraps local HuggingFace pipeline models for structured decoding of a subset of the JSON Schema.📄️ KoboldAI APIKoboldAI is a "a browser-based front-end for AI-assisted writing with multiple local & remote AI models...". It has a public and local API that is able to be used in langchain.📄️ Llama.cppllama-cpp-python is a Python binding for llama.cpp.📄️ LLM Caching integrationsThis notebook covers how to cache results of individual LLM calls using different caches.📄️ ManifestThis notebook goes over how to use Manifest and LangChain.📄️ MinimaxMinimax is a Chinese startup that provides natural language processing models for companies and individuals.📄️ ModalThe Modal cloud platform provides convenient, on-demand access to serverless cloud compute from Python scripts on your local computer.📄️ MosaicMLMosaicML offers a managed inference service. You can either use a variety of open source models, or deploy your own.📄️ NLP CloudThe NLP Cloud serves high performance pre-trained or custom models for NER, sentiment-analysis, classification, summarization, paraphrasing, grammar and spelling correction, keywords and keyphrases extraction, chatbot, product description and ad generation, intent classification, text generation, image generation, blog post generation, code generation, question answering, automatic speech recognition, machine translation, language detection, semantic search, semantic similarity, tokenization, POS tagging, embeddings, and dependency parsing. It is ready for production, served through a REST API.📄️ OctoAIOctoML is a service with efficient compute. It enables users to integrate their choice of AI models into applications. The OctoAI compute service helps you run, tune, and scale AI applications.📄️ OllamaOllama allows you to run open-source large language models, such as Llama 2, locally.📄️ OpaquePromptsOpaquePrompts is a service that enables applications to leverage the power of language models without compromising user privacy. Designed for composability and ease of integration into existing applications and services, OpaquePrompts is consumable via a simple Python library as well as through LangChain. Perhaps more importantly, OpaquePrompts leverages the power of confidential computing to ensure that even the OpaquePrompts service itself cannot access the data it is protecting.📄️ OpenAIOpenAI offers a spectrum of models with different levels of power suitable for different tasks.📄️ OpenLLM🦾 OpenLLM is an open platform for operating large language models (LLMs) in production. It enables developers to easily run inference with any open-source LLMs, deploy to the cloud or on-premises, and build powerful AI apps.📄️ OpenLMOpenLM is a zero-dependency OpenAI-compatible LLM provider that can call different inference endpoints directly via HTTP.📄️ PetalsPetals runs 100B+ language models at home, BitTorrent-style.📄️ PipelineAIPipelineAI allows you to run your ML models at scale in the cloud. It also provides API access to several LLM models.📄️ PredibasePredibase allows you to train, finetune, and deploy any ML model—from linear regression to large language model.📄️ Prediction GuardBasic LLM usage📄️ PromptLayer OpenAIPromptLayer is the first platform that allows you to track, manage, and share your GPT prompt engineering. PromptLayer acts a middleware between your code and OpenAI’s python library.📄️ RELLMRELLM is a library that wraps local Hugging Face pipeline models for structured decoding.📄️ ReplicateReplicate runs machine learning models in the cloud. We have a library of open-source models that you can run with a few lines of code. If you're building your own machine learning models, Replicate makes it easy to deploy them at scale.📄️ RunhouseThe Runhouse allows remote compute and data across environments and users. See the Runhouse docs.📄️ SageMakerEndpointAmazon SageMaker is a system that can build, train, and deploy machine learning (ML) models for any use case with fully managed infrastructure, tools, and workflows.📄️ StochasticAIStochastic Acceleration Platform aims to simplify the life cycle of a Deep Learning model. From uploading and versioning the model, through training, compression and acceleration to putting it into production.📄️ Nebula (Symbl.ai)Nebula is a large language model (LLM) built by Symbl.ai. It is trained to perform generative tasks on human conversations. Nebula excels at modeling the nuanced details of a conversation and performing tasks on the conversation.📄️ TextGenGitHub:oobabooga/text-generation-webui A gradio web UI for running Large Language Models like LLaMA, llama.cpp, GPT-J, Pythia, OPT, and GALACTICA.📄️ Titan TakeoffTitanML helps businesses build and deploy better, smaller, cheaper, and faster NLP models through our training, compression, and inference optimization platform.📄️ Tongyi QwenTongyi Qwen is a large-scale language model developed by Alibaba's Damo Academy. It is capable of understanding user intent through natural language understanding and semantic analysis, based on user input in natural language. It provides services and assistance to users in different domains and tasks. By providing clear and detailed instructions, you can obtain results that better align with your expectations.📄️ vLLMvLLM is a fast and easy-to-use library for LLM inference and serving, offering:📄️ WriterWriter is a platform to generate different language content.📄️ Xorbits Inference (Xinference)Xinference is a powerful and versatile library designed to serve LLMs, Previous OpenAI Functions Metadata Tagger Next AI21 CommunityDiscordTwitterGitHubPythonJS/TSMoreHomepageBlogCopyright © 2023 LangChain, Inc. CommunityDiscordTwitterGitHubPythonJS/TSMoreHomepageBlog CommunityDiscordTwitter Community GitHubPythonJS/TS GitHub MoreHomepageBlog More Copyright © 2023 LangChain, Inc. Copyright © 2023 LangChain, Inc. Skip to main content (#docusaurus_skipToContent_fallback) 🦜️🔗 LangChain (/) Docs (/docs/get_started/introduction) Use cases (/docs/use_cases) Integrations (/docs/integrations) API (https://api.python.langchain.com) LangSmith (https://smith.langchain.com) JS/TS Docs (https://js.langchain.com/docs)  (https://github.com/hwchase17/langchain) Integrations (/docs/integrations) Callbacks (/docs/integrations/callbacks/) Chat models (/docs/integrations/chat/) Chat loaders (/docs/integrations/chat_loaders/) Document loaders (/docs/integrations/document_loaders/) Document transformers (/docs/integrations/document_transformers/) LLMs (/docs/integrations/llms/) AI21 (/docs/integrations/llms/ai21) Aleph Alpha (/docs/integrations/llms/aleph_alpha) Amazon API Gateway (/docs/integrations/llms/amazon_api_gateway) Anyscale (/docs/integrations/llms/anyscale) Azure ML (/docs/integrations/llms/azure_ml) Azure OpenAI (/docs/integrations/llms/azure_openai) Banana (/docs/integrations/llms/banana) Baseten (/docs/integrations/llms/baseten) Beam (/docs/integrations/llms/beam) Bedrock (/docs/integrations/llms/bedrock) Bittensor (/docs/integrations/llms/bittensor) CerebriumAI (/docs/integrations/llms/cerebriumai) ChatGLM (/docs/integrations/llms/chatglm) Clarifai (/docs/integrations/llms/clarifai) Cohere (/docs/integrations/llms/cohere) C Transformers (/docs/integrations/llms/ctransformers) CTranslate2 (/docs/integrations/llms/ctranslate2) Databricks (/docs/integrations/llms/databricks) DeepInfra (/docs/integrations/llms/deepinfra) DeepSparse (/docs/integrations/llms/deepsparse) Eden AI (/docs/integrations/llms/edenai) Fireworks (/docs/integrations/llms/fireworks) ForefrontAI (/docs/integrations/llms/forefrontai) Google Vertex AI PaLM (/docs/integrations/llms/google_vertex_ai_palm) GooseAI (/docs/integrations/llms/gooseai) GPT4All (/docs/integrations/llms/gpt4all) Hugging Face Hub (/docs/integrations/llms/huggingface_hub) Hugging Face Local Pipelines (/docs/integrations/llms/huggingface_pipelines) Huggingface TextGen Inference (/docs/integrations/llms/huggingface_textgen_inference) JSONFormer (/docs/integrations/llms/jsonformer_experimental) KoboldAI API (/docs/integrations/llms/koboldai) Llama.cpp (/docs/integrations/llms/llamacpp) LLM Caching integrations (/docs/integrations/llms/llm_caching) Manifest (/docs/integrations/llms/manifest) Minimax (/docs/integrations/llms/minimax) Modal (/docs/integrations/llms/modal) MosaicML (/docs/integrations/llms/mosaicml) NLP Cloud (/docs/integrations/llms/nlpcloud) OctoAI (/docs/integrations/llms/octoai) Ollama (/docs/integrations/llms/ollama) OpaquePrompts (/docs/integrations/llms/opaqueprompts) OpenAI (/docs/integrations/llms/openai) OpenLLM (/docs/integrations/llms/openllm) OpenLM (/docs/integrations/llms/openlm) Petals (/docs/integrations/llms/petals) PipelineAI (/docs/integrations/llms/pipelineai) Predibase (/docs/integrations/llms/predibase) Prediction Guard (/docs/integrations/llms/predictionguard) PromptLayer OpenAI (/docs/integrations/llms/promptlayer_openai) RELLM (/docs/integrations/llms/rellm_experimental) Replicate (/docs/integrations/llms/replicate) Runhouse (/docs/integrations/llms/runhouse) SageMakerEndpoint (/docs/integrations/llms/sagemaker) StochasticAI (/docs/integrations/llms/stochasticai) Nebula (Symbl.ai) (/docs/integrations/llms/symblai_nebula) TextGen (/docs/integrations/llms/textgen) Titan Takeoff (/docs/integrations/llms/titan_takeoff) Tongyi Qwen (/docs/integrations/llms/tongyi) vLLM (/docs/integrations/llms/vllm) Writer (/docs/integrations/llms/writer) Xorbits Inference (Xinference) (/docs/integrations/llms/xinference) Memory (/docs/integrations/memory/) Retrievers (/docs/integrations/retrievers/) Text embedding models (/docs/integrations/text_embedding/) Agents & Toolkits (/docs/integrations/toolkits/) Tools (/docs/integrations/tools/) Vector stores (/docs/integrations/vectorstores/) Grouped by provider (/docs/integrations/providers/)  (/) Integrations (/docs/integrations) 📄️ AI21AI21 Studio provides API access to Jurassic-2 large language models. (/docs/integrations/llms/ai21) 📄️ Aleph AlphaThe Luminous series is a family of large language models. (/docs/integrations/llms/aleph_alpha) 📄️ Amazon API GatewayAmazon API Gateway is a fully managed service that makes it easy for developers to create, publish, maintain, monitor, and secure APIs at any scale. APIs act as the "front door" for applications to access data, business logic, or functionality from your backend services. Using API Gateway, you can create RESTful APIs and WebSocket APIs that enable real-time two-way communication applications. API Gateway supports containerized and serverless workloads, as well as web applications. (/docs/integrations/llms/amazon_api_gateway) 📄️ AnyscaleAnyscale is a fully-managed Ray platform, on which you can build, deploy, and manage scalable AI and Python applications (/docs/integrations/llms/anyscale) 📄️ Azure MLAzure ML is a platform used to build, train, and deploy machine learning models. Users can explore the types of models to deploy in the Model Catalog, which provides Azure Foundation Models and OpenAI Models. Azure Foundation Models include various open-source models and popular Hugging Face models. Users can also import models of their liking into AzureML. (/docs/integrations/llms/azure_ml) 📄️ Azure OpenAIThis notebook goes over how to use Langchain with Azure OpenAI. (/docs/integrations/llms/azure_openai) 📄️ BananaBanana is focused on building the machine learning infrastructure. (/docs/integrations/llms/banana) 📄️ BasetenBaseten provides all the infrastructure you need to deploy and serve ML models performantly, scalably, and cost-efficiently. (/docs/integrations/llms/baseten) 📄️ BeamCalls the Beam API wrapper to deploy and make subsequent calls to an instance of the gpt2 LLM in a cloud deployment. Requires installation of the Beam library and registration of Beam Client ID and Client Secret. By calling the wrapper an instance of the model is created and run, with returned text relating to the prompt. Additional calls can then be made by directly calling the Beam API. (/docs/integrations/llms/beam) 📄️ BedrockAmazon Bedrock is a fully managed service that makes FMs from leading AI startups and Amazon available via an API, so you can choose from a wide range of FMs to find the model that is best suited for your use case (/docs/integrations/llms/bedrock) 📄️ BittensorBittensor is a mining network, similar to Bitcoin, that includes built-in incentives designed to encourage miners to contribute compute + knowledge. (/docs/integrations/llms/bittensor) 📄️ CerebriumAICerebrium is an AWS Sagemaker alternative. It also provides API access to several LLM models. (/docs/integrations/llms/cerebriumai) 📄️ ChatGLMChatGLM-6B is an open bilingual language model based on General Language Model (GLM) framework, with 6.2 billion parameters. With the quantization technique, users can deploy locally on consumer-grade graphics cards (only 6GB of GPU memory is required at the INT4 quantization level). (/docs/integrations/llms/chatglm) 📄️ ClarifaiClarifai is an AI Platform that provides the full AI lifecycle ranging from data exploration, data labeling, model training, evaluation, and inference. (/docs/integrations/llms/clarifai) 📄️ CohereCohere is a Canadian startup that provides natural language processing models that help companies improve human-machine interactions. (/docs/integrations/llms/cohere) 📄️ C TransformersThe C Transformers library provides Python bindings for GGML models. (/docs/integrations/llms/ctransformers) 📄️ CTranslate2CTranslate2 is a C++ and Python library for efficient inference with Transformer models. (/docs/integrations/llms/ctranslate2) 📄️ DatabricksThe Databricks Lakehouse Platform unifies data, analytics, and AI on one platform. (/docs/integrations/llms/databricks) 📄️ DeepInfraDeepInfra provides several LLMs. (/docs/integrations/llms/deepinfra) 📄️ DeepSparseThis page covers how to use the DeepSparse inference runtime within LangChain. (/docs/integrations/llms/deepsparse) 📄️ Eden AIEden AI is revolutionizing the AI landscape by uniting the best AI providers, empowering users to unlock limitless possibilities and tap into the true potential of artificial intelligence. With an all-in-one comprehensive and hassle-free platform, it allows users to deploy AI features to production lightning fast, enabling effortless access to the full breadth of AI capabilities via a single API. (website//edenai.co/) (/docs/integrations/llms/edenai) 📄️ FireworksFireworks accelerates product development on generative AI by creating an innovative AI experiment and production platform. (/docs/integrations/llms/fireworks) 📄️ ForefrontAIThe Forefront platform gives you the ability to fine-tune and use open source large language models. (/docs/integrations/llms/forefrontai) 📄️ Google Vertex AI PaLMNote: This is seperate from the Google PaLM integration, it exposes Vertex AI PaLM API on Google Cloud. (/docs/integrations/llms/google_vertex_ai_palm) 📄️ GooseAIGooseAI is a fully managed NLP-as-a-Service, delivered via API. GooseAI provides access to these models. (/docs/integrations/llms/gooseai) 📄️ GPT4AllGitHub:nomic-ai/gpt4all an ecosystem of open-source chatbots trained on a massive collections of clean assistant data including code, stories and dialogue. (/docs/integrations/llms/gpt4all) 📄️ Hugging Face HubThe Hugging Face Hub is a platform with over 120k models, 20k datasets, and 50k demo apps (Spaces), all open source and publicly available, in an online platform where people can easily collaborate and build ML together. (/docs/integrations/llms/huggingface_hub) 📄️ Hugging Face Local PipelinesHugging Face models can be run locally through the HuggingFacePipeline class. (/docs/integrations/llms/huggingface_pipelines) 📄️ Huggingface TextGen InferenceText Generation Inference is a Rust, Python and gRPC server for text generation inference. Used in production at HuggingFace to power LLMs api-inference widgets. (/docs/integrations/llms/huggingface_textgen_inference) 📄️ JSONFormerJSONFormer is a library that wraps local HuggingFace pipeline models for structured decoding of a subset of the JSON Schema. (/docs/integrations/llms/jsonformer_experimental) 📄️ KoboldAI APIKoboldAI is a "a browser-based front-end for AI-assisted writing with multiple local & remote AI models...". It has a public and local API that is able to be used in langchain. (/docs/integrations/llms/koboldai) 📄️ Llama.cppllama-cpp-python is a Python binding for llama.cpp. (/docs/integrations/llms/llamacpp) 📄️ LLM Caching integrationsThis notebook covers how to cache results of individual LLM calls using different caches. (/docs/integrations/llms/llm_caching) 📄️ ManifestThis notebook goes over how to use Manifest and LangChain. (/docs/integrations/llms/manifest) 📄️ MinimaxMinimax is a Chinese startup that provides natural language processing models for companies and individuals. (/docs/integrations/llms/minimax) 📄️ ModalThe Modal cloud platform provides convenient, on-demand access to serverless cloud compute from Python scripts on your local computer. (/docs/integrations/llms/modal) 📄️ MosaicMLMosaicML offers a managed inference service. You can either use a variety of open source models, or deploy your own. (/docs/integrations/llms/mosaicml) 📄️ NLP CloudThe NLP Cloud serves high performance pre-trained or custom models for NER, sentiment-analysis, classification, summarization, paraphrasing, grammar and spelling correction, keywords and keyphrases extraction, chatbot, product description and ad generation, intent classification, text generation, image generation, blog post generation, code generation, question answering, automatic speech recognition, machine translation, language detection, semantic search, semantic similarity, tokenization, POS tagging, embeddings, and dependency parsing. It is ready for production, served through a REST API. (/docs/integrations/llms/nlpcloud) 📄️ OctoAIOctoML is a service with efficient compute. It enables users to integrate their choice of AI models into applications. The OctoAI compute service helps you run, tune, and scale AI applications. (/docs/integrations/llms/octoai) 📄️ OllamaOllama allows you to run open-source large language models, such as Llama 2, locally. (/docs/integrations/llms/ollama) 📄️ OpaquePromptsOpaquePrompts is a service that enables applications to leverage the power of language models without compromising user privacy. Designed for composability and ease of integration into existing applications and services, OpaquePrompts is consumable via a simple Python library as well as through LangChain. Perhaps more importantly, OpaquePrompts leverages the power of confidential computing to ensure that even the OpaquePrompts service itself cannot access the data it is protecting. (/docs/integrations/llms/opaqueprompts) 📄️ OpenAIOpenAI offers a spectrum of models with different levels of power suitable for different tasks. (/docs/integrations/llms/openai) 📄️ OpenLLM🦾 OpenLLM is an open platform for operating large language models (LLMs) in production. It enables developers to easily run inference with any open-source LLMs, deploy to the cloud or on-premises, and build powerful AI apps. (/docs/integrations/llms/openllm) 📄️ OpenLMOpenLM is a zero-dependency OpenAI-compatible LLM provider that can call different inference endpoints directly via HTTP. (/docs/integrations/llms/openlm) 📄️ PetalsPetals runs 100B+ language models at home, BitTorrent-style. (/docs/integrations/llms/petals) 📄️ PipelineAIPipelineAI allows you to run your ML models at scale in the cloud. It also provides API access to several LLM models. (/docs/integrations/llms/pipelineai) 📄️ PredibasePredibase allows you to train, finetune, and deploy any ML model—from linear regression to large language model. (/docs/integrations/llms/predibase) 📄️ Prediction GuardBasic LLM usage (/docs/integrations/llms/predictionguard) 📄️ PromptLayer OpenAIPromptLayer is the first platform that allows you to track, manage, and share your GPT prompt engineering. PromptLayer acts a middleware between your code and OpenAI’s python library. (/docs/integrations/llms/promptlayer_openai) 📄️ RELLMRELLM is a library that wraps local Hugging Face pipeline models for structured decoding. (/docs/integrations/llms/rellm_experimental) 📄️ ReplicateReplicate runs machine learning models in the cloud. We have a library of open-source models that you can run with a few lines of code. If you're building your own machine learning models, Replicate makes it easy to deploy them at scale. (/docs/integrations/llms/replicate) 📄️ RunhouseThe Runhouse allows remote compute and data across environments and users. See the Runhouse docs. (/docs/integrations/llms/runhouse) 📄️ SageMakerEndpointAmazon SageMaker is a system that can build, train, and deploy machine learning (ML) models for any use case with fully managed infrastructure, tools, and workflows. (/docs/integrations/llms/sagemaker) 📄️ StochasticAIStochastic Acceleration Platform aims to simplify the life cycle of a Deep Learning model. From uploading and versioning the model, through training, compression and acceleration to putting it into production. (/docs/integrations/llms/stochasticai) 📄️ Nebula (Symbl.ai)Nebula is a large language model (LLM) built by Symbl.ai. It is trained to perform generative tasks on human conversations. Nebula excels at modeling the nuanced details of a conversation and performing tasks on the conversation. (/docs/integrations/llms/symblai_nebula) 📄️ TextGenGitHub:oobabooga/text-generation-webui A gradio web UI for running Large Language Models like LLaMA, llama.cpp, GPT-J, Pythia, OPT, and GALACTICA. (/docs/integrations/llms/textgen) 📄️ Titan TakeoffTitanML helps businesses build and deploy better, smaller, cheaper, and faster NLP models through our training, compression, and inference optimization platform. (/docs/integrations/llms/titan_takeoff) 📄️ Tongyi QwenTongyi Qwen is a large-scale language model developed by Alibaba's Damo Academy. It is capable of understanding user intent through natural language understanding and semantic analysis, based on user input in natural language. It provides services and assistance to users in different domains and tasks. By providing clear and detailed instructions, you can obtain results that better align with your expectations. (/docs/integrations/llms/tongyi) 📄️ vLLMvLLM is a fast and easy-to-use library for LLM inference and serving, offering: (/docs/integrations/llms/vllm) 📄️ WriterWriter is a platform to generate different language content. (/docs/integrations/llms/writer) 📄️ Xorbits Inference (Xinference)Xinference is a powerful and versatile library designed to serve LLMs, (/docs/integrations/llms/xinference) PreviousOpenAI Functions Metadata Tagger (/docs/integrations/document_transformers/openai_metadata_tagger) NextAI21 (/docs/integrations/llms/ai21) Discord (https://discord.gg/cU2adEyC7w) Twitter (https://twitter.com/LangChainAI) Python (https://github.com/hwchase17/langchain) JS/TS (https://github.com/hwchase17/langchainjs) Homepage (https://langchain.com) Blog (https://blog.langchain.dev)