Arthur is a model monitoring and observability platform. The following guide shows how to run a registered chat LLM with the Arthur callback handler to automatically log model inferences to Arthur. If you do not have a model currently onboarded to Arthur, visit our onboarding guide for generative text models. For more information about how to use the Arthur SDK, visit our docs. Place Arthur credentials here Create Langchain LLM with Arthur callback handler Running the chat LLM with this run function will save the chat history in an ongoing list so that the conversation can reference earlier messages and log each response to the Arthur platform. You can view the history of this model's inferences on your model dashboard page. Enter q to quit the run loop IntegrationsCallbacksChat modelsChat loadersDocument loadersDocument transformersLLMsMemoryRetrieversText embedding modelsAgents & ToolkitsToolsVector storesGrouped by providerActiveloop Deep LakeAI21 LabsAimAINetworkAirbyteAirtableAleph AlphaAlibaba Cloud OpensearchAmazon API GatewayAnalyticDBAnnoyAnyscaleApifyArangoDBArgillaArthurArxivAtlasAwaDBAWS S3 DirectoryAZLyricsAzure Blob StorageAzure Cognitive SearchAzure OpenAIBagelDBBananaBasetenBeamBedrockBiliBiliNIBittensorBlackboardBrave SearchCassandraCerebriumAIChaindeskChromaClarifaiClearMLClickHouseCnosDBCohereCollege ConfidentialCometConfident AIConfluenceC TransformersDashVectorDatabricksDatadog TracingDatadog LogsDataForSEODeepInfraDeepSparseDiffbotDingoDiscordDocArrayDocugamiDuckDBElasticsearchEpsillaEverNoteFacebook ChatFacebook FaissFigmaFireworksFlyteForefrontAIGitGitBookGoldenGoogle BigQueryGoogle Cloud StorageGoogle DriveGoogle SearchGoogle SerperGoogle Vertex AI MatchingEngineGooseAIGPT4AllGraphsignalGrobidGutenbergHacker NewsHazy ResearchHeliconeHologresHugging FaceiFixitIMSDbInfinoJinaKonkoLanceDBLangChain Decorators âœ¨Llama.cppLog10MarqoMediaWikiDumpMeilisearchMetalMicrosoft OneDriveMicrosoft PowerPointMicrosoft WordMilvusMinimaxMLflow AI GatewayMLflowModalModelScopeModern TreasuryMomentoMongoDB AtlasMotherduckMyScaleNeo4jNLPCloudNotion DBObsidianOpenAIOpenLLMOpenSearchOpenWeatherMapPetalsPostgres EmbeddingPGVectorPineconePipelineAIPortkeyPredibasePrediction GuardPromptLayerPsychicPubMedQdrantRay ServeRebuffRedditRedisReplicateRoamRocksetRunhouseRWKV-4SageMaker EndpointSageMaker TrackingScaNNSearxNG Search APISerpAPIShale ProtocolSingleStoreDBscikit-learnSlackspaCySpreedlyStarRocksStochasticAIStripeSupabase (Postgres)NebulaTairTelegramTencentVectorDBTensorFlow DatasetsTigris2MarkdownTrelloTruLensTwitterTypesenseUnstructuredUSearchVearchVectaraVespaWandB TracingWeights & BiasesWeatherWeaviateWhatsAppWhyLabsWikipediaWolfram AlphaWriterXataXorbits Inference (Xinference)Yeager.aiYouTubeZepZilliz Callbacks Chat models Chat loaders Document loaders Document transformers LLMs Memory Retrievers Text embedding models Agents & Toolkits Tools Vector stores Grouped by providerActiveloop Deep LakeAI21 LabsAimAINetworkAirbyteAirtableAleph AlphaAlibaba Cloud OpensearchAmazon API GatewayAnalyticDBAnnoyAnyscaleApifyArangoDBArgillaArthurArxivAtlasAwaDBAWS S3 DirectoryAZLyricsAzure Blob StorageAzure Cognitive SearchAzure OpenAIBagelDBBananaBasetenBeamBedrockBiliBiliNIBittensorBlackboardBrave SearchCassandraCerebriumAIChaindeskChromaClarifaiClearMLClickHouseCnosDBCohereCollege ConfidentialCometConfident AIConfluenceC TransformersDashVectorDatabricksDatadog TracingDatadog LogsDataForSEODeepInfraDeepSparseDiffbotDingoDiscordDocArrayDocugamiDuckDBElasticsearchEpsillaEverNoteFacebook ChatFacebook FaissFigmaFireworksFlyteForefrontAIGitGitBookGoldenGoogle BigQueryGoogle Cloud StorageGoogle DriveGoogle SearchGoogle SerperGoogle Vertex AI MatchingEngineGooseAIGPT4AllGraphsignalGrobidGutenbergHacker NewsHazy ResearchHeliconeHologresHugging FaceiFixitIMSDbInfinoJinaKonkoLanceDBLangChain Decorators âœ¨Llama.cppLog10MarqoMediaWikiDumpMeilisearchMetalMicrosoft OneDriveMicrosoft PowerPointMicrosoft WordMilvusMinimaxMLflow AI GatewayMLflowModalModelScopeModern TreasuryMomentoMongoDB AtlasMotherduckMyScaleNeo4jNLPCloudNotion DBObsidianOpenAIOpenLLMOpenSearchOpenWeatherMapPetalsPostgres EmbeddingPGVectorPineconePipelineAIPortkeyPredibasePrediction GuardPromptLayerPsychicPubMedQdrantRay ServeRebuffRedditRedisReplicateRoamRocksetRunhouseRWKV-4SageMaker EndpointSageMaker TrackingScaNNSearxNG Search APISerpAPIShale ProtocolSingleStoreDBscikit-learnSlackspaCySpreedlyStarRocksStochasticAIStripeSupabase (Postgres)NebulaTairTelegramTencentVectorDBTensorFlow DatasetsTigris2MarkdownTrelloTruLensTwitterTypesenseUnstructuredUSearchVearchVectaraVespaWandB TracingWeights & BiasesWeatherWeaviateWhatsAppWhyLabsWikipediaWolfram AlphaWriterXataXorbits Inference (Xinference)Yeager.aiYouTubeZepZilliz Activeloop Deep Lake AI21 Labs Aim AINetwork Airbyte Airtable Aleph Alpha Alibaba Cloud Opensearch Amazon API Gateway AnalyticDB Annoy Anyscale Apify ArangoDB Argilla Arthur Arxiv Atlas AwaDB AWS S3 Directory AZLyrics Azure Blob Storage Azure Cognitive Search Azure OpenAI BagelDB Banana Baseten Beam Bedrock BiliBili NIBittensor Blackboard Brave Search Cassandra CerebriumAI Chaindesk Chroma Clarifai ClearML ClickHouse CnosDB Cohere College Confidential Comet Confident AI Confluence C Transformers DashVector Databricks Datadog Tracing Datadog Logs DataForSEO DeepInfra DeepSparse Diffbot Dingo Discord DocArray Docugami DuckDB Elasticsearch Epsilla EverNote Facebook Chat Facebook Faiss Figma Fireworks Flyte ForefrontAI Git GitBook Golden Google BigQuery Google Cloud Storage Google Drive Google Search Google Serper Google Vertex AI MatchingEngine GooseAI GPT4All Graphsignal Grobid Gutenberg Hacker News Hazy Research Helicone Hologres Hugging Face iFixit IMSDb Infino Jina Konko LanceDB LangChain Decorators âœ¨ Llama.cpp Log10 Marqo MediaWikiDump Meilisearch Metal Microsoft OneDrive Microsoft PowerPoint Microsoft Word Milvus Minimax MLflow AI Gateway MLflow Modal ModelScope Modern Treasury Momento MongoDB Atlas Motherduck MyScale Neo4j NLPCloud Notion DB Obsidian OpenAI OpenLLM OpenSearch OpenWeatherMap Petals Postgres Embedding PGVector Pinecone PipelineAI Portkey Predibase Prediction Guard PromptLayer Psychic PubMed Qdrant Ray Serve Rebuff Reddit Redis Replicate Roam Rockset Runhouse RWKV-4 SageMaker Endpoint SageMaker Tracking ScaNN SearxNG Search API SerpAPI Shale Protocol SingleStoreDB scikit-learn Slack spaCy Spreedly StarRocks StochasticAI Stripe Supabase (Postgres) Nebula Tair Telegram TencentVectorDB TensorFlow Datasets Tigris 2Markdown Trello TruLens Twitter Typesense Unstructured USearch Vearch Vectara Vespa WandB Tracing Weights & Biases Weather Weaviate WhatsApp WhyLabs Wikipedia Wolfram Alpha Writer Xata Xorbits Inference (Xinference) Yeager.ai YouTube Zep Zilliz  Integrations Grouped by provider Arthur ArthurCallbackHandler StreamingStdOutCallbackHandler ChatOpenAI HumanMessage Discord Twitter Python JS/TS Homepage Blog Skip to main contentðŸ¦œï¸ðŸ”— LangChainDocsUse casesIntegrationsAPILangSmithJS/TS DocsCTRLKIntegrationsCallbacksChat modelsChat loadersDocument loadersDocument transformersLLMsMemoryRetrieversText embedding modelsAgents & ToolkitsToolsVector storesGrouped by providerActiveloop Deep LakeAI21 LabsAimAINetworkAirbyteAirtableAleph AlphaAlibaba Cloud OpensearchAmazon API GatewayAnalyticDBAnnoyAnyscaleApifyArangoDBArgillaArthurArxivAtlasAwaDBAWS S3 DirectoryAZLyricsAzure Blob StorageAzure Cognitive SearchAzure OpenAIBagelDBBananaBasetenBeamBedrockBiliBiliNIBittensorBlackboardBrave SearchCassandraCerebriumAIChaindeskChromaClarifaiClearMLClickHouseCnosDBCohereCollege ConfidentialCometConfident AIConfluenceC TransformersDashVectorDatabricksDatadog TracingDatadog LogsDataForSEODeepInfraDeepSparseDiffbotDingoDiscordDocArrayDocugamiDuckDBElasticsearchEpsillaEverNoteFacebook ChatFacebook FaissFigmaFireworksFlyteForefrontAIGitGitBookGoldenGoogle BigQueryGoogle Cloud StorageGoogle DriveGoogle SearchGoogle SerperGoogle Vertex AI MatchingEngineGooseAIGPT4AllGraphsignalGrobidGutenbergHacker NewsHazy ResearchHeliconeHologresHugging FaceiFixitIMSDbInfinoJinaKonkoLanceDBLangChain Decorators âœ¨Llama.cppLog10MarqoMediaWikiDumpMeilisearchMetalMicrosoft OneDriveMicrosoft PowerPointMicrosoft WordMilvusMinimaxMLflow AI GatewayMLflowModalModelScopeModern TreasuryMomentoMongoDB AtlasMotherduckMyScaleNeo4jNLPCloudNotion DBObsidianOpenAIOpenLLMOpenSearchOpenWeatherMapPetalsPostgres EmbeddingPGVectorPineconePipelineAIPortkeyPredibasePrediction GuardPromptLayerPsychicPubMedQdrantRay ServeRebuffRedditRedisReplicateRoamRocksetRunhouseRWKV-4SageMaker EndpointSageMaker TrackingScaNNSearxNG Search APISerpAPIShale ProtocolSingleStoreDBscikit-learnSlackspaCySpreedlyStarRocksStochasticAIStripeSupabase (Postgres)NebulaTairTelegramTencentVectorDBTensorFlow DatasetsTigris2MarkdownTrelloTruLensTwitterTypesenseUnstructuredUSearchVearchVectaraVespaWandB TracingWeights & BiasesWeatherWeaviateWhatsAppWhyLabsWikipediaWolfram AlphaWriterXataXorbits Inference (Xinference)Yeager.aiYouTubeZepZillizIntegrationsGrouped by providerArthurArthurArthur is a model monitoring and observability platform.The following guide shows how to run a registered chat LLM with the Arthur callback handler to automatically log model inferences to Arthur.If you do not have a model currently onboarded to Arthur, visit our onboarding guide for generative text models. For more information about how to use the Arthur SDK, visit our docs.from langchain.callbacks import ArthurCallbackHandlerfrom langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandlerfrom langchain.chat_models import ChatOpenAIfrom langchain.schema import HumanMessageAPI Reference:ArthurCallbackHandlerStreamingStdOutCallbackHandlerChatOpenAIHumanMessagePlace Arthur credentials herearthur_url = "https://app.arthur.ai"arthur_login = "your-arthur-login-username-here"arthur_model_id = "your-arthur-model-id-here"Create Langchain LLM with Arthur callback handlerdef make_langchain_chat_llm(chat_model=):    return ChatOpenAI(        streaming=True,        temperature=0.1,        callbacks=[            StreamingStdOutCallbackHandler(),            ArthurCallbackHandler.from_credentials(                arthur_model_id,                 arthur_url=arthur_url,                 arthur_login=arthur_login)        ])chatgpt = make_langchain_chat_llm()    Please enter password for admin: Â·Â·Â·Â·Â·Â·Â·Â·Running the chat LLM with this run function will save the chat history in an ongoing list so that the conversation can reference earlier messages and log each response to the Arthur platform. You can view the history of this model's inferences on your model dashboard page.Enter q to quit the run loopdef run(llm):    history = []    while True:        user_input = input("\n>>> input >>>\n>>>: ")        if user_input == "q":            break        history.append(HumanMessage(content=user_input))        history.append(llm(history))run(chatgpt)        >>> input >>>    >>>: What is a callback handler?    A callback handler, also known as a callback function or callback method, is a piece of code that is executed in response to a specific event or condition. It is commonly used in programming languages that support event-driven or asynchronous programming paradigms.        The purpose of a callback handler is to provide a way for developers to define custom behavior that should be executed when a certain event occurs. Instead of waiting for a result or blocking the execution, the program registers a callback function and continues with other tasks. When the event is triggered, the callback function is invoked, allowing the program to respond accordingly.        Callback handlers are commonly used in various scenarios, such as handling user input, responding to network requests, processing asynchronous operations, and implementing event-driven architectures. They provide a flexible and modular way to handle events and decouple different components of a system.    >>> input >>>    >>>: What do I need to do to get the full benefits of this    To get the full benefits of using a callback handler, you should consider the following:        1. Understand the event or condition: Identify the specific event or condition that you want to respond to with a callback handler. This could be user input, network requests, or any other asynchronous operation.        2. Define the callback function: Create a function that will be executed when the event or condition occurs. This function should contain the desired behavior or actions you want to take in response to the event.        3. Register the callback function: Depending on the programming language or framework you are using, you may need to register or attach the callback function to the appropriate event or condition. This ensures that the callback function is invoked when the event occurs.        4. Handle the callback: Implement the necessary logic within the callback function to handle the event or condition. This could involve updating the user interface, processing data, making further requests, or triggering other actions.        5. Consider error handling: It's important to handle any potential errors or exceptions that may occur within the callback function. This ensures that your program can gracefully handle unexpected situations and prevent crashes or undesired behavior.        6. Maintain code readability and modularity: As your codebase grows, it's crucial to keep your callback handlers organized and maintainable. Consider using design patterns or architectural principles to structure your code in a modular and scalable way.        By following these steps, you can leverage the benefits of callback handlers, such as asynchronous and event-driven programming, improved responsiveness, and modular code design.    >>> input >>>    >>>: qPreviousArgillaNextArxivCommunityDiscordTwitterGitHubPythonJS/TSMoreHomepageBlogCopyright Â© 2023 LangChain, Inc. Skip to main content ðŸ¦œï¸ðŸ”— LangChainDocsUse casesIntegrationsAPILangSmithJS/TS DocsCTRLK ðŸ¦œï¸ðŸ”— LangChainDocsUse casesIntegrationsAPI LangSmithJS/TS DocsCTRLK  CTRLK CTRLK  CTRLK   IntegrationsCallbacksChat modelsChat loadersDocument loadersDocument transformersLLMsMemoryRetrieversText embedding modelsAgents & ToolkitsToolsVector storesGrouped by providerActiveloop Deep LakeAI21 LabsAimAINetworkAirbyteAirtableAleph AlphaAlibaba Cloud OpensearchAmazon API GatewayAnalyticDBAnnoyAnyscaleApifyArangoDBArgillaArthurArxivAtlasAwaDBAWS S3 DirectoryAZLyricsAzure Blob StorageAzure Cognitive SearchAzure OpenAIBagelDBBananaBasetenBeamBedrockBiliBiliNIBittensorBlackboardBrave SearchCassandraCerebriumAIChaindeskChromaClarifaiClearMLClickHouseCnosDBCohereCollege ConfidentialCometConfident AIConfluenceC TransformersDashVectorDatabricksDatadog TracingDatadog LogsDataForSEODeepInfraDeepSparseDiffbotDingoDiscordDocArrayDocugamiDuckDBElasticsearchEpsillaEverNoteFacebook ChatFacebook FaissFigmaFireworksFlyteForefrontAIGitGitBookGoldenGoogle BigQueryGoogle Cloud StorageGoogle DriveGoogle SearchGoogle SerperGoogle Vertex AI MatchingEngineGooseAIGPT4AllGraphsignalGrobidGutenbergHacker NewsHazy ResearchHeliconeHologresHugging FaceiFixitIMSDbInfinoJinaKonkoLanceDBLangChain Decorators âœ¨Llama.cppLog10MarqoMediaWikiDumpMeilisearchMetalMicrosoft OneDriveMicrosoft PowerPointMicrosoft WordMilvusMinimaxMLflow AI GatewayMLflowModalModelScopeModern TreasuryMomentoMongoDB AtlasMotherduckMyScaleNeo4jNLPCloudNotion DBObsidianOpenAIOpenLLMOpenSearchOpenWeatherMapPetalsPostgres EmbeddingPGVectorPineconePipelineAIPortkeyPredibasePrediction GuardPromptLayerPsychicPubMedQdrantRay ServeRebuffRedditRedisReplicateRoamRocksetRunhouseRWKV-4SageMaker EndpointSageMaker TrackingScaNNSearxNG Search APISerpAPIShale ProtocolSingleStoreDBscikit-learnSlackspaCySpreedlyStarRocksStochasticAIStripeSupabase (Postgres)NebulaTairTelegramTencentVectorDBTensorFlow DatasetsTigris2MarkdownTrelloTruLensTwitterTypesenseUnstructuredUSearchVearchVectaraVespaWandB TracingWeights & BiasesWeatherWeaviateWhatsAppWhyLabsWikipediaWolfram AlphaWriterXataXorbits Inference (Xinference)Yeager.aiYouTubeZepZillizIntegrationsGrouped by providerArthurArthurArthur is a model monitoring and observability platform.The following guide shows how to run a registered chat LLM with the Arthur callback handler to automatically log model inferences to Arthur.If you do not have a model currently onboarded to Arthur, visit our onboarding guide for generative text models. For more information about how to use the Arthur SDK, visit our docs.from langchain.callbacks import ArthurCallbackHandlerfrom langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandlerfrom langchain.chat_models import ChatOpenAIfrom langchain.schema import HumanMessageAPI Reference:ArthurCallbackHandlerStreamingStdOutCallbackHandlerChatOpenAIHumanMessagePlace Arthur credentials herearthur_url = "https://app.arthur.ai"arthur_login = "your-arthur-login-username-here"arthur_model_id = "your-arthur-model-id-here"Create Langchain LLM with Arthur callback handlerdef make_langchain_chat_llm(chat_model=):    return ChatOpenAI(        streaming=True,        temperature=0.1,        callbacks=[            StreamingStdOutCallbackHandler(),            ArthurCallbackHandler.from_credentials(                arthur_model_id,                 arthur_url=arthur_url,                 arthur_login=arthur_login)        ])chatgpt = make_langchain_chat_llm()    Please enter password for admin: Â·Â·Â·Â·Â·Â·Â·Â·Running the chat LLM with this run function will save the chat history in an ongoing list so that the conversation can reference earlier messages and log each response to the Arthur platform. You can view the history of this model's inferences on your model dashboard page.Enter q to quit the run loopdef run(llm):    history = []    while True:        user_input = input("\n>>> input >>>\n>>>: ")        if user_input == "q":            break        history.append(HumanMessage(content=user_input))        history.append(llm(history))run(chatgpt)        >>> input >>>    >>>: What is a callback handler?    A callback handler, also known as a callback function or callback method, is a piece of code that is executed in response to a specific event or condition. It is commonly used in programming languages that support event-driven or asynchronous programming paradigms.        The purpose of a callback handler is to provide a way for developers to define custom behavior that should be executed when a certain event occurs. Instead of waiting for a result or blocking the execution, the program registers a callback function and continues with other tasks. When the event is triggered, the callback function is invoked, allowing the program to respond accordingly.        Callback handlers are commonly used in various scenarios, such as handling user input, responding to network requests, processing asynchronous operations, and implementing event-driven architectures. They provide a flexible and modular way to handle events and decouple different components of a system.    >>> input >>>    >>>: What do I need to do to get the full benefits of this    To get the full benefits of using a callback handler, you should consider the following:        1. Understand the event or condition: Identify the specific event or condition that you want to respond to with a callback handler. This could be user input, network requests, or any other asynchronous operation.        2. Define the callback function: Create a function that will be executed when the event or condition occurs. This function should contain the desired behavior or actions you want to take in response to the event.        3. Register the callback function: Depending on the programming language or framework you are using, you may need to register or attach the callback function to the appropriate event or condition. This ensures that the callback function is invoked when the event occurs.        4. Handle the callback: Implement the necessary logic within the callback function to handle the event or condition. This could involve updating the user interface, processing data, making further requests, or triggering other actions.        5. Consider error handling: It's important to handle any potential errors or exceptions that may occur within the callback function. This ensures that your program can gracefully handle unexpected situations and prevent crashes or undesired behavior.        6. Maintain code readability and modularity: As your codebase grows, it's crucial to keep your callback handlers organized and maintainable. Consider using design patterns or architectural principles to structure your code in a modular and scalable way.        By following these steps, you can leverage the benefits of callback handlers, such as asynchronous and event-driven programming, improved responsiveness, and modular code design.    >>> input >>>    >>>: qPreviousArgillaNextArxiv IntegrationsCallbacksChat modelsChat loadersDocument loadersDocument transformersLLMsMemoryRetrieversText embedding modelsAgents & ToolkitsToolsVector storesGrouped by providerActiveloop Deep LakeAI21 LabsAimAINetworkAirbyteAirtableAleph AlphaAlibaba Cloud OpensearchAmazon API GatewayAnalyticDBAnnoyAnyscaleApifyArangoDBArgillaArthurArxivAtlasAwaDBAWS S3 DirectoryAZLyricsAzure Blob StorageAzure Cognitive SearchAzure OpenAIBagelDBBananaBasetenBeamBedrockBiliBiliNIBittensorBlackboardBrave SearchCassandraCerebriumAIChaindeskChromaClarifaiClearMLClickHouseCnosDBCohereCollege ConfidentialCometConfident AIConfluenceC TransformersDashVectorDatabricksDatadog TracingDatadog LogsDataForSEODeepInfraDeepSparseDiffbotDingoDiscordDocArrayDocugamiDuckDBElasticsearchEpsillaEverNoteFacebook ChatFacebook FaissFigmaFireworksFlyteForefrontAIGitGitBookGoldenGoogle BigQueryGoogle Cloud StorageGoogle DriveGoogle SearchGoogle SerperGoogle Vertex AI MatchingEngineGooseAIGPT4AllGraphsignalGrobidGutenbergHacker NewsHazy ResearchHeliconeHologresHugging FaceiFixitIMSDbInfinoJinaKonkoLanceDBLangChain Decorators âœ¨Llama.cppLog10MarqoMediaWikiDumpMeilisearchMetalMicrosoft OneDriveMicrosoft PowerPointMicrosoft WordMilvusMinimaxMLflow AI GatewayMLflowModalModelScopeModern TreasuryMomentoMongoDB AtlasMotherduckMyScaleNeo4jNLPCloudNotion DBObsidianOpenAIOpenLLMOpenSearchOpenWeatherMapPetalsPostgres EmbeddingPGVectorPineconePipelineAIPortkeyPredibasePrediction GuardPromptLayerPsychicPubMedQdrantRay ServeRebuffRedditRedisReplicateRoamRocksetRunhouseRWKV-4SageMaker EndpointSageMaker TrackingScaNNSearxNG Search APISerpAPIShale ProtocolSingleStoreDBscikit-learnSlackspaCySpreedlyStarRocksStochasticAIStripeSupabase (Postgres)NebulaTairTelegramTencentVectorDBTensorFlow DatasetsTigris2MarkdownTrelloTruLensTwitterTypesenseUnstructuredUSearchVearchVectaraVespaWandB TracingWeights & BiasesWeatherWeaviateWhatsAppWhyLabsWikipediaWolfram AlphaWriterXataXorbits Inference (Xinference)Yeager.aiYouTubeZepZillizIntegrationsGrouped by providerArthurArthurArthur is a model monitoring and observability platform.The following guide shows how to run a registered chat LLM with the Arthur callback handler to automatically log model inferences to Arthur.If you do not have a model currently onboarded to Arthur, visit our onboarding guide for generative text models. For more information about how to use the Arthur SDK, visit our docs.from langchain.callbacks import ArthurCallbackHandlerfrom langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandlerfrom langchain.chat_models import ChatOpenAIfrom langchain.schema import HumanMessageAPI Reference:ArthurCallbackHandlerStreamingStdOutCallbackHandlerChatOpenAIHumanMessagePlace Arthur credentials herearthur_url = "https://app.arthur.ai"arthur_login = "your-arthur-login-username-here"arthur_model_id = "your-arthur-model-id-here"Create Langchain LLM with Arthur callback handlerdef make_langchain_chat_llm(chat_model=):    return ChatOpenAI(        streaming=True,        temperature=0.1,        callbacks=[            StreamingStdOutCallbackHandler(),            ArthurCallbackHandler.from_credentials(                arthur_model_id,                 arthur_url=arthur_url,                 arthur_login=arthur_login)        ])chatgpt = make_langchain_chat_llm()    Please enter password for admin: Â·Â·Â·Â·Â·Â·Â·Â·Running the chat LLM with this run function will save the chat history in an ongoing list so that the conversation can reference earlier messages and log each response to the Arthur platform. You can view the history of this model's inferences on your model dashboard page.Enter q to quit the run loopdef run(llm):    history = []    while True:        user_input = input("\n>>> input >>>\n>>>: ")        if user_input == "q":            break        history.append(HumanMessage(content=user_input))        history.append(llm(history))run(chatgpt)        >>> input >>>    >>>: What is a callback handler?    A callback handler, also known as a callback function or callback method, is a piece of code that is executed in response to a specific event or condition. It is commonly used in programming languages that support event-driven or asynchronous programming paradigms.        The purpose of a callback handler is to provide a way for developers to define custom behavior that should be executed when a certain event occurs. Instead of waiting for a result or blocking the execution, the program registers a callback function and continues with other tasks. When the event is triggered, the callback function is invoked, allowing the program to respond accordingly.        Callback handlers are commonly used in various scenarios, such as handling user input, responding to network requests, processing asynchronous operations, and implementing event-driven architectures. They provide a flexible and modular way to handle events and decouple different components of a system.    >>> input >>>    >>>: What do I need to do to get the full benefits of this    To get the full benefits of using a callback handler, you should consider the following:        1. Understand the event or condition: Identify the specific event or condition that you want to respond to with a callback handler. This could be user input, network requests, or any other asynchronous operation.        2. Define the callback function: Create a function that will be executed when the event or condition occurs. This function should contain the desired behavior or actions you want to take in response to the event.        3. Register the callback function: Depending on the programming language or framework you are using, you may need to register or attach the callback function to the appropriate event or condition. This ensures that the callback function is invoked when the event occurs.        4. Handle the callback: Implement the necessary logic within the callback function to handle the event or condition. This could involve updating the user interface, processing data, making further requests, or triggering other actions.        5. Consider error handling: It's important to handle any potential errors or exceptions that may occur within the callback function. This ensures that your program can gracefully handle unexpected situations and prevent crashes or undesired behavior.        6. Maintain code readability and modularity: As your codebase grows, it's crucial to keep your callback handlers organized and maintainable. Consider using design patterns or architectural principles to structure your code in a modular and scalable way.        By following these steps, you can leverage the benefits of callback handlers, such as asynchronous and event-driven programming, improved responsiveness, and modular code design.    >>> input >>>    >>>: qPreviousArgillaNextArxiv IntegrationsCallbacksChat modelsChat loadersDocument loadersDocument transformersLLMsMemoryRetrieversText embedding modelsAgents & ToolkitsToolsVector storesGrouped by providerActiveloop Deep LakeAI21 LabsAimAINetworkAirbyteAirtableAleph AlphaAlibaba Cloud OpensearchAmazon API GatewayAnalyticDBAnnoyAnyscaleApifyArangoDBArgillaArthurArxivAtlasAwaDBAWS S3 DirectoryAZLyricsAzure Blob StorageAzure Cognitive SearchAzure OpenAIBagelDBBananaBasetenBeamBedrockBiliBiliNIBittensorBlackboardBrave SearchCassandraCerebriumAIChaindeskChromaClarifaiClearMLClickHouseCnosDBCohereCollege ConfidentialCometConfident AIConfluenceC TransformersDashVectorDatabricksDatadog TracingDatadog LogsDataForSEODeepInfraDeepSparseDiffbotDingoDiscordDocArrayDocugamiDuckDBElasticsearchEpsillaEverNoteFacebook ChatFacebook FaissFigmaFireworksFlyteForefrontAIGitGitBookGoldenGoogle BigQueryGoogle Cloud StorageGoogle DriveGoogle SearchGoogle SerperGoogle Vertex AI MatchingEngineGooseAIGPT4AllGraphsignalGrobidGutenbergHacker NewsHazy ResearchHeliconeHologresHugging FaceiFixitIMSDbInfinoJinaKonkoLanceDBLangChain Decorators âœ¨Llama.cppLog10MarqoMediaWikiDumpMeilisearchMetalMicrosoft OneDriveMicrosoft PowerPointMicrosoft WordMilvusMinimaxMLflow AI GatewayMLflowModalModelScopeModern TreasuryMomentoMongoDB AtlasMotherduckMyScaleNeo4jNLPCloudNotion DBObsidianOpenAIOpenLLMOpenSearchOpenWeatherMapPetalsPostgres EmbeddingPGVectorPineconePipelineAIPortkeyPredibasePrediction GuardPromptLayerPsychicPubMedQdrantRay ServeRebuffRedditRedisReplicateRoamRocksetRunhouseRWKV-4SageMaker EndpointSageMaker TrackingScaNNSearxNG Search APISerpAPIShale ProtocolSingleStoreDBscikit-learnSlackspaCySpreedlyStarRocksStochasticAIStripeSupabase (Postgres)NebulaTairTelegramTencentVectorDBTensorFlow DatasetsTigris2MarkdownTrelloTruLensTwitterTypesenseUnstructuredUSearchVearchVectaraVespaWandB TracingWeights & BiasesWeatherWeaviateWhatsAppWhyLabsWikipediaWolfram AlphaWriterXataXorbits Inference (Xinference)Yeager.aiYouTubeZepZilliz IntegrationsCallbacksChat modelsChat loadersDocument loadersDocument transformersLLMsMemoryRetrieversText embedding modelsAgents & ToolkitsToolsVector storesGrouped by providerActiveloop Deep LakeAI21 LabsAimAINetworkAirbyteAirtableAleph AlphaAlibaba Cloud OpensearchAmazon API GatewayAnalyticDBAnnoyAnyscaleApifyArangoDBArgillaArthurArxivAtlasAwaDBAWS S3 DirectoryAZLyricsAzure Blob StorageAzure Cognitive SearchAzure OpenAIBagelDBBananaBasetenBeamBedrockBiliBiliNIBittensorBlackboardBrave SearchCassandraCerebriumAIChaindeskChromaClarifaiClearMLClickHouseCnosDBCohereCollege ConfidentialCometConfident AIConfluenceC TransformersDashVectorDatabricksDatadog TracingDatadog LogsDataForSEODeepInfraDeepSparseDiffbotDingoDiscordDocArrayDocugamiDuckDBElasticsearchEpsillaEverNoteFacebook ChatFacebook FaissFigmaFireworksFlyteForefrontAIGitGitBookGoldenGoogle BigQueryGoogle Cloud StorageGoogle DriveGoogle SearchGoogle SerperGoogle Vertex AI MatchingEngineGooseAIGPT4AllGraphsignalGrobidGutenbergHacker NewsHazy ResearchHeliconeHologresHugging FaceiFixitIMSDbInfinoJinaKonkoLanceDBLangChain Decorators âœ¨Llama.cppLog10MarqoMediaWikiDumpMeilisearchMetalMicrosoft OneDriveMicrosoft PowerPointMicrosoft WordMilvusMinimaxMLflow AI GatewayMLflowModalModelScopeModern TreasuryMomentoMongoDB AtlasMotherduckMyScaleNeo4jNLPCloudNotion DBObsidianOpenAIOpenLLMOpenSearchOpenWeatherMapPetalsPostgres EmbeddingPGVectorPineconePipelineAIPortkeyPredibasePrediction GuardPromptLayerPsychicPubMedQdrantRay ServeRebuffRedditRedisReplicateRoamRocksetRunhouseRWKV-4SageMaker EndpointSageMaker TrackingScaNNSearxNG Search APISerpAPIShale ProtocolSingleStoreDBscikit-learnSlackspaCySpreedlyStarRocksStochasticAIStripeSupabase (Postgres)NebulaTairTelegramTencentVectorDBTensorFlow DatasetsTigris2MarkdownTrelloTruLensTwitterTypesenseUnstructuredUSearchVearchVectaraVespaWandB TracingWeights & BiasesWeatherWeaviateWhatsAppWhyLabsWikipediaWolfram AlphaWriterXataXorbits Inference (Xinference)Yeager.aiYouTubeZepZilliz Integrations Callbacks Chat models Chat loaders Document loaders Document transformers LLMs Memory Retrievers Text embedding models Agents & Toolkits Tools Vector stores Grouped by provider Portkey Vectara IntegrationsGrouped by providerArthurArthurArthur is a model monitoring and observability platform.The following guide shows how to run a registered chat LLM with the Arthur callback handler to automatically log model inferences to Arthur.If you do not have a model currently onboarded to Arthur, visit our onboarding guide for generative text models. For more information about how to use the Arthur SDK, visit our docs.from langchain.callbacks import ArthurCallbackHandlerfrom langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandlerfrom langchain.chat_models import ChatOpenAIfrom langchain.schema import HumanMessageAPI Reference:ArthurCallbackHandlerStreamingStdOutCallbackHandlerChatOpenAIHumanMessagePlace Arthur credentials herearthur_url = "https://app.arthur.ai"arthur_login = "your-arthur-login-username-here"arthur_model_id = "your-arthur-model-id-here"Create Langchain LLM with Arthur callback handlerdef make_langchain_chat_llm(chat_model=):    return ChatOpenAI(        streaming=True,        temperature=0.1,        callbacks=[            StreamingStdOutCallbackHandler(),            ArthurCallbackHandler.from_credentials(                arthur_model_id,                 arthur_url=arthur_url,                 arthur_login=arthur_login)        ])chatgpt = make_langchain_chat_llm()    Please enter password for admin: Â·Â·Â·Â·Â·Â·Â·Â·Running the chat LLM with this run function will save the chat history in an ongoing list so that the conversation can reference earlier messages and log each response to the Arthur platform. You can view the history of this model's inferences on your model dashboard page.Enter q to quit the run loopdef run(llm):    history = []    while True:        user_input = input("\n>>> input >>>\n>>>: ")        if user_input == "q":            break        history.append(HumanMessage(content=user_input))        history.append(llm(history))run(chatgpt)        >>> input >>>    >>>: What is a callback handler?    A callback handler, also known as a callback function or callback method, is a piece of code that is executed in response to a specific event or condition. It is commonly used in programming languages that support event-driven or asynchronous programming paradigms.        The purpose of a callback handler is to provide a way for developers to define custom behavior that should be executed when a certain event occurs. Instead of waiting for a result or blocking the execution, the program registers a callback function and continues with other tasks. When the event is triggered, the callback function is invoked, allowing the program to respond accordingly.        Callback handlers are commonly used in various scenarios, such as handling user input, responding to network requests, processing asynchronous operations, and implementing event-driven architectures. They provide a flexible and modular way to handle events and decouple different components of a system.    >>> input >>>    >>>: What do I need to do to get the full benefits of this    To get the full benefits of using a callback handler, you should consider the following:        1. Understand the event or condition: Identify the specific event or condition that you want to respond to with a callback handler. This could be user input, network requests, or any other asynchronous operation.        2. Define the callback function: Create a function that will be executed when the event or condition occurs. This function should contain the desired behavior or actions you want to take in response to the event.        3. Register the callback function: Depending on the programming language or framework you are using, you may need to register or attach the callback function to the appropriate event or condition. This ensures that the callback function is invoked when the event occurs.        4. Handle the callback: Implement the necessary logic within the callback function to handle the event or condition. This could involve updating the user interface, processing data, making further requests, or triggering other actions.        5. Consider error handling: It's important to handle any potential errors or exceptions that may occur within the callback function. This ensures that your program can gracefully handle unexpected situations and prevent crashes or undesired behavior.        6. Maintain code readability and modularity: As your codebase grows, it's crucial to keep your callback handlers organized and maintainable. Consider using design patterns or architectural principles to structure your code in a modular and scalable way.        By following these steps, you can leverage the benefits of callback handlers, such as asynchronous and event-driven programming, improved responsiveness, and modular code design.    >>> input >>>    >>>: qPreviousArgillaNextArxiv IntegrationsGrouped by providerArthurArthurArthur is a model monitoring and observability platform.The following guide shows how to run a registered chat LLM with the Arthur callback handler to automatically log model inferences to Arthur.If you do not have a model currently onboarded to Arthur, visit our onboarding guide for generative text models. For more information about how to use the Arthur SDK, visit our docs.from langchain.callbacks import ArthurCallbackHandlerfrom langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandlerfrom langchain.chat_models import ChatOpenAIfrom langchain.schema import HumanMessageAPI Reference:ArthurCallbackHandlerStreamingStdOutCallbackHandlerChatOpenAIHumanMessagePlace Arthur credentials herearthur_url = "https://app.arthur.ai"arthur_login = "your-arthur-login-username-here"arthur_model_id = "your-arthur-model-id-here"Create Langchain LLM with Arthur callback handlerdef make_langchain_chat_llm(chat_model=):    return ChatOpenAI(        streaming=True,        temperature=0.1,        callbacks=[            StreamingStdOutCallbackHandler(),            ArthurCallbackHandler.from_credentials(                arthur_model_id,                 arthur_url=arthur_url,                 arthur_login=arthur_login)        ])chatgpt = make_langchain_chat_llm()    Please enter password for admin: Â·Â·Â·Â·Â·Â·Â·Â·Running the chat LLM with this run function will save the chat history in an ongoing list so that the conversation can reference earlier messages and log each response to the Arthur platform. You can view the history of this model's inferences on your model dashboard page.Enter q to quit the run loopdef run(llm):    history = []    while True:        user_input = input("\n>>> input >>>\n>>>: ")        if user_input == "q":            break        history.append(HumanMessage(content=user_input))        history.append(llm(history))run(chatgpt)        >>> input >>>    >>>: What is a callback handler?    A callback handler, also known as a callback function or callback method, is a piece of code that is executed in response to a specific event or condition. It is commonly used in programming languages that support event-driven or asynchronous programming paradigms.        The purpose of a callback handler is to provide a way for developers to define custom behavior that should be executed when a certain event occurs. Instead of waiting for a result or blocking the execution, the program registers a callback function and continues with other tasks. When the event is triggered, the callback function is invoked, allowing the program to respond accordingly.        Callback handlers are commonly used in various scenarios, such as handling user input, responding to network requests, processing asynchronous operations, and implementing event-driven architectures. They provide a flexible and modular way to handle events and decouple different components of a system.    >>> input >>>    >>>: What do I need to do to get the full benefits of this    To get the full benefits of using a callback handler, you should consider the following:        1. Understand the event or condition: Identify the specific event or condition that you want to respond to with a callback handler. This could be user input, network requests, or any other asynchronous operation.        2. Define the callback function: Create a function that will be executed when the event or condition occurs. This function should contain the desired behavior or actions you want to take in response to the event.        3. Register the callback function: Depending on the programming language or framework you are using, you may need to register or attach the callback function to the appropriate event or condition. This ensures that the callback function is invoked when the event occurs.        4. Handle the callback: Implement the necessary logic within the callback function to handle the event or condition. This could involve updating the user interface, processing data, making further requests, or triggering other actions.        5. Consider error handling: It's important to handle any potential errors or exceptions that may occur within the callback function. This ensures that your program can gracefully handle unexpected situations and prevent crashes or undesired behavior.        6. Maintain code readability and modularity: As your codebase grows, it's crucial to keep your callback handlers organized and maintainable. Consider using design patterns or architectural principles to structure your code in a modular and scalable way.        By following these steps, you can leverage the benefits of callback handlers, such as asynchronous and event-driven programming, improved responsiveness, and modular code design.    >>> input >>>    >>>: qPreviousArgillaNextArxiv IntegrationsGrouped by providerArthurArthurArthur is a model monitoring and observability platform.The following guide shows how to run a registered chat LLM with the Arthur callback handler to automatically log model inferences to Arthur.If you do not have a model currently onboarded to Arthur, visit our onboarding guide for generative text models. For more information about how to use the Arthur SDK, visit our docs.from langchain.callbacks import ArthurCallbackHandlerfrom langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandlerfrom langchain.chat_models import ChatOpenAIfrom langchain.schema import HumanMessageAPI Reference:ArthurCallbackHandlerStreamingStdOutCallbackHandlerChatOpenAIHumanMessagePlace Arthur credentials herearthur_url = "https://app.arthur.ai"arthur_login = "your-arthur-login-username-here"arthur_model_id = "your-arthur-model-id-here"Create Langchain LLM with Arthur callback handlerdef make_langchain_chat_llm(chat_model=):    return ChatOpenAI(        streaming=True,        temperature=0.1,        callbacks=[            StreamingStdOutCallbackHandler(),            ArthurCallbackHandler.from_credentials(                arthur_model_id,                 arthur_url=arthur_url,                 arthur_login=arthur_login)        ])chatgpt = make_langchain_chat_llm()    Please enter password for admin: Â·Â·Â·Â·Â·Â·Â·Â·Running the chat LLM with this run function will save the chat history in an ongoing list so that the conversation can reference earlier messages and log each response to the Arthur platform. You can view the history of this model's inferences on your model dashboard page.Enter q to quit the run loopdef run(llm):    history = []    while True:        user_input = input("\n>>> input >>>\n>>>: ")        if user_input == "q":            break        history.append(HumanMessage(content=user_input))        history.append(llm(history))run(chatgpt)        >>> input >>>    >>>: What is a callback handler?    A callback handler, also known as a callback function or callback method, is a piece of code that is executed in response to a specific event or condition. It is commonly used in programming languages that support event-driven or asynchronous programming paradigms.        The purpose of a callback handler is to provide a way for developers to define custom behavior that should be executed when a certain event occurs. Instead of waiting for a result or blocking the execution, the program registers a callback function and continues with other tasks. When the event is triggered, the callback function is invoked, allowing the program to respond accordingly.        Callback handlers are commonly used in various scenarios, such as handling user input, responding to network requests, processing asynchronous operations, and implementing event-driven architectures. They provide a flexible and modular way to handle events and decouple different components of a system.    >>> input >>>    >>>: What do I need to do to get the full benefits of this    To get the full benefits of using a callback handler, you should consider the following:        1. Understand the event or condition: Identify the specific event or condition that you want to respond to with a callback handler. This could be user input, network requests, or any other asynchronous operation.        2. Define the callback function: Create a function that will be executed when the event or condition occurs. This function should contain the desired behavior or actions you want to take in response to the event.        3. Register the callback function: Depending on the programming language or framework you are using, you may need to register or attach the callback function to the appropriate event or condition. This ensures that the callback function is invoked when the event occurs.        4. Handle the callback: Implement the necessary logic within the callback function to handle the event or condition. This could involve updating the user interface, processing data, making further requests, or triggering other actions.        5. Consider error handling: It's important to handle any potential errors or exceptions that may occur within the callback function. This ensures that your program can gracefully handle unexpected situations and prevent crashes or undesired behavior.        6. Maintain code readability and modularity: As your codebase grows, it's crucial to keep your callback handlers organized and maintainable. Consider using design patterns or architectural principles to structure your code in a modular and scalable way.        By following these steps, you can leverage the benefits of callback handlers, such as asynchronous and event-driven programming, improved responsiveness, and modular code design.    >>> input >>>    >>>: qPreviousArgillaNextArxiv IntegrationsGrouped by providerArthurArthurArthur is a model monitoring and observability platform.The following guide shows how to run a registered chat LLM with the Arthur callback handler to automatically log model inferences to Arthur.If you do not have a model currently onboarded to Arthur, visit our onboarding guide for generative text models. For more information about how to use the Arthur SDK, visit our docs.from langchain.callbacks import ArthurCallbackHandlerfrom langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandlerfrom langchain.chat_models import ChatOpenAIfrom langchain.schema import HumanMessageAPI Reference:ArthurCallbackHandlerStreamingStdOutCallbackHandlerChatOpenAIHumanMessagePlace Arthur credentials herearthur_url = "https://app.arthur.ai"arthur_login = "your-arthur-login-username-here"arthur_model_id = "your-arthur-model-id-here"Create Langchain LLM with Arthur callback handlerdef make_langchain_chat_llm(chat_model=):    return ChatOpenAI(        streaming=True,        temperature=0.1,        callbacks=[            StreamingStdOutCallbackHandler(),            ArthurCallbackHandler.from_credentials(                arthur_model_id,                 arthur_url=arthur_url,                 arthur_login=arthur_login)        ])chatgpt = make_langchain_chat_llm()    Please enter password for admin: Â·Â·Â·Â·Â·Â·Â·Â·Running the chat LLM with this run function will save the chat history in an ongoing list so that the conversation can reference earlier messages and log each response to the Arthur platform. You can view the history of this model's inferences on your model dashboard page.Enter q to quit the run loopdef run(llm):    history = []    while True:        user_input = input("\n>>> input >>>\n>>>: ")        if user_input == "q":            break        history.append(HumanMessage(content=user_input))        history.append(llm(history))run(chatgpt)        >>> input >>>    >>>: What is a callback handler?    A callback handler, also known as a callback function or callback method, is a piece of code that is executed in response to a specific event or condition. It is commonly used in programming languages that support event-driven or asynchronous programming paradigms.        The purpose of a callback handler is to provide a way for developers to define custom behavior that should be executed when a certain event occurs. Instead of waiting for a result or blocking the execution, the program registers a callback function and continues with other tasks. When the event is triggered, the callback function is invoked, allowing the program to respond accordingly.        Callback handlers are commonly used in various scenarios, such as handling user input, responding to network requests, processing asynchronous operations, and implementing event-driven architectures. They provide a flexible and modular way to handle events and decouple different components of a system.    >>> input >>>    >>>: What do I need to do to get the full benefits of this    To get the full benefits of using a callback handler, you should consider the following:        1. Understand the event or condition: Identify the specific event or condition that you want to respond to with a callback handler. This could be user input, network requests, or any other asynchronous operation.        2. Define the callback function: Create a function that will be executed when the event or condition occurs. This function should contain the desired behavior or actions you want to take in response to the event.        3. Register the callback function: Depending on the programming language or framework you are using, you may need to register or attach the callback function to the appropriate event or condition. This ensures that the callback function is invoked when the event occurs.        4. Handle the callback: Implement the necessary logic within the callback function to handle the event or condition. This could involve updating the user interface, processing data, making further requests, or triggering other actions.        5. Consider error handling: It's important to handle any potential errors or exceptions that may occur within the callback function. This ensures that your program can gracefully handle unexpected situations and prevent crashes or undesired behavior.        6. Maintain code readability and modularity: As your codebase grows, it's crucial to keep your callback handlers organized and maintainable. Consider using design patterns or architectural principles to structure your code in a modular and scalable way.        By following these steps, you can leverage the benefits of callback handlers, such as asynchronous and event-driven programming, improved responsiveness, and modular code design.    >>> input >>>    >>>: qPreviousArgillaNextArxiv ArthurArthur is a model monitoring and observability platform.The following guide shows how to run a registered chat LLM with the Arthur callback handler to automatically log model inferences to Arthur.If you do not have a model currently onboarded to Arthur, visit our onboarding guide for generative text models. For more information about how to use the Arthur SDK, visit our docs.from langchain.callbacks import ArthurCallbackHandlerfrom langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandlerfrom langchain.chat_models import ChatOpenAIfrom langchain.schema import HumanMessageAPI Reference:ArthurCallbackHandlerStreamingStdOutCallbackHandlerChatOpenAIHumanMessagePlace Arthur credentials herearthur_url = "https://app.arthur.ai"arthur_login = "your-arthur-login-username-here"arthur_model_id = "your-arthur-model-id-here"Create Langchain LLM with Arthur callback handlerdef make_langchain_chat_llm(chat_model=):    return ChatOpenAI(        streaming=True,        temperature=0.1,        callbacks=[            StreamingStdOutCallbackHandler(),            ArthurCallbackHandler.from_credentials(                arthur_model_id,                 arthur_url=arthur_url,                 arthur_login=arthur_login)        ])chatgpt = make_langchain_chat_llm()    Please enter password for admin: Â·Â·Â·Â·Â·Â·Â·Â·Running the chat LLM with this run function will save the chat history in an ongoing list so that the conversation can reference earlier messages and log each response to the Arthur platform. You can view the history of this model's inferences on your model dashboard page.Enter q to quit the run loopdef run(llm):    history = []    while True:        user_input = input("\n>>> input >>>\n>>>: ")        if user_input == "q":            break        history.append(HumanMessage(content=user_input))        history.append(llm(history))run(chatgpt)        >>> input >>>    >>>: What is a callback handler?    A callback handler, also known as a callback function or callback method, is a piece of code that is executed in response to a specific event or condition. It is commonly used in programming languages that support event-driven or asynchronous programming paradigms.        The purpose of a callback handler is to provide a way for developers to define custom behavior that should be executed when a certain event occurs. Instead of waiting for a result or blocking the execution, the program registers a callback function and continues with other tasks. When the event is triggered, the callback function is invoked, allowing the program to respond accordingly.        Callback handlers are commonly used in various scenarios, such as handling user input, responding to network requests, processing asynchronous operations, and implementing event-driven architectures. They provide a flexible and modular way to handle events and decouple different components of a system.    >>> input >>>    >>>: What do I need to do to get the full benefits of this    To get the full benefits of using a callback handler, you should consider the following:        1. Understand the event or condition: Identify the specific event or condition that you want to respond to with a callback handler. This could be user input, network requests, or any other asynchronous operation.        2. Define the callback function: Create a function that will be executed when the event or condition occurs. This function should contain the desired behavior or actions you want to take in response to the event.        3. Register the callback function: Depending on the programming language or framework you are using, you may need to register or attach the callback function to the appropriate event or condition. This ensures that the callback function is invoked when the event occurs.        4. Handle the callback: Implement the necessary logic within the callback function to handle the event or condition. This could involve updating the user interface, processing data, making further requests, or triggering other actions.        5. Consider error handling: It's important to handle any potential errors or exceptions that may occur within the callback function. This ensures that your program can gracefully handle unexpected situations and prevent crashes or undesired behavior.        6. Maintain code readability and modularity: As your codebase grows, it's crucial to keep your callback handlers organized and maintainable. Consider using design patterns or architectural principles to structure your code in a modular and scalable way.        By following these steps, you can leverage the benefits of callback handlers, such as asynchronous and event-driven programming, improved responsiveness, and modular code design.    >>> input >>>    >>>: q from langchain.callbacks import ArthurCallbackHandlerfrom langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandlerfrom langchain.chat_models import ChatOpenAIfrom langchain.schema import HumanMessage from langchain.callbacks import ArthurCallbackHandlerfrom langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandlerfrom langchain.chat_models import ChatOpenAIfrom langchain.schema import HumanMessage  API Reference:ArthurCallbackHandlerStreamingStdOutCallbackHandlerChatOpenAIHumanMessage arthur_url = "https://app.arthur.ai"arthur_login = "your-arthur-login-username-here"arthur_model_id = "your-arthur-model-id-here" arthur_url = "https://app.arthur.ai"arthur_login = "your-arthur-login-username-here"arthur_model_id = "your-arthur-model-id-here"  def make_langchain_chat_llm(chat_model=):    return ChatOpenAI(        streaming=True,        temperature=0.1,        callbacks=[            StreamingStdOutCallbackHandler(),            ArthurCallbackHandler.from_credentials(                arthur_model_id,                 arthur_url=arthur_url,                 arthur_login=arthur_login)        ]) def make_langchain_chat_llm(chat_model=):    return ChatOpenAI(        streaming=True,        temperature=0.1,        callbacks=[            StreamingStdOutCallbackHandler(),            ArthurCallbackHandler.from_credentials(                arthur_model_id,                 arthur_url=arthur_url,                 arthur_login=arthur_login)        ])  chatgpt = make_langchain_chat_llm() chatgpt = make_langchain_chat_llm()      Please enter password for admin: Â·Â·Â·Â·Â·Â·Â·Â·     Please enter password for admin: Â·Â·Â·Â·Â·Â·Â·Â·     Please enter password for admin: Â·Â·Â·Â·Â·Â·Â·Â·  def run(llm):    history = []    while True:        user_input = input("\n>>> input >>>\n>>>: ")        if user_input == "q":            break        history.append(HumanMessage(content=user_input))        history.append(llm(history)) def run(llm):    history = []    while True:        user_input = input("\n>>> input >>>\n>>>: ")        if user_input == "q":            break        history.append(HumanMessage(content=user_input))        history.append(llm(history))  run(chatgpt) run(chatgpt)          >>> input >>>    >>>: What is a callback handler?    A callback handler, also known as a callback function or callback method, is a piece of code that is executed in response to a specific event or condition. It is commonly used in programming languages that support event-driven or asynchronous programming paradigms.        The purpose of a callback handler is to provide a way for developers to define custom behavior that should be executed when a certain event occurs. Instead of waiting for a result or blocking the execution, the program registers a callback function and continues with other tasks. When the event is triggered, the callback function is invoked, allowing the program to respond accordingly.        Callback handlers are commonly used in various scenarios, such as handling user input, responding to network requests, processing asynchronous operations, and implementing event-driven architectures. They provide a flexible and modular way to handle events and decouple different components of a system.    >>> input >>>    >>>: What do I need to do to get the full benefits of this    To get the full benefits of using a callback handler, you should consider the following:        1. Understand the event or condition: Identify the specific event or condition that you want to respond to with a callback handler. This could be user input, network requests, or any other asynchronous operation.        2. Define the callback function: Create a function that will be executed when the event or condition occurs. This function should contain the desired behavior or actions you want to take in response to the event.        3. Register the callback function: Depending on the programming language or framework you are using, you may need to register or attach the callback function to the appropriate event or condition. This ensures that the callback function is invoked when the event occurs.        4. Handle the callback: Implement the necessary logic within the callback function to handle the event or condition. This could involve updating the user interface, processing data, making further requests, or triggering other actions.        5. Consider error handling: It's important to handle any potential errors or exceptions that may occur within the callback function. This ensures that your program can gracefully handle unexpected situations and prevent crashes or undesired behavior.        6. Maintain code readability and modularity: As your codebase grows, it's crucial to keep your callback handlers organized and maintainable. Consider using design patterns or architectural principles to structure your code in a modular and scalable way.        By following these steps, you can leverage the benefits of callback handlers, such as asynchronous and event-driven programming, improved responsiveness, and modular code design.    >>> input >>>    >>>: q         >>> input >>>    >>>: What is a callback handler?    A callback handler, also known as a callback function or callback method, is a piece of code that is executed in response to a specific event or condition. It is commonly used in programming languages that support event-driven or asynchronous programming paradigms.        The purpose of a callback handler is to provide a way for developers to define custom behavior that should be executed when a certain event occurs. Instead of waiting for a result or blocking the execution, the program registers a callback function and continues with other tasks. When the event is triggered, the callback function is invoked, allowing the program to respond accordingly.        Callback handlers are commonly used in various scenarios, such as handling user input, responding to network requests, processing asynchronous operations, and implementing event-driven architectures. They provide a flexible and modular way to handle events and decouple different components of a system.    >>> input >>>    >>>: What do I need to do to get the full benefits of this    To get the full benefits of using a callback handler, you should consider the following:        1. Understand the event or condition: Identify the specific event or condition that you want to respond to with a callback handler. This could be user input, network requests, or any other asynchronous operation.        2. Define the callback function: Create a function that will be executed when the event or condition occurs. This function should contain the desired behavior or actions you want to take in response to the event.        3. Register the callback function: Depending on the programming language or framework you are using, you may need to register or attach the callback function to the appropriate event or condition. This ensures that the callback function is invoked when the event occurs.        4. Handle the callback: Implement the necessary logic within the callback function to handle the event or condition. This could involve updating the user interface, processing data, making further requests, or triggering other actions.        5. Consider error handling: It's important to handle any potential errors or exceptions that may occur within the callback function. This ensures that your program can gracefully handle unexpected situations and prevent crashes or undesired behavior.        6. Maintain code readability and modularity: As your codebase grows, it's crucial to keep your callback handlers organized and maintainable. Consider using design patterns or architectural principles to structure your code in a modular and scalable way.        By following these steps, you can leverage the benefits of callback handlers, such as asynchronous and event-driven programming, improved responsiveness, and modular code design.    >>> input >>>    >>>: q         >>> input >>>    >>>: What is a callback handler?    A callback handler, also known as a callback function or callback method, is a piece of code that is executed in response to a specific event or condition. It is commonly used in programming languages that support event-driven or asynchronous programming paradigms.        The purpose of a callback handler is to provide a way for developers to define custom behavior that should be executed when a certain event occurs. Instead of waiting for a result or blocking the execution, the program registers a callback function and continues with other tasks. When the event is triggered, the callback function is invoked, allowing the program to respond accordingly.        Callback handlers are commonly used in various scenarios, such as handling user input, responding to network requests, processing asynchronous operations, and implementing event-driven architectures. They provide a flexible and modular way to handle events and decouple different components of a system.    >>> input >>>    >>>: What do I need to do to get the full benefits of this    To get the full benefits of using a callback handler, you should consider the following:        1. Understand the event or condition: Identify the specific event or condition that you want to respond to with a callback handler. This could be user input, network requests, or any other asynchronous operation.        2. Define the callback function: Create a function that will be executed when the event or condition occurs. This function should contain the desired behavior or actions you want to take in response to the event.        3. Register the callback function: Depending on the programming language or framework you are using, you may need to register or attach the callback function to the appropriate event or condition. This ensures that the callback function is invoked when the event occurs.        4. Handle the callback: Implement the necessary logic within the callback function to handle the event or condition. This could involve updating the user interface, processing data, making further requests, or triggering other actions.        5. Consider error handling: It's important to handle any potential errors or exceptions that may occur within the callback function. This ensures that your program can gracefully handle unexpected situations and prevent crashes or undesired behavior.        6. Maintain code readability and modularity: As your codebase grows, it's crucial to keep your callback handlers organized and maintainable. Consider using design patterns or architectural principles to structure your code in a modular and scalable way.        By following these steps, you can leverage the benefits of callback handlers, such as asynchronous and event-driven programming, improved responsiveness, and modular code design.    >>> input >>>    >>>: q  Previous Argilla Next Arxiv CommunityDiscordTwitterGitHubPythonJS/TSMoreHomepageBlogCopyright Â© 2023 LangChain, Inc. CommunityDiscordTwitterGitHubPythonJS/TSMoreHomepageBlog CommunityDiscordTwitter Community GitHubPythonJS/TS GitHub MoreHomepageBlog More Copyright Â© 2023 LangChain, Inc. Copyright Â© 2023 LangChain, Inc. Skip to main content (#docusaurus_skipToContent_fallback) ðŸ¦œï¸ðŸ”— LangChain (/) Docs (/docs/get_started/introduction) Use cases (/docs/use_cases) Integrations (/docs/integrations) API (https://api.python.langchain.com) LangSmith (https://smith.langchain.com) JS/TS Docs (https://js.langchain.com/docs)  (https://github.com/hwchase17/langchain) Integrations (/docs/integrations) Callbacks (/docs/integrations/callbacks/) Chat models (/docs/integrations/chat/) Chat loaders (/docs/integrations/chat_loaders/) Document loaders (/docs/integrations/document_loaders/) Document transformers (/docs/integrations/document_transformers/) LLMs (/docs/integrations/llms/) Memory (/docs/integrations/memory/) Retrievers (/docs/integrations/retrievers/) Text embedding models (/docs/integrations/text_embedding/) Agents & Toolkits (/docs/integrations/toolkits/) Tools (/docs/integrations/tools/) Vector stores (/docs/integrations/vectorstores/) Grouped by provider (/docs/integrations/providers/) Activeloop Deep Lake (/docs/integrations/providers/activeloop_deeplake) AI21 Labs (/docs/integrations/providers/ai21) Aim (/docs/integrations/providers/aim_tracking) AINetwork (/docs/integrations/providers/ainetwork) Airbyte (/docs/integrations/providers/airbyte) Airtable (/docs/integrations/providers/airtable) Aleph Alpha (/docs/integrations/providers/aleph_alpha) Alibaba Cloud Opensearch (/docs/integrations/providers/alibabacloud_opensearch) Amazon API Gateway (/docs/integrations/providers/amazon_api_gateway) AnalyticDB (/docs/integrations/providers/analyticdb) Annoy (/docs/integrations/providers/annoy) Anyscale (/docs/integrations/providers/anyscale) Apify (/docs/integrations/providers/apify) ArangoDB (/docs/integrations/providers/arangodb) Argilla (/docs/integrations/providers/argilla) Arthur (/docs/integrations/providers/arthur_tracking) Arxiv (/docs/integrations/providers/arxiv) Atlas (/docs/integrations/providers/atlas) AwaDB (/docs/integrations/providers/awadb) AWS S3 Directory (/docs/integrations/providers/aws_s3) AZLyrics (/docs/integrations/providers/azlyrics) Azure Blob Storage (/docs/integrations/providers/azure_blob_storage) Azure Cognitive Search (/docs/integrations/providers/azure_cognitive_search_) Azure OpenAI (/docs/integrations/providers/azure_openai) BagelDB (/docs/integrations/providers/bageldb) Banana (/docs/integrations/providers/bananadev) Baseten (/docs/integrations/providers/baseten) Beam (/docs/integrations/providers/beam) Bedrock (/docs/integrations/providers/bedrock) BiliBili (/docs/integrations/providers/bilibili) NIBittensor (/docs/integrations/providers/bittensor) Blackboard (/docs/integrations/providers/blackboard) Brave Search (/docs/integrations/providers/brave_search) Cassandra (/docs/integrations/providers/cassandra) CerebriumAI (/docs/integrations/providers/cerebriumai) Chaindesk (/docs/integrations/providers/chaindesk) Chroma (/docs/integrations/providers/chroma) Clarifai (/docs/integrations/providers/clarifai) ClearML (/docs/integrations/providers/clearml_tracking) ClickHouse (/docs/integrations/providers/clickhouse) CnosDB (/docs/integrations/providers/cnosdb) Cohere (/docs/integrations/providers/cohere) College Confidential (/docs/integrations/providers/college_confidential) Comet (/docs/integrations/providers/comet_tracking) Confident AI (/docs/integrations/providers/confident) Confluence (/docs/integrations/providers/confluence) C Transformers (/docs/integrations/providers/ctransformers) DashVector (/docs/integrations/providers/dashvector) Databricks (/docs/integrations/providers/databricks) Datadog Tracing (/docs/integrations/providers/datadog) Datadog Logs (/docs/integrations/providers/datadog_logs) DataForSEO (/docs/integrations/providers/dataforseo) DeepInfra (/docs/integrations/providers/deepinfra) DeepSparse (/docs/integrations/providers/deepsparse) Diffbot (/docs/integrations/providers/diffbot) Dingo (/docs/integrations/providers/dingo) Discord (/docs/integrations/providers/discord) DocArray (/docs/integrations/providers/docarray) Docugami (/docs/integrations/providers/docugami) DuckDB (/docs/integrations/providers/duckdb) Elasticsearch (/docs/integrations/providers/elasticsearch) Epsilla (/docs/integrations/providers/epsilla) EverNote (/docs/integrations/providers/evernote) Facebook Chat (/docs/integrations/providers/facebook_chat) Facebook Faiss (/docs/integrations/providers/facebook_faiss) Figma (/docs/integrations/providers/figma) Fireworks (/docs/integrations/providers/fireworks) Flyte (/docs/integrations/providers/flyte) ForefrontAI (/docs/integrations/providers/forefrontai) Git (/docs/integrations/providers/git) GitBook (/docs/integrations/providers/gitbook) Golden (/docs/integrations/providers/golden) Google BigQuery (/docs/integrations/providers/google_bigquery) Google Cloud Storage (/docs/integrations/providers/google_cloud_storage) Google Drive (/docs/integrations/providers/google_drive) Google Search (/docs/integrations/providers/google_search) Google Serper (/docs/integrations/providers/google_serper) Google Vertex AI MatchingEngine (/docs/integrations/providers/google_vertex_ai_matchingengine) GooseAI (/docs/integrations/providers/gooseai) GPT4All (/docs/integrations/providers/gpt4all) Graphsignal (/docs/integrations/providers/graphsignal) Grobid (/docs/integrations/providers/grobid) Gutenberg (/docs/integrations/providers/gutenberg) Hacker News (/docs/integrations/providers/hacker_news) Hazy Research (/docs/integrations/providers/hazy_research) Helicone (/docs/integrations/providers/helicone) Hologres (/docs/integrations/providers/hologres) Hugging Face (/docs/integrations/providers/huggingface) iFixit (/docs/integrations/providers/ifixit) IMSDb (/docs/integrations/providers/imsdb) Infino (/docs/integrations/providers/infino) Jina (/docs/integrations/providers/jina) Konko (/docs/integrations/providers/konko) LanceDB (/docs/integrations/providers/lancedb) LangChain Decorators âœ¨ (/docs/integrations/providers/langchain_decorators) Llama.cpp (/docs/integrations/providers/llamacpp) Log10 (/docs/integrations/providers/log10) Marqo (/docs/integrations/providers/marqo) MediaWikiDump (/docs/integrations/providers/mediawikidump) Meilisearch (/docs/integrations/providers/meilisearch) Metal (/docs/integrations/providers/metal) Microsoft OneDrive (/docs/integrations/providers/microsoft_onedrive) Microsoft PowerPoint (/docs/integrations/providers/microsoft_powerpoint) Microsoft Word (/docs/integrations/providers/microsoft_word) Milvus (/docs/integrations/providers/milvus) Minimax (/docs/integrations/providers/minimax) MLflow AI Gateway (/docs/integrations/providers/mlflow_ai_gateway) MLflow (/docs/integrations/providers/mlflow_tracking) Modal (/docs/integrations/providers/modal) ModelScope (/docs/integrations/providers/modelscope) Modern Treasury (/docs/integrations/providers/modern_treasury) Momento (/docs/integrations/providers/momento) MongoDB Atlas (/docs/integrations/providers/mongodb_atlas) Motherduck (/docs/integrations/providers/motherduck) MyScale (/docs/integrations/providers/myscale) Neo4j (/docs/integrations/providers/neo4j) NLPCloud (/docs/integrations/providers/nlpcloud) Notion DB (/docs/integrations/providers/notion) Obsidian (/docs/integrations/providers/obsidian) OpenAI (/docs/integrations/providers/openai) OpenLLM (/docs/integrations/providers/openllm) OpenSearch (/docs/integrations/providers/opensearch) OpenWeatherMap (/docs/integrations/providers/openweathermap) Petals (/docs/integrations/providers/petals) Postgres Embedding (/docs/integrations/providers/pg_embedding) PGVector (/docs/integrations/providers/pgvector) Pinecone (/docs/integrations/providers/pinecone) PipelineAI (/docs/integrations/providers/pipelineai) Portkey (/docs/integrations/providers/portkey/) Predibase (/docs/integrations/providers/predibase) Prediction Guard (/docs/integrations/providers/predictionguard) PromptLayer (/docs/integrations/providers/promptlayer) Psychic (/docs/integrations/providers/psychic) PubMed (/docs/integrations/providers/pubmed) Qdrant (/docs/integrations/providers/qdrant) Ray Serve (/docs/integrations/providers/ray_serve) Rebuff (/docs/integrations/providers/rebuff) Reddit (/docs/integrations/providers/reddit) Redis (/docs/integrations/providers/redis) Replicate (/docs/integrations/providers/replicate) Roam (/docs/integrations/providers/roam) Rockset (/docs/integrations/providers/rockset) Runhouse (/docs/integrations/providers/runhouse) RWKV-4 (/docs/integrations/providers/rwkv) SageMaker Endpoint (/docs/integrations/providers/sagemaker_endpoint) SageMaker Tracking (/docs/integrations/providers/sagemaker_tracking) ScaNN (/docs/integrations/providers/scann) SearxNG Search API (/docs/integrations/providers/searx) SerpAPI (/docs/integrations/providers/serpapi) Shale Protocol (/docs/integrations/providers/shaleprotocol) SingleStoreDB (/docs/integrations/providers/singlestoredb) scikit-learn (/docs/integrations/providers/sklearn) Slack (/docs/integrations/providers/slack) spaCy (/docs/integrations/providers/spacy) Spreedly (/docs/integrations/providers/spreedly) StarRocks (/docs/integrations/providers/starrocks) StochasticAI (/docs/integrations/providers/stochasticai) Stripe (/docs/integrations/providers/stripe) Supabase (Postgres) (/docs/integrations/providers/supabase) Nebula (/docs/integrations/providers/symblai_nebula) Tair (/docs/integrations/providers/tair) Telegram (/docs/integrations/providers/telegram) TencentVectorDB (/docs/integrations/providers/tencentvectordb) TensorFlow Datasets (/docs/integrations/providers/tensorflow_datasets) Tigris (/docs/integrations/providers/tigris) 2Markdown (/docs/integrations/providers/tomarkdown) Trello (/docs/integrations/providers/trello) TruLens (/docs/integrations/providers/trulens) Twitter (/docs/integrations/providers/twitter) Typesense (/docs/integrations/providers/typesense) Unstructured (/docs/integrations/providers/unstructured) USearch (/docs/integrations/providers/usearch) Vearch (/docs/integrations/providers/vearch) Vectara (/docs/integrations/providers/vectara/) Vespa (/docs/integrations/providers/vespa) WandB Tracing (/docs/integrations/providers/wandb_tracing) Weights & Biases (/docs/integrations/providers/wandb_tracking) Weather (/docs/integrations/providers/weather) Weaviate (/docs/integrations/providers/weaviate) WhatsApp (/docs/integrations/providers/whatsapp) WhyLabs (/docs/integrations/providers/whylabs_profiling) Wikipedia (/docs/integrations/providers/wikipedia) Wolfram Alpha (/docs/integrations/providers/wolfram_alpha) Writer (/docs/integrations/providers/writer) Xata (/docs/integrations/providers/xata) Xorbits Inference (Xinference) (/docs/integrations/providers/xinference) Yeager.ai (/docs/integrations/providers/yeagerai) YouTube (/docs/integrations/providers/youtube) Zep (/docs/integrations/providers/zep) Zilliz (/docs/integrations/providers/zilliz)  (/) Integrations (/docs/integrations) Grouped by provider (/docs/integrations/providers/) Arthur (https://arthur.ai) onboarding guide for generative text models (https://docs.arthur.ai/user-guide/walkthroughs/model-onboarding/generative_text_onboarding.html) docs (https://docs.arthur.ai/) ArthurCallbackHandler (https://api.python.langchain.com/en/latest/callbacks/langchain.callbacks.arthur_callback.ArthurCallbackHandler.html) StreamingStdOutCallbackHandler (https://api.python.langchain.com/en/latest/callbacks/langchain.callbacks.streaming_stdout.StreamingStdOutCallbackHandler.html) ChatOpenAI (https://api.python.langchain.com/en/latest/chat_models/langchain.chat_models.openai.ChatOpenAI.html) HumanMessage (https://api.python.langchain.com/en/latest/schema/langchain.schema.messages.HumanMessage.html) model dashboard page (https://app.arthur.ai/) PreviousArgilla (/docs/integrations/providers/argilla) NextArxiv (/docs/integrations/providers/arxiv) Discord (https://discord.gg/cU2adEyC7w) Twitter (https://twitter.com/LangChainAI) Python (https://github.com/hwchase17/langchain) JS/TS (https://github.com/hwchase17/langchainjs) Homepage (https://langchain.com) Blog (https://blog.langchain.dev)