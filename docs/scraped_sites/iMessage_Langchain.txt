This notebook shows how to use the iMessage chat loader. This class helps convert iMessage conversations to LangChain chat messages. On MacOS, iMessage stores conversations in a sqlite database at ~/Library/Messages/chat.db (at least for macOS Ventura 13.4). The IMessageChatLoader loads from this database file.  It's likely that your terminal is denied access to ~/Library/Messages. To use this class, you can copy the DB to an accessible directory (e.g., Documents) and load from there. Alternatively (and not recommended), you can grant full disk access for your terminal emulator in System Settings > Securityand Privacy > Full Disk Access. We have created an example database you can use at this linked drive file. Provide the loader with the file path to the zip directory. You can optionally specify the user id that maps to an ai message as well an configure whether to merge message runs. The load() (or lazy_load) methods return a list of "ChatSessions" that currently just contain a list of messages per loaded conversation. All messages are mapped to "HumanMessage" objects to start.  You can optionally choose to merge message "runs" (consecutive messages from the same sender) and select a sender to represent the "AI". The fine-tuned LLM will learn to generate these AI messages. Now it's time to convert our chat  messages to OpenAI dictionaries. We can use the convert_messages_for_finetuning utility to do so. It's time to fine-tune the model. Make sure you have openai installed and have set your OPENAI_API_KEY appropriately With the file ready, it's time to kick off a training job. Grab a cup of tea while your model is being prepared. This may take some time! You can use the resulting model ID directly the ChatOpenAI model class. IntegrationsCallbacksChat modelsChat loadersDiscordFacebook MessengerGMailiMessageSlackTelegramTwitter (via Apify)WhatsAppDocument loadersDocument transformersLLMsMemoryRetrieversText embedding modelsAgents & ToolkitsToolsVector storesGrouped by provider Callbacks Chat models Chat loadersDiscordFacebook MessengerGMailiMessageSlackTelegramTwitter (via Apify)WhatsApp Discord Facebook Messenger GMail iMessage Slack Telegram Twitter (via Apify) WhatsApp Document loaders Document transformers LLMs Memory Retrievers Text embedding models Agents & Toolkits Tools Vector stores Grouped by provider  Integrations Chat loaders iMessage Create the IMessageChatLoader with the file path pointed to chat.db database you'd like to process. Call loader.load() (or loader.lazy_load()) to perform the conversion. Optionally use merge_chat_runs to combine message from the same sender in sequence, and/or map_ai_messages to convert messages from the specified sender to the "AIMessage" class. IMessageChatLoader ChatSession map_ai_messages merge_chat_runs convert_messages_for_finetuning ChatOpenAI ChatPromptTemplate StrOutputParser 1. Access Chat DB 2. Create the Chat Loader 3. Load messages 3. Prepare for fine-tuning 4. Fine-tune the model 5. Use in LangChain Discord Twitter Python JS/TS Homepage Blog Skip to main content🦜️🔗 LangChainDocsUse casesIntegrationsAPILangSmithJS/TS DocsCTRLKIntegrationsCallbacksChat modelsChat loadersDiscordFacebook MessengerGMailiMessageSlackTelegramTwitter (via Apify)WhatsAppDocument loadersDocument transformersLLMsMemoryRetrieversText embedding modelsAgents & ToolkitsToolsVector storesGrouped by providerIntegrationsChat loadersiMessageOn this pageiMessageThis notebook shows how to use the iMessage chat loader. This class helps convert iMessage conversations to LangChain chat messages.On MacOS, iMessage stores conversations in a sqlite database at ~/Library/Messages/chat.db (at least for macOS Ventura 13.4). The IMessageChatLoader loads from this database file. Create the IMessageChatLoader with the file path pointed to chat.db database you'd like to process.Call loader.load() (or loader.lazy_load()) to perform the conversion. Optionally use merge_chat_runs to combine message from the same sender in sequence, and/or map_ai_messages to convert messages from the specified sender to the "AIMessage" class.1. Access Chat DB​It's likely that your terminal is denied access to ~/Library/Messages. To use this class, you can copy the DB to an accessible directory (e.g., Documents) and load from there. Alternatively (and not recommended), you can grant full disk access for your terminal emulator in System Settings > Securityand Privacy > Full Disk Access.We have created an example database you can use at this linked drive file.# This uses some example dataimport requestsdef download_drive_file(url: str, output_path: str = 'chat.db') -> None:    file_id = url.split('/')[-2]    download_url = f'https://drive.google.com/uc?export=download&id={file_id}'    response = requests.get(download_url)    if response.status_code != 200:        print('Failed to download the file.')        return    with open(output_path, 'wb') as file:        file.write(response.content)        print(f'File {output_path} downloaded.')url = 'https://drive.google.com/file/d/1NebNKqTA2NXApCmeH6mu0unJD2tANZzo/view?usp=sharing'# Download file to chat.dbdownload_drive_file(url)    File chat.db downloaded.2. Create the Chat Loader​Provide the loader with the file path to the zip directory. You can optionally specify the user id that maps to an ai message as well an configure whether to merge message runs.from langchain.chat_loaders.imessage import IMessageChatLoaderAPI Reference:IMessageChatLoaderloader = IMessageChatLoader(    path="./chat.db",)3. Load messages​The load() (or lazy_load) methods return a list of "ChatSessions" that currently just contain a list of messages per loaded conversation. All messages are mapped to "HumanMessage" objects to start. You can optionally choose to merge message "runs" (consecutive messages from the same sender) and select a sender to represent the "AI". The fine-tuned LLM will learn to generate these AI messages.from typing import Listfrom langchain.chat_loaders.base import ChatSessionfrom langchain.chat_loaders.utils import (    map_ai_messages,    merge_chat_runs,)raw_messages = loader.lazy_load()# Merge consecutive messages from the same sender into a single messagemerged_messages = merge_chat_runs(raw_messages)# Convert messages from "Tortoise" to AI messages. Do you have a guess who these conversations are between?chat_sessions: List[ChatSession] = list(map_ai_messages(merged_messages, sender="Tortoise"))API Reference:ChatSessionmap_ai_messagesmerge_chat_runs# Now all of the Tortoise's messages will take the AI message class# which maps to the 'assistant' role in OpenAI's training formatalternating_sessions[0]['messages'][:3]    [AIMessage(content="Slow and steady, that's my motto.", additional_kwargs={'message_time': 1693182723, 'sender': 'Tortoise'}, example=False),     HumanMessage(content='Speed is key!', additional_kwargs={'message_time': 1693182753, 'sender': 'Hare'}, example=False),     AIMessage(content='A balanced approach is more reliable.', additional_kwargs={'message_time': 1693182783, 'sender': 'Tortoise'}, example=False)]3. Prepare for fine-tuning​Now it's time to convert our chat  messages to OpenAI dictionaries. We can use the convert_messages_for_finetuning utility to do so.from langchain.adapters.openai import convert_messages_for_finetuningAPI Reference:convert_messages_for_finetuningtraining_data = convert_messages_for_finetuning(alternating_sessions)print(f"Prepared {len(training_data)} dialogues for training")    Prepared 10 dialogues for training4. Fine-tune the model​It's time to fine-tune the model. Make sure you have openai installed and have set your OPENAI_API_KEY appropriately# %pip install -U openai --quietimport jsonfrom io import BytesIOimport timeimport openai# We will write the jsonl file in memorymy_file = BytesIO()for m in training_data:    my_file.write((json.dumps({"messages": m}) + "\n").encode('utf-8'))my_file.seek(0)training_file = openai.File.create(  file=my_file,  purpose='fine-tune')# OpenAI audits each training file for compliance reasons.# This make take a few minutesstatus = openai.File.retrieve(training_file.id).statusstart_time = time.time()while status != "processed":    print(f"Status=[{status}]... {time.time() - start_time:.2f}s", end="\r", flush=True)    time.sleep(5)    status = openai.File.retrieve(training_file.id).statusprint(f"File {training_file.id} ready after {time.time() - start_time:.2f} seconds.")    File file-zHIgf4r8LltZG3RFpkGd4Sjf ready after 10.19 seconds.With the file ready, it's time to kick off a training job.job = openai.FineTuningJob.create(    training_file=training_file.id,    model="gpt-3.5-turbo",)Grab a cup of tea while your model is being prepared. This may take some time!status = openai.FineTuningJob.retrieve(job.id).statusstart_time = time.time()while status != "succeeded":    print(f"Status=[{status}]... {time.time() - start_time:.2f}s", end="\r", flush=True)    time.sleep(5)    job = openai.FineTuningJob.retrieve(job.id)    status = job.status    Status=[running]... 524.95sprint(job.fine_tuned_model)    ft:gpt-3.5-turbo-0613:personal::7sKoRdlz5. Use in LangChain​You can use the resulting model ID directly the ChatOpenAI model class.from langchain.chat_models import ChatOpenAImodel = ChatOpenAI(    model=job.fine_tuned_model,    temperature=1,)API Reference:ChatOpenAIfrom langchain.prompts import ChatPromptTemplatefrom langchain.schema.output_parser import StrOutputParserprompt = ChatPromptTemplate.from_messages(    [        ("system", "You are speaking to hare."),        ("human", "{input}"),    ])chain = prompt | model | StrOutputParser()API Reference:ChatPromptTemplateStrOutputParserfor tok in chain.stream({"input": "What's the golden thread?"}):    print(tok, end="", flush=True)    A symbol of interconnectedness.PreviousGMailNextSlack1. Access Chat DB2. Create the Chat Loader3. Load messages3. Prepare for fine-tuning4. Fine-tune the model5. Use in LangChainCommunityDiscordTwitterGitHubPythonJS/TSMoreHomepageBlogCopyright © 2023 LangChain, Inc. Skip to main content 🦜️🔗 LangChainDocsUse casesIntegrationsAPILangSmithJS/TS DocsCTRLK 🦜️🔗 LangChainDocsUse casesIntegrationsAPI LangSmithJS/TS DocsCTRLK  CTRLK CTRLK  CTRLK   IntegrationsCallbacksChat modelsChat loadersDiscordFacebook MessengerGMailiMessageSlackTelegramTwitter (via Apify)WhatsAppDocument loadersDocument transformersLLMsMemoryRetrieversText embedding modelsAgents & ToolkitsToolsVector storesGrouped by providerIntegrationsChat loadersiMessageOn this pageiMessageThis notebook shows how to use the iMessage chat loader. This class helps convert iMessage conversations to LangChain chat messages.On MacOS, iMessage stores conversations in a sqlite database at ~/Library/Messages/chat.db (at least for macOS Ventura 13.4). and have set your OPENAI_API_KEY appropriately# %pip install -U openai --quietimport jsonfrom io import BytesIOimport timeimport openai# We will write the jsonl file in memorymy_file = BytesIO()for m in training_data:    my_file.write((json.dumps({"messages": m}) + "\n").encode('utf-8'))my_file.seek(0)training_file = openai.File.create(  file=my_file,  purpose='fine-tune')# OpenAI audits each training file for compliance reasons.# This make take a few minutesstatus = openai.File.retrieve(training_file.id).statusstart_time = time.time()while status != "processed":    print(f"Status=[{status}]... {time.time() - start_time:.2f}s", end="\r", flush=True)    time.sleep(5)    status = openai.File.retrieve(training_file.id).statusprint(f"File {training_file.id} ready after {time.time() - start_time:.2f} seconds.")    File file-zHIgf4r8LltZG3RFpkGd4Sjf ready after 10.19 seconds.With the file ready, it's time to kick off a training job.job = openai.FineTuningJob.create(    training_file=training_file.id,    model="gpt-3.5-turbo",)Grab a cup of tea while your model is being prepared. This may take some time!status = openai.FineTuningJob.retrieve(job.id).statusstart_time = time.time()while status != "succeeded":    print(f"Status=[{status}]... {time.time() - start_time:.2f}s", end="\r", flush=True)    time.sleep(5)    job = openai.FineTuningJob.retrieve(job.id)    status = job.status    Status=[running]... 524.95sprint(job.fine_tuned_model)    ft:gpt-3.5-turbo-0613:personal::7sKoRdlz5. Use in LangChain​You can use the resulting model ID directly the ChatOpenAI model class.from langchain.chat_models import ChatOpenAImodel = ChatOpenAI(    model=job.fine_tuned_model,    temperature=1,)API Reference:ChatOpenAIfrom langchain.prompts import ChatPromptTemplatefrom langchain.schema.output_parser import StrOutputParserprompt = ChatPromptTemplate.from_messages(    [        ("system", "You are speaking to hare."),        ("human", "{input}"),    ])chain = prompt | model | StrOutputParser()API Reference:ChatPromptTemplateStrOutputParserfor tok in chain.stream({"input": "What's the golden thread?"}):    print(tok, end="", flush=True)    A symbol of interconnectedness.PreviousGMailNextSlack1. Access Chat DB2. Create the Chat Loader3. Load messages3. Prepare for fine-tuning4. Fine-tune the model5. Use in LangChain IntegrationsCallbacksChat modelsChat loadersDiscordFacebook MessengerGMailiMessageSlackTelegramTwitter (via Apify)WhatsAppDocument loadersDocument transformersLLMsMemoryRetrieversText embedding modelsAgents & ToolkitsToolsVector storesGrouped by providerIntegrationsChat loadersiMessageOn this pageiMessageThis notebook shows how to use the iMessage chat loader. This class helps convert iMessage conversations to LangChain chat messages.On MacOS, iMessage stores conversations in a sqlite database at ~/Library/Messages/chat.db (at least for macOS Ventura 13.4). and have set your OPENAI_API_KEY appropriately# %pip install -U openai --quietimport jsonfrom io import BytesIOimport timeimport openai# We will write the jsonl file in memorymy_file = BytesIO()for m in training_data:    my_file.write((json.dumps({"messages": m}) + "\n").encode('utf-8'))my_file.seek(0)training_file = openai.File.create(  file=my_file,  purpose='fine-tune')# OpenAI audits each training file for compliance reasons.# This make take a few minutesstatus = openai.File.retrieve(training_file.id).statusstart_time = time.time()while status != "processed":    print(f"Status=[{status}]... {time.time() - start_time:.2f}s", end="\r", flush=True)    time.sleep(5)    status = openai.File.retrieve(training_file.id).statusprint(f"File {training_file.id} ready after {time.time() - start_time:.2f} seconds.")    File file-zHIgf4r8LltZG3RFpkGd4Sjf ready after 10.19 seconds.With the file ready, it's time to kick off a training job.job = openai.FineTuningJob.create(    training_file=training_file.id,    model="gpt-3.5-turbo",)Grab a cup of tea while your model is being prepared. This may take some time!status = openai.FineTuningJob.retrieve(job.id).statusstart_time = time.time()while status != "succeeded":    print(f"Status=[{status}]... {time.time() - start_time:.2f}s", end="\r", flush=True)    time.sleep(5)    job = openai.FineTuningJob.retrieve(job.id)    status = job.status    Status=[running]... 524.95sprint(job.fine_tuned_model)    ft:gpt-3.5-turbo-0613:personal::7sKoRdlz5. Use in LangChain​You can use the resulting model ID directly the ChatOpenAI model class.from langchain.chat_models import ChatOpenAImodel = ChatOpenAI(    model=job.fine_tuned_model,    temperature=1,)API Reference:ChatOpenAIfrom langchain.prompts import ChatPromptTemplatefrom langchain.schema.output_parser import StrOutputParserprompt = ChatPromptTemplate.from_messages(    [        ("system", "You are speaking to hare."),        ("human", "{input}"),    ])chain = prompt | model | StrOutputParser()API Reference:ChatPromptTemplateStrOutputParserfor tok in chain.stream({"input": "What's the golden thread?"}):    print(tok, end="", flush=True)    A symbol of interconnectedness.PreviousGMailNextSlack1. Access Chat DB2. Create the Chat Loader3. Load messages3. Prepare for fine-tuning4. Fine-tune the model5. Use in LangChain IntegrationsCallbacksChat modelsChat loadersDiscordFacebook MessengerGMailiMessageSlackTelegramTwitter (via Apify)WhatsAppDocument loadersDocument transformersLLMsMemoryRetrieversText embedding modelsAgents & ToolkitsToolsVector storesGrouped by provider IntegrationsCallbacksChat modelsChat loadersDiscordFacebook MessengerGMailiMessageSlackTelegramTwitter (via Apify)WhatsAppDocument loadersDocument transformersLLMsMemoryRetrieversText embedding modelsAgents & ToolkitsToolsVector storesGrouped by provider Integrations Callbacks Chat models Chat loaders Document loaders Document transformers LLMs Memory Retrievers Text embedding models Agents & Toolkits Tools Vector stores Grouped by provider IntegrationsChat loadersiMessageOn this pageiMessageThis notebook shows how to use the iMessage chat loader. This class helps convert iMessage conversations to LangChain chat messages.On MacOS, iMessage stores conversations in a sqlite database at ~/Library/Messages/chat.db (at least for macOS Ventura 13.4). and have set your OPENAI_API_KEY appropriately# %pip install -U openai --quietimport jsonfrom io import BytesIOimport timeimport openai# We will write the jsonl file in memorymy_file = BytesIO()for m in training_data:    my_file.write((json.dumps({"messages": m}) + "\n").encode('utf-8'))my_file.seek(0)training_file = openai.File.create(  file=my_file,  purpose='fine-tune')# OpenAI audits each training file for compliance reasons.# This make take a few minutesstatus = openai.File.retrieve(training_file.id).statusstart_time = time.time()while status != "processed":    print(f"Status=[{status}]... {time.time() - start_time:.2f}s", end="\r", flush=True)    time.sleep(5)    status = openai.File.retrieve(training_file.id).statusprint(f"File {training_file.id} ready after {time.time() - start_time:.2f} seconds.")    File file-zHIgf4r8LltZG3RFpkGd4Sjf ready after 10.19 seconds.With the file ready, it's time to kick off a training job.job = openai.FineTuningJob.create(    training_file=training_file.id,    model="gpt-3.5-turbo",)Grab a cup of tea while your model is being prepared. This may take some time!status = openai.FineTuningJob.retrieve(job.id).statusstart_time = time.time()while status != "succeeded":    print(f"Status=[{status}]... {time.time() - start_time:.2f}s", end="\r", flush=True)    time.sleep(5)    job = openai.FineTuningJob.retrieve(job.id)    status = job.status    Status=[running]... 524.95sprint(job.fine_tuned_model)    ft:gpt-3.5-turbo-0613:personal::7sKoRdlz5. Use in LangChain​You can use the resulting model ID directly the ChatOpenAI model class.from langchain.chat_models import ChatOpenAImodel = ChatOpenAI(    model=job.fine_tuned_model,    temperature=1,)API Reference:ChatOpenAIfrom langchain.prompts import ChatPromptTemplatefrom langchain.schema.output_parser import StrOutputParserprompt = ChatPromptTemplate.from_messages(    [        ("system", "You are speaking to hare."),        ("human", "{input}"),    ])chain = prompt | model | StrOutputParser()API Reference:ChatPromptTemplateStrOutputParserfor tok in chain.stream({"input": "What's the golden thread?"}):    print(tok, end="", flush=True)    A symbol of interconnectedness.PreviousGMailNextSlack1. Access Chat DB2. Create the Chat Loader3. Load messages3. Prepare for fine-tuning4. Fine-tune the model5. Use in LangChain IntegrationsChat loadersiMessageOn this pageiMessageThis notebook shows how to use the iMessage chat loader. This class helps convert iMessage conversations to LangChain chat messages.On MacOS, iMessage stores conversations in a sqlite database at ~/Library/Messages/chat.db (at least for macOS Ventura 13.4). and have set your OPENAI_API_KEY appropriately# %pip install -U openai --quietimport jsonfrom io import BytesIOimport timeimport openai# We will write the jsonl file in memorymy_file = BytesIO()for m in training_data:    my_file.write((json.dumps({"messages": m}) + "\n").encode('utf-8'))my_file.seek(0)training_file = openai.File.create(  file=my_file,  purpose='fine-tune')# OpenAI audits each training file for compliance reasons.# This make take a few minutesstatus = openai.File.retrieve(training_file.id).statusstart_time = time.time()while status != "processed":    print(f"Status=[{status}]... {time.time() - start_time:.2f}s", end="\r", flush=True)    time.sleep(5)    status = openai.File.retrieve(training_file.id).statusprint(f"File {training_file.id} ready after {time.time() - start_time:.2f} seconds.")    File file-zHIgf4r8LltZG3RFpkGd4Sjf ready after 10.19 seconds.With the file ready, it's time to kick off a training job.job = openai.FineTuningJob.create(    training_file=training_file.id,    model="gpt-3.5-turbo",)Grab a cup of tea while your model is being prepared. This may take some time!status = openai.FineTuningJob.retrieve(job.id).statusstart_time = time.time()while status != "succeeded":    print(f"Status=[{status}]... {time.time() - start_time:.2f}s", end="\r", flush=True)    time.sleep(5)    job = openai.FineTuningJob.retrieve(job.id)    status = job.status    Status=[running]... 524.95sprint(job.fine_tuned_model)    ft:gpt-3.5-turbo-0613:personal::7sKoRdlz5. Use in LangChain​You can use the resulting model ID directly the ChatOpenAI model class.from langchain.chat_models import ChatOpenAImodel = ChatOpenAI(    model=job.fine_tuned_model,    temperature=1,)API Reference:ChatOpenAIfrom langchain.prompts import ChatPromptTemplatefrom langchain.schema.output_parser import StrOutputParserprompt = ChatPromptTemplate.from_messages(    [        ("system", "You are speaking to hare."),        ("human", "{input}"),    ])chain = prompt | model | StrOutputParser()API Reference:ChatPromptTemplateStrOutputParserfor tok in chain.stream({"input": "What's the golden thread?"}):    print(tok, end="", flush=True)    A symbol of interconnectedness.PreviousGMailNextSlack IntegrationsChat loadersiMessageOn this pageiMessageThis notebook shows how to use the iMessage chat loader. This class helps convert iMessage conversations to LangChain chat messages.On MacOS, iMessage stores conversations in a sqlite database at ~/Library/Messages/chat.db (at least for macOS Ventura 13.4). and have set your OPENAI_API_KEY appropriately# %pip install -U openai --quietimport jsonfrom io import BytesIOimport timeimport openai# We will write the jsonl file in memorymy_file = BytesIO()for m in training_data:    my_file.write((json.dumps({"messages": m}) + "\n").encode('utf-8'))my_file.seek(0)training_file = openai.File.create(  file=my_file,  purpose='fine-tune')# OpenAI audits each training file for compliance reasons.# This make take a few minutesstatus = openai.File.retrieve(training_file.id).statusstart_time = time.time()while status != "processed":    print(f"Status=[{status}]... {time.time() - start_time:.2f}s", end="\r", flush=True)    time.sleep(5)    status = openai.File.retrieve(training_file.id).statusprint(f"File {training_file.id} ready after {time.time() - start_time:.2f} seconds.")    File file-zHIgf4r8LltZG3RFpkGd4Sjf ready after 10.19 seconds.With the file ready, it's time to kick off a training job.job = openai.FineTuningJob.create(    training_file=training_file.id,    model="gpt-3.5-turbo",)Grab a cup of tea while your model is being prepared. This may take some time!status = openai.FineTuningJob.retrieve(job.id).statusstart_time = time.time()while status != "succeeded":    print(f"Status=[{status}]... {time.time() - start_time:.2f}s", end="\r", flush=True)    time.sleep(5)    job = openai.FineTuningJob.retrieve(job.id)    status = job.status    Status=[running]... 524.95sprint(job.fine_tuned_model)    ft:gpt-3.5-turbo-0613:personal::7sKoRdlz5. Use in LangChain​You can use the resulting model ID directly the ChatOpenAI model class.from langchain.chat_models import ChatOpenAImodel = ChatOpenAI(    model=job.fine_tuned_model,    temperature=1,)API Reference:ChatOpenAIfrom langchain.prompts import ChatPromptTemplatefrom langchain.schema.output_parser import StrOutputParserprompt = ChatPromptTemplate.from_messages(    [        ("system", "You are speaking to hare."),        ("human", "{input}"),    ])chain = prompt | model | StrOutputParser()API Reference:ChatPromptTemplateStrOutputParserfor tok in chain.stream({"input": "What's the golden thread?"}):    print(tok, end="", flush=True)    A symbol of interconnectedness.PreviousGMailNextSlack On this page iMessageThis notebook shows how to use the iMessage chat loader. This class helps convert iMessage conversations to LangChain chat messages.On MacOS, iMessage stores conversations in a sqlite database at ~/Library/Messages/chat.db (at least for macOS Ventura 13.4). and have set your OPENAI_API_KEY appropriately# %pip install -U openai --quietimport jsonfrom io import BytesIOimport timeimport openai# We will write the jsonl file in memorymy_file = BytesIO()for m in training_data:    my_file.write((json.dumps({"messages": m}) + "\n").encode('utf-8'))my_file.seek(0)training_file = openai.File.create(  file=my_file,  purpose='fine-tune')# OpenAI audits each training file for compliance reasons.# This make take a few minutesstatus = openai.File.retrieve(training_file.id).statusstart_time = time.time()while status != "processed":    print(f"Status=[{status}]... {time.time() - start_time:.2f}s", end="\r", flush=True)    time.sleep(5)    status = openai.File.retrieve(training_file.id).statusprint(f"File {training_file.id} ready after {time.time() - start_time:.2f} seconds.")    File file-zHIgf4r8LltZG3RFpkGd4Sjf ready after 10.19 seconds.With the file ready, it's time to kick off a training job.job = openai.FineTuningJob.create(    training_file=training_file.id,    model="gpt-3.5-turbo",)Grab a cup of tea while your model is being prepared. This may take some time!status = openai.FineTuningJob.retrieve(job.id).statusstart_time = time.time()while status != "succeeded":    print(f"Status=[{status}]... {time.time() - start_time:.2f}s", end="\r", flush=True)    time.sleep(5)    job = openai.FineTuningJob.retrieve(job.id)    status = job.status    Status=[running]... 524.95sprint(job.fine_tuned_model)    ft:gpt-3.5-turbo-0613:personal::7sKoRdlz5. Use in LangChain​You can use the resulting model ID directly the ChatOpenAI model class.from langchain.chat_models import ChatOpenAImodel = ChatOpenAI(    model=job.fine_tuned_model,    temperature=1,)API Reference:ChatOpenAIfrom langchain.prompts import ChatPromptTemplatefrom langchain.schema.output_parser import StrOutputParserprompt = ChatPromptTemplate.from_messages(    [        ("system", "You are speaking to hare."),        ("human", "{input}"),    ])chain = prompt | model | StrOutputParser()API Reference:ChatPromptTemplateStrOutputParserfor tok in chain.stream({"input": "What's the golden thread?"}):    print(tok, end="", flush=True)    A symbol of interconnectedness. # This uses some example dataimport requestsdef download_drive_file(url: str, output_path: str = 'chat.db') -> None:    file_id = url.split('/')[-2]    download_url = f'https://drive.google.com/uc?export=download&id={file_id}'    response = requests.get(download_url)    if response.status_code != 200:        print('Failed to download the file.')        return    with open(output_path, 'wb') as file:        file.write(response.content)        print(f'File {output_path} downloaded.')url = 'https://drive.google.com/file/d/1NebNKqTA2NXApCmeH6mu0unJD2tANZzo/view?usp=sharing'# Download file to chat.dbdownload_drive_file(url) # This uses some example dataimport requestsdef download_drive_file(url: str, output_path: str = 'chat.db') -> None:    file_id = url.split('/')[-2]    download_url = f'https://drive.google.com/uc?export=download&id={file_id}'    response = requests.get(download_url)    if response.status_code != 200:        print('Failed to download the file.')        return    with open(output_path, 'wb') as file:        file.write(response.content)        print(f'File {output_path} downloaded.')url = 'https://drive.google.com/file/d/1NebNKqTA2NXApCmeH6mu0unJD2tANZzo/view?usp=sharing'# Download file to chat.dbdownload_drive_file(url)      File chat.db downloaded.     File chat.db downloaded.     File chat.db downloaded.  from langchain.chat_loaders.imessage import IMessageChatLoader from langchain.chat_loaders.imessage import IMessageChatLoader  API Reference:IMessageChatLoader loader = IMessageChatLoader(    path="./chat.db",) loader = IMessageChatLoader(    path="./chat.db",)  from typing import Listfrom langchain.chat_loaders.base import ChatSessionfrom langchain.chat_loaders.utils import (    map_ai_messages,    merge_chat_runs,)raw_messages = loader.lazy_load()# Merge consecutive messages from the same sender into a single messagemerged_messages = merge_chat_runs(raw_messages)# Convert messages from "Tortoise" to AI messages. Do you have a guess who these conversations are between?chat_sessions: List[ChatSession] = list(map_ai_messages(merged_messages, sender="Tortoise")) from typing import Listfrom langchain.chat_loaders.base import ChatSessionfrom langchain.chat_loaders.utils import (    map_ai_messages,    merge_chat_runs,)raw_messages = loader.lazy_load()# Merge consecutive messages from the same sender into a single messagemerged_messages = merge_chat_runs(raw_messages)# Convert messages from "Tortoise" to AI messages. Do you have a guess who these conversations are between?chat_sessions: List[ChatSession] = list(map_ai_messages(merged_messages, sender="Tortoise"))  API Reference:ChatSessionmap_ai_messagesmerge_chat_runs # Now all of the Tortoise's messages will take the AI message class# which maps to the 'assistant' role in OpenAI's training formatalternating_sessions[0]['messages'][:3] # Now all of the Tortoise's messages will take the AI message class# which maps to the 'assistant' role in OpenAI's training formatalternating_sessions[0]['messages'][:3]      [AIMessage(content="Slow and steady, that's my motto.", additional_kwargs={'message_time': 1693182723, 'sender': 'Tortoise'}, example=False),     HumanMessage(content='Speed is key!', additional_kwargs={'message_time': 1693182753, 'sender': 'Hare'}, example=False),     AIMessage(content='A balanced approach is more reliable.', additional_kwargs={'message_time': 1693182783, 'sender': 'Tortoise'}, example=False)]     [AIMessage(content="Slow and steady, that's my motto.", additional_kwargs={'message_time': 1693182723, 'sender': 'Tortoise'}, example=False),     HumanMessage(content='Speed is key!', additional_kwargs={'message_time': 1693182753, 'sender': 'Hare'}, example=False),     AIMessage(content='A balanced approach is more reliable.', additional_kwargs={'message_time': 1693182783, 'sender': 'Tortoise'}, example=False)]     [AIMessage(content="Slow and steady, that's my motto.", additional_kwargs={'message_time': 1693182723, 'sender': 'Tortoise'}, example=False),     HumanMessage(content='Speed is key!', additional_kwargs={'message_time': 1693182753, 'sender': 'Hare'}, example=False),     AIMessage(content='A balanced approach is more reliable.', additional_kwargs={'message_time': 1693182783, 'sender': 'Tortoise'}, example=False)]  from langchain.adapters.openai import convert_messages_for_finetuning from langchain.adapters.openai import convert_messages_for_finetuning  API Reference:convert_messages_for_finetuning training_data = convert_messages_for_finetuning(alternating_sessions)print(f"Prepared {len(training_data)} dialogues for training") training_data = convert_messages_for_finetuning(alternating_sessions)print(f"Prepared {len(training_data)} dialogues for training")      Prepared 10 dialogues for training     Prepared 10 dialogues for training     Prepared 10 dialogues for training  # %pip install -U openai --quiet # %pip install -U openai --quiet  import jsonfrom io import BytesIOimport timeimport openai# We will write the jsonl file in memorymy_file = BytesIO()for m in training_data:    my_file.write((json.dumps({"messages": m}) + "\n").encode('utf-8'))my_file.seek(0)training_file = openai.File.create(  file=my_file,  purpose='fine-tune')# OpenAI audits each training file for compliance reasons.# This make take a few minutesstatus = openai.File.retrieve(training_file.id).statusstart_time = time.time()while status != "processed":    print(f"Status=[{status}]... {time.time() - start_time:.2f}s", end="\r", flush=True)    time.sleep(5)    status = openai.File.retrieve(training_file.id).statusprint(f"File {training_file.id} ready after {time.time() - start_time:.2f} seconds.") import jsonfrom io import BytesIOimport timeimport openai# We will write the jsonl file in memorymy_file = BytesIO()for m in training_data:    my_file.write((json.dumps({"messages": m}) + "\n").encode('utf-8'))my_file.seek(0)training_file = openai.File.create(  file=my_file,  purpose='fine-tune')# OpenAI audits each training file for compliance reasons.# This make take a few minutesstatus = openai.File.retrieve(training_file.id).statusstart_time = time.time()while status != "processed":    print(f"Status=[{status}]... {time.time() - start_time:.2f}s", end="\r", flush=True)    time.sleep(5)    status = openai.File.retrieve(training_file.id).statusprint(f"File {training_file.id} ready after {time.time() - start_time:.2f} seconds.")      File file-zHIgf4r8LltZG3RFpkGd4Sjf ready after 10.19 seconds.     File file-zHIgf4r8LltZG3RFpkGd4Sjf ready after 10.19 seconds.     File file-zHIgf4r8LltZG3RFpkGd4Sjf ready after 10.19 seconds.  job = openai.FineTuningJob.create(    training_file=training_file.id,    model="gpt-3.5-turbo",) job = openai.FineTuningJob.create(    training_file=training_file.id,    model="gpt-3.5-turbo",)  status = openai.FineTuningJob.retrieve(job.id).statusstart_time = time.time()while status != "succeeded":    print(f"Status=[{status}]... {time.time() - start_time:.2f}s", end="\r", flush=True)    time.sleep(5)    job = openai.FineTuningJob.retrieve(job.id)    status = job.status status = openai.FineTuningJob.retrieve(job.id).statusstart_time = time.time()while status != "succeeded":    print(f"Status=[{status}]... {time.time() - start_time:.2f}s", end="\r", flush=True)    time.sleep(5)    job = openai.FineTuningJob.retrieve(job.id)    status = job.status      Status=[running]... 524.95s     Status=[running]... 524.95s     Status=[running]... 524.95s  print(job.fine_tuned_model) print(job.fine_tuned_model)      ft:gpt-3.5-turbo-0613:personal::7sKoRdlz     ft:gpt-3.5-turbo-0613:personal::7sKoRdlz     ft:gpt-3.5-turbo-0613:personal::7sKoRdlz  from langchain.chat_models import ChatOpenAImodel = ChatOpenAI(    model=job.fine_tuned_model,    temperature=1,) from langchain.chat_models import ChatOpenAImodel = ChatOpenAI(    model=job.fine_tuned_model,    temperature=1,)  API Reference:ChatOpenAI from langchain.prompts import ChatPromptTemplatefrom langchain.schema.output_parser import StrOutputParserprompt = ChatPromptTemplate.from_messages(    [        ("system", "You are speaking to hare."),        ("human", "{input}"),    ])chain = prompt | model | StrOutputParser() from langchain.prompts import ChatPromptTemplatefrom langchain.schema.output_parser import StrOutputParserprompt = ChatPromptTemplate.from_messages(    [        ("system", "You are speaking to hare."),        ("human", "{input}"),    ])chain = prompt | model | StrOutputParser()  API Reference:ChatPromptTemplateStrOutputParser for tok in chain.stream({"input": "What's the golden thread?"}):    print(tok, end="", flush=True) for tok in chain.stream({"input": "What's the golden thread?"}):    print(tok, end="", flush=True)      A symbol of interconnectedness.     A symbol of interconnectedness.     A symbol of interconnectedness.  Previous GMail Next Slack 1. Access Chat DB2. Create the Chat Loader3. Load messages3. Prepare for fine-tuning4. Fine-tune the model5. Use in LangChain 1. Access Chat DB2. Create the Chat Loader3. Load messages3. Prepare for fine-tuning4. Fine-tune the model5. Use in LangChain CommunityDiscordTwitterGitHubPythonJS/TSMoreHomepageBlogCopyright © 2023 LangChain, Inc. CommunityDiscordTwitterGitHubPythonJS/TSMoreHomepageBlog CommunityDiscordTwitter Community GitHubPythonJS/TS GitHub MoreHomepageBlog More Copyright © 2023 LangChain, Inc. Copyright © 2023 LangChain, Inc. Skip to main content (#docusaurus_skipToContent_fallback) 🦜️🔗 LangChain (/) Docs (/docs/get_started/introduction) Use cases (/docs/use_cases) Integrations (/docs/integrations) API (https://api.python.langchain.com) LangSmith (https://smith.langchain.com) JS/TS Docs (https://js.langchain.com/docs)  (https://github.com/hwchase17/langchain) Integrations (/docs/integrations) Callbacks (/docs/integrations/callbacks/) Chat models (/docs/integrations/chat/) Chat loaders (/docs/integrations/chat_loaders/) Discord (/docs/integrations/chat_loaders/discord) Facebook Messenger (/docs/integrations/chat_loaders/facebook) GMail (/docs/integrations/chat_loaders/gmail) iMessage (/docs/integrations/chat_loaders/imessage) Slack (/docs/integrations/chat_loaders/slack) Telegram (/docs/integrations/chat_loaders/telegram) Twitter (via Apify) (/docs/integrations/chat_loaders/twitter) WhatsApp (/docs/integrations/chat_loaders/whatsapp) Document loaders (/docs/integrations/document_loaders/) Document transformers (/docs/integrations/document_transformers/) LLMs (/docs/integrations/llms/) Memory (/docs/integrations/memory/) Retrievers (/docs/integrations/retrievers/) Text embedding models (/docs/integrations/text_embedding/) Agents & Toolkits (/docs/integrations/toolkits/) Tools (/docs/integrations/tools/) Vector stores (/docs/integrations/vectorstores/) Grouped by provider (/docs/integrations/providers/)  (/) Integrations (/docs/integrations) Chat loaders (/docs/integrations/chat_loaders/) ​ (#1-access-chat-db) this linked drive file (https://drive.google.com/file/d/1NebNKqTA2NXApCmeH6mu0unJD2tANZzo/view?usp=sharing) ​ (#2-create-the-chat-loader) IMessageChatLoader (https://api.python.langchain.com/en/latest/chat_loaders/langchain.chat_loaders.imessage.IMessageChatLoader.html) ​ (#3-load-messages) ChatSession (https://api.python.langchain.com/en/latest/chat_loaders/langchain.chat_loaders.base.ChatSession.html) map_ai_messages (https://api.python.langchain.com/en/latest/chat_loaders/langchain.chat_loaders.utils.map_ai_messages.html) merge_chat_runs (https://api.python.langchain.com/en/latest/chat_loaders/langchain.chat_loaders.utils.merge_chat_runs.html) ​ (#3-prepare-for-fine-tuning) convert_messages_for_finetuning (https://api.python.langchain.com/en/latest/adapters/langchain.adapters.openai.convert_messages_for_finetuning.html) ​ (#4-fine-tune-the-model) ​ (#5-use-in-langchain) ChatOpenAI (https://api.python.langchain.com/en/latest/chat_models/langchain.chat_models.openai.ChatOpenAI.html) ChatPromptTemplate (https://api.python.langchain.com/en/latest/prompts/langchain.prompts.chat.ChatPromptTemplate.html) StrOutputParser (https://api.python.langchain.com/en/latest/schema/langchain.schema.output_parser.StrOutputParser.html) PreviousGMail (/docs/integrations/chat_loaders/gmail) NextSlack (/docs/integrations/chat_loaders/slack) 1. Access Chat DB (#1-access-chat-db) 2. Create the Chat Loader (#2-create-the-chat-loader) 3. Load messages (#3-load-messages) 3. Prepare for fine-tuning (#3-prepare-for-fine-tuning) 4. Fine-tune the model (#4-fine-tune-the-model) 5. Use in LangChain (#5-use-in-langchain) Discord (https://discord.gg/cU2adEyC7w) Twitter (https://twitter.com/LangChainAI) Python (https://github.com/hwchase17/langchain) JS/TS (https://github.com/hwchase17/langchainjs) Homepage (https://langchain.com) Blog (https://blog.langchain.dev)