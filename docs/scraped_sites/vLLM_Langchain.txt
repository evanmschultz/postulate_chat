vLLM is a fast and easy-to-use library for LLM inference and serving, offering: This notebooks goes over how to use a LLM with langchain and vLLM. To use, you should have the vllm python package installed. vLLM supports distributed tensor-parallel inference and serving.  To run multi-GPU inference with the LLM class, set the tensor_parallel_size argument to the number of GPUs you want to use. For example, to run inference on 4 GPUs vLLM can be deployed as a server that mimics the OpenAI API protocol. This allows vLLM to be used as a drop-in replacement for applications using OpenAI API. This server can be queried in the same format as OpenAI API. IntegrationsCallbacksChat modelsChat loadersDocument loadersDocument transformersLLMsAI21Aleph AlphaAmazon API GatewayAnyscaleAzure MLAzure OpenAIBananaBasetenBeamBedrockBittensorCerebriumAIChatGLMClarifaiCohereC TransformersCTranslate2DatabricksDeepInfraDeepSparseEden AIFireworksForefrontAIGoogle Vertex AI PaLMGooseAIGPT4AllHugging Face HubHugging Face Local PipelinesHuggingface TextGen InferenceJSONFormerKoboldAI APILlama.cppLLM Caching integrationsManifestMinimaxModalMosaicMLNLP CloudOctoAIOllamaOpaquePromptsOpenAIOpenLLMOpenLMPetalsPipelineAIPredibasePrediction GuardPromptLayer OpenAIRELLMReplicateRunhouseSageMakerEndpointStochasticAINebula (Symbl.ai)TextGenTitan TakeoffTongyi QwenvLLMWriterXorbits Inference (Xinference)MemoryRetrieversText embedding modelsAgents & ToolkitsToolsVector storesGrouped by provider Callbacks Chat models Chat loaders Document loaders Document transformers LLMsAI21Aleph AlphaAmazon API GatewayAnyscaleAzure MLAzure OpenAIBananaBasetenBeamBedrockBittensorCerebriumAIChatGLMClarifaiCohereC TransformersCTranslate2DatabricksDeepInfraDeepSparseEden AIFireworksForefrontAIGoogle Vertex AI PaLMGooseAIGPT4AllHugging Face HubHugging Face Local PipelinesHuggingface TextGen InferenceJSONFormerKoboldAI APILlama.cppLLM Caching integrationsManifestMinimaxModalMosaicMLNLP CloudOctoAIOllamaOpaquePromptsOpenAIOpenLLMOpenLMPetalsPipelineAIPredibasePrediction GuardPromptLayer OpenAIRELLMReplicateRunhouseSageMakerEndpointStochasticAINebula (Symbl.ai)TextGenTitan TakeoffTongyi QwenvLLMWriterXorbits Inference (Xinference) AI21 Aleph Alpha Amazon API Gateway Anyscale Azure ML Azure OpenAI Banana Baseten Beam Bedrock Bittensor CerebriumAI ChatGLM Clarifai Cohere C Transformers CTranslate2 Databricks DeepInfra DeepSparse Eden AI Fireworks ForefrontAI Google Vertex AI PaLM GooseAI GPT4All Hugging Face Hub Hugging Face Local Pipelines Huggingface TextGen Inference JSONFormer KoboldAI API Llama.cpp LLM Caching integrations Manifest Minimax Modal MosaicML NLP Cloud OctoAI Ollama OpaquePrompts OpenAI OpenLLM OpenLM Petals PipelineAI Predibase Prediction Guard PromptLayer OpenAI RELLM Replicate Runhouse SageMakerEndpoint StochasticAI Nebula (Symbl.ai) TextGen Titan Takeoff Tongyi Qwen vLLM Writer Xorbits Inference (Xinference) Memory Retrievers Text embedding models Agents & Toolkits Tools Vector stores Grouped by provider  Integrations LLMs vLLM State-of-the-art serving throughput  Efficient management of attention key and value memory with PagedAttention Continuous batching of incoming requests Optimized CUDA kernels VLLM VLLM VLLMOpenAI Integrate the model in an LLMChain Distributed Inference OpenAI-Compatible ServerOpenAI-Compatible Completion OpenAI-Compatible Completion Discord Twitter Python JS/TS Homepage Blog Skip to main contentü¶úÔ∏èüîó LangChainDocsUse casesIntegrationsAPILangSmithJS/TS DocsCTRLKIntegrationsCallbacksChat modelsChat loadersDocument loadersDocument transformersLLMsAI21Aleph AlphaAmazon API GatewayAnyscaleAzure MLAzure OpenAIBananaBasetenBeamBedrockBittensorCerebriumAIChatGLMClarifaiCohereC TransformersCTranslate2DatabricksDeepInfraDeepSparseEden AIFireworksForefrontAIGoogle Vertex AI PaLMGooseAIGPT4AllHugging Face HubHugging Face Local PipelinesHuggingface TextGen InferenceJSONFormerKoboldAI APILlama.cppLLM Caching integrationsManifestMinimaxModalMosaicMLNLP CloudOctoAIOllamaOpaquePromptsOpenAIOpenLLMOpenLMPetalsPipelineAIPredibasePrediction GuardPromptLayer OpenAIRELLMReplicateRunhouseSageMakerEndpointStochasticAINebula (Symbl.ai)TextGenTitan TakeoffTongyi QwenvLLMWriterXorbits Inference (Xinference)MemoryRetrieversText embedding modelsAgents & ToolkitsToolsVector storesGrouped by providerIntegrationsLLMsvLLMOn this pagevLLMvLLM is a fast and easy-to-use library for LLM inference and serving, offering:State-of-the-art serving throughput Efficient management of attention key and value memory with PagedAttentionContinuous batching of incoming requestsOptimized CUDA kernelsThis notebooks goes over how to use a LLM with langchain and vLLM.To use, you should have the vllm python package installed.#!pip install vllm -qfrom langchain.llms import VLLMllm = VLLM(model="mosaicml/mpt-7b",           trust_remote_code=True,  # mandatory for hf models           max_new_tokens=128,           top_k=10,           top_p=0.95,           temperature=0.8,)print(llm("What is the capital of France ?"))API Reference:VLLM    INFO 08-06 11:37:33 llm_engine.py:70] Initializing an LLM engine with config: model='mosaicml/mpt-7b', tokenizer='mosaicml/mpt-7b', tokenizer_mode=auto, trust_remote_code=True, dtype=torch.bfloat16, use_dummy_weights=False, download_dir=None, use_np_weights=False, tensor_parallel_size=1, seed=0)    INFO 08-06 11:37:41 llm_engine.py:196] # GPU blocks: 861, # CPU blocks: 512    Processed prompts: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.00it/s]        What is the capital of France ? The capital of France is Paris.    Integrate the model in an LLMChain‚Äãfrom langchain import PromptTemplate, LLMChaintemplate = """Question: {question}Answer: Let's think step by step."""prompt = PromptTemplate(template=template, input_variables=["question"])llm_chain = LLMChain(prompt=prompt, llm=llm)question = "Who was the US president in the year the first Pokemon game was released?"print(llm_chain.run(question))    Processed prompts: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:01<00:00,  1.34s/it]            1. The first Pokemon game was released in 1996.    2. The president was Bill Clinton.    3. Clinton was president from 1993 to 2001.    4. The answer is Clinton.        Distributed Inference‚ÄãvLLM supports distributed tensor-parallel inference and serving. To run multi-GPU inference with the LLM class, set the tensor_parallel_size argument to the number of GPUs you want to use. For example, to run inference on 4 GPUsfrom langchain.llms import VLLMllm = VLLM(model="mosaicml/mpt-30b",           tensor_parallel_size=4,           trust_remote_code=True,  # mandatory for hf models)llm("What is the future of AI?")API Reference:VLLMOpenAI-Compatible Server‚ÄãvLLM can be deployed as a server that mimics the OpenAI API protocol. This allows vLLM to be used as a drop-in replacement for applications using OpenAI API.This server can be queried in the same format as OpenAI API.OpenAI-Compatible Completion‚Äãfrom langchain.llms import VLLMOpenAIllm = VLLMOpenAI(    openai_api_key="EMPTY",    openai_api_base="http://localhost:8000/v1",    model_name="tiiuae/falcon-7b",    model_kwargs={"stop": ["."]})print(llm("Rome is"))API Reference:VLLMOpenAI     a city that is filled with history, ancient buildings, and art around every cornerPreviousTongyi QwenNextWriterIntegrate the model in an LLMChainDistributed InferenceOpenAI-Compatible ServerOpenAI-Compatible CompletionCommunityDiscordTwitterGitHubPythonJS/TSMoreHomepageBlogCopyright ¬© 2023 LangChain, Inc. Skip to main content ü¶úÔ∏èüîó LangChainDocsUse casesIntegrationsAPILangSmithJS/TS DocsCTRLK ü¶úÔ∏èüîó LangChainDocsUse casesIntegrationsAPI LangSmithJS/TS DocsCTRLK  CTRLK CTRLK  CTRLK   IntegrationsCallbacksChat modelsChat loadersDocument loadersDocument transformersLLMsAI21Aleph AlphaAmazon API GatewayAnyscaleAzure MLAzure OpenAIBananaBasetenBeamBedrockBittensorCerebriumAIChatGLMClarifaiCohereC TransformersCTranslate2DatabricksDeepInfraDeepSparseEden AIFireworksForefrontAIGoogle Vertex AI PaLMGooseAIGPT4AllHugging Face HubHugging Face Local PipelinesHuggingface TextGen InferenceJSONFormerKoboldAI APILlama.cppLLM Caching integrationsManifestMinimaxModalMosaicMLNLP CloudOctoAIOllamaOpaquePromptsOpenAIOpenLLMOpenLMPetalsPipelineAIPredibasePrediction GuardPromptLayer OpenAIRELLMReplicateRunhouseSageMakerEndpointStochasticAINebula (Symbl.ai)TextGenTitan TakeoffTongyi QwenvLLMWriterXorbits Inference (Xinference)MemoryRetrieversText embedding modelsAgents & ToolkitsToolsVector storesGrouped by providerIntegrationsLLMsvLLMOn this pagevLLMvLLM is a fast and easy-to-use library for LLM inference and serving, offering:State-of-the-art serving throughput Efficient management of attention key and value memory with PagedAttentionContinuous batching of incoming requestsOptimized CUDA kernelsThis notebooks goes over how to use a LLM with langchain and vLLM.To use, you should have the vllm python package installed.#!pip install vllm -qfrom langchain.llms import VLLMllm = VLLM(model="mosaicml/mpt-7b",           trust_remote_code=True,  # mandatory for hf models           max_new_tokens=128,           top_k=10,           top_p=0.95,           temperature=0.8,)print(llm("What is the capital of France ?"))API Reference:VLLM    INFO 08-06 11:37:33 llm_engine.py:70] Initializing an LLM engine with config: model='mosaicml/mpt-7b', tokenizer='mosaicml/mpt-7b', tokenizer_mode=auto, trust_remote_code=True, dtype=torch.bfloat16, use_dummy_weights=False, download_dir=None, use_np_weights=False, tensor_parallel_size=1, seed=0)    INFO 08-06 11:37:41 llm_engine.py:196] # GPU blocks: 861, # CPU blocks: 512    Processed prompts: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.00it/s]        What is the capital of France ? The capital of France is Paris.    Integrate the model in an LLMChain‚Äãfrom langchain import PromptTemplate, LLMChaintemplate = """Question: {question}Answer: Let's think step by step."""prompt = PromptTemplate(template=template, input_variables=["question"])llm_chain = LLMChain(prompt=prompt, llm=llm)question = "Who was the US president in the year the first Pokemon game was released?"print(llm_chain.run(question))    Processed prompts: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:01<00:00,  1.34s/it]            1. The first Pokemon game was released in 1996.    2. The president was Bill Clinton.    3. Clinton was president from 1993 to 2001.    4. The answer is Clinton.        Distributed Inference‚ÄãvLLM supports distributed tensor-parallel inference and serving. To run multi-GPU inference with the LLM class, set the tensor_parallel_size argument to the number of GPUs you want to use. For example, to run inference on 4 GPUsfrom langchain.llms import VLLMllm = VLLM(model="mosaicml/mpt-30b",           tensor_parallel_size=4,           trust_remote_code=True,  # mandatory for hf models)llm("What is the future of AI?")API Reference:VLLMOpenAI-Compatible Server‚ÄãvLLM can be deployed as a server that mimics the OpenAI API protocol. This allows vLLM to be used as a drop-in replacement for applications using OpenAI API.This server can be queried in the same format as OpenAI API.OpenAI-Compatible Completion‚Äãfrom langchain.llms import VLLMOpenAIllm = VLLMOpenAI(    openai_api_key="EMPTY",    openai_api_base="http://localhost:8000/v1",    model_name="tiiuae/falcon-7b",    model_kwargs={"stop": ["."]})print(llm("Rome is"))API Reference:VLLMOpenAI     a city that is filled with history, ancient buildings, and art around every cornerPreviousTongyi QwenNextWriterIntegrate the model in an LLMChainDistributed InferenceOpenAI-Compatible ServerOpenAI-Compatible Completion IntegrationsCallbacksChat modelsChat loadersDocument loadersDocument transformersLLMsAI21Aleph AlphaAmazon API GatewayAnyscaleAzure MLAzure OpenAIBananaBasetenBeamBedrockBittensorCerebriumAIChatGLMClarifaiCohereC TransformersCTranslate2DatabricksDeepInfraDeepSparseEden AIFireworksForefrontAIGoogle Vertex AI PaLMGooseAIGPT4AllHugging Face HubHugging Face Local PipelinesHuggingface TextGen InferenceJSONFormerKoboldAI APILlama.cppLLM Caching integrationsManifestMinimaxModalMosaicMLNLP CloudOctoAIOllamaOpaquePromptsOpenAIOpenLLMOpenLMPetalsPipelineAIPredibasePrediction GuardPromptLayer OpenAIRELLMReplicateRunhouseSageMakerEndpointStochasticAINebula (Symbl.ai)TextGenTitan TakeoffTongyi QwenvLLMWriterXorbits Inference (Xinference)MemoryRetrieversText embedding modelsAgents & ToolkitsToolsVector storesGrouped by providerIntegrationsLLMsvLLMOn this pagevLLMvLLM is a fast and easy-to-use library for LLM inference and serving, offering:State-of-the-art serving throughput Efficient management of attention key and value memory with PagedAttentionContinuous batching of incoming requestsOptimized CUDA kernelsThis notebooks goes over how to use a LLM with langchain and vLLM.To use, you should have the vllm python package installed.#!pip install vllm -qfrom langchain.llms import VLLMllm = VLLM(model="mosaicml/mpt-7b",           trust_remote_code=True,  # mandatory for hf models           max_new_tokens=128,           top_k=10,           top_p=0.95,           temperature=0.8,)print(llm("What is the capital of France ?"))API Reference:VLLM    INFO 08-06 11:37:33 llm_engine.py:70] Initializing an LLM engine with config: model='mosaicml/mpt-7b', tokenizer='mosaicml/mpt-7b', tokenizer_mode=auto, trust_remote_code=True, dtype=torch.bfloat16, use_dummy_weights=False, download_dir=None, use_np_weights=False, tensor_parallel_size=1, seed=0)    INFO 08-06 11:37:41 llm_engine.py:196] # GPU blocks: 861, # CPU blocks: 512    Processed prompts: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.00it/s]        What is the capital of France ? The capital of France is Paris.    Integrate the model in an LLMChain‚Äãfrom langchain import PromptTemplate, LLMChaintemplate = """Question: {question}Answer: Let's think step by step."""prompt = PromptTemplate(template=template, input_variables=["question"])llm_chain = LLMChain(prompt=prompt, llm=llm)question = "Who was the US president in the year the first Pokemon game was released?"print(llm_chain.run(question))    Processed prompts: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:01<00:00,  1.34s/it]            1. The first Pokemon game was released in 1996.    2. The president was Bill Clinton.    3. Clinton was president from 1993 to 2001.    4. The answer is Clinton.        Distributed Inference‚ÄãvLLM supports distributed tensor-parallel inference and serving. To run multi-GPU inference with the LLM class, set the tensor_parallel_size argument to the number of GPUs you want to use. For example, to run inference on 4 GPUsfrom langchain.llms import VLLMllm = VLLM(model="mosaicml/mpt-30b",           tensor_parallel_size=4,           trust_remote_code=True,  # mandatory for hf models)llm("What is the future of AI?")API Reference:VLLMOpenAI-Compatible Server‚ÄãvLLM can be deployed as a server that mimics the OpenAI API protocol. This allows vLLM to be used as a drop-in replacement for applications using OpenAI API.This server can be queried in the same format as OpenAI API.OpenAI-Compatible Completion‚Äãfrom langchain.llms import VLLMOpenAIllm = VLLMOpenAI(    openai_api_key="EMPTY",    openai_api_base="http://localhost:8000/v1",    model_name="tiiuae/falcon-7b",    model_kwargs={"stop": ["."]})print(llm("Rome is"))API Reference:VLLMOpenAI     a city that is filled with history, ancient buildings, and art around every cornerPreviousTongyi QwenNextWriterIntegrate the model in an LLMChainDistributed InferenceOpenAI-Compatible ServerOpenAI-Compatible Completion IntegrationsCallbacksChat modelsChat loadersDocument loadersDocument transformersLLMsAI21Aleph AlphaAmazon API GatewayAnyscaleAzure MLAzure OpenAIBananaBasetenBeamBedrockBittensorCerebriumAIChatGLMClarifaiCohereC TransformersCTranslate2DatabricksDeepInfraDeepSparseEden AIFireworksForefrontAIGoogle Vertex AI PaLMGooseAIGPT4AllHugging Face HubHugging Face Local PipelinesHuggingface TextGen InferenceJSONFormerKoboldAI APILlama.cppLLM Caching integrationsManifestMinimaxModalMosaicMLNLP CloudOctoAIOllamaOpaquePromptsOpenAIOpenLLMOpenLMPetalsPipelineAIPredibasePrediction GuardPromptLayer OpenAIRELLMReplicateRunhouseSageMakerEndpointStochasticAINebula (Symbl.ai)TextGenTitan TakeoffTongyi QwenvLLMWriterXorbits Inference (Xinference)MemoryRetrieversText embedding modelsAgents & ToolkitsToolsVector storesGrouped by provider IntegrationsCallbacksChat modelsChat loadersDocument loadersDocument transformersLLMsAI21Aleph AlphaAmazon API GatewayAnyscaleAzure MLAzure OpenAIBananaBasetenBeamBedrockBittensorCerebriumAIChatGLMClarifaiCohereC TransformersCTranslate2DatabricksDeepInfraDeepSparseEden AIFireworksForefrontAIGoogle Vertex AI PaLMGooseAIGPT4AllHugging Face HubHugging Face Local PipelinesHuggingface TextGen InferenceJSONFormerKoboldAI APILlama.cppLLM Caching integrationsManifestMinimaxModalMosaicMLNLP CloudOctoAIOllamaOpaquePromptsOpenAIOpenLLMOpenLMPetalsPipelineAIPredibasePrediction GuardPromptLayer OpenAIRELLMReplicateRunhouseSageMakerEndpointStochasticAINebula (Symbl.ai)TextGenTitan TakeoffTongyi QwenvLLMWriterXorbits Inference (Xinference)MemoryRetrieversText embedding modelsAgents & ToolkitsToolsVector storesGrouped by provider Integrations Callbacks Chat models Chat loaders Document loaders Document transformers LLMs Memory Retrievers Text embedding models Agents & Toolkits Tools Vector stores Grouped by provider IntegrationsLLMsvLLMOn this pagevLLMvLLM is a fast and easy-to-use library for LLM inference and serving, offering:State-of-the-art serving throughput Efficient management of attention key and value memory with PagedAttentionContinuous batching of incoming requestsOptimized CUDA kernelsThis notebooks goes over how to use a LLM with langchain and vLLM.To use, you should have the vllm python package installed.#!pip install vllm -qfrom langchain.llms import VLLMllm = VLLM(model="mosaicml/mpt-7b",           trust_remote_code=True,  # mandatory for hf models           max_new_tokens=128,           top_k=10,           top_p=0.95,           temperature=0.8,)print(llm("What is the capital of France ?"))API Reference:VLLM    INFO 08-06 11:37:33 llm_engine.py:70] Initializing an LLM engine with config: model='mosaicml/mpt-7b', tokenizer='mosaicml/mpt-7b', tokenizer_mode=auto, trust_remote_code=True, dtype=torch.bfloat16, use_dummy_weights=False, download_dir=None, use_np_weights=False, tensor_parallel_size=1, seed=0)    INFO 08-06 11:37:41 llm_engine.py:196] # GPU blocks: 861, # CPU blocks: 512    Processed prompts: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.00it/s]        What is the capital of France ? The capital of France is Paris.    Integrate the model in an LLMChain‚Äãfrom langchain import PromptTemplate, LLMChaintemplate = """Question: {question}Answer: Let's think step by step."""prompt = PromptTemplate(template=template, input_variables=["question"])llm_chain = LLMChain(prompt=prompt, llm=llm)question = "Who was the US president in the year the first Pokemon game was released?"print(llm_chain.run(question))    Processed prompts: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:01<00:00,  1.34s/it]            1. The first Pokemon game was released in 1996.    2. The president was Bill Clinton.    3. Clinton was president from 1993 to 2001.    4. The answer is Clinton.        Distributed Inference‚ÄãvLLM supports distributed tensor-parallel inference and serving. To run multi-GPU inference with the LLM class, set the tensor_parallel_size argument to the number of GPUs you want to use. For example, to run inference on 4 GPUsfrom langchain.llms import VLLMllm = VLLM(model="mosaicml/mpt-30b",           tensor_parallel_size=4,           trust_remote_code=True,  # mandatory for hf models)llm("What is the future of AI?")API Reference:VLLMOpenAI-Compatible Server‚ÄãvLLM can be deployed as a server that mimics the OpenAI API protocol. This allows vLLM to be used as a drop-in replacement for applications using OpenAI API.This server can be queried in the same format as OpenAI API.OpenAI-Compatible Completion‚Äãfrom langchain.llms import VLLMOpenAIllm = VLLMOpenAI(    openai_api_key="EMPTY",    openai_api_base="http://localhost:8000/v1",    model_name="tiiuae/falcon-7b",    model_kwargs={"stop": ["."]})print(llm("Rome is"))API Reference:VLLMOpenAI     a city that is filled with history, ancient buildings, and art around every cornerPreviousTongyi QwenNextWriterIntegrate the model in an LLMChainDistributed InferenceOpenAI-Compatible ServerOpenAI-Compatible Completion IntegrationsLLMsvLLMOn this pagevLLMvLLM is a fast and easy-to-use library for LLM inference and serving, offering:State-of-the-art serving throughput Efficient management of attention key and value memory with PagedAttentionContinuous batching of incoming requestsOptimized CUDA kernelsThis notebooks goes over how to use a LLM with langchain and vLLM.To use, you should have the vllm python package installed.#!pip install vllm -qfrom langchain.llms import VLLMllm = VLLM(model="mosaicml/mpt-7b",           trust_remote_code=True,  # mandatory for hf models           max_new_tokens=128,           top_k=10,           top_p=0.95,           temperature=0.8,)print(llm("What is the capital of France ?"))API Reference:VLLM    INFO 08-06 11:37:33 llm_engine.py:70] Initializing an LLM engine with config: model='mosaicml/mpt-7b', tokenizer='mosaicml/mpt-7b', tokenizer_mode=auto, trust_remote_code=True, dtype=torch.bfloat16, use_dummy_weights=False, download_dir=None, use_np_weights=False, tensor_parallel_size=1, seed=0)    INFO 08-06 11:37:41 llm_engine.py:196] # GPU blocks: 861, # CPU blocks: 512    Processed prompts: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.00it/s]        What is the capital of France ? The capital of France is Paris.    Integrate the model in an LLMChain‚Äãfrom langchain import PromptTemplate, LLMChaintemplate = """Question: {question}Answer: Let's think step by step."""prompt = PromptTemplate(template=template, input_variables=["question"])llm_chain = LLMChain(prompt=prompt, llm=llm)question = "Who was the US president in the year the first Pokemon game was released?"print(llm_chain.run(question))    Processed prompts: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:01<00:00,  1.34s/it]            1. The first Pokemon game was released in 1996.    2. The president was Bill Clinton.    3. Clinton was president from 1993 to 2001.    4. The answer is Clinton.        Distributed Inference‚ÄãvLLM supports distributed tensor-parallel inference and serving. To run multi-GPU inference with the LLM class, set the tensor_parallel_size argument to the number of GPUs you want to use. For example, to run inference on 4 GPUsfrom langchain.llms import VLLMllm = VLLM(model="mosaicml/mpt-30b",           tensor_parallel_size=4,           trust_remote_code=True,  # mandatory for hf models)llm("What is the future of AI?")API Reference:VLLMOpenAI-Compatible Server‚ÄãvLLM can be deployed as a server that mimics the OpenAI API protocol. This allows vLLM to be used as a drop-in replacement for applications using OpenAI API.This server can be queried in the same format as OpenAI API.OpenAI-Compatible Completion‚Äãfrom langchain.llms import VLLMOpenAIllm = VLLMOpenAI(    openai_api_key="EMPTY",    openai_api_base="http://localhost:8000/v1",    model_name="tiiuae/falcon-7b",    model_kwargs={"stop": ["."]})print(llm("Rome is"))API Reference:VLLMOpenAI     a city that is filled with history, ancient buildings, and art around every cornerPreviousTongyi QwenNextWriterIntegrate the model in an LLMChainDistributed InferenceOpenAI-Compatible ServerOpenAI-Compatible Completion IntegrationsLLMsvLLMOn this pagevLLMvLLM is a fast and easy-to-use library for LLM inference and serving, offering:State-of-the-art serving throughput Efficient management of attention key and value memory with PagedAttentionContinuous batching of incoming requestsOptimized CUDA kernelsThis notebooks goes over how to use a LLM with langchain and vLLM.To use, you should have the vllm python package installed.#!pip install vllm -qfrom langchain.llms import VLLMllm = VLLM(model="mosaicml/mpt-7b",           trust_remote_code=True,  # mandatory for hf models           max_new_tokens=128,           top_k=10,           top_p=0.95,           temperature=0.8,)print(llm("What is the capital of France ?"))API Reference:VLLM    INFO 08-06 11:37:33 llm_engine.py:70] Initializing an LLM engine with config: model='mosaicml/mpt-7b', tokenizer='mosaicml/mpt-7b', tokenizer_mode=auto, trust_remote_code=True, dtype=torch.bfloat16, use_dummy_weights=False, download_dir=None, use_np_weights=False, tensor_parallel_size=1, seed=0)    INFO 08-06 11:37:41 llm_engine.py:196] # GPU blocks: 861, # CPU blocks: 512    Processed prompts: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.00it/s]        What is the capital of France ? The capital of France is Paris.    Integrate the model in an LLMChain‚Äãfrom langchain import PromptTemplate, LLMChaintemplate = """Question: {question}Answer: Let's think step by step."""prompt = PromptTemplate(template=template, input_variables=["question"])llm_chain = LLMChain(prompt=prompt, llm=llm)question = "Who was the US president in the year the first Pokemon game was released?"print(llm_chain.run(question))    Processed prompts: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:01<00:00,  1.34s/it]            1. The first Pokemon game was released in 1996.    2. The president was Bill Clinton.    3. Clinton was president from 1993 to 2001.    4. The answer is Clinton.        Distributed Inference‚ÄãvLLM supports distributed tensor-parallel inference and serving. To run multi-GPU inference with the LLM class, set the tensor_parallel_size argument to the number of GPUs you want to use. For example, to run inference on 4 GPUsfrom langchain.llms import VLLMllm = VLLM(model="mosaicml/mpt-30b",           tensor_parallel_size=4,           trust_remote_code=True,  # mandatory for hf models)llm("What is the future of AI?")API Reference:VLLMOpenAI-Compatible Server‚ÄãvLLM can be deployed as a server that mimics the OpenAI API protocol. This allows vLLM to be used as a drop-in replacement for applications using OpenAI API.This server can be queried in the same format as OpenAI API.OpenAI-Compatible Completion‚Äãfrom langchain.llms import VLLMOpenAIllm = VLLMOpenAI(    openai_api_key="EMPTY",    openai_api_base="http://localhost:8000/v1",    model_name="tiiuae/falcon-7b",    model_kwargs={"stop": ["."]})print(llm("Rome is"))API Reference:VLLMOpenAI     a city that is filled with history, ancient buildings, and art around every cornerPreviousTongyi QwenNextWriter IntegrationsLLMsvLLMOn this pagevLLMvLLM is a fast and easy-to-use library for LLM inference and serving, offering:State-of-the-art serving throughput Efficient management of attention key and value memory with PagedAttentionContinuous batching of incoming requestsOptimized CUDA kernelsThis notebooks goes over how to use a LLM with langchain and vLLM.To use, you should have the vllm python package installed.#!pip install vllm -qfrom langchain.llms import VLLMllm = VLLM(model="mosaicml/mpt-7b",           trust_remote_code=True,  # mandatory for hf models           max_new_tokens=128,           top_k=10,           top_p=0.95,           temperature=0.8,)print(llm("What is the capital of France ?"))API Reference:VLLM    INFO 08-06 11:37:33 llm_engine.py:70] Initializing an LLM engine with config: model='mosaicml/mpt-7b', tokenizer='mosaicml/mpt-7b', tokenizer_mode=auto, trust_remote_code=True, dtype=torch.bfloat16, use_dummy_weights=False, download_dir=None, use_np_weights=False, tensor_parallel_size=1, seed=0)    INFO 08-06 11:37:41 llm_engine.py:196] # GPU blocks: 861, # CPU blocks: 512    Processed prompts: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.00it/s]        What is the capital of France ? The capital of France is Paris.    Integrate the model in an LLMChain‚Äãfrom langchain import PromptTemplate, LLMChaintemplate = """Question: {question}Answer: Let's think step by step."""prompt = PromptTemplate(template=template, input_variables=["question"])llm_chain = LLMChain(prompt=prompt, llm=llm)question = "Who was the US president in the year the first Pokemon game was released?"print(llm_chain.run(question))    Processed prompts: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:01<00:00,  1.34s/it]            1. The first Pokemon game was released in 1996.    2. The president was Bill Clinton.    3. Clinton was president from 1993 to 2001.    4. The answer is Clinton.        Distributed Inference‚ÄãvLLM supports distributed tensor-parallel inference and serving. To run multi-GPU inference with the LLM class, set the tensor_parallel_size argument to the number of GPUs you want to use. For example, to run inference on 4 GPUsfrom langchain.llms import VLLMllm = VLLM(model="mosaicml/mpt-30b",           tensor_parallel_size=4,           trust_remote_code=True,  # mandatory for hf models)llm("What is the future of AI?")API Reference:VLLMOpenAI-Compatible Server‚ÄãvLLM can be deployed as a server that mimics the OpenAI API protocol. This allows vLLM to be used as a drop-in replacement for applications using OpenAI API.This server can be queried in the same format as OpenAI API.OpenAI-Compatible Completion‚Äãfrom langchain.llms import VLLMOpenAIllm = VLLMOpenAI(    openai_api_key="EMPTY",    openai_api_base="http://localhost:8000/v1",    model_name="tiiuae/falcon-7b",    model_kwargs={"stop": ["."]})print(llm("Rome is"))API Reference:VLLMOpenAI     a city that is filled with history, ancient buildings, and art around every cornerPreviousTongyi QwenNextWriter On this page vLLMvLLM is a fast and easy-to-use library for LLM inference and serving, offering:State-of-the-art serving throughput Efficient management of attention key and value memory with PagedAttentionContinuous batching of incoming requestsOptimized CUDA kernelsThis notebooks goes over how to use a LLM with langchain and vLLM.To use, you should have the vllm python package installed.#!pip install vllm -qfrom langchain.llms import VLLMllm = VLLM(model="mosaicml/mpt-7b",           trust_remote_code=True,  # mandatory for hf models           max_new_tokens=128,           top_k=10,           top_p=0.95,           temperature=0.8,)print(llm("What is the capital of France ?"))API Reference:VLLM    INFO 08-06 11:37:33 llm_engine.py:70] Initializing an LLM engine with config: model='mosaicml/mpt-7b', tokenizer='mosaicml/mpt-7b', tokenizer_mode=auto, trust_remote_code=True, dtype=torch.bfloat16, use_dummy_weights=False, download_dir=None, use_np_weights=False, tensor_parallel_size=1, seed=0)    INFO 08-06 11:37:41 llm_engine.py:196] # GPU blocks: 861, # CPU blocks: 512    Processed prompts: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.00it/s]        What is the capital of France ? The capital of France is Paris.    Integrate the model in an LLMChain‚Äãfrom langchain import PromptTemplate, LLMChaintemplate = """Question: {question}Answer: Let's think step by step."""prompt = PromptTemplate(template=template, input_variables=["question"])llm_chain = LLMChain(prompt=prompt, llm=llm)question = "Who was the US president in the year the first Pokemon game was released?"print(llm_chain.run(question))    Processed prompts: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:01<00:00,  1.34s/it]            1. The first Pokemon game was released in 1996.    2. The president was Bill Clinton.    3. Clinton was president from 1993 to 2001.    4. The answer is Clinton.        Distributed Inference‚ÄãvLLM supports distributed tensor-parallel inference and serving. To run multi-GPU inference with the LLM class, set the tensor_parallel_size argument to the number of GPUs you want to use. For example, to run inference on 4 GPUsfrom langchain.llms import VLLMllm = VLLM(model="mosaicml/mpt-30b",           tensor_parallel_size=4,           trust_remote_code=True,  # mandatory for hf models)llm("What is the future of AI?")API Reference:VLLMOpenAI-Compatible Server‚ÄãvLLM can be deployed as a server that mimics the OpenAI API protocol. This allows vLLM to be used as a drop-in replacement for applications using OpenAI API.This server can be queried in the same format as OpenAI API.OpenAI-Compatible Completion‚Äãfrom langchain.llms import VLLMOpenAIllm = VLLMOpenAI(    openai_api_key="EMPTY",    openai_api_base="http://localhost:8000/v1",    model_name="tiiuae/falcon-7b",    model_kwargs={"stop": ["."]})print(llm("Rome is"))API Reference:VLLMOpenAI     a city that is filled with history, ancient buildings, and art around every corner #!pip install vllm -q #!pip install vllm -q  from langchain.llms import VLLMllm = VLLM(model="mosaicml/mpt-7b",           trust_remote_code=True,  # mandatory for hf models           max_new_tokens=128,           top_k=10,           top_p=0.95,           temperature=0.8,)print(llm("What is the capital of France ?")) from langchain.llms import VLLMllm = VLLM(model="mosaicml/mpt-7b",           trust_remote_code=True,  # mandatory for hf models           max_new_tokens=128,           top_k=10,           top_p=0.95,           temperature=0.8,)print(llm("What is the capital of France ?"))  API Reference:VLLM     INFO 08-06 11:37:33 llm_engine.py:70] Initializing an LLM engine with config: model='mosaicml/mpt-7b', tokenizer='mosaicml/mpt-7b', tokenizer_mode=auto, trust_remote_code=True, dtype=torch.bfloat16, use_dummy_weights=False, download_dir=None, use_np_weights=False, tensor_parallel_size=1, seed=0)    INFO 08-06 11:37:41 llm_engine.py:196] # GPU blocks: 861, # CPU blocks: 512    Processed prompts: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.00it/s]        What is the capital of France ? The capital of France is Paris.         INFO 08-06 11:37:33 llm_engine.py:70] Initializing an LLM engine with config: model='mosaicml/mpt-7b', tokenizer='mosaicml/mpt-7b', tokenizer_mode=auto, trust_remote_code=True, dtype=torch.bfloat16, use_dummy_weights=False, download_dir=None, use_np_weights=False, tensor_parallel_size=1, seed=0)    INFO 08-06 11:37:41 llm_engine.py:196] # GPU blocks: 861, # CPU blocks: 512    Processed prompts: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.00it/s]        What is the capital of France ? The capital of France is Paris.         INFO 08-06 11:37:33 llm_engine.py:70] Initializing an LLM engine with config: model='mosaicml/mpt-7b', tokenizer='mosaicml/mpt-7b', tokenizer_mode=auto, trust_remote_code=True, dtype=torch.bfloat16, use_dummy_weights=False, download_dir=None, use_np_weights=False, tensor_parallel_size=1, seed=0)    INFO 08-06 11:37:41 llm_engine.py:196] # GPU blocks: 861, # CPU blocks: 512    Processed prompts: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.00it/s]        What is the capital of France ? The capital of France is Paris.      from langchain import PromptTemplate, LLMChaintemplate = """Question: {question}Answer: Let's think step by step."""prompt = PromptTemplate(template=template, input_variables=["question"])llm_chain = LLMChain(prompt=prompt, llm=llm)question = "Who was the US president in the year the first Pokemon game was released?"print(llm_chain.run(question)) from langchain import PromptTemplate, LLMChaintemplate = """Question: {question}Answer: Let's think step by step."""prompt = PromptTemplate(template=template, input_variables=["question"])llm_chain = LLMChain(prompt=prompt, llm=llm)question = "Who was the US president in the year the first Pokemon game was released?"print(llm_chain.run(question))      Processed prompts: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:01<00:00,  1.34s/it]            1. The first Pokemon game was released in 1996.    2. The president was Bill Clinton.    3. Clinton was president from 1993 to 2001.    4. The answer is Clinton.             Processed prompts: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:01<00:00,  1.34s/it]            1. The first Pokemon game was released in 1996.    2. The president was Bill Clinton.    3. Clinton was president from 1993 to 2001.    4. The answer is Clinton.             Processed prompts: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:01<00:00,  1.34s/it]            1. The first Pokemon game was released in 1996.    2. The president was Bill Clinton.    3. Clinton was president from 1993 to 2001.    4. The answer is Clinton.          from langchain.llms import VLLMllm = VLLM(model="mosaicml/mpt-30b",           tensor_parallel_size=4,           trust_remote_code=True,  # mandatory for hf models)llm("What is the future of AI?") from langchain.llms import VLLMllm = VLLM(model="mosaicml/mpt-30b",           tensor_parallel_size=4,           trust_remote_code=True,  # mandatory for hf models)llm("What is the future of AI?")  API Reference:VLLM from langchain.llms import VLLMOpenAIllm = VLLMOpenAI(    openai_api_key="EMPTY",    openai_api_base="http://localhost:8000/v1",    model_name="tiiuae/falcon-7b",    model_kwargs={"stop": ["."]})print(llm("Rome is")) from langchain.llms import VLLMOpenAIllm = VLLMOpenAI(    openai_api_key="EMPTY",    openai_api_base="http://localhost:8000/v1",    model_name="tiiuae/falcon-7b",    model_kwargs={"stop": ["."]})print(llm("Rome is"))  API Reference:VLLMOpenAI      a city that is filled with history, ancient buildings, and art around every corner      a city that is filled with history, ancient buildings, and art around every corner      a city that is filled with history, ancient buildings, and art around every corner  Previous Tongyi Qwen Next Writer Integrate the model in an LLMChainDistributed InferenceOpenAI-Compatible ServerOpenAI-Compatible Completion Integrate the model in an LLMChainDistributed InferenceOpenAI-Compatible ServerOpenAI-Compatible Completion CommunityDiscordTwitterGitHubPythonJS/TSMoreHomepageBlogCopyright ¬© 2023 LangChain, Inc. CommunityDiscordTwitterGitHubPythonJS/TSMoreHomepageBlog CommunityDiscordTwitter Community GitHubPythonJS/TS GitHub MoreHomepageBlog More Copyright ¬© 2023 LangChain, Inc. Copyright ¬© 2023 LangChain, Inc. Skip to main content (#docusaurus_skipToContent_fallback) ü¶úÔ∏èüîó LangChain (/) Docs (/docs/get_started/introduction) Use cases (/docs/use_cases) Integrations (/docs/integrations) API (https://api.python.langchain.com) LangSmith (https://smith.langchain.com) JS/TS Docs (https://js.langchain.com/docs)  (https://github.com/hwchase17/langchain) Integrations (/docs/integrations) Callbacks (/docs/integrations/callbacks/) Chat models (/docs/integrations/chat/) Chat loaders (/docs/integrations/chat_loaders/) Document loaders (/docs/integrations/document_loaders/) Document transformers (/docs/integrations/document_transformers/) LLMs (/docs/integrations/llms/) AI21 (/docs/integrations/llms/ai21) Aleph Alpha (/docs/integrations/llms/aleph_alpha) Amazon API Gateway (/docs/integrations/llms/amazon_api_gateway) Anyscale (/docs/integrations/llms/anyscale) Azure ML (/docs/integrations/llms/azure_ml) Azure OpenAI (/docs/integrations/llms/azure_openai) Banana (/docs/integrations/llms/banana) Baseten (/docs/integrations/llms/baseten) Beam (/docs/integrations/llms/beam) Bedrock (/docs/integrations/llms/bedrock) Bittensor (/docs/integrations/llms/bittensor) CerebriumAI (/docs/integrations/llms/cerebriumai) ChatGLM (/docs/integrations/llms/chatglm) Clarifai (/docs/integrations/llms/clarifai) Cohere (/docs/integrations/llms/cohere) C Transformers (/docs/integrations/llms/ctransformers) CTranslate2 (/docs/integrations/llms/ctranslate2) Databricks (/docs/integrations/llms/databricks) DeepInfra (/docs/integrations/llms/deepinfra) DeepSparse (/docs/integrations/llms/deepsparse) Eden AI (/docs/integrations/llms/edenai) Fireworks (/docs/integrations/llms/fireworks) ForefrontAI (/docs/integrations/llms/forefrontai) Google Vertex AI PaLM (/docs/integrations/llms/google_vertex_ai_palm) GooseAI (/docs/integrations/llms/gooseai) GPT4All (/docs/integrations/llms/gpt4all) Hugging Face Hub (/docs/integrations/llms/huggingface_hub) Hugging Face Local Pipelines (/docs/integrations/llms/huggingface_pipelines) Huggingface TextGen Inference (/docs/integrations/llms/huggingface_textgen_inference) JSONFormer (/docs/integrations/llms/jsonformer_experimental) KoboldAI API (/docs/integrations/llms/koboldai) Llama.cpp (/docs/integrations/llms/llamacpp) LLM Caching integrations (/docs/integrations/llms/llm_caching) Manifest (/docs/integrations/llms/manifest) Minimax (/docs/integrations/llms/minimax) Modal (/docs/integrations/llms/modal) MosaicML (/docs/integrations/llms/mosaicml) NLP Cloud (/docs/integrations/llms/nlpcloud) OctoAI (/docs/integrations/llms/octoai) Ollama (/docs/integrations/llms/ollama) OpaquePrompts (/docs/integrations/llms/opaqueprompts) OpenAI (/docs/integrations/llms/openai) OpenLLM (/docs/integrations/llms/openllm) OpenLM (/docs/integrations/llms/openlm) Petals (/docs/integrations/llms/petals) PipelineAI (/docs/integrations/llms/pipelineai) Predibase (/docs/integrations/llms/predibase) Prediction Guard (/docs/integrations/llms/predictionguard) PromptLayer OpenAI (/docs/integrations/llms/promptlayer_openai) RELLM (/docs/integrations/llms/rellm_experimental) Replicate (/docs/integrations/llms/replicate) Runhouse (/docs/integrations/llms/runhouse) SageMakerEndpoint (/docs/integrations/llms/sagemaker) StochasticAI (/docs/integrations/llms/stochasticai) Nebula (Symbl.ai) (/docs/integrations/llms/symblai_nebula) TextGen (/docs/integrations/llms/textgen) Titan Takeoff (/docs/integrations/llms/titan_takeoff) Tongyi Qwen (/docs/integrations/llms/tongyi) vLLM (/docs/integrations/llms/vllm) Writer (/docs/integrations/llms/writer) Xorbits Inference (Xinference) (/docs/integrations/llms/xinference) Memory (/docs/integrations/memory/) Retrievers (/docs/integrations/retrievers/) Text embedding models (/docs/integrations/text_embedding/) Agents & Toolkits (/docs/integrations/toolkits/) Tools (/docs/integrations/tools/) Vector stores (/docs/integrations/vectorstores/) Grouped by provider (/docs/integrations/providers/)  (/) Integrations (/docs/integrations) LLMs (/docs/integrations/llms/) vLLM (https://vllm.readthedocs.io/en/latest/index.html) VLLM (https://api.python.langchain.com/en/latest/llms/langchain.llms.vllm.VLLM.html) ‚Äã (#integrate-the-model-in-an-llmchain) ‚Äã (#distributed-inference) VLLM (https://api.python.langchain.com/en/latest/llms/langchain.llms.vllm.VLLM.html) ‚Äã (#openai-compatible-server) ‚Äã (#openai-compatible-completion) VLLMOpenAI (https://api.python.langchain.com/en/latest/llms/langchain.llms.vllm.VLLMOpenAI.html) PreviousTongyi Qwen (/docs/integrations/llms/tongyi) NextWriter (/docs/integrations/llms/writer) Integrate the model in an LLMChain (#integrate-the-model-in-an-llmchain) Distributed Inference (#distributed-inference) OpenAI-Compatible Server (#openai-compatible-server) OpenAI-Compatible Completion (#openai-compatible-completion) Discord (https://discord.gg/cU2adEyC7w) Twitter (https://twitter.com/LangChainAI) Python (https://github.com/hwchase17/langchain) JS/TS (https://github.com/hwchase17/langchainjs) Homepage (https://langchain.com) Blog (https://blog.langchain.dev)