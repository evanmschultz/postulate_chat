Rockset is a real-time analytics database which enables queries on massive, semi-structured data without operational burden. With Rockset, ingested data is queryable within one second and analytical queries against that data typically execute in milliseconds. Rockset is compute optimized, making it suitable for serving high concurrency applications in the sub-100TB range (or larger than 100s of TBs with rollups). This notebook demonstrates how to use Rockset as a document loader in langchain. To get started, make sure you have a Rockset account and an API key available. The Rockset integration with LangChain allows you to load documents from Rockset collections with SQL queries. In order to do this you must construct a RocksetLoader object. Here is an example snippet that initializes a RocksetLoader. Here, you can see that the following query is run: The text column in the collection is used as the page content, and the record's id and date columns are used as metadata (if you do not pass anything into metadata_keys, the whole Rockset document will be used as metadata).  To execute the query and access an iterator over the resulting Documents, run: To execute the query and access all resulting Documents at once, run: Here is an example response of loader.load(): You can choose to use multiple columns as content: Assuming the "sentence1" field is "This is the first sentence." and the "sentence2" field is "This is the second sentence.", the page_content of the resulting Document would be: You can define you own function to join content columns by setting the content_columns_joiner argument in the RocksetLoader constructor. content_columns_joiner is a method that takes in a List[Tuple[str, Any]]] as an argument, representing a list of tuples of (column name, column value). By default, this is a method that joins each column value with a new line. For example, if you wanted to join sentence1 and sentence2 with a space instead of a new line, you could set content_columns_joiner like so: The page_content of the resulting Document would be: Oftentimes you want to include the column name in the page_content. You can do that like this: This would result in the following page_content: IntegrationsCallbacksChat modelsChat loadersDocument loadersEtherscan LoaderacreomAirbyte CDKAirbyte GongAirbyte HubspotAirbyte JSONAirbyte SalesforceAirbyte ShopifyAirbyte StripeAirbyte TypeformAirbyte Zendesk SupportAirtableAlibaba Cloud MaxComputeApify DatasetArcGISArxivAssemblyAI Audio TranscriptsAsync ChromiumAsyncHtmlLoaderAWS S3 DirectoryAWS S3 FileAZLyricsAzure Blob Storage ContainerAzure Blob Storage FileAzure Document IntelligenceBibTeXBiliBiliBlackboardBlockchainBrave SearchBrowserlessChatGPT DataCollege ConfidentialConcurrent LoaderConfluenceCoNLL-UCopy PasteCSVCube Semantic LayerDatadog LogsDiffbotDiscordDocugamiDropboxDuckDBEmailEmbaasEPubEverNoteexample_dataMicrosoft ExcelFacebook ChatFaunaFigmaGeopandasGitGitBookGitHubGoogle BigQueryGoogle Cloud Storage DirectoryGoogle Cloud Storage FileGoogle DriveGrobidGutenbergHacker NewsHuawei OBS DirectoryHuawei OBS FileHuggingFace datasetiFixitImagesImage captionsIMSDbIuguJoplinJupyter NotebookLarkSuite (FeiShu)MastodonMediaWikiDumpMergeDocLoadermhtmlMicrosoft OneDriveMicrosoft PowerPointMicrosoft SharePointMicrosoft WordModern TreasuryNews URLNotion DB 1/2Notion DB 2/2Nuclia Understanding API document loaderObsidianOpen Document Format (ODT)Open City DataOrg-modePandas DataFrameAmazon TextractPolars DataFramePsychicPubMedPySpark DataFrame LoaderReadTheDocs DocumentationRecursive URL LoaderRedditRoamRocksetRSS FeedsRSTSitemapSlackSnowflakeSource CodeSpreedlyStripeSubtitleTelegramTencent COS DirectoryTencent COS FileTensorFlow Datasets2MarkdownTOMLTrelloTSVTwitterUnstructured FileURLWeatherWebBaseLoaderWhatsApp ChatWikipediaXMLXorbits Pandas DataFrameLoading documents from a YouTube urlYouTube transcriptsDocument transformersLLMsMemoryRetrieversText embedding modelsAgents & ToolkitsToolsVector storesGrouped by provider Callbacks Chat models Chat loaders Document loadersEtherscan LoaderacreomAirbyte CDKAirbyte GongAirbyte HubspotAirbyte JSONAirbyte SalesforceAirbyte ShopifyAirbyte StripeAirbyte TypeformAirbyte Zendesk SupportAirtableAlibaba Cloud MaxComputeApify DatasetArcGISArxivAssemblyAI Audio TranscriptsAsync ChromiumAsyncHtmlLoaderAWS S3 DirectoryAWS S3 FileAZLyricsAzure Blob Storage ContainerAzure Blob Storage FileAzure Document IntelligenceBibTeXBiliBiliBlackboardBlockchainBrave SearchBrowserlessChatGPT DataCollege ConfidentialConcurrent LoaderConfluenceCoNLL-UCopy PasteCSVCube Semantic LayerDatadog LogsDiffbotDiscordDocugamiDropboxDuckDBEmailEmbaasEPubEverNoteexample_dataMicrosoft ExcelFacebook ChatFaunaFigmaGeopandasGitGitBookGitHubGoogle BigQueryGoogle Cloud Storage DirectoryGoogle Cloud Storage FileGoogle DriveGrobidGutenbergHacker NewsHuawei OBS DirectoryHuawei OBS FileHuggingFace datasetiFixitImagesImage captionsIMSDbIuguJoplinJupyter NotebookLarkSuite (FeiShu)MastodonMediaWikiDumpMergeDocLoadermhtmlMicrosoft OneDriveMicrosoft PowerPointMicrosoft SharePointMicrosoft WordModern TreasuryNews URLNotion DB 1/2Notion DB 2/2Nuclia Understanding API document loaderObsidianOpen Document Format (ODT)Open City DataOrg-modePandas DataFrameAmazon TextractPolars DataFramePsychicPubMedPySpark DataFrame LoaderReadTheDocs DocumentationRecursive URL LoaderRedditRoamRocksetRSS FeedsRSTSitemapSlackSnowflakeSource CodeSpreedlyStripeSubtitleTelegramTencent COS DirectoryTencent COS FileTensorFlow Datasets2MarkdownTOMLTrelloTSVTwitterUnstructured FileURLWeatherWebBaseLoaderWhatsApp ChatWikipediaXMLXorbits Pandas DataFrameLoading documents from a YouTube urlYouTube transcripts Etherscan Loader acreom Airbyte CDK Airbyte Gong Airbyte Hubspot Airbyte JSON Airbyte Salesforce Airbyte Shopify Airbyte Stripe Airbyte Typeform Airbyte Zendesk Support Airtable Alibaba Cloud MaxCompute Apify Dataset ArcGIS Arxiv AssemblyAI Audio Transcripts Async Chromium AsyncHtmlLoader AWS S3 Directory AWS S3 File AZLyrics Azure Blob Storage Container Azure Blob Storage File Azure Document Intelligence BibTeX BiliBili Blackboard Blockchain Brave Search Browserless ChatGPT Data College Confidential Concurrent Loader Confluence CoNLL-U Copy Paste CSV Cube Semantic Layer Datadog Logs Diffbot Discord Docugami Dropbox DuckDB Email Embaas EPub EverNote example_data Microsoft Excel Facebook Chat Fauna Figma Geopandas Git GitBook GitHub Google BigQuery Google Cloud Storage Directory Google Cloud Storage File Google Drive Grobid Gutenberg Hacker News Huawei OBS Directory Huawei OBS File HuggingFace dataset iFixit Images Image captions IMSDb Iugu Joplin Jupyter Notebook LarkSuite (FeiShu) Mastodon MediaWikiDump MergeDocLoader mhtml Microsoft OneDrive Microsoft PowerPoint Microsoft SharePoint Microsoft Word Modern Treasury News URL Notion DB 1/2 Notion DB 2/2 Nuclia Understanding API document loader Obsidian Open Document Format (ODT) Open City Data Org-mode Pandas DataFrame Amazon Textract Polars DataFrame Psychic PubMed PySpark DataFrame Loader ReadTheDocs Documentation Recursive URL Loader Reddit Roam Rockset RSS Feeds RST Sitemap Slack Snowflake Source Code Spreedly Stripe Subtitle Telegram Tencent COS Directory Tencent COS File TensorFlow Datasets 2Markdown TOML Trello TSV Twitter Unstructured File URL Weather WebBaseLoader WhatsApp Chat Wikipedia XML Xorbits Pandas DataFrame Loading documents from a YouTube url YouTube transcripts Document transformers LLMs Memory Retrievers Text embedding models Agents & Toolkits Tools Vector stores Grouped by provider  Integrations Document loaders Rockset Go to the Rockset console and get an API key. Find your API region from the API reference. For the purpose of this notebook, we will assume you're using Rockset from Oregon(us-west-2). Set your the environment variable ROCKSET_API_KEY. Install the Rockset python client, which will be used by langchain to interact with the Rockset database. RocksetLoader RocksetLoader Setting up the environment Using multiple columns as content Discord Twitter Python JS/TS Homepage Blog Skip to main contentü¶úÔ∏èüîó LangChainDocsUse casesIntegrationsAPILangSmithJS/TS DocsCTRLKIntegrationsCallbacksChat modelsChat loadersDocument loadersEtherscan LoaderacreomAirbyte CDKAirbyte GongAirbyte HubspotAirbyte JSONAirbyte SalesforceAirbyte ShopifyAirbyte StripeAirbyte TypeformAirbyte Zendesk SupportAirtableAlibaba Cloud MaxComputeApify DatasetArcGISArxivAssemblyAI Audio TranscriptsAsync ChromiumAsyncHtmlLoaderAWS S3 DirectoryAWS S3 FileAZLyricsAzure Blob Storage ContainerAzure Blob Storage FileAzure Document IntelligenceBibTeXBiliBiliBlackboardBlockchainBrave SearchBrowserlessChatGPT DataCollege ConfidentialConcurrent LoaderConfluenceCoNLL-UCopy PasteCSVCube Semantic LayerDatadog LogsDiffbotDiscordDocugamiDropboxDuckDBEmailEmbaasEPubEverNoteexample_dataMicrosoft ExcelFacebook ChatFaunaFigmaGeopandasGitGitBookGitHubGoogle BigQueryGoogle Cloud Storage DirectoryGoogle Cloud Storage FileGoogle DriveGrobidGutenbergHacker NewsHuawei OBS DirectoryHuawei OBS FileHuggingFace datasetiFixitImagesImage captionsIMSDbIuguJoplinJupyter NotebookLarkSuite (FeiShu)MastodonMediaWikiDumpMergeDocLoadermhtmlMicrosoft OneDriveMicrosoft PowerPointMicrosoft SharePointMicrosoft WordModern TreasuryNews URLNotion DB 1/2Notion DB 2/2Nuclia Understanding API document loaderObsidianOpen Document Format (ODT)Open City DataOrg-modePandas DataFrameAmazon TextractPolars DataFramePsychicPubMedPySpark DataFrame LoaderReadTheDocs DocumentationRecursive URL LoaderRedditRoamRocksetRSS FeedsRSTSitemapSlackSnowflakeSource CodeSpreedlyStripeSubtitleTelegramTencent COS DirectoryTencent COS FileTensorFlow Datasets2MarkdownTOMLTrelloTSVTwitterUnstructured FileURLWeatherWebBaseLoaderWhatsApp ChatWikipediaXMLXorbits Pandas DataFrameLoading documents from a YouTube urlYouTube transcriptsDocument transformersLLMsMemoryRetrieversText embedding modelsAgents & ToolkitsToolsVector storesGrouped by providerIntegrationsDocument loadersRocksetOn this pageRocksetRockset is a real-time analytics database which enables queries on massive, semi-structured data without operational burden. With Rockset, ingested data is queryable within one second and analytical queries against that data typically execute in milliseconds. Rockset is compute optimized, making it suitable for serving high concurrency applications in the sub-100TB range (or larger than 100s of TBs with rollups).This notebook demonstrates how to use Rockset as a document loader in langchain. To get started, make sure you have a Rockset account and an API key available.Setting up the environment‚ÄãGo to the Rockset console and get an API key. Find your API region from the API reference. For the purpose of this notebook, we will assume you're using Rockset from Oregon(us-west-2).Set your the environment variable ROCKSET_API_KEY.Install the Rockset python client, which will be used by langchain to interact with the Rockset database.$ pip3 install rocksetLoading DocumentsThe Rockset integration with LangChain allows you to load documents from Rockset collections with SQL queries. In order to do this you must construct a RocksetLoader object. Here is an example snippet that initializes a RocksetLoader.from langchain.document_loaders import RocksetLoaderfrom rockset import RocksetClient, Regions, modelsloader = RocksetLoader(    RocksetClient(Regions.usw2a1, "<api key>"),    models.QueryRequestSql(query="SELECT * FROM langchain_demo LIMIT 3"),  # SQL query    ["text"],  # content columns    metadata_keys=["id", "date"],  # metadata columns)API Reference:RocksetLoaderHere, you can see that the following query is run:SELECT * FROM langchain_demo LIMIT 3The text column in the collection is used as the page content, and the record's id and date columns are used as metadata (if you do not pass anything into metadata_keys, the whole Rockset document will be used as metadata). To execute the query and access an iterator over the resulting Documents, run:loader.lazy_load()To execute the query and access all resulting Documents at once, run:loader.load()Here is an example response of loader.load():[    Document(        page_content="Lorem ipsum dolor sit amet, consectetur adipiscing elit. Maecenas a libero porta, dictum ipsum eget, hendrerit neque. Morbi blandit, ex ut suscipit viverra, enim velit tincidunt tellus, a tempor velit nunc et ex. Proin hendrerit odio nec convallis lobortis. Aenean in purus dolor. Vestibulum orci orci, laoreet eget magna in, commodo euismod justo.",         metadata={"id": 83209, "date": "2022-11-13T18:26:45.000000Z"}    ),    Document(        page_content="Integer at finibus odio. Nam sit amet enim cursus lacus gravida feugiat vestibulum sed libero. Aenean eleifend est quis elementum tincidunt. Curabitur sit amet ornare erat. Nulla id dolor ut magna volutpat sodales fringilla vel ipsum. Donec ultricies, lacus sed fermentum dignissim, lorem elit aliquam ligula, sed suscipit sapien purus nec ligula.",         metadata={"id": 89313, "date": "2022-11-13T18:28:53.000000Z"}    ),    Document(        page_content="Morbi tortor enim, commodo id efficitur vitae, fringilla nec mi. Nullam molestie faucibus aliquet. Praesent a est facilisis, condimentum justo sit amet, viverra erat. Fusce volutpat nisi vel purus blandit, et facilisis felis accumsan. Phasellus luctus ligula ultrices tellus tempor hendrerit. Donec at ultricies leo.",         metadata={"id": 87732, "date": "2022-11-13T18:49:04.000000Z"}    )]Using multiple columns as content‚ÄãYou can choose to use multiple columns as content:from langchain.document_loaders import RocksetLoaderfrom rockset import RocksetClient, Regions, modelsloader = RocksetLoader(    RocksetClient(Regions.usw2a1, "<api key>"),    models.QueryRequestSql(query="SELECT * FROM langchain_demo LIMIT 1 WHERE id=38"),    ["sentence1", "sentence2"],  # TWO content columns)API Reference:RocksetLoaderAssuming the "sentence1" field is "This is the first sentence." and the "sentence2" field is "This is the second sentence.", the page_content of the resulting Document would be:This is the first sentence.This is the second sentence.You can define you own function to join content columns by setting the content_columns_joiner argument in the RocksetLoader constructor. content_columns_joiner is a method that takes in a List[Tuple[str, Any]]] as an argument, representing a list of tuples of (column name, column value). By default, this is a method that joins each column value with a new line.For example, if you wanted to join sentence1 and sentence2 with a space instead of a new line, you could set content_columns_joiner like so:RocksetLoader(    RocksetClient(Regions.usw2a1, "<api key>"),    models.QueryRequestSql(query="SELECT * FROM langchain_demo LIMIT 1 WHERE id=38"),    ["sentence1", "sentence2"],    content_columns_joiner=lambda docs: " ".join(        [doc[1] for doc in docs]    ),  # join with space instead of /n)The page_content of the resulting Document would be:This is the first sentence. This is the second sentence.Oftentimes you want to include the column name in the page_content. You can do that like this:RocksetLoader(    RocksetClient(Regions.usw2a1, "<api key>"),    models.QueryRequestSql(query="SELECT * FROM langchain_demo LIMIT 1 WHERE id=38"),    ["sentence1", "sentence2"],    content_columns_joiner=lambda docs: "\n".join(        [f"{doc[0]}: {doc[1]}" for doc in docs]    ),)This would result in the following page_content:sentence1: This is the first sentence.sentence2: This is the second sentence.PreviousRoamNextRSS FeedsSetting up the environmentUsing multiple columns as contentCommunityDiscordTwitterGitHubPythonJS/TSMoreHomepageBlogCopyright ¬© 2023 LangChain, Inc. Skip to main content ü¶úÔ∏èüîó LangChainDocsUse casesIntegrationsAPILangSmithJS/TS DocsCTRLK ü¶úÔ∏èüîó LangChainDocsUse casesIntegrationsAPI LangSmithJS/TS DocsCTRLK  CTRLK CTRLK  CTRLK   IntegrationsCallbacksChat modelsChat loadersDocument loadersEtherscan LoaderacreomAirbyte CDKAirbyte GongAirbyte HubspotAirbyte JSONAirbyte SalesforceAirbyte ShopifyAirbyte StripeAirbyte TypeformAirbyte Zendesk SupportAirtableAlibaba Cloud MaxComputeApify DatasetArcGISArxivAssemblyAI Audio TranscriptsAsync ChromiumAsyncHtmlLoaderAWS S3 DirectoryAWS S3 FileAZLyricsAzure Blob Storage ContainerAzure Blob Storage FileAzure Document IntelligenceBibTeXBiliBiliBlackboardBlockchainBrave SearchBrowserlessChatGPT DataCollege ConfidentialConcurrent LoaderConfluenceCoNLL-UCopy PasteCSVCube Semantic LayerDatadog LogsDiffbotDiscordDocugamiDropboxDuckDBEmailEmbaasEPubEverNoteexample_dataMicrosoft ExcelFacebook ChatFaunaFigmaGeopandasGitGitBookGitHubGoogle BigQueryGoogle Cloud Storage DirectoryGoogle Cloud Storage FileGoogle DriveGrobidGutenbergHacker NewsHuawei OBS DirectoryHuawei OBS FileHuggingFace datasetiFixitImagesImage captionsIMSDbIuguJoplinJupyter NotebookLarkSuite (FeiShu)MastodonMediaWikiDumpMergeDocLoadermhtmlMicrosoft OneDriveMicrosoft PowerPointMicrosoft SharePointMicrosoft WordModern TreasuryNews URLNotion DB 1/2Notion DB 2/2Nuclia Understanding API document loaderObsidianOpen Document Format (ODT)Open City DataOrg-modePandas DataFrameAmazon TextractPolars DataFramePsychicPubMedPySpark DataFrame LoaderReadTheDocs DocumentationRecursive URL LoaderRedditRoamRocksetRSS FeedsRSTSitemapSlackSnowflakeSource CodeSpreedlyStripeSubtitleTelegramTencent COS DirectoryTencent COS FileTensorFlow Datasets2MarkdownTOMLTrelloTSVTwitterUnstructured FileURLWeatherWebBaseLoaderWhatsApp ChatWikipediaXMLXorbits Pandas DataFrameLoading documents from a YouTube urlYouTube transcriptsDocument transformersLLMsMemoryRetrieversText embedding modelsAgents & ToolkitsToolsVector storesGrouped by providerIntegrationsDocument loadersRocksetOn this pageRocksetRockset is a real-time analytics database which enables queries on massive, semi-structured data without operational burden. With Rockset, ingested data is queryable within one second and analytical queries against that data typically execute in milliseconds. Rockset is compute optimized, making it suitable for serving high concurrency applications in the sub-100TB range (or larger than 100s of TBs with rollups).This notebook demonstrates how to use Rockset as a document loader in langchain. To get started, make sure you have a Rockset account and an API key available.Setting up the environment‚ÄãGo to the Rockset console and get an API key. Find your API region from the API reference. For the purpose of this notebook, we will assume you're using Rockset from Oregon(us-west-2).Set your the environment variable ROCKSET_API_KEY.Install the Rockset python client, which will be used by langchain to interact with the Rockset database.$ pip3 install rocksetLoading DocumentsThe Rockset integration with LangChain allows you to load documents from Rockset collections with SQL queries. In order to do this you must construct a RocksetLoader object. Here is an example snippet that initializes a RocksetLoader.from langchain.document_loaders import RocksetLoaderfrom rockset import RocksetClient, Regions, modelsloader = RocksetLoader(    RocksetClient(Regions.usw2a1, "<api key>"),    models.QueryRequestSql(query="SELECT * FROM langchain_demo LIMIT 3"),  # SQL query    ["text"],  # content columns    metadata_keys=["id", "date"],  # metadata columns)API Reference:RocksetLoaderHere, you can see that the following query is run:SELECT * FROM langchain_demo LIMIT 3The text column in the collection is used as the page content, and the record's id and date columns are used as metadata (if you do not pass anything into metadata_keys, the whole Rockset document will be used as metadata). To execute the query and access an iterator over the resulting Documents, run:loader.lazy_load()To execute the query and access all resulting Documents at once, run:loader.load()Here is an example response of loader.load():[    Document(        page_content="Lorem ipsum dolor sit amet, consectetur adipiscing elit. Maecenas a libero porta, dictum ipsum eget, hendrerit neque. Morbi blandit, ex ut suscipit viverra, enim velit tincidunt tellus, a tempor velit nunc et ex. Proin hendrerit odio nec convallis lobortis. Aenean in purus dolor. Vestibulum orci orci, laoreet eget magna in, commodo euismod justo.",         metadata={"id": 83209, "date": "2022-11-13T18:26:45.000000Z"}    ),    Document(        page_content="Integer at finibus odio. Nam sit amet enim cursus lacus gravida feugiat vestibulum sed libero. Aenean eleifend est quis elementum tincidunt. Curabitur sit amet ornare erat. Nulla id dolor ut magna volutpat sodales fringilla vel ipsum. Donec ultricies, lacus sed fermentum dignissim, lorem elit aliquam ligula, sed suscipit sapien purus nec ligula.",         metadata={"id": 89313, "date": "2022-11-13T18:28:53.000000Z"}    ),    Document(        page_content="Morbi tortor enim, commodo id efficitur vitae, fringilla nec mi. Nullam molestie faucibus aliquet. Praesent a est facilisis, condimentum justo sit amet, viverra erat. Fusce volutpat nisi vel purus blandit, et facilisis felis accumsan. Phasellus luctus ligula ultrices tellus tempor hendrerit. Donec at ultricies leo.",         metadata={"id": 87732, "date": "2022-11-13T18:49:04.000000Z"}    )]Using multiple columns as content‚ÄãYou can choose to use multiple columns as content:from langchain.document_loaders import RocksetLoaderfrom rockset import RocksetClient, Regions, modelsloader = RocksetLoader(    RocksetClient(Regions.usw2a1, "<api key>"),    models.QueryRequestSql(query="SELECT * FROM langchain_demo LIMIT 1 WHERE id=38"),    ["sentence1", "sentence2"],  # TWO content columns)API Reference:RocksetLoaderAssuming the "sentence1" field is "This is the first sentence." and the "sentence2" field is "This is the second sentence.", the page_content of the resulting Document would be:This is the first sentence.This is the second sentence.You can define you own function to join content columns by setting the content_columns_joiner argument in the RocksetLoader constructor. content_columns_joiner is a method that takes in a List[Tuple[str, Any]]] as an argument, representing a list of tuples of (column name, column value). By default, this is a method that joins each column value with a new line.For example, if you wanted to join sentence1 and sentence2 with a space instead of a new line, you could set content_columns_joiner like so:RocksetLoader(    RocksetClient(Regions.usw2a1, "<api key>"),    models.QueryRequestSql(query="SELECT * FROM langchain_demo LIMIT 1 WHERE id=38"),    ["sentence1", "sentence2"],    content_columns_joiner=lambda docs: " ".join(        [doc[1] for doc in docs]    ),  # join with space instead of /n)The page_content of the resulting Document would be:This is the first sentence. This is the second sentence.Oftentimes you want to include the column name in the page_content. You can do that like this:RocksetLoader(    RocksetClient(Regions.usw2a1, "<api key>"),    models.QueryRequestSql(query="SELECT * FROM langchain_demo LIMIT 1 WHERE id=38"),    ["sentence1", "sentence2"],    content_columns_joiner=lambda docs: "\n".join(        [f"{doc[0]}: {doc[1]}" for doc in docs]    ),)This would result in the following page_content:sentence1: This is the first sentence.sentence2: This is the second sentence.PreviousRoamNextRSS FeedsSetting up the environmentUsing multiple columns as content IntegrationsCallbacksChat modelsChat loadersDocument loadersEtherscan LoaderacreomAirbyte CDKAirbyte GongAirbyte HubspotAirbyte JSONAirbyte SalesforceAirbyte ShopifyAirbyte StripeAirbyte TypeformAirbyte Zendesk SupportAirtableAlibaba Cloud MaxComputeApify DatasetArcGISArxivAssemblyAI Audio TranscriptsAsync ChromiumAsyncHtmlLoaderAWS S3 DirectoryAWS S3 FileAZLyricsAzure Blob Storage ContainerAzure Blob Storage FileAzure Document IntelligenceBibTeXBiliBiliBlackboardBlockchainBrave SearchBrowserlessChatGPT DataCollege ConfidentialConcurrent LoaderConfluenceCoNLL-UCopy PasteCSVCube Semantic LayerDatadog LogsDiffbotDiscordDocugamiDropboxDuckDBEmailEmbaasEPubEverNoteexample_dataMicrosoft ExcelFacebook ChatFaunaFigmaGeopandasGitGitBookGitHubGoogle BigQueryGoogle Cloud Storage DirectoryGoogle Cloud Storage FileGoogle DriveGrobidGutenbergHacker NewsHuawei OBS DirectoryHuawei OBS FileHuggingFace datasetiFixitImagesImage captionsIMSDbIuguJoplinJupyter NotebookLarkSuite (FeiShu)MastodonMediaWikiDumpMergeDocLoadermhtmlMicrosoft OneDriveMicrosoft PowerPointMicrosoft SharePointMicrosoft WordModern TreasuryNews URLNotion DB 1/2Notion DB 2/2Nuclia Understanding API document loaderObsidianOpen Document Format (ODT)Open City DataOrg-modePandas DataFrameAmazon TextractPolars DataFramePsychicPubMedPySpark DataFrame LoaderReadTheDocs DocumentationRecursive URL LoaderRedditRoamRocksetRSS FeedsRSTSitemapSlackSnowflakeSource CodeSpreedlyStripeSubtitleTelegramTencent COS DirectoryTencent COS FileTensorFlow Datasets2MarkdownTOMLTrelloTSVTwitterUnstructured FileURLWeatherWebBaseLoaderWhatsApp ChatWikipediaXMLXorbits Pandas DataFrameLoading documents from a YouTube urlYouTube transcriptsDocument transformersLLMsMemoryRetrieversText embedding modelsAgents & ToolkitsToolsVector storesGrouped by providerIntegrationsDocument loadersRocksetOn this pageRocksetRockset is a real-time analytics database which enables queries on massive, semi-structured data without operational burden. With Rockset, ingested data is queryable within one second and analytical queries against that data typically execute in milliseconds. Rockset is compute optimized, making it suitable for serving high concurrency applications in the sub-100TB range (or larger than 100s of TBs with rollups).This notebook demonstrates how to use Rockset as a document loader in langchain. To get started, make sure you have a Rockset account and an API key available.Setting up the environment‚ÄãGo to the Rockset console and get an API key. Find your API region from the API reference. For the purpose of this notebook, we will assume you're using Rockset from Oregon(us-west-2).Set your the environment variable ROCKSET_API_KEY.Install the Rockset python client, which will be used by langchain to interact with the Rockset database.$ pip3 install rocksetLoading DocumentsThe Rockset integration with LangChain allows you to load documents from Rockset collections with SQL queries. In order to do this you must construct a RocksetLoader object. Here is an example snippet that initializes a RocksetLoader.from langchain.document_loaders import RocksetLoaderfrom rockset import RocksetClient, Regions, modelsloader = RocksetLoader(    RocksetClient(Regions.usw2a1, "<api key>"),    models.QueryRequestSql(query="SELECT * FROM langchain_demo LIMIT 3"),  # SQL query    ["text"],  # content columns    metadata_keys=["id", "date"],  # metadata columns)API Reference:RocksetLoaderHere, you can see that the following query is run:SELECT * FROM langchain_demo LIMIT 3The text column in the collection is used as the page content, and the record's id and date columns are used as metadata (if you do not pass anything into metadata_keys, the whole Rockset document will be used as metadata). To execute the query and access an iterator over the resulting Documents, run:loader.lazy_load()To execute the query and access all resulting Documents at once, run:loader.load()Here is an example response of loader.load():[    Document(        page_content="Lorem ipsum dolor sit amet, consectetur adipiscing elit. Maecenas a libero porta, dictum ipsum eget, hendrerit neque. Morbi blandit, ex ut suscipit viverra, enim velit tincidunt tellus, a tempor velit nunc et ex. Proin hendrerit odio nec convallis lobortis. Aenean in purus dolor. Vestibulum orci orci, laoreet eget magna in, commodo euismod justo.",         metadata={"id": 83209, "date": "2022-11-13T18:26:45.000000Z"}    ),    Document(        page_content="Integer at finibus odio. Nam sit amet enim cursus lacus gravida feugiat vestibulum sed libero. Aenean eleifend est quis elementum tincidunt. Curabitur sit amet ornare erat. Nulla id dolor ut magna volutpat sodales fringilla vel ipsum. Donec ultricies, lacus sed fermentum dignissim, lorem elit aliquam ligula, sed suscipit sapien purus nec ligula.",         metadata={"id": 89313, "date": "2022-11-13T18:28:53.000000Z"}    ),    Document(        page_content="Morbi tortor enim, commodo id efficitur vitae, fringilla nec mi. Nullam molestie faucibus aliquet. Praesent a est facilisis, condimentum justo sit amet, viverra erat. Fusce volutpat nisi vel purus blandit, et facilisis felis accumsan. Phasellus luctus ligula ultrices tellus tempor hendrerit. Donec at ultricies leo.",         metadata={"id": 87732, "date": "2022-11-13T18:49:04.000000Z"}    )]Using multiple columns as content‚ÄãYou can choose to use multiple columns as content:from langchain.document_loaders import RocksetLoaderfrom rockset import RocksetClient, Regions, modelsloader = RocksetLoader(    RocksetClient(Regions.usw2a1, "<api key>"),    models.QueryRequestSql(query="SELECT * FROM langchain_demo LIMIT 1 WHERE id=38"),    ["sentence1", "sentence2"],  # TWO content columns)API Reference:RocksetLoaderAssuming the "sentence1" field is "This is the first sentence." and the "sentence2" field is "This is the second sentence.", the page_content of the resulting Document would be:This is the first sentence.This is the second sentence.You can define you own function to join content columns by setting the content_columns_joiner argument in the RocksetLoader constructor. content_columns_joiner is a method that takes in a List[Tuple[str, Any]]] as an argument, representing a list of tuples of (column name, column value). By default, this is a method that joins each column value with a new line.For example, if you wanted to join sentence1 and sentence2 with a space instead of a new line, you could set content_columns_joiner like so:RocksetLoader(    RocksetClient(Regions.usw2a1, "<api key>"),    models.QueryRequestSql(query="SELECT * FROM langchain_demo LIMIT 1 WHERE id=38"),    ["sentence1", "sentence2"],    content_columns_joiner=lambda docs: " ".join(        [doc[1] for doc in docs]    ),  # join with space instead of /n)The page_content of the resulting Document would be:This is the first sentence. This is the second sentence.Oftentimes you want to include the column name in the page_content. You can do that like this:RocksetLoader(    RocksetClient(Regions.usw2a1, "<api key>"),    models.QueryRequestSql(query="SELECT * FROM langchain_demo LIMIT 1 WHERE id=38"),    ["sentence1", "sentence2"],    content_columns_joiner=lambda docs: "\n".join(        [f"{doc[0]}: {doc[1]}" for doc in docs]    ),)This would result in the following page_content:sentence1: This is the first sentence.sentence2: This is the second sentence.PreviousRoamNextRSS FeedsSetting up the environmentUsing multiple columns as content IntegrationsCallbacksChat modelsChat loadersDocument loadersEtherscan LoaderacreomAirbyte CDKAirbyte GongAirbyte HubspotAirbyte JSONAirbyte SalesforceAirbyte ShopifyAirbyte StripeAirbyte TypeformAirbyte Zendesk SupportAirtableAlibaba Cloud MaxComputeApify DatasetArcGISArxivAssemblyAI Audio TranscriptsAsync ChromiumAsyncHtmlLoaderAWS S3 DirectoryAWS S3 FileAZLyricsAzure Blob Storage ContainerAzure Blob Storage FileAzure Document IntelligenceBibTeXBiliBiliBlackboardBlockchainBrave SearchBrowserlessChatGPT DataCollege ConfidentialConcurrent LoaderConfluenceCoNLL-UCopy PasteCSVCube Semantic LayerDatadog LogsDiffbotDiscordDocugamiDropboxDuckDBEmailEmbaasEPubEverNoteexample_dataMicrosoft ExcelFacebook ChatFaunaFigmaGeopandasGitGitBookGitHubGoogle BigQueryGoogle Cloud Storage DirectoryGoogle Cloud Storage FileGoogle DriveGrobidGutenbergHacker NewsHuawei OBS DirectoryHuawei OBS FileHuggingFace datasetiFixitImagesImage captionsIMSDbIuguJoplinJupyter NotebookLarkSuite (FeiShu)MastodonMediaWikiDumpMergeDocLoadermhtmlMicrosoft OneDriveMicrosoft PowerPointMicrosoft SharePointMicrosoft WordModern TreasuryNews URLNotion DB 1/2Notion DB 2/2Nuclia Understanding API document loaderObsidianOpen Document Format (ODT)Open City DataOrg-modePandas DataFrameAmazon TextractPolars DataFramePsychicPubMedPySpark DataFrame LoaderReadTheDocs DocumentationRecursive URL LoaderRedditRoamRocksetRSS FeedsRSTSitemapSlackSnowflakeSource CodeSpreedlyStripeSubtitleTelegramTencent COS DirectoryTencent COS FileTensorFlow Datasets2MarkdownTOMLTrelloTSVTwitterUnstructured FileURLWeatherWebBaseLoaderWhatsApp ChatWikipediaXMLXorbits Pandas DataFrameLoading documents from a YouTube urlYouTube transcriptsDocument transformersLLMsMemoryRetrieversText embedding modelsAgents & ToolkitsToolsVector storesGrouped by provider IntegrationsCallbacksChat modelsChat loadersDocument loadersEtherscan LoaderacreomAirbyte CDKAirbyte GongAirbyte HubspotAirbyte JSONAirbyte SalesforceAirbyte ShopifyAirbyte StripeAirbyte TypeformAirbyte Zendesk SupportAirtableAlibaba Cloud MaxComputeApify DatasetArcGISArxivAssemblyAI Audio TranscriptsAsync ChromiumAsyncHtmlLoaderAWS S3 DirectoryAWS S3 FileAZLyricsAzure Blob Storage ContainerAzure Blob Storage FileAzure Document IntelligenceBibTeXBiliBiliBlackboardBlockchainBrave SearchBrowserlessChatGPT DataCollege ConfidentialConcurrent LoaderConfluenceCoNLL-UCopy PasteCSVCube Semantic LayerDatadog LogsDiffbotDiscordDocugamiDropboxDuckDBEmailEmbaasEPubEverNoteexample_dataMicrosoft ExcelFacebook ChatFaunaFigmaGeopandasGitGitBookGitHubGoogle BigQueryGoogle Cloud Storage DirectoryGoogle Cloud Storage FileGoogle DriveGrobidGutenbergHacker NewsHuawei OBS DirectoryHuawei OBS FileHuggingFace datasetiFixitImagesImage captionsIMSDbIuguJoplinJupyter NotebookLarkSuite (FeiShu)MastodonMediaWikiDumpMergeDocLoadermhtmlMicrosoft OneDriveMicrosoft PowerPointMicrosoft SharePointMicrosoft WordModern TreasuryNews URLNotion DB 1/2Notion DB 2/2Nuclia Understanding API document loaderObsidianOpen Document Format (ODT)Open City DataOrg-modePandas DataFrameAmazon TextractPolars DataFramePsychicPubMedPySpark DataFrame LoaderReadTheDocs DocumentationRecursive URL LoaderRedditRoamRocksetRSS FeedsRSTSitemapSlackSnowflakeSource CodeSpreedlyStripeSubtitleTelegramTencent COS DirectoryTencent COS FileTensorFlow Datasets2MarkdownTOMLTrelloTSVTwitterUnstructured FileURLWeatherWebBaseLoaderWhatsApp ChatWikipediaXMLXorbits Pandas DataFrameLoading documents from a YouTube urlYouTube transcriptsDocument transformersLLMsMemoryRetrieversText embedding modelsAgents & ToolkitsToolsVector storesGrouped by provider Integrations Callbacks Chat models Chat loaders Document loaders example_data Document transformers LLMs Memory Retrievers Text embedding models Agents & Toolkits Tools Vector stores Grouped by provider IntegrationsDocument loadersRocksetOn this pageRocksetRockset is a real-time analytics database which enables queries on massive, semi-structured data without operational burden. With Rockset, ingested data is queryable within one second and analytical queries against that data typically execute in milliseconds. Rockset is compute optimized, making it suitable for serving high concurrency applications in the sub-100TB range (or larger than 100s of TBs with rollups).This notebook demonstrates how to use Rockset as a document loader in langchain. To get started, make sure you have a Rockset account and an API key available.Setting up the environment‚ÄãGo to the Rockset console and get an API key. Find your API region from the API reference. For the purpose of this notebook, we will assume you're using Rockset from Oregon(us-west-2).Set your the environment variable ROCKSET_API_KEY.Install the Rockset python client, which will be used by langchain to interact with the Rockset database.$ pip3 install rocksetLoading DocumentsThe Rockset integration with LangChain allows you to load documents from Rockset collections with SQL queries. In order to do this you must construct a RocksetLoader object. Here is an example snippet that initializes a RocksetLoader.from langchain.document_loaders import RocksetLoaderfrom rockset import RocksetClient, Regions, modelsloader = RocksetLoader(    RocksetClient(Regions.usw2a1, "<api key>"),    models.QueryRequestSql(query="SELECT * FROM langchain_demo LIMIT 3"),  # SQL query    ["text"],  # content columns    metadata_keys=["id", "date"],  # metadata columns)API Reference:RocksetLoaderHere, you can see that the following query is run:SELECT * FROM langchain_demo LIMIT 3The text column in the collection is used as the page content, and the record's id and date columns are used as metadata (if you do not pass anything into metadata_keys, the whole Rockset document will be used as metadata). To execute the query and access an iterator over the resulting Documents, run:loader.lazy_load()To execute the query and access all resulting Documents at once, run:loader.load()Here is an example response of loader.load():[    Document(        page_content="Lorem ipsum dolor sit amet, consectetur adipiscing elit. Maecenas a libero porta, dictum ipsum eget, hendrerit neque. Morbi blandit, ex ut suscipit viverra, enim velit tincidunt tellus, a tempor velit nunc et ex. Proin hendrerit odio nec convallis lobortis. Aenean in purus dolor. Vestibulum orci orci, laoreet eget magna in, commodo euismod justo.",         metadata={"id": 83209, "date": "2022-11-13T18:26:45.000000Z"}    ),    Document(        page_content="Integer at finibus odio. Nam sit amet enim cursus lacus gravida feugiat vestibulum sed libero. Aenean eleifend est quis elementum tincidunt. Curabitur sit amet ornare erat. Nulla id dolor ut magna volutpat sodales fringilla vel ipsum. Donec ultricies, lacus sed fermentum dignissim, lorem elit aliquam ligula, sed suscipit sapien purus nec ligula.",         metadata={"id": 89313, "date": "2022-11-13T18:28:53.000000Z"}    ),    Document(        page_content="Morbi tortor enim, commodo id efficitur vitae, fringilla nec mi. Nullam molestie faucibus aliquet. Praesent a est facilisis, condimentum justo sit amet, viverra erat. Fusce volutpat nisi vel purus blandit, et facilisis felis accumsan. Phasellus luctus ligula ultrices tellus tempor hendrerit. Donec at ultricies leo.",         metadata={"id": 87732, "date": "2022-11-13T18:49:04.000000Z"}    )]Using multiple columns as content‚ÄãYou can choose to use multiple columns as content:from langchain.document_loaders import RocksetLoaderfrom rockset import RocksetClient, Regions, modelsloader = RocksetLoader(    RocksetClient(Regions.usw2a1, "<api key>"),    models.QueryRequestSql(query="SELECT * FROM langchain_demo LIMIT 1 WHERE id=38"),    ["sentence1", "sentence2"],  # TWO content columns)API Reference:RocksetLoaderAssuming the "sentence1" field is "This is the first sentence." and the "sentence2" field is "This is the second sentence.", the page_content of the resulting Document would be:This is the first sentence.This is the second sentence.You can define you own function to join content columns by setting the content_columns_joiner argument in the RocksetLoader constructor. content_columns_joiner is a method that takes in a List[Tuple[str, Any]]] as an argument, representing a list of tuples of (column name, column value). By default, this is a method that joins each column value with a new line.For example, if you wanted to join sentence1 and sentence2 with a space instead of a new line, you could set content_columns_joiner like so:RocksetLoader(    RocksetClient(Regions.usw2a1, "<api key>"),    models.QueryRequestSql(query="SELECT * FROM langchain_demo LIMIT 1 WHERE id=38"),    ["sentence1", "sentence2"],    content_columns_joiner=lambda docs: " ".join(        [doc[1] for doc in docs]    ),  # join with space instead of /n)The page_content of the resulting Document would be:This is the first sentence. This is the second sentence.Oftentimes you want to include the column name in the page_content. You can do that like this:RocksetLoader(    RocksetClient(Regions.usw2a1, "<api key>"),    models.QueryRequestSql(query="SELECT * FROM langchain_demo LIMIT 1 WHERE id=38"),    ["sentence1", "sentence2"],    content_columns_joiner=lambda docs: "\n".join(        [f"{doc[0]}: {doc[1]}" for doc in docs]    ),)This would result in the following page_content:sentence1: This is the first sentence.sentence2: This is the second sentence.PreviousRoamNextRSS FeedsSetting up the environmentUsing multiple columns as content IntegrationsDocument loadersRocksetOn this pageRocksetRockset is a real-time analytics database which enables queries on massive, semi-structured data without operational burden. With Rockset, ingested data is queryable within one second and analytical queries against that data typically execute in milliseconds. Rockset is compute optimized, making it suitable for serving high concurrency applications in the sub-100TB range (or larger than 100s of TBs with rollups).This notebook demonstrates how to use Rockset as a document loader in langchain. To get started, make sure you have a Rockset account and an API key available.Setting up the environment‚ÄãGo to the Rockset console and get an API key. Find your API region from the API reference. For the purpose of this notebook, we will assume you're using Rockset from Oregon(us-west-2).Set your the environment variable ROCKSET_API_KEY.Install the Rockset python client, which will be used by langchain to interact with the Rockset database.$ pip3 install rocksetLoading DocumentsThe Rockset integration with LangChain allows you to load documents from Rockset collections with SQL queries. In order to do this you must construct a RocksetLoader object. Here is an example snippet that initializes a RocksetLoader.from langchain.document_loaders import RocksetLoaderfrom rockset import RocksetClient, Regions, modelsloader = RocksetLoader(    RocksetClient(Regions.usw2a1, "<api key>"),    models.QueryRequestSql(query="SELECT * FROM langchain_demo LIMIT 3"),  # SQL query    ["text"],  # content columns    metadata_keys=["id", "date"],  # metadata columns)API Reference:RocksetLoaderHere, you can see that the following query is run:SELECT * FROM langchain_demo LIMIT 3The text column in the collection is used as the page content, and the record's id and date columns are used as metadata (if you do not pass anything into metadata_keys, the whole Rockset document will be used as metadata). To execute the query and access an iterator over the resulting Documents, run:loader.lazy_load()To execute the query and access all resulting Documents at once, run:loader.load()Here is an example response of loader.load():[    Document(        page_content="Lorem ipsum dolor sit amet, consectetur adipiscing elit. Maecenas a libero porta, dictum ipsum eget, hendrerit neque. Morbi blandit, ex ut suscipit viverra, enim velit tincidunt tellus, a tempor velit nunc et ex. Proin hendrerit odio nec convallis lobortis. Aenean in purus dolor. Vestibulum orci orci, laoreet eget magna in, commodo euismod justo.",         metadata={"id": 83209, "date": "2022-11-13T18:26:45.000000Z"}    ),    Document(        page_content="Integer at finibus odio. Nam sit amet enim cursus lacus gravida feugiat vestibulum sed libero. Aenean eleifend est quis elementum tincidunt. Curabitur sit amet ornare erat. Nulla id dolor ut magna volutpat sodales fringilla vel ipsum. Donec ultricies, lacus sed fermentum dignissim, lorem elit aliquam ligula, sed suscipit sapien purus nec ligula.",         metadata={"id": 89313, "date": "2022-11-13T18:28:53.000000Z"}    ),    Document(        page_content="Morbi tortor enim, commodo id efficitur vitae, fringilla nec mi. Nullam molestie faucibus aliquet. Praesent a est facilisis, condimentum justo sit amet, viverra erat. Fusce volutpat nisi vel purus blandit, et facilisis felis accumsan. Phasellus luctus ligula ultrices tellus tempor hendrerit. Donec at ultricies leo.",         metadata={"id": 87732, "date": "2022-11-13T18:49:04.000000Z"}    )]Using multiple columns as content‚ÄãYou can choose to use multiple columns as content:from langchain.document_loaders import RocksetLoaderfrom rockset import RocksetClient, Regions, modelsloader = RocksetLoader(    RocksetClient(Regions.usw2a1, "<api key>"),    models.QueryRequestSql(query="SELECT * FROM langchain_demo LIMIT 1 WHERE id=38"),    ["sentence1", "sentence2"],  # TWO content columns)API Reference:RocksetLoaderAssuming the "sentence1" field is "This is the first sentence." and the "sentence2" field is "This is the second sentence.", the page_content of the resulting Document would be:This is the first sentence.This is the second sentence.You can define you own function to join content columns by setting the content_columns_joiner argument in the RocksetLoader constructor. content_columns_joiner is a method that takes in a List[Tuple[str, Any]]] as an argument, representing a list of tuples of (column name, column value). By default, this is a method that joins each column value with a new line.For example, if you wanted to join sentence1 and sentence2 with a space instead of a new line, you could set content_columns_joiner like so:RocksetLoader(    RocksetClient(Regions.usw2a1, "<api key>"),    models.QueryRequestSql(query="SELECT * FROM langchain_demo LIMIT 1 WHERE id=38"),    ["sentence1", "sentence2"],    content_columns_joiner=lambda docs: " ".join(        [doc[1] for doc in docs]    ),  # join with space instead of /n)The page_content of the resulting Document would be:This is the first sentence. This is the second sentence.Oftentimes you want to include the column name in the page_content. You can do that like this:RocksetLoader(    RocksetClient(Regions.usw2a1, "<api key>"),    models.QueryRequestSql(query="SELECT * FROM langchain_demo LIMIT 1 WHERE id=38"),    ["sentence1", "sentence2"],    content_columns_joiner=lambda docs: "\n".join(        [f"{doc[0]}: {doc[1]}" for doc in docs]    ),)This would result in the following page_content:sentence1: This is the first sentence.sentence2: This is the second sentence.PreviousRoamNextRSS FeedsSetting up the environmentUsing multiple columns as content IntegrationsDocument loadersRocksetOn this pageRocksetRockset is a real-time analytics database which enables queries on massive, semi-structured data without operational burden. With Rockset, ingested data is queryable within one second and analytical queries against that data typically execute in milliseconds. Rockset is compute optimized, making it suitable for serving high concurrency applications in the sub-100TB range (or larger than 100s of TBs with rollups).This notebook demonstrates how to use Rockset as a document loader in langchain. To get started, make sure you have a Rockset account and an API key available.Setting up the environment‚ÄãGo to the Rockset console and get an API key. Find your API region from the API reference. For the purpose of this notebook, we will assume you're using Rockset from Oregon(us-west-2).Set your the environment variable ROCKSET_API_KEY.Install the Rockset python client, which will be used by langchain to interact with the Rockset database.$ pip3 install rocksetLoading DocumentsThe Rockset integration with LangChain allows you to load documents from Rockset collections with SQL queries. In order to do this you must construct a RocksetLoader object. Here is an example snippet that initializes a RocksetLoader.from langchain.document_loaders import RocksetLoaderfrom rockset import RocksetClient, Regions, modelsloader = RocksetLoader(    RocksetClient(Regions.usw2a1, "<api key>"),    models.QueryRequestSql(query="SELECT * FROM langchain_demo LIMIT 3"),  # SQL query    ["text"],  # content columns    metadata_keys=["id", "date"],  # metadata columns)API Reference:RocksetLoaderHere, you can see that the following query is run:SELECT * FROM langchain_demo LIMIT 3The text column in the collection is used as the page content, and the record's id and date columns are used as metadata (if you do not pass anything into metadata_keys, the whole Rockset document will be used as metadata). To execute the query and access an iterator over the resulting Documents, run:loader.lazy_load()To execute the query and access all resulting Documents at once, run:loader.load()Here is an example response of loader.load():[    Document(        page_content="Lorem ipsum dolor sit amet, consectetur adipiscing elit. Maecenas a libero porta, dictum ipsum eget, hendrerit neque. Morbi blandit, ex ut suscipit viverra, enim velit tincidunt tellus, a tempor velit nunc et ex. Proin hendrerit odio nec convallis lobortis. Aenean in purus dolor. Vestibulum orci orci, laoreet eget magna in, commodo euismod justo.",         metadata={"id": 83209, "date": "2022-11-13T18:26:45.000000Z"}    ),    Document(        page_content="Integer at finibus odio. Nam sit amet enim cursus lacus gravida feugiat vestibulum sed libero. Aenean eleifend est quis elementum tincidunt. Curabitur sit amet ornare erat. Nulla id dolor ut magna volutpat sodales fringilla vel ipsum. Donec ultricies, lacus sed fermentum dignissim, lorem elit aliquam ligula, sed suscipit sapien purus nec ligula.",         metadata={"id": 89313, "date": "2022-11-13T18:28:53.000000Z"}    ),    Document(        page_content="Morbi tortor enim, commodo id efficitur vitae, fringilla nec mi. Nullam molestie faucibus aliquet. Praesent a est facilisis, condimentum justo sit amet, viverra erat. Fusce volutpat nisi vel purus blandit, et facilisis felis accumsan. Phasellus luctus ligula ultrices tellus tempor hendrerit. Donec at ultricies leo.",         metadata={"id": 87732, "date": "2022-11-13T18:49:04.000000Z"}    )]Using multiple columns as content‚ÄãYou can choose to use multiple columns as content:from langchain.document_loaders import RocksetLoaderfrom rockset import RocksetClient, Regions, modelsloader = RocksetLoader(    RocksetClient(Regions.usw2a1, "<api key>"),    models.QueryRequestSql(query="SELECT * FROM langchain_demo LIMIT 1 WHERE id=38"),    ["sentence1", "sentence2"],  # TWO content columns)API Reference:RocksetLoaderAssuming the "sentence1" field is "This is the first sentence." and the "sentence2" field is "This is the second sentence.", the page_content of the resulting Document would be:This is the first sentence.This is the second sentence.You can define you own function to join content columns by setting the content_columns_joiner argument in the RocksetLoader constructor. content_columns_joiner is a method that takes in a List[Tuple[str, Any]]] as an argument, representing a list of tuples of (column name, column value). By default, this is a method that joins each column value with a new line.For example, if you wanted to join sentence1 and sentence2 with a space instead of a new line, you could set content_columns_joiner like so:RocksetLoader(    RocksetClient(Regions.usw2a1, "<api key>"),    models.QueryRequestSql(query="SELECT * FROM langchain_demo LIMIT 1 WHERE id=38"),    ["sentence1", "sentence2"],    content_columns_joiner=lambda docs: " ".join(        [doc[1] for doc in docs]    ),  # join with space instead of /n)The page_content of the resulting Document would be:This is the first sentence. This is the second sentence.Oftentimes you want to include the column name in the page_content. You can do that like this:RocksetLoader(    RocksetClient(Regions.usw2a1, "<api key>"),    models.QueryRequestSql(query="SELECT * FROM langchain_demo LIMIT 1 WHERE id=38"),    ["sentence1", "sentence2"],    content_columns_joiner=lambda docs: "\n".join(        [f"{doc[0]}: {doc[1]}" for doc in docs]    ),)This would result in the following page_content:sentence1: This is the first sentence.sentence2: This is the second sentence.PreviousRoamNextRSS Feeds IntegrationsDocument loadersRocksetOn this pageRocksetRockset is a real-time analytics database which enables queries on massive, semi-structured data without operational burden. With Rockset, ingested data is queryable within one second and analytical queries against that data typically execute in milliseconds. Rockset is compute optimized, making it suitable for serving high concurrency applications in the sub-100TB range (or larger than 100s of TBs with rollups).This notebook demonstrates how to use Rockset as a document loader in langchain. To get started, make sure you have a Rockset account and an API key available.Setting up the environment‚ÄãGo to the Rockset console and get an API key. Find your API region from the API reference. For the purpose of this notebook, we will assume you're using Rockset from Oregon(us-west-2).Set your the environment variable ROCKSET_API_KEY.Install the Rockset python client, which will be used by langchain to interact with the Rockset database.$ pip3 install rocksetLoading DocumentsThe Rockset integration with LangChain allows you to load documents from Rockset collections with SQL queries. In order to do this you must construct a RocksetLoader object. Here is an example snippet that initializes a RocksetLoader.from langchain.document_loaders import RocksetLoaderfrom rockset import RocksetClient, Regions, modelsloader = RocksetLoader(    RocksetClient(Regions.usw2a1, "<api key>"),    models.QueryRequestSql(query="SELECT * FROM langchain_demo LIMIT 3"),  # SQL query    ["text"],  # content columns    metadata_keys=["id", "date"],  # metadata columns)API Reference:RocksetLoaderHere, you can see that the following query is run:SELECT * FROM langchain_demo LIMIT 3The text column in the collection is used as the page content, and the record's id and date columns are used as metadata (if you do not pass anything into metadata_keys, the whole Rockset document will be used as metadata). To execute the query and access an iterator over the resulting Documents, run:loader.lazy_load()To execute the query and access all resulting Documents at once, run:loader.load()Here is an example response of loader.load():[    Document(        page_content="Lorem ipsum dolor sit amet, consectetur adipiscing elit. Maecenas a libero porta, dictum ipsum eget, hendrerit neque. Morbi blandit, ex ut suscipit viverra, enim velit tincidunt tellus, a tempor velit nunc et ex. Proin hendrerit odio nec convallis lobortis. Aenean in purus dolor. Vestibulum orci orci, laoreet eget magna in, commodo euismod justo.",         metadata={"id": 83209, "date": "2022-11-13T18:26:45.000000Z"}    ),    Document(        page_content="Integer at finibus odio. Nam sit amet enim cursus lacus gravida feugiat vestibulum sed libero. Aenean eleifend est quis elementum tincidunt. Curabitur sit amet ornare erat. Nulla id dolor ut magna volutpat sodales fringilla vel ipsum. Donec ultricies, lacus sed fermentum dignissim, lorem elit aliquam ligula, sed suscipit sapien purus nec ligula.",         metadata={"id": 89313, "date": "2022-11-13T18:28:53.000000Z"}    ),    Document(        page_content="Morbi tortor enim, commodo id efficitur vitae, fringilla nec mi. Nullam molestie faucibus aliquet. Praesent a est facilisis, condimentum justo sit amet, viverra erat. Fusce volutpat nisi vel purus blandit, et facilisis felis accumsan. Phasellus luctus ligula ultrices tellus tempor hendrerit. Donec at ultricies leo.",         metadata={"id": 87732, "date": "2022-11-13T18:49:04.000000Z"}    )]Using multiple columns as content‚ÄãYou can choose to use multiple columns as content:from langchain.document_loaders import RocksetLoaderfrom rockset import RocksetClient, Regions, modelsloader = RocksetLoader(    RocksetClient(Regions.usw2a1, "<api key>"),    models.QueryRequestSql(query="SELECT * FROM langchain_demo LIMIT 1 WHERE id=38"),    ["sentence1", "sentence2"],  # TWO content columns)API Reference:RocksetLoaderAssuming the "sentence1" field is "This is the first sentence." and the "sentence2" field is "This is the second sentence.", the page_content of the resulting Document would be:This is the first sentence.This is the second sentence.You can define you own function to join content columns by setting the content_columns_joiner argument in the RocksetLoader constructor. content_columns_joiner is a method that takes in a List[Tuple[str, Any]]] as an argument, representing a list of tuples of (column name, column value). By default, this is a method that joins each column value with a new line.For example, if you wanted to join sentence1 and sentence2 with a space instead of a new line, you could set content_columns_joiner like so:RocksetLoader(    RocksetClient(Regions.usw2a1, "<api key>"),    models.QueryRequestSql(query="SELECT * FROM langchain_demo LIMIT 1 WHERE id=38"),    ["sentence1", "sentence2"],    content_columns_joiner=lambda docs: " ".join(        [doc[1] for doc in docs]    ),  # join with space instead of /n)The page_content of the resulting Document would be:This is the first sentence. This is the second sentence.Oftentimes you want to include the column name in the page_content. You can do that like this:RocksetLoader(    RocksetClient(Regions.usw2a1, "<api key>"),    models.QueryRequestSql(query="SELECT * FROM langchain_demo LIMIT 1 WHERE id=38"),    ["sentence1", "sentence2"],    content_columns_joiner=lambda docs: "\n".join(        [f"{doc[0]}: {doc[1]}" for doc in docs]    ),)This would result in the following page_content:sentence1: This is the first sentence.sentence2: This is the second sentence.PreviousRoamNextRSS Feeds On this page RocksetRockset is a real-time analytics database which enables queries on massive, semi-structured data without operational burden. With Rockset, ingested data is queryable within one second and analytical queries against that data typically execute in milliseconds. Rockset is compute optimized, making it suitable for serving high concurrency applications in the sub-100TB range (or larger than 100s of TBs with rollups).This notebook demonstrates how to use Rockset as a document loader in langchain. To get started, make sure you have a Rockset account and an API key available.Setting up the environment‚ÄãGo to the Rockset console and get an API key. Find your API region from the API reference. For the purpose of this notebook, we will assume you're using Rockset from Oregon(us-west-2).Set your the environment variable ROCKSET_API_KEY.Install the Rockset python client, which will be used by langchain to interact with the Rockset database.$ pip3 install rocksetLoading DocumentsThe Rockset integration with LangChain allows you to load documents from Rockset collections with SQL queries. In order to do this you must construct a RocksetLoader object. Here is an example snippet that initializes a RocksetLoader.from langchain.document_loaders import RocksetLoaderfrom rockset import RocksetClient, Regions, modelsloader = RocksetLoader(    RocksetClient(Regions.usw2a1, "<api key>"),    models.QueryRequestSql(query="SELECT * FROM langchain_demo LIMIT 3"),  # SQL query    ["text"],  # content columns    metadata_keys=["id", "date"],  # metadata columns)API Reference:RocksetLoaderHere, you can see that the following query is run:SELECT * FROM langchain_demo LIMIT 3The text column in the collection is used as the page content, and the record's id and date columns are used as metadata (if you do not pass anything into metadata_keys, the whole Rockset document will be used as metadata). To execute the query and access an iterator over the resulting Documents, run:loader.lazy_load()To execute the query and access all resulting Documents at once, run:loader.load()Here is an example response of loader.load():[    Document(        page_content="Lorem ipsum dolor sit amet, consectetur adipiscing elit. Maecenas a libero porta, dictum ipsum eget, hendrerit neque. Morbi blandit, ex ut suscipit viverra, enim velit tincidunt tellus, a tempor velit nunc et ex. Proin hendrerit odio nec convallis lobortis. Aenean in purus dolor. Vestibulum orci orci, laoreet eget magna in, commodo euismod justo.",         metadata={"id": 83209, "date": "2022-11-13T18:26:45.000000Z"}    ),    Document(        page_content="Integer at finibus odio. Nam sit amet enim cursus lacus gravida feugiat vestibulum sed libero. Aenean eleifend est quis elementum tincidunt. Curabitur sit amet ornare erat. Nulla id dolor ut magna volutpat sodales fringilla vel ipsum. Donec ultricies, lacus sed fermentum dignissim, lorem elit aliquam ligula, sed suscipit sapien purus nec ligula.",         metadata={"id": 89313, "date": "2022-11-13T18:28:53.000000Z"}    ),    Document(        page_content="Morbi tortor enim, commodo id efficitur vitae, fringilla nec mi. Nullam molestie faucibus aliquet. Praesent a est facilisis, condimentum justo sit amet, viverra erat. Fusce volutpat nisi vel purus blandit, et facilisis felis accumsan. Phasellus luctus ligula ultrices tellus tempor hendrerit. Donec at ultricies leo.",         metadata={"id": 87732, "date": "2022-11-13T18:49:04.000000Z"}    )]Using multiple columns as content‚ÄãYou can choose to use multiple columns as content:from langchain.document_loaders import RocksetLoaderfrom rockset import RocksetClient, Regions, modelsloader = RocksetLoader(    RocksetClient(Regions.usw2a1, "<api key>"),    models.QueryRequestSql(query="SELECT * FROM langchain_demo LIMIT 1 WHERE id=38"),    ["sentence1", "sentence2"],  # TWO content columns)API Reference:RocksetLoaderAssuming the "sentence1" field is "This is the first sentence." and the "sentence2" field is "This is the second sentence.", the page_content of the resulting Document would be:This is the first sentence.This is the second sentence.You can define you own function to join content columns by setting the content_columns_joiner argument in the RocksetLoader constructor. content_columns_joiner is a method that takes in a List[Tuple[str, Any]]] as an argument, representing a list of tuples of (column name, column value). By default, this is a method that joins each column value with a new line.For example, if you wanted to join sentence1 and sentence2 with a space instead of a new line, you could set content_columns_joiner like so:RocksetLoader(    RocksetClient(Regions.usw2a1, "<api key>"),    models.QueryRequestSql(query="SELECT * FROM langchain_demo LIMIT 1 WHERE id=38"),    ["sentence1", "sentence2"],    content_columns_joiner=lambda docs: " ".join(        [doc[1] for doc in docs]    ),  # join with space instead of /n)The page_content of the resulting Document would be:This is the first sentence. This is the second sentence.Oftentimes you want to include the column name in the page_content. You can do that like this:RocksetLoader(    RocksetClient(Regions.usw2a1, "<api key>"),    models.QueryRequestSql(query="SELECT * FROM langchain_demo LIMIT 1 WHERE id=38"),    ["sentence1", "sentence2"],    content_columns_joiner=lambda docs: "\n".join(        [f"{doc[0]}: {doc[1]}" for doc in docs]    ),)This would result in the following page_content:sentence1: This is the first sentence.sentence2: This is the second sentence. $ pip3 install rockset $ pip3 install rockset  from langchain.document_loaders import RocksetLoaderfrom rockset import RocksetClient, Regions, modelsloader = RocksetLoader(    RocksetClient(Regions.usw2a1, "<api key>"),    models.QueryRequestSql(query="SELECT * FROM langchain_demo LIMIT 3"),  # SQL query    ["text"],  # content columns    metadata_keys=["id", "date"],  # metadata columns) from langchain.document_loaders import RocksetLoaderfrom rockset import RocksetClient, Regions, modelsloader = RocksetLoader(    RocksetClient(Regions.usw2a1, "<api key>"),    models.QueryRequestSql(query="SELECT * FROM langchain_demo LIMIT 3"),  # SQL query    ["text"],  # content columns    metadata_keys=["id", "date"],  # metadata columns)  API Reference:RocksetLoader SELECT * FROM langchain_demo LIMIT 3 SELECT * FROM langchain_demo LIMIT 3  loader.lazy_load() loader.lazy_load()  loader.load() loader.load()  [    Document(        page_content="Lorem ipsum dolor sit amet, consectetur adipiscing elit. Maecenas a libero porta, dictum ipsum eget, hendrerit neque. Morbi blandit, ex ut suscipit viverra, enim velit tincidunt tellus, a tempor velit nunc et ex. Proin hendrerit odio nec convallis lobortis. Aenean in purus dolor. Vestibulum orci orci, laoreet eget magna in, commodo euismod justo.",         metadata={"id": 83209, "date": "2022-11-13T18:26:45.000000Z"}    ),    Document(        page_content="Integer at finibus odio. Nam sit amet enim cursus lacus gravida feugiat vestibulum sed libero. Aenean eleifend est quis elementum tincidunt. Curabitur sit amet ornare erat. Nulla id dolor ut magna volutpat sodales fringilla vel ipsum. Donec ultricies, lacus sed fermentum dignissim, lorem elit aliquam ligula, sed suscipit sapien purus nec ligula.",         metadata={"id": 89313, "date": "2022-11-13T18:28:53.000000Z"}    ),    Document(        page_content="Morbi tortor enim, commodo id efficitur vitae, fringilla nec mi. Nullam molestie faucibus aliquet. Praesent a est facilisis, condimentum justo sit amet, viverra erat. Fusce volutpat nisi vel purus blandit, et facilisis felis accumsan. Phasellus luctus ligula ultrices tellus tempor hendrerit. Donec at ultricies leo.",         metadata={"id": 87732, "date": "2022-11-13T18:49:04.000000Z"}    )] [    Document(        page_content="Lorem ipsum dolor sit amet, consectetur adipiscing elit. Maecenas a libero porta, dictum ipsum eget, hendrerit neque. Morbi blandit, ex ut suscipit viverra, enim velit tincidunt tellus, a tempor velit nunc et ex. Proin hendrerit odio nec convallis lobortis. Aenean in purus dolor. Vestibulum orci orci, laoreet eget magna in, commodo euismod justo.",         metadata={"id": 83209, "date": "2022-11-13T18:26:45.000000Z"}    ),    Document(        page_content="Integer at finibus odio. Nam sit amet enim cursus lacus gravida feugiat vestibulum sed libero. Aenean eleifend est quis elementum tincidunt. Curabitur sit amet ornare erat. Nulla id dolor ut magna volutpat sodales fringilla vel ipsum. Donec ultricies, lacus sed fermentum dignissim, lorem elit aliquam ligula, sed suscipit sapien purus nec ligula.",         metadata={"id": 89313, "date": "2022-11-13T18:28:53.000000Z"}    ),    Document(        page_content="Morbi tortor enim, commodo id efficitur vitae, fringilla nec mi. Nullam molestie faucibus aliquet. Praesent a est facilisis, condimentum justo sit amet, viverra erat. Fusce volutpat nisi vel purus blandit, et facilisis felis accumsan. Phasellus luctus ligula ultrices tellus tempor hendrerit. Donec at ultricies leo.",         metadata={"id": 87732, "date": "2022-11-13T18:49:04.000000Z"}    )]  from langchain.document_loaders import RocksetLoaderfrom rockset import RocksetClient, Regions, modelsloader = RocksetLoader(    RocksetClient(Regions.usw2a1, "<api key>"),    models.QueryRequestSql(query="SELECT * FROM langchain_demo LIMIT 1 WHERE id=38"),    ["sentence1", "sentence2"],  # TWO content columns) from langchain.document_loaders import RocksetLoaderfrom rockset import RocksetClient, Regions, modelsloader = RocksetLoader(    RocksetClient(Regions.usw2a1, "<api key>"),    models.QueryRequestSql(query="SELECT * FROM langchain_demo LIMIT 1 WHERE id=38"),    ["sentence1", "sentence2"],  # TWO content columns)  API Reference:RocksetLoader This is the first sentence.This is the second sentence. This is the first sentence.This is the second sentence.  RocksetLoader(    RocksetClient(Regions.usw2a1, "<api key>"),    models.QueryRequestSql(query="SELECT * FROM langchain_demo LIMIT 1 WHERE id=38"),    ["sentence1", "sentence2"],    content_columns_joiner=lambda docs: " ".join(        [doc[1] for doc in docs]    ),  # join with space instead of /n) RocksetLoader(    RocksetClient(Regions.usw2a1, "<api key>"),    models.QueryRequestSql(query="SELECT * FROM langchain_demo LIMIT 1 WHERE id=38"),    ["sentence1", "sentence2"],    content_columns_joiner=lambda docs: " ".join(        [doc[1] for doc in docs]    ),  # join with space instead of /n)  This is the first sentence. This is the second sentence. This is the first sentence. This is the second sentence.  RocksetLoader(    RocksetClient(Regions.usw2a1, "<api key>"),    models.QueryRequestSql(query="SELECT * FROM langchain_demo LIMIT 1 WHERE id=38"),    ["sentence1", "sentence2"],    content_columns_joiner=lambda docs: "\n".join(        [f"{doc[0]}: {doc[1]}" for doc in docs]    ),) RocksetLoader(    RocksetClient(Regions.usw2a1, "<api key>"),    models.QueryRequestSql(query="SELECT * FROM langchain_demo LIMIT 1 WHERE id=38"),    ["sentence1", "sentence2"],    content_columns_joiner=lambda docs: "\n".join(        [f"{doc[0]}: {doc[1]}" for doc in docs]    ),)  sentence1: This is the first sentence.sentence2: This is the second sentence. sentence1: This is the first sentence.sentence2: This is the second sentence.  Previous Roam Next RSS Feeds Setting up the environmentUsing multiple columns as content Setting up the environmentUsing multiple columns as content CommunityDiscordTwitterGitHubPythonJS/TSMoreHomepageBlogCopyright ¬© 2023 LangChain, Inc. CommunityDiscordTwitterGitHubPythonJS/TSMoreHomepageBlog CommunityDiscordTwitter Community GitHubPythonJS/TS GitHub MoreHomepageBlog More Copyright ¬© 2023 LangChain, Inc. Copyright ¬© 2023 LangChain, Inc. Skip to main content (#docusaurus_skipToContent_fallback) ü¶úÔ∏èüîó LangChain (/) Docs (/docs/get_started/introduction) Use cases (/docs/use_cases) Integrations (/docs/integrations) API (https://api.python.langchain.com) LangSmith (https://smith.langchain.com) JS/TS Docs (https://js.langchain.com/docs)  (https://github.com/hwchase17/langchain) Integrations (/docs/integrations) Callbacks (/docs/integrations/callbacks/) Chat models (/docs/integrations/chat/) Chat loaders (/docs/integrations/chat_loaders/) Document loaders (/docs/integrations/document_loaders/) Etherscan Loader (/docs/integrations/document_loaders/Etherscan) acreom (/docs/integrations/document_loaders/acreom) Airbyte CDK (/docs/integrations/document_loaders/airbyte_cdk) Airbyte Gong (/docs/integrations/document_loaders/airbyte_gong) Airbyte Hubspot (/docs/integrations/document_loaders/airbyte_hubspot) Airbyte JSON (/docs/integrations/document_loaders/airbyte_json) Airbyte Salesforce (/docs/integrations/document_loaders/airbyte_salesforce) Airbyte Shopify (/docs/integrations/document_loaders/airbyte_shopify) Airbyte Stripe (/docs/integrations/document_loaders/airbyte_stripe) Airbyte Typeform (/docs/integrations/document_loaders/airbyte_typeform) Airbyte Zendesk Support (/docs/integrations/document_loaders/airbyte_zendesk_support) Airtable (/docs/integrations/document_loaders/airtable) Alibaba Cloud MaxCompute (/docs/integrations/document_loaders/alibaba_cloud_maxcompute) Apify Dataset (/docs/integrations/document_loaders/apify_dataset) ArcGIS (/docs/integrations/document_loaders/arcgis) Arxiv (/docs/integrations/document_loaders/arxiv) AssemblyAI Audio Transcripts (/docs/integrations/document_loaders/assemblyai) Async Chromium (/docs/integrations/document_loaders/async_chromium) AsyncHtmlLoader (/docs/integrations/document_loaders/async_html) AWS S3 Directory (/docs/integrations/document_loaders/aws_s3_directory) AWS S3 File (/docs/integrations/document_loaders/aws_s3_file) AZLyrics (/docs/integrations/document_loaders/azlyrics) Azure Blob Storage Container (/docs/integrations/document_loaders/azure_blob_storage_container) Azure Blob Storage File (/docs/integrations/document_loaders/azure_blob_storage_file) Azure Document Intelligence (/docs/integrations/document_loaders/azure_document_intelligence) BibTeX (/docs/integrations/document_loaders/bibtex) BiliBili (/docs/integrations/document_loaders/bilibili) Blackboard (/docs/integrations/document_loaders/blackboard) Blockchain (/docs/integrations/document_loaders/blockchain) Brave Search (/docs/integrations/document_loaders/brave_search) Browserless (/docs/integrations/document_loaders/browserless) ChatGPT Data (/docs/integrations/document_loaders/chatgpt_loader) College Confidential (/docs/integrations/document_loaders/college_confidential) Concurrent Loader (/docs/integrations/document_loaders/concurrent) Confluence (/docs/integrations/document_loaders/confluence) CoNLL-U (/docs/integrations/document_loaders/conll-u) Copy Paste (/docs/integrations/document_loaders/copypaste) CSV (/docs/integrations/document_loaders/csv) Cube Semantic Layer (/docs/integrations/document_loaders/cube_semantic) Datadog Logs (/docs/integrations/document_loaders/datadog_logs) Diffbot (/docs/integrations/document_loaders/diffbot) Discord (/docs/integrations/document_loaders/discord) Docugami (/docs/integrations/document_loaders/docugami) Dropbox (/docs/integrations/document_loaders/dropbox) DuckDB (/docs/integrations/document_loaders/duckdb) Email (/docs/integrations/document_loaders/email) Embaas (/docs/integrations/document_loaders/embaas) EPub (/docs/integrations/document_loaders/epub) EverNote (/docs/integrations/document_loaders/evernote) example_data (/docs/integrations/document_loaders/example_data/notebook) Microsoft Excel (/docs/integrations/document_loaders/excel) Facebook Chat (/docs/integrations/document_loaders/facebook_chat) Fauna (/docs/integrations/document_loaders/fauna) Figma (/docs/integrations/document_loaders/figma) Geopandas (/docs/integrations/document_loaders/geopandas) Git (/docs/integrations/document_loaders/git) GitBook (/docs/integrations/document_loaders/gitbook) GitHub (/docs/integrations/document_loaders/github) Google BigQuery (/docs/integrations/document_loaders/google_bigquery) Google Cloud Storage Directory (/docs/integrations/document_loaders/google_cloud_storage_directory) Google Cloud Storage File (/docs/integrations/document_loaders/google_cloud_storage_file) Google Drive (/docs/integrations/document_loaders/google_drive) Grobid (/docs/integrations/document_loaders/grobid) Gutenberg (/docs/integrations/document_loaders/gutenberg) Hacker News (/docs/integrations/document_loaders/hacker_news) Huawei OBS Directory (/docs/integrations/document_loaders/huawei_obs_directory) Huawei OBS File (/docs/integrations/document_loaders/huawei_obs_file) HuggingFace dataset (/docs/integrations/document_loaders/hugging_face_dataset) iFixit (/docs/integrations/document_loaders/ifixit) Images (/docs/integrations/document_loaders/image) Image captions (/docs/integrations/document_loaders/image_captions) IMSDb (/docs/integrations/document_loaders/imsdb) Iugu (/docs/integrations/document_loaders/iugu) Joplin (/docs/integrations/document_loaders/joplin) Jupyter Notebook (/docs/integrations/document_loaders/jupyter_notebook) LarkSuite (FeiShu) (/docs/integrations/document_loaders/larksuite) Mastodon (/docs/integrations/document_loaders/mastodon) MediaWikiDump (/docs/integrations/document_loaders/mediawikidump) MergeDocLoader (/docs/integrations/document_loaders/merge_doc_loader) mhtml (/docs/integrations/document_loaders/mhtml) Microsoft OneDrive (/docs/integrations/document_loaders/microsoft_onedrive) Microsoft PowerPoint (/docs/integrations/document_loaders/microsoft_powerpoint) Microsoft SharePoint (/docs/integrations/document_loaders/microsoft_sharepoint) Microsoft Word (/docs/integrations/document_loaders/microsoft_word) Modern Treasury (/docs/integrations/document_loaders/modern_treasury) News URL (/docs/integrations/document_loaders/news) Notion DB 1/2 (/docs/integrations/document_loaders/notion) Notion DB 2/2 (/docs/integrations/document_loaders/notiondb) Nuclia Understanding API document loader (/docs/integrations/document_loaders/nuclia) Obsidian (/docs/integrations/document_loaders/obsidian) Open Document Format (ODT) (/docs/integrations/document_loaders/odt) Open City Data (/docs/integrations/document_loaders/open_city_data) Org-mode (/docs/integrations/document_loaders/org_mode) Pandas DataFrame (/docs/integrations/document_loaders/pandas_dataframe) Amazon Textract (/docs/integrations/document_loaders/pdf-amazonTextractPDFLoader) Polars DataFrame (/docs/integrations/document_loaders/polars_dataframe) Psychic (/docs/integrations/document_loaders/psychic) PubMed (/docs/integrations/document_loaders/pubmed) PySpark DataFrame Loader (/docs/integrations/document_loaders/pyspark_dataframe) ReadTheDocs Documentation (/docs/integrations/document_loaders/readthedocs_documentation) Recursive URL Loader (/docs/integrations/document_loaders/recursive_url_loader) Reddit (/docs/integrations/document_loaders/reddit) Roam (/docs/integrations/document_loaders/roam) Rockset (/docs/integrations/document_loaders/rockset) RSS Feeds (/docs/integrations/document_loaders/rss) RST (/docs/integrations/document_loaders/rst) Sitemap (/docs/integrations/document_loaders/sitemap) Slack (/docs/integrations/document_loaders/slack) Snowflake (/docs/integrations/document_loaders/snowflake) Source Code (/docs/integrations/document_loaders/source_code) Spreedly (/docs/integrations/document_loaders/spreedly) Stripe (/docs/integrations/document_loaders/stripe) Subtitle (/docs/integrations/document_loaders/subtitle) Telegram (/docs/integrations/document_loaders/telegram) Tencent COS Directory (/docs/integrations/document_loaders/tencent_cos_directory) Tencent COS File (/docs/integrations/document_loaders/tencent_cos_file) TensorFlow Datasets (/docs/integrations/document_loaders/tensorflow_datasets) 2Markdown (/docs/integrations/document_loaders/tomarkdown) TOML (/docs/integrations/document_loaders/toml) Trello (/docs/integrations/document_loaders/trello) TSV (/docs/integrations/document_loaders/tsv) Twitter (/docs/integrations/document_loaders/twitter) Unstructured File (/docs/integrations/document_loaders/unstructured_file) URL (/docs/integrations/document_loaders/url) Weather (/docs/integrations/document_loaders/weather) WebBaseLoader (/docs/integrations/document_loaders/web_base) WhatsApp Chat (/docs/integrations/document_loaders/whatsapp_chat) Wikipedia (/docs/integrations/document_loaders/wikipedia) XML (/docs/integrations/document_loaders/xml) Xorbits Pandas DataFrame (/docs/integrations/document_loaders/xorbits) Loading documents from a YouTube url (/docs/integrations/document_loaders/youtube_audio) YouTube transcripts (/docs/integrations/document_loaders/youtube_transcript) Document transformers (/docs/integrations/document_transformers/) LLMs (/docs/integrations/llms/) Memory (/docs/integrations/memory/) Retrievers (/docs/integrations/retrievers/) Text embedding models (/docs/integrations/text_embedding/) Agents & Toolkits (/docs/integrations/toolkits/) Tools (/docs/integrations/tools/) Vector stores (/docs/integrations/vectorstores/) Grouped by provider (/docs/integrations/providers/)  (/) Integrations (/docs/integrations) Document loaders (/docs/integrations/document_loaders/) ‚Äã (#setting-up-the-environment) Rockset console (https://console.rockset.com/apikeys) API reference (https://rockset.com/docs/rest-api/#introduction) RocksetLoader (https://api.python.langchain.com/en/latest/document_loaders/langchain.document_loaders.rocksetdb.RocksetLoader.html) ‚Äã (#using-multiple-columns-as-content) RocksetLoader (https://api.python.langchain.com/en/latest/document_loaders/langchain.document_loaders.rocksetdb.RocksetLoader.html) PreviousRoam (/docs/integrations/document_loaders/roam) NextRSS Feeds (/docs/integrations/document_loaders/rss) Setting up the environment (#setting-up-the-environment) Using multiple columns as content (#using-multiple-columns-as-content) Discord (https://discord.gg/cU2adEyC7w) Twitter (https://twitter.com/LangChainAI) Python (https://github.com/hwchase17/langchain) JS/TS (https://github.com/hwchase17/langchainjs) Homepage (https://langchain.com) Blog (https://blog.langchain.dev)