ü¶æ OpenLLM is an open platform for operating large language models (LLMs) in production. It enables developers to easily run inference with any open-source LLMs, deploy to the cloud or on-premises, and build powerful AI apps. Install openllm through PyPI To start an LLM server, use openllm start command. For example, to start a dolly-v2 server, run the following command from a terminal: You may also choose to initialize an LLM managed by OpenLLM locally from current process. This is useful for development purpose and allows developers to quickly try out different types of LLMs. When moving LLM applications to production, we recommend deploying the OpenLLM server separately and access via the server_url option demonstrated above. To load an LLM locally via the LangChain wrapper: IntegrationsCallbacksChat modelsChat loadersDocument loadersDocument transformersLLMsAI21Aleph AlphaAmazon API GatewayAnyscaleAzure MLAzure OpenAIBananaBasetenBeamBedrockBittensorCerebriumAIChatGLMClarifaiCohereC TransformersCTranslate2DatabricksDeepInfraDeepSparseEden AIFireworksForefrontAIGoogle Vertex AI PaLMGooseAIGPT4AllHugging Face HubHugging Face Local PipelinesHuggingface TextGen InferenceJSONFormerKoboldAI APILlama.cppLLM Caching integrationsManifestMinimaxModalMosaicMLNLP CloudOctoAIOllamaOpaquePromptsOpenAIOpenLLMOpenLMPetalsPipelineAIPredibasePrediction GuardPromptLayer OpenAIRELLMReplicateRunhouseSageMakerEndpointStochasticAINebula (Symbl.ai)TextGenTitan TakeoffTongyi QwenvLLMWriterXorbits Inference (Xinference)MemoryRetrieversText embedding modelsAgents & ToolkitsToolsVector storesGrouped by provider Callbacks Chat models Chat loaders Document loaders Document transformers LLMsAI21Aleph AlphaAmazon API GatewayAnyscaleAzure MLAzure OpenAIBananaBasetenBeamBedrockBittensorCerebriumAIChatGLMClarifaiCohereC TransformersCTranslate2DatabricksDeepInfraDeepSparseEden AIFireworksForefrontAIGoogle Vertex AI PaLMGooseAIGPT4AllHugging Face HubHugging Face Local PipelinesHuggingface TextGen InferenceJSONFormerKoboldAI APILlama.cppLLM Caching integrationsManifestMinimaxModalMosaicMLNLP CloudOctoAIOllamaOpaquePromptsOpenAIOpenLLMOpenLMPetalsPipelineAIPredibasePrediction GuardPromptLayer OpenAIRELLMReplicateRunhouseSageMakerEndpointStochasticAINebula (Symbl.ai)TextGenTitan TakeoffTongyi QwenvLLMWriterXorbits Inference (Xinference) AI21 Aleph Alpha Amazon API Gateway Anyscale Azure ML Azure OpenAI Banana Baseten Beam Bedrock Bittensor CerebriumAI ChatGLM Clarifai Cohere C Transformers CTranslate2 Databricks DeepInfra DeepSparse Eden AI Fireworks ForefrontAI Google Vertex AI PaLM GooseAI GPT4All Hugging Face Hub Hugging Face Local Pipelines Huggingface TextGen Inference JSONFormer KoboldAI API Llama.cpp LLM Caching integrations Manifest Minimax Modal MosaicML NLP Cloud OctoAI Ollama OpaquePrompts OpenAI OpenLLM OpenLM Petals PipelineAI Predibase Prediction Guard PromptLayer OpenAI RELLM Replicate Runhouse SageMakerEndpoint StochasticAI Nebula (Symbl.ai) TextGen Titan Takeoff Tongyi Qwen vLLM Writer Xorbits Inference (Xinference) Memory Retrievers Text embedding models Agents & Toolkits Tools Vector stores Grouped by provider  Integrations LLMs OpenLLM OpenLLM OpenLLM Installation Launch OpenLLM server locally WrapperOptional: Local LLM InferenceIntegrate with a LLMChain Optional: Local LLM Inference Integrate with a LLMChain Discord Twitter Python JS/TS Homepage Blog Skip to main contentü¶úÔ∏èüîó LangChainDocsUse casesIntegrationsAPILangSmithJS/TS DocsCTRLKIntegrationsCallbacksChat modelsChat loadersDocument loadersDocument transformersLLMsAI21Aleph AlphaAmazon API GatewayAnyscaleAzure MLAzure OpenAIBananaBasetenBeamBedrockBittensorCerebriumAIChatGLMClarifaiCohereC TransformersCTranslate2DatabricksDeepInfraDeepSparseEden AIFireworksForefrontAIGoogle Vertex AI PaLMGooseAIGPT4AllHugging Face HubHugging Face Local PipelinesHuggingface TextGen InferenceJSONFormerKoboldAI APILlama.cppLLM Caching integrationsManifestMinimaxModalMosaicMLNLP CloudOctoAIOllamaOpaquePromptsOpenAIOpenLLMOpenLMPetalsPipelineAIPredibasePrediction GuardPromptLayer OpenAIRELLMReplicateRunhouseSageMakerEndpointStochasticAINebula (Symbl.ai)TextGenTitan TakeoffTongyi QwenvLLMWriterXorbits Inference (Xinference)MemoryRetrieversText embedding modelsAgents & ToolkitsToolsVector storesGrouped by providerIntegrationsLLMsOpenLLMOn this pageOpenLLMü¶æ OpenLLM is an open platform for operating large language models (LLMs) in production. It enables developers to easily run inference with any open-source LLMs, deploy to the cloud or on-premises, and build powerful AI apps.Installation‚ÄãInstall openllm through PyPIpip install openllmLaunch OpenLLM server locally‚ÄãTo start an LLM server, use openllm start command. For example, to start a dolly-v2 server, run the following command from a terminal:openllm start dolly-v2Wrapper‚Äãfrom langchain.llms import OpenLLMserver_url = "http://localhost:3000"  # Replace with remote host if you are running on a remote serverllm = OpenLLM(server_url=server_url)API Reference:OpenLLMOptional: Local LLM Inference‚ÄãYou may also choose to initialize an LLM managed by OpenLLM locally from current process. This is useful for development purpose and allows developers to quickly try out different types of LLMs.When moving LLM applications to production, we recommend deploying the OpenLLM server separately and access via the server_url option demonstrated above.To load an LLM locally via the LangChain wrapper:from langchain.llms import OpenLLMllm = OpenLLM(    model_name="dolly-v2",    model_id="databricks/dolly-v2-3b",    temperature=0.94,    repetition_penalty=1.2,)API Reference:OpenLLMIntegrate with a LLMChain‚Äãfrom langchain import PromptTemplate, LLMChaintemplate = "What is a good name for a company that makes {product}?"prompt = PromptTemplate(template=template, input_variables=["product"])llm_chain = LLMChain(prompt=prompt, llm=llm)generated = llm_chain.run(product="mechanical keyboard")print(generated)    iLkbPreviousOpenAINextOpenLMInstallationLaunch OpenLLM server locallyWrapperOptional: Local LLM InferenceIntegrate with a LLMChainCommunityDiscordTwitterGitHubPythonJS/TSMoreHomepageBlogCopyright ¬© 2023 LangChain, Inc. Skip to main content ü¶úÔ∏èüîó LangChainDocsUse casesIntegrationsAPILangSmithJS/TS DocsCTRLK ü¶úÔ∏èüîó LangChainDocsUse casesIntegrationsAPI LangSmithJS/TS DocsCTRLK  CTRLK CTRLK  CTRLK   IntegrationsCallbacksChat modelsChat loadersDocument loadersDocument transformersLLMsAI21Aleph AlphaAmazon API GatewayAnyscaleAzure MLAzure OpenAIBananaBasetenBeamBedrockBittensorCerebriumAIChatGLMClarifaiCohereC TransformersCTranslate2DatabricksDeepInfraDeepSparseEden AIFireworksForefrontAIGoogle Vertex AI PaLMGooseAIGPT4AllHugging Face HubHugging Face Local PipelinesHuggingface TextGen InferenceJSONFormerKoboldAI APILlama.cppLLM Caching integrationsManifestMinimaxModalMosaicMLNLP CloudOctoAIOllamaOpaquePromptsOpenAIOpenLLMOpenLMPetalsPipelineAIPredibasePrediction GuardPromptLayer OpenAIRELLMReplicateRunhouseSageMakerEndpointStochasticAINebula (Symbl.ai)TextGenTitan TakeoffTongyi QwenvLLMWriterXorbits Inference (Xinference)MemoryRetrieversText embedding modelsAgents & ToolkitsToolsVector storesGrouped by providerIntegrationsLLMsOpenLLMOn this pageOpenLLMü¶æ OpenLLM is an open platform for operating large language models (LLMs) in production. It enables developers to easily run inference with any open-source LLMs, deploy to the cloud or on-premises, and build powerful AI apps.Installation‚ÄãInstall openllm through PyPIpip install openllmLaunch OpenLLM server locally‚ÄãTo start an LLM server, use openllm start command. For example, to start a dolly-v2 server, run the following command from a terminal:openllm start dolly-v2Wrapper‚Äãfrom langchain.llms import OpenLLMserver_url = "http://localhost:3000"  # Replace with remote host if you are running on a remote serverllm = OpenLLM(server_url=server_url)API Reference:OpenLLMOptional: Local LLM Inference‚ÄãYou may also choose to initialize an LLM managed by OpenLLM locally from current process. This is useful for development purpose and allows developers to quickly try out different types of LLMs.When moving LLM applications to production, we recommend deploying the OpenLLM server separately and access via the server_url option demonstrated above.To load an LLM locally via the LangChain wrapper:from langchain.llms import OpenLLMllm = OpenLLM(    model_name="dolly-v2",    model_id="databricks/dolly-v2-3b",    temperature=0.94,    repetition_penalty=1.2,)API Reference:OpenLLMIntegrate with a LLMChain‚Äãfrom langchain import PromptTemplate, LLMChaintemplate = "What is a good name for a company that makes {product}?"prompt = PromptTemplate(template=template, input_variables=["product"])llm_chain = LLMChain(prompt=prompt, llm=llm)generated = llm_chain.run(product="mechanical keyboard")print(generated)    iLkbPreviousOpenAINextOpenLMInstallationLaunch OpenLLM server locallyWrapperOptional: Local LLM InferenceIntegrate with a LLMChain IntegrationsCallbacksChat modelsChat loadersDocument loadersDocument transformersLLMsAI21Aleph AlphaAmazon API GatewayAnyscaleAzure MLAzure OpenAIBananaBasetenBeamBedrockBittensorCerebriumAIChatGLMClarifaiCohereC TransformersCTranslate2DatabricksDeepInfraDeepSparseEden AIFireworksForefrontAIGoogle Vertex AI PaLMGooseAIGPT4AllHugging Face HubHugging Face Local PipelinesHuggingface TextGen InferenceJSONFormerKoboldAI APILlama.cppLLM Caching integrationsManifestMinimaxModalMosaicMLNLP CloudOctoAIOllamaOpaquePromptsOpenAIOpenLLMOpenLMPetalsPipelineAIPredibasePrediction GuardPromptLayer OpenAIRELLMReplicateRunhouseSageMakerEndpointStochasticAINebula (Symbl.ai)TextGenTitan TakeoffTongyi QwenvLLMWriterXorbits Inference (Xinference)MemoryRetrieversText embedding modelsAgents & ToolkitsToolsVector storesGrouped by providerIntegrationsLLMsOpenLLMOn this pageOpenLLMü¶æ OpenLLM is an open platform for operating large language models (LLMs) in production. It enables developers to easily run inference with any open-source LLMs, deploy to the cloud or on-premises, and build powerful AI apps.Installation‚ÄãInstall openllm through PyPIpip install openllmLaunch OpenLLM server locally‚ÄãTo start an LLM server, use openllm start command. For example, to start a dolly-v2 server, run the following command from a terminal:openllm start dolly-v2Wrapper‚Äãfrom langchain.llms import OpenLLMserver_url = "http://localhost:3000"  # Replace with remote host if you are running on a remote serverllm = OpenLLM(server_url=server_url)API Reference:OpenLLMOptional: Local LLM Inference‚ÄãYou may also choose to initialize an LLM managed by OpenLLM locally from current process. This is useful for development purpose and allows developers to quickly try out different types of LLMs.When moving LLM applications to production, we recommend deploying the OpenLLM server separately and access via the server_url option demonstrated above.To load an LLM locally via the LangChain wrapper:from langchain.llms import OpenLLMllm = OpenLLM(    model_name="dolly-v2",    model_id="databricks/dolly-v2-3b",    temperature=0.94,    repetition_penalty=1.2,)API Reference:OpenLLMIntegrate with a LLMChain‚Äãfrom langchain import PromptTemplate, LLMChaintemplate = "What is a good name for a company that makes {product}?"prompt = PromptTemplate(template=template, input_variables=["product"])llm_chain = LLMChain(prompt=prompt, llm=llm)generated = llm_chain.run(product="mechanical keyboard")print(generated)    iLkbPreviousOpenAINextOpenLMInstallationLaunch OpenLLM server locallyWrapperOptional: Local LLM InferenceIntegrate with a LLMChain IntegrationsCallbacksChat modelsChat loadersDocument loadersDocument transformersLLMsAI21Aleph AlphaAmazon API GatewayAnyscaleAzure MLAzure OpenAIBananaBasetenBeamBedrockBittensorCerebriumAIChatGLMClarifaiCohereC TransformersCTranslate2DatabricksDeepInfraDeepSparseEden AIFireworksForefrontAIGoogle Vertex AI PaLMGooseAIGPT4AllHugging Face HubHugging Face Local PipelinesHuggingface TextGen InferenceJSONFormerKoboldAI APILlama.cppLLM Caching integrationsManifestMinimaxModalMosaicMLNLP CloudOctoAIOllamaOpaquePromptsOpenAIOpenLLMOpenLMPetalsPipelineAIPredibasePrediction GuardPromptLayer OpenAIRELLMReplicateRunhouseSageMakerEndpointStochasticAINebula (Symbl.ai)TextGenTitan TakeoffTongyi QwenvLLMWriterXorbits Inference (Xinference)MemoryRetrieversText embedding modelsAgents & ToolkitsToolsVector storesGrouped by provider IntegrationsCallbacksChat modelsChat loadersDocument loadersDocument transformersLLMsAI21Aleph AlphaAmazon API GatewayAnyscaleAzure MLAzure OpenAIBananaBasetenBeamBedrockBittensorCerebriumAIChatGLMClarifaiCohereC TransformersCTranslate2DatabricksDeepInfraDeepSparseEden AIFireworksForefrontAIGoogle Vertex AI PaLMGooseAIGPT4AllHugging Face HubHugging Face Local PipelinesHuggingface TextGen InferenceJSONFormerKoboldAI APILlama.cppLLM Caching integrationsManifestMinimaxModalMosaicMLNLP CloudOctoAIOllamaOpaquePromptsOpenAIOpenLLMOpenLMPetalsPipelineAIPredibasePrediction GuardPromptLayer OpenAIRELLMReplicateRunhouseSageMakerEndpointStochasticAINebula (Symbl.ai)TextGenTitan TakeoffTongyi QwenvLLMWriterXorbits Inference (Xinference)MemoryRetrieversText embedding modelsAgents & ToolkitsToolsVector storesGrouped by provider Integrations Callbacks Chat models Chat loaders Document loaders Document transformers LLMs Memory Retrievers Text embedding models Agents & Toolkits Tools Vector stores Grouped by provider IntegrationsLLMsOpenLLMOn this pageOpenLLMü¶æ OpenLLM is an open platform for operating large language models (LLMs) in production. It enables developers to easily run inference with any open-source LLMs, deploy to the cloud or on-premises, and build powerful AI apps.Installation‚ÄãInstall openllm through PyPIpip install openllmLaunch OpenLLM server locally‚ÄãTo start an LLM server, use openllm start command. For example, to start a dolly-v2 server, run the following command from a terminal:openllm start dolly-v2Wrapper‚Äãfrom langchain.llms import OpenLLMserver_url = "http://localhost:3000"  # Replace with remote host if you are running on a remote serverllm = OpenLLM(server_url=server_url)API Reference:OpenLLMOptional: Local LLM Inference‚ÄãYou may also choose to initialize an LLM managed by OpenLLM locally from current process. This is useful for development purpose and allows developers to quickly try out different types of LLMs.When moving LLM applications to production, we recommend deploying the OpenLLM server separately and access via the server_url option demonstrated above.To load an LLM locally via the LangChain wrapper:from langchain.llms import OpenLLMllm = OpenLLM(    model_name="dolly-v2",    model_id="databricks/dolly-v2-3b",    temperature=0.94,    repetition_penalty=1.2,)API Reference:OpenLLMIntegrate with a LLMChain‚Äãfrom langchain import PromptTemplate, LLMChaintemplate = "What is a good name for a company that makes {product}?"prompt = PromptTemplate(template=template, input_variables=["product"])llm_chain = LLMChain(prompt=prompt, llm=llm)generated = llm_chain.run(product="mechanical keyboard")print(generated)    iLkbPreviousOpenAINextOpenLMInstallationLaunch OpenLLM server locallyWrapperOptional: Local LLM InferenceIntegrate with a LLMChain IntegrationsLLMsOpenLLMOn this pageOpenLLMü¶æ OpenLLM is an open platform for operating large language models (LLMs) in production. It enables developers to easily run inference with any open-source LLMs, deploy to the cloud or on-premises, and build powerful AI apps.Installation‚ÄãInstall openllm through PyPIpip install openllmLaunch OpenLLM server locally‚ÄãTo start an LLM server, use openllm start command. For example, to start a dolly-v2 server, run the following command from a terminal:openllm start dolly-v2Wrapper‚Äãfrom langchain.llms import OpenLLMserver_url = "http://localhost:3000"  # Replace with remote host if you are running on a remote serverllm = OpenLLM(server_url=server_url)API Reference:OpenLLMOptional: Local LLM Inference‚ÄãYou may also choose to initialize an LLM managed by OpenLLM locally from current process. This is useful for development purpose and allows developers to quickly try out different types of LLMs.When moving LLM applications to production, we recommend deploying the OpenLLM server separately and access via the server_url option demonstrated above.To load an LLM locally via the LangChain wrapper:from langchain.llms import OpenLLMllm = OpenLLM(    model_name="dolly-v2",    model_id="databricks/dolly-v2-3b",    temperature=0.94,    repetition_penalty=1.2,)API Reference:OpenLLMIntegrate with a LLMChain‚Äãfrom langchain import PromptTemplate, LLMChaintemplate = "What is a good name for a company that makes {product}?"prompt = PromptTemplate(template=template, input_variables=["product"])llm_chain = LLMChain(prompt=prompt, llm=llm)generated = llm_chain.run(product="mechanical keyboard")print(generated)    iLkbPreviousOpenAINextOpenLMInstallationLaunch OpenLLM server locallyWrapperOptional: Local LLM InferenceIntegrate with a LLMChain IntegrationsLLMsOpenLLMOn this pageOpenLLMü¶æ OpenLLM is an open platform for operating large language models (LLMs) in production. It enables developers to easily run inference with any open-source LLMs, deploy to the cloud or on-premises, and build powerful AI apps.Installation‚ÄãInstall openllm through PyPIpip install openllmLaunch OpenLLM server locally‚ÄãTo start an LLM server, use openllm start command. For example, to start a dolly-v2 server, run the following command from a terminal:openllm start dolly-v2Wrapper‚Äãfrom langchain.llms import OpenLLMserver_url = "http://localhost:3000"  # Replace with remote host if you are running on a remote serverllm = OpenLLM(server_url=server_url)API Reference:OpenLLMOptional: Local LLM Inference‚ÄãYou may also choose to initialize an LLM managed by OpenLLM locally from current process. This is useful for development purpose and allows developers to quickly try out different types of LLMs.When moving LLM applications to production, we recommend deploying the OpenLLM server separately and access via the server_url option demonstrated above.To load an LLM locally via the LangChain wrapper:from langchain.llms import OpenLLMllm = OpenLLM(    model_name="dolly-v2",    model_id="databricks/dolly-v2-3b",    temperature=0.94,    repetition_penalty=1.2,)API Reference:OpenLLMIntegrate with a LLMChain‚Äãfrom langchain import PromptTemplate, LLMChaintemplate = "What is a good name for a company that makes {product}?"prompt = PromptTemplate(template=template, input_variables=["product"])llm_chain = LLMChain(prompt=prompt, llm=llm)generated = llm_chain.run(product="mechanical keyboard")print(generated)    iLkbPreviousOpenAINextOpenLM IntegrationsLLMsOpenLLMOn this pageOpenLLMü¶æ OpenLLM is an open platform for operating large language models (LLMs) in production. It enables developers to easily run inference with any open-source LLMs, deploy to the cloud or on-premises, and build powerful AI apps.Installation‚ÄãInstall openllm through PyPIpip install openllmLaunch OpenLLM server locally‚ÄãTo start an LLM server, use openllm start command. For example, to start a dolly-v2 server, run the following command from a terminal:openllm start dolly-v2Wrapper‚Äãfrom langchain.llms import OpenLLMserver_url = "http://localhost:3000"  # Replace with remote host if you are running on a remote serverllm = OpenLLM(server_url=server_url)API Reference:OpenLLMOptional: Local LLM Inference‚ÄãYou may also choose to initialize an LLM managed by OpenLLM locally from current process. This is useful for development purpose and allows developers to quickly try out different types of LLMs.When moving LLM applications to production, we recommend deploying the OpenLLM server separately and access via the server_url option demonstrated above.To load an LLM locally via the LangChain wrapper:from langchain.llms import OpenLLMllm = OpenLLM(    model_name="dolly-v2",    model_id="databricks/dolly-v2-3b",    temperature=0.94,    repetition_penalty=1.2,)API Reference:OpenLLMIntegrate with a LLMChain‚Äãfrom langchain import PromptTemplate, LLMChaintemplate = "What is a good name for a company that makes {product}?"prompt = PromptTemplate(template=template, input_variables=["product"])llm_chain = LLMChain(prompt=prompt, llm=llm)generated = llm_chain.run(product="mechanical keyboard")print(generated)    iLkbPreviousOpenAINextOpenLM On this page OpenLLMü¶æ OpenLLM is an open platform for operating large language models (LLMs) in production. It enables developers to easily run inference with any open-source LLMs, deploy to the cloud or on-premises, and build powerful AI apps.Installation‚ÄãInstall openllm through PyPIpip install openllmLaunch OpenLLM server locally‚ÄãTo start an LLM server, use openllm start command. For example, to start a dolly-v2 server, run the following command from a terminal:openllm start dolly-v2Wrapper‚Äãfrom langchain.llms import OpenLLMserver_url = "http://localhost:3000"  # Replace with remote host if you are running on a remote serverllm = OpenLLM(server_url=server_url)API Reference:OpenLLMOptional: Local LLM Inference‚ÄãYou may also choose to initialize an LLM managed by OpenLLM locally from current process. This is useful for development purpose and allows developers to quickly try out different types of LLMs.When moving LLM applications to production, we recommend deploying the OpenLLM server separately and access via the server_url option demonstrated above.To load an LLM locally via the LangChain wrapper:from langchain.llms import OpenLLMllm = OpenLLM(    model_name="dolly-v2",    model_id="databricks/dolly-v2-3b",    temperature=0.94,    repetition_penalty=1.2,)API Reference:OpenLLMIntegrate with a LLMChain‚Äãfrom langchain import PromptTemplate, LLMChaintemplate = "What is a good name for a company that makes {product}?"prompt = PromptTemplate(template=template, input_variables=["product"])llm_chain = LLMChain(prompt=prompt, llm=llm)generated = llm_chain.run(product="mechanical keyboard")print(generated)    iLkb pip install openllm pip install openllm  openllm start dolly-v2 openllm start dolly-v2  from langchain.llms import OpenLLMserver_url = "http://localhost:3000"  # Replace with remote host if you are running on a remote serverllm = OpenLLM(server_url=server_url) from langchain.llms import OpenLLMserver_url = "http://localhost:3000"  # Replace with remote host if you are running on a remote serverllm = OpenLLM(server_url=server_url)  API Reference:OpenLLM from langchain.llms import OpenLLMllm = OpenLLM(    model_name="dolly-v2",    model_id="databricks/dolly-v2-3b",    temperature=0.94,    repetition_penalty=1.2,) from langchain.llms import OpenLLMllm = OpenLLM(    model_name="dolly-v2",    model_id="databricks/dolly-v2-3b",    temperature=0.94,    repetition_penalty=1.2,)  API Reference:OpenLLM from langchain import PromptTemplate, LLMChaintemplate = "What is a good name for a company that makes {product}?"prompt = PromptTemplate(template=template, input_variables=["product"])llm_chain = LLMChain(prompt=prompt, llm=llm)generated = llm_chain.run(product="mechanical keyboard")print(generated) from langchain import PromptTemplate, LLMChaintemplate = "What is a good name for a company that makes {product}?"prompt = PromptTemplate(template=template, input_variables=["product"])llm_chain = LLMChain(prompt=prompt, llm=llm)generated = llm_chain.run(product="mechanical keyboard")print(generated)      iLkb     iLkb     iLkb  Previous OpenAI Next OpenLM InstallationLaunch OpenLLM server locallyWrapperOptional: Local LLM InferenceIntegrate with a LLMChain InstallationLaunch OpenLLM server locallyWrapperOptional: Local LLM InferenceIntegrate with a LLMChain CommunityDiscordTwitterGitHubPythonJS/TSMoreHomepageBlogCopyright ¬© 2023 LangChain, Inc. CommunityDiscordTwitterGitHubPythonJS/TSMoreHomepageBlog CommunityDiscordTwitter Community GitHubPythonJS/TS GitHub MoreHomepageBlog More Copyright ¬© 2023 LangChain, Inc. Copyright ¬© 2023 LangChain, Inc. Skip to main content (#docusaurus_skipToContent_fallback) ü¶úÔ∏èüîó LangChain (/) Docs (/docs/get_started/introduction) Use cases (/docs/use_cases) Integrations (/docs/integrations) API (https://api.python.langchain.com) LangSmith (https://smith.langchain.com) JS/TS Docs (https://js.langchain.com/docs)  (https://github.com/hwchase17/langchain) Integrations (/docs/integrations) Callbacks (/docs/integrations/callbacks/) Chat models (/docs/integrations/chat/) Chat loaders (/docs/integrations/chat_loaders/) Document loaders (/docs/integrations/document_loaders/) Document transformers (/docs/integrations/document_transformers/) LLMs (/docs/integrations/llms/) AI21 (/docs/integrations/llms/ai21) Aleph Alpha (/docs/integrations/llms/aleph_alpha) Amazon API Gateway (/docs/integrations/llms/amazon_api_gateway) Anyscale (/docs/integrations/llms/anyscale) Azure ML (/docs/integrations/llms/azure_ml) Azure OpenAI (/docs/integrations/llms/azure_openai) Banana (/docs/integrations/llms/banana) Baseten (/docs/integrations/llms/baseten) Beam (/docs/integrations/llms/beam) Bedrock (/docs/integrations/llms/bedrock) Bittensor (/docs/integrations/llms/bittensor) CerebriumAI (/docs/integrations/llms/cerebriumai) ChatGLM (/docs/integrations/llms/chatglm) Clarifai (/docs/integrations/llms/clarifai) Cohere (/docs/integrations/llms/cohere) C Transformers (/docs/integrations/llms/ctransformers) CTranslate2 (/docs/integrations/llms/ctranslate2) Databricks (/docs/integrations/llms/databricks) DeepInfra (/docs/integrations/llms/deepinfra) DeepSparse (/docs/integrations/llms/deepsparse) Eden AI (/docs/integrations/llms/edenai) Fireworks (/docs/integrations/llms/fireworks) ForefrontAI (/docs/integrations/llms/forefrontai) Google Vertex AI PaLM (/docs/integrations/llms/google_vertex_ai_palm) GooseAI (/docs/integrations/llms/gooseai) GPT4All (/docs/integrations/llms/gpt4all) Hugging Face Hub (/docs/integrations/llms/huggingface_hub) Hugging Face Local Pipelines (/docs/integrations/llms/huggingface_pipelines) Huggingface TextGen Inference (/docs/integrations/llms/huggingface_textgen_inference) JSONFormer (/docs/integrations/llms/jsonformer_experimental) KoboldAI API (/docs/integrations/llms/koboldai) Llama.cpp (/docs/integrations/llms/llamacpp) LLM Caching integrations (/docs/integrations/llms/llm_caching) Manifest (/docs/integrations/llms/manifest) Minimax (/docs/integrations/llms/minimax) Modal (/docs/integrations/llms/modal) MosaicML (/docs/integrations/llms/mosaicml) NLP Cloud (/docs/integrations/llms/nlpcloud) OctoAI (/docs/integrations/llms/octoai) Ollama (/docs/integrations/llms/ollama) OpaquePrompts (/docs/integrations/llms/opaqueprompts) OpenAI (/docs/integrations/llms/openai) OpenLLM (/docs/integrations/llms/openllm) OpenLM (/docs/integrations/llms/openlm) Petals (/docs/integrations/llms/petals) PipelineAI (/docs/integrations/llms/pipelineai) Predibase (/docs/integrations/llms/predibase) Prediction Guard (/docs/integrations/llms/predictionguard) PromptLayer OpenAI (/docs/integrations/llms/promptlayer_openai) RELLM (/docs/integrations/llms/rellm_experimental) Replicate (/docs/integrations/llms/replicate) Runhouse (/docs/integrations/llms/runhouse) SageMakerEndpoint (/docs/integrations/llms/sagemaker) StochasticAI (/docs/integrations/llms/stochasticai) Nebula (Symbl.ai) (/docs/integrations/llms/symblai_nebula) TextGen (/docs/integrations/llms/textgen) Titan Takeoff (/docs/integrations/llms/titan_takeoff) Tongyi Qwen (/docs/integrations/llms/tongyi) vLLM (/docs/integrations/llms/vllm) Writer (/docs/integrations/llms/writer) Xorbits Inference (Xinference) (/docs/integrations/llms/xinference) Memory (/docs/integrations/memory/) Retrievers (/docs/integrations/retrievers/) Text embedding models (/docs/integrations/text_embedding/) Agents & Toolkits (/docs/integrations/toolkits/) Tools (/docs/integrations/tools/) Vector stores (/docs/integrations/vectorstores/) Grouped by provider (/docs/integrations/providers/)  (/) Integrations (/docs/integrations) LLMs (/docs/integrations/llms/) ü¶æ OpenLLM (https://github.com/bentoml/OpenLLM) ‚Äã (#installation) PyPI (https://pypi.org/project/openllm/) ‚Äã (#launch-openllm-server-locally) ‚Äã (#wrapper) OpenLLM (https://api.python.langchain.com/en/latest/llms/langchain.llms.openllm.OpenLLM.html) ‚Äã (#optional-local-llm-inference) OpenLLM (https://api.python.langchain.com/en/latest/llms/langchain.llms.openllm.OpenLLM.html) ‚Äã (#integrate-with-a-llmchain) PreviousOpenAI (/docs/integrations/llms/openai) NextOpenLM (/docs/integrations/llms/openlm) Installation (#installation) Launch OpenLLM server locally (#launch-openllm-server-locally) Wrapper (#wrapper) Optional: Local LLM Inference (#optional-local-llm-inference) Integrate with a LLMChain (#integrate-with-a-llmchain) Discord (https://discord.gg/cU2adEyC7w) Twitter (https://twitter.com/LangChainAI) Python (https://github.com/hwchase17/langchain) JS/TS (https://github.com/hwchase17/langchainjs) Homepage (https://langchain.com) Blog (https://blog.langchain.dev)