LiteLLM is a library that simplifies calling Anthropic, Azure, Huggingface, Replicate, etc.  This notebook covers how to get started with using Langchain + the LiteLLM I/O library.  IntegrationsCallbacksChat modelsAnthropicAnthropic FunctionsAnyscaleAzureAzureML Chat Online EndpointBedrock ChatERNIE-Bot ChatGoogle Cloud Platform Vertex AI PaLMJinaChatKonkoüöÖ LiteLLMLlama APIOllamaOpenAIPromptLayer ChatOpenAIChat loadersDocument loadersDocument transformersLLMsMemoryRetrieversText embedding modelsAgents & ToolkitsToolsVector storesGrouped by provider Callbacks Chat modelsAnthropicAnthropic FunctionsAnyscaleAzureAzureML Chat Online EndpointBedrock ChatERNIE-Bot ChatGoogle Cloud Platform Vertex AI PaLMJinaChatKonkoüöÖ LiteLLMLlama APIOllamaOpenAIPromptLayer ChatOpenAI Anthropic Anthropic Functions Anyscale Azure AzureML Chat Online Endpoint Bedrock Chat ERNIE-Bot Chat Google Cloud Platform Vertex AI PaLM JinaChat Konko üöÖ LiteLLM Llama API Ollama OpenAI PromptLayer ChatOpenAI Chat loaders Document loaders Document transformers LLMs Memory Retrievers Text embedding models Agents & Toolkits Tools Vector stores Grouped by provider  Integrations Chat models üöÖ LiteLLM ChatLiteLLM ChatPromptTemplate SystemMessagePromptTemplate AIMessagePromptTemplate HumanMessagePromptTemplate AIMessage HumanMessage SystemMessage CallbackManager StreamingStdOutCallbackHandler ChatLiteLLM also supports async and streaming functionality: Discord Twitter Python JS/TS Homepage Blog Skip to main contentü¶úÔ∏èüîó LangChainDocsUse casesIntegrationsAPILangSmithJS/TS DocsCTRLKIntegrationsCallbacksChat modelsAnthropicAnthropic FunctionsAnyscaleAzureAzureML Chat Online EndpointBedrock ChatERNIE-Bot ChatGoogle Cloud Platform Vertex AI PaLMJinaChatKonkoüöÖ LiteLLMLlama APIOllamaOpenAIPromptLayer ChatOpenAIChat loadersDocument loadersDocument transformersLLMsMemoryRetrieversText embedding modelsAgents & ToolkitsToolsVector storesGrouped by providerIntegrationsChat modelsüöÖ LiteLLMOn this pageüöÖ LiteLLMLiteLLM is a library that simplifies calling Anthropic, Azure, Huggingface, Replicate, etc. This notebook covers how to get started with using Langchain + the LiteLLM I/O library. from langchain.chat_models import ChatLiteLLMfrom langchain.prompts.chat import (    ChatPromptTemplate,    SystemMessagePromptTemplate,    AIMessagePromptTemplate,    HumanMessagePromptTemplate,)from langchain.schema import AIMessage, HumanMessage, SystemMessageAPI Reference:ChatLiteLLMChatPromptTemplateSystemMessagePromptTemplateAIMessagePromptTemplateHumanMessagePromptTemplateAIMessageHumanMessageSystemMessagechat = ChatLiteLLM(model="gpt-3.5-turbo")messages = [    HumanMessage(        content="Translate this sentence from English to French. I love programming."    )]chat(messages)    AIMessage(content=" J'aime la programmation.", additional_kwargs={}, example=False)ChatLiteLLM also supports async and streaming functionality:‚Äãfrom langchain.callbacks.manager import CallbackManagerfrom langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandlerAPI Reference:CallbackManagerStreamingStdOutCallbackHandlerawait chat.agenerate([messages])    LLMResult(generations=[[ChatGeneration(text=" J'aime programmer.", generation_info=None, message=AIMessage(content=" J'aime programmer.", additional_kwargs={}, example=False))]], llm_output={}, run=[RunInfo(run_id=UUID('8cc8fb68-1c35-439c-96a0-695036a93652'))])chat = ChatLiteLLM(    streaming=True,    verbose=True,    callback_manager=CallbackManager([StreamingStdOutCallbackHandler()]),)chat(messages)     J'aime la programmation.    AIMessage(content=" J'aime la programmation.", additional_kwargs={}, example=False)PreviousKonkoNextLlama APIChatLiteLLM also supports async and streaming functionality:CommunityDiscordTwitterGitHubPythonJS/TSMoreHomepageBlogCopyright ¬© 2023 LangChain, Inc. Skip to main content ü¶úÔ∏èüîó LangChainDocsUse casesIntegrationsAPILangSmithJS/TS DocsCTRLK ü¶úÔ∏èüîó LangChainDocsUse casesIntegrationsAPI LangSmithJS/TS DocsCTRLK  CTRLK CTRLK  CTRLK   IntegrationsCallbacksChat modelsAnthropicAnthropic FunctionsAnyscaleAzureAzureML Chat Online EndpointBedrock ChatERNIE-Bot ChatGoogle Cloud Platform Vertex AI PaLMJinaChatKonkoüöÖ LiteLLMLlama APIOllamaOpenAIPromptLayer ChatOpenAIChat loadersDocument loadersDocument transformersLLMsMemoryRetrieversText embedding modelsAgents & ToolkitsToolsVector storesGrouped by providerIntegrationsChat modelsüöÖ LiteLLMOn this pageüöÖ LiteLLMLiteLLM is a library that simplifies calling Anthropic, Azure, Huggingface, Replicate, etc. This notebook covers how to get started with using Langchain + the LiteLLM I/O library. from langchain.chat_models import ChatLiteLLMfrom langchain.prompts.chat import (    ChatPromptTemplate,    SystemMessagePromptTemplate,    AIMessagePromptTemplate,    HumanMessagePromptTemplate,)from langchain.schema import AIMessage, HumanMessage, SystemMessageAPI Reference:ChatLiteLLMChatPromptTemplateSystemMessagePromptTemplateAIMessagePromptTemplateHumanMessagePromptTemplateAIMessageHumanMessageSystemMessagechat = ChatLiteLLM(model="gpt-3.5-turbo")messages = [    HumanMessage(        content="Translate this sentence from English to French. I love programming."    )]chat(messages)    AIMessage(content=" J'aime la programmation.", additional_kwargs={}, example=False)ChatLiteLLM also supports async and streaming functionality:‚Äãfrom langchain.callbacks.manager import CallbackManagerfrom langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandlerAPI Reference:CallbackManagerStreamingStdOutCallbackHandlerawait chat.agenerate([messages])    LLMResult(generations=[[ChatGeneration(text=" J'aime programmer.", generation_info=None, message=AIMessage(content=" J'aime programmer.", additional_kwargs={}, example=False))]], llm_output={}, run=[RunInfo(run_id=UUID('8cc8fb68-1c35-439c-96a0-695036a93652'))])chat = ChatLiteLLM(    streaming=True,    verbose=True,    callback_manager=CallbackManager([StreamingStdOutCallbackHandler()]),)chat(messages)     J'aime la programmation.    AIMessage(content=" J'aime la programmation.", additional_kwargs={}, example=False)PreviousKonkoNextLlama APIChatLiteLLM also supports async and streaming functionality: IntegrationsCallbacksChat modelsAnthropicAnthropic FunctionsAnyscaleAzureAzureML Chat Online EndpointBedrock ChatERNIE-Bot ChatGoogle Cloud Platform Vertex AI PaLMJinaChatKonkoüöÖ LiteLLMLlama APIOllamaOpenAIPromptLayer ChatOpenAIChat loadersDocument loadersDocument transformersLLMsMemoryRetrieversText embedding modelsAgents & ToolkitsToolsVector storesGrouped by providerIntegrationsChat modelsüöÖ LiteLLMOn this pageüöÖ LiteLLMLiteLLM is a library that simplifies calling Anthropic, Azure, Huggingface, Replicate, etc. This notebook covers how to get started with using Langchain + the LiteLLM I/O library. from langchain.chat_models import ChatLiteLLMfrom langchain.prompts.chat import (    ChatPromptTemplate,    SystemMessagePromptTemplate,    AIMessagePromptTemplate,    HumanMessagePromptTemplate,)from langchain.schema import AIMessage, HumanMessage, SystemMessageAPI Reference:ChatLiteLLMChatPromptTemplateSystemMessagePromptTemplateAIMessagePromptTemplateHumanMessagePromptTemplateAIMessageHumanMessageSystemMessagechat = ChatLiteLLM(model="gpt-3.5-turbo")messages = [    HumanMessage(        content="Translate this sentence from English to French. I love programming."    )]chat(messages)    AIMessage(content=" J'aime la programmation.", additional_kwargs={}, example=False)ChatLiteLLM also supports async and streaming functionality:‚Äãfrom langchain.callbacks.manager import CallbackManagerfrom langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandlerAPI Reference:CallbackManagerStreamingStdOutCallbackHandlerawait chat.agenerate([messages])    LLMResult(generations=[[ChatGeneration(text=" J'aime programmer.", generation_info=None, message=AIMessage(content=" J'aime programmer.", additional_kwargs={}, example=False))]], llm_output={}, run=[RunInfo(run_id=UUID('8cc8fb68-1c35-439c-96a0-695036a93652'))])chat = ChatLiteLLM(    streaming=True,    verbose=True,    callback_manager=CallbackManager([StreamingStdOutCallbackHandler()]),)chat(messages)     J'aime la programmation.    AIMessage(content=" J'aime la programmation.", additional_kwargs={}, example=False)PreviousKonkoNextLlama APIChatLiteLLM also supports async and streaming functionality: IntegrationsCallbacksChat modelsAnthropicAnthropic FunctionsAnyscaleAzureAzureML Chat Online EndpointBedrock ChatERNIE-Bot ChatGoogle Cloud Platform Vertex AI PaLMJinaChatKonkoüöÖ LiteLLMLlama APIOllamaOpenAIPromptLayer ChatOpenAIChat loadersDocument loadersDocument transformersLLMsMemoryRetrieversText embedding modelsAgents & ToolkitsToolsVector storesGrouped by provider IntegrationsCallbacksChat modelsAnthropicAnthropic FunctionsAnyscaleAzureAzureML Chat Online EndpointBedrock ChatERNIE-Bot ChatGoogle Cloud Platform Vertex AI PaLMJinaChatKonkoüöÖ LiteLLMLlama APIOllamaOpenAIPromptLayer ChatOpenAIChat loadersDocument loadersDocument transformersLLMsMemoryRetrieversText embedding modelsAgents & ToolkitsToolsVector storesGrouped by provider Integrations Callbacks Chat models Chat loaders Document loaders Document transformers LLMs Memory Retrievers Text embedding models Agents & Toolkits Tools Vector stores Grouped by provider IntegrationsChat modelsüöÖ LiteLLMOn this pageüöÖ LiteLLMLiteLLM is a library that simplifies calling Anthropic, Azure, Huggingface, Replicate, etc. This notebook covers how to get started with using Langchain + the LiteLLM I/O library. from langchain.chat_models import ChatLiteLLMfrom langchain.prompts.chat import (    ChatPromptTemplate,    SystemMessagePromptTemplate,    AIMessagePromptTemplate,    HumanMessagePromptTemplate,)from langchain.schema import AIMessage, HumanMessage, SystemMessageAPI Reference:ChatLiteLLMChatPromptTemplateSystemMessagePromptTemplateAIMessagePromptTemplateHumanMessagePromptTemplateAIMessageHumanMessageSystemMessagechat = ChatLiteLLM(model="gpt-3.5-turbo")messages = [    HumanMessage(        content="Translate this sentence from English to French. I love programming."    )]chat(messages)    AIMessage(content=" J'aime la programmation.", additional_kwargs={}, example=False)ChatLiteLLM also supports async and streaming functionality:‚Äãfrom langchain.callbacks.manager import CallbackManagerfrom langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandlerAPI Reference:CallbackManagerStreamingStdOutCallbackHandlerawait chat.agenerate([messages])    LLMResult(generations=[[ChatGeneration(text=" J'aime programmer.", generation_info=None, message=AIMessage(content=" J'aime programmer.", additional_kwargs={}, example=False))]], llm_output={}, run=[RunInfo(run_id=UUID('8cc8fb68-1c35-439c-96a0-695036a93652'))])chat = ChatLiteLLM(    streaming=True,    verbose=True,    callback_manager=CallbackManager([StreamingStdOutCallbackHandler()]),)chat(messages)     J'aime la programmation.    AIMessage(content=" J'aime la programmation.", additional_kwargs={}, example=False)PreviousKonkoNextLlama APIChatLiteLLM also supports async and streaming functionality: IntegrationsChat modelsüöÖ LiteLLMOn this pageüöÖ LiteLLMLiteLLM is a library that simplifies calling Anthropic, Azure, Huggingface, Replicate, etc. This notebook covers how to get started with using Langchain + the LiteLLM I/O library. from langchain.chat_models import ChatLiteLLMfrom langchain.prompts.chat import (    ChatPromptTemplate,    SystemMessagePromptTemplate,    AIMessagePromptTemplate,    HumanMessagePromptTemplate,)from langchain.schema import AIMessage, HumanMessage, SystemMessageAPI Reference:ChatLiteLLMChatPromptTemplateSystemMessagePromptTemplateAIMessagePromptTemplateHumanMessagePromptTemplateAIMessageHumanMessageSystemMessagechat = ChatLiteLLM(model="gpt-3.5-turbo")messages = [    HumanMessage(        content="Translate this sentence from English to French. I love programming."    )]chat(messages)    AIMessage(content=" J'aime la programmation.", additional_kwargs={}, example=False)ChatLiteLLM also supports async and streaming functionality:‚Äãfrom langchain.callbacks.manager import CallbackManagerfrom langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandlerAPI Reference:CallbackManagerStreamingStdOutCallbackHandlerawait chat.agenerate([messages])    LLMResult(generations=[[ChatGeneration(text=" J'aime programmer.", generation_info=None, message=AIMessage(content=" J'aime programmer.", additional_kwargs={}, example=False))]], llm_output={}, run=[RunInfo(run_id=UUID('8cc8fb68-1c35-439c-96a0-695036a93652'))])chat = ChatLiteLLM(    streaming=True,    verbose=True,    callback_manager=CallbackManager([StreamingStdOutCallbackHandler()]),)chat(messages)     J'aime la programmation.    AIMessage(content=" J'aime la programmation.", additional_kwargs={}, example=False)PreviousKonkoNextLlama APIChatLiteLLM also supports async and streaming functionality: IntegrationsChat modelsüöÖ LiteLLMOn this pageüöÖ LiteLLMLiteLLM is a library that simplifies calling Anthropic, Azure, Huggingface, Replicate, etc. This notebook covers how to get started with using Langchain + the LiteLLM I/O library. from langchain.chat_models import ChatLiteLLMfrom langchain.prompts.chat import (    ChatPromptTemplate,    SystemMessagePromptTemplate,    AIMessagePromptTemplate,    HumanMessagePromptTemplate,)from langchain.schema import AIMessage, HumanMessage, SystemMessageAPI Reference:ChatLiteLLMChatPromptTemplateSystemMessagePromptTemplateAIMessagePromptTemplateHumanMessagePromptTemplateAIMessageHumanMessageSystemMessagechat = ChatLiteLLM(model="gpt-3.5-turbo")messages = [    HumanMessage(        content="Translate this sentence from English to French. I love programming."    )]chat(messages)    AIMessage(content=" J'aime la programmation.", additional_kwargs={}, example=False)ChatLiteLLM also supports async and streaming functionality:‚Äãfrom langchain.callbacks.manager import CallbackManagerfrom langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandlerAPI Reference:CallbackManagerStreamingStdOutCallbackHandlerawait chat.agenerate([messages])    LLMResult(generations=[[ChatGeneration(text=" J'aime programmer.", generation_info=None, message=AIMessage(content=" J'aime programmer.", additional_kwargs={}, example=False))]], llm_output={}, run=[RunInfo(run_id=UUID('8cc8fb68-1c35-439c-96a0-695036a93652'))])chat = ChatLiteLLM(    streaming=True,    verbose=True,    callback_manager=CallbackManager([StreamingStdOutCallbackHandler()]),)chat(messages)     J'aime la programmation.    AIMessage(content=" J'aime la programmation.", additional_kwargs={}, example=False)PreviousKonkoNextLlama API IntegrationsChat modelsüöÖ LiteLLMOn this pageüöÖ LiteLLMLiteLLM is a library that simplifies calling Anthropic, Azure, Huggingface, Replicate, etc. This notebook covers how to get started with using Langchain + the LiteLLM I/O library. from langchain.chat_models import ChatLiteLLMfrom langchain.prompts.chat import (    ChatPromptTemplate,    SystemMessagePromptTemplate,    AIMessagePromptTemplate,    HumanMessagePromptTemplate,)from langchain.schema import AIMessage, HumanMessage, SystemMessageAPI Reference:ChatLiteLLMChatPromptTemplateSystemMessagePromptTemplateAIMessagePromptTemplateHumanMessagePromptTemplateAIMessageHumanMessageSystemMessagechat = ChatLiteLLM(model="gpt-3.5-turbo")messages = [    HumanMessage(        content="Translate this sentence from English to French. I love programming."    )]chat(messages)    AIMessage(content=" J'aime la programmation.", additional_kwargs={}, example=False)ChatLiteLLM also supports async and streaming functionality:‚Äãfrom langchain.callbacks.manager import CallbackManagerfrom langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandlerAPI Reference:CallbackManagerStreamingStdOutCallbackHandlerawait chat.agenerate([messages])    LLMResult(generations=[[ChatGeneration(text=" J'aime programmer.", generation_info=None, message=AIMessage(content=" J'aime programmer.", additional_kwargs={}, example=False))]], llm_output={}, run=[RunInfo(run_id=UUID('8cc8fb68-1c35-439c-96a0-695036a93652'))])chat = ChatLiteLLM(    streaming=True,    verbose=True,    callback_manager=CallbackManager([StreamingStdOutCallbackHandler()]),)chat(messages)     J'aime la programmation.    AIMessage(content=" J'aime la programmation.", additional_kwargs={}, example=False)PreviousKonkoNextLlama API On this page üöÖ LiteLLMLiteLLM is a library that simplifies calling Anthropic, Azure, Huggingface, Replicate, etc. This notebook covers how to get started with using Langchain + the LiteLLM I/O library. from langchain.chat_models import ChatLiteLLMfrom langchain.prompts.chat import (    ChatPromptTemplate,    SystemMessagePromptTemplate,    AIMessagePromptTemplate,    HumanMessagePromptTemplate,)from langchain.schema import AIMessage, HumanMessage, SystemMessageAPI Reference:ChatLiteLLMChatPromptTemplateSystemMessagePromptTemplateAIMessagePromptTemplateHumanMessagePromptTemplateAIMessageHumanMessageSystemMessagechat = ChatLiteLLM(model="gpt-3.5-turbo")messages = [    HumanMessage(        content="Translate this sentence from English to French. I love programming."    )]chat(messages)    AIMessage(content=" J'aime la programmation.", additional_kwargs={}, example=False)ChatLiteLLM also supports async and streaming functionality:‚Äãfrom langchain.callbacks.manager import CallbackManagerfrom langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandlerAPI Reference:CallbackManagerStreamingStdOutCallbackHandlerawait chat.agenerate([messages])    LLMResult(generations=[[ChatGeneration(text=" J'aime programmer.", generation_info=None, message=AIMessage(content=" J'aime programmer.", additional_kwargs={}, example=False))]], llm_output={}, run=[RunInfo(run_id=UUID('8cc8fb68-1c35-439c-96a0-695036a93652'))])chat = ChatLiteLLM(    streaming=True,    verbose=True,    callback_manager=CallbackManager([StreamingStdOutCallbackHandler()]),)chat(messages)     J'aime la programmation.    AIMessage(content=" J'aime la programmation.", additional_kwargs={}, example=False) from langchain.chat_models import ChatLiteLLMfrom langchain.prompts.chat import (    ChatPromptTemplate,    SystemMessagePromptTemplate,    AIMessagePromptTemplate,    HumanMessagePromptTemplate,)from langchain.schema import AIMessage, HumanMessage, SystemMessage from langchain.chat_models import ChatLiteLLMfrom langchain.prompts.chat import (    ChatPromptTemplate,    SystemMessagePromptTemplate,    AIMessagePromptTemplate,    HumanMessagePromptTemplate,)from langchain.schema import AIMessage, HumanMessage, SystemMessage  API Reference:ChatLiteLLMChatPromptTemplateSystemMessagePromptTemplateAIMessagePromptTemplateHumanMessagePromptTemplateAIMessageHumanMessageSystemMessage chat = ChatLiteLLM(model="gpt-3.5-turbo") chat = ChatLiteLLM(model="gpt-3.5-turbo")  messages = [    HumanMessage(        content="Translate this sentence from English to French. I love programming."    )]chat(messages) messages = [    HumanMessage(        content="Translate this sentence from English to French. I love programming."    )]chat(messages)      AIMessage(content=" J'aime la programmation.", additional_kwargs={}, example=False)     AIMessage(content=" J'aime la programmation.", additional_kwargs={}, example=False)     AIMessage(content=" J'aime la programmation.", additional_kwargs={}, example=False)  from langchain.callbacks.manager import CallbackManagerfrom langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler from langchain.callbacks.manager import CallbackManagerfrom langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler  API Reference:CallbackManagerStreamingStdOutCallbackHandler await chat.agenerate([messages]) await chat.agenerate([messages])      LLMResult(generations=[[ChatGeneration(text=" J'aime programmer.", generation_info=None, message=AIMessage(content=" J'aime programmer.", additional_kwargs={}, example=False))]], llm_output={}, run=[RunInfo(run_id=UUID('8cc8fb68-1c35-439c-96a0-695036a93652'))])     LLMResult(generations=[[ChatGeneration(text=" J'aime programmer.", generation_info=None, message=AIMessage(content=" J'aime programmer.", additional_kwargs={}, example=False))]], llm_output={}, run=[RunInfo(run_id=UUID('8cc8fb68-1c35-439c-96a0-695036a93652'))])     LLMResult(generations=[[ChatGeneration(text=" J'aime programmer.", generation_info=None, message=AIMessage(content=" J'aime programmer.", additional_kwargs={}, example=False))]], llm_output={}, run=[RunInfo(run_id=UUID('8cc8fb68-1c35-439c-96a0-695036a93652'))])  chat = ChatLiteLLM(    streaming=True,    verbose=True,    callback_manager=CallbackManager([StreamingStdOutCallbackHandler()]),)chat(messages) chat = ChatLiteLLM(    streaming=True,    verbose=True,    callback_manager=CallbackManager([StreamingStdOutCallbackHandler()]),)chat(messages)       J'aime la programmation.    AIMessage(content=" J'aime la programmation.", additional_kwargs={}, example=False)      J'aime la programmation.    AIMessage(content=" J'aime la programmation.", additional_kwargs={}, example=False)      J'aime la programmation.    AIMessage(content=" J'aime la programmation.", additional_kwargs={}, example=False)  Previous Konko Next Llama API ChatLiteLLM also supports async and streaming functionality: ChatLiteLLM also supports async and streaming functionality: CommunityDiscordTwitterGitHubPythonJS/TSMoreHomepageBlogCopyright ¬© 2023 LangChain, Inc. CommunityDiscordTwitterGitHubPythonJS/TSMoreHomepageBlog CommunityDiscordTwitter Community GitHubPythonJS/TS GitHub MoreHomepageBlog More Copyright ¬© 2023 LangChain, Inc. Copyright ¬© 2023 LangChain, Inc. Skip to main content (#docusaurus_skipToContent_fallback) ü¶úÔ∏èüîó LangChain (/) Docs (/docs/get_started/introduction) Use cases (/docs/use_cases) Integrations (/docs/integrations) API (https://api.python.langchain.com) LangSmith (https://smith.langchain.com) JS/TS Docs (https://js.langchain.com/docs)  (https://github.com/hwchase17/langchain) Integrations (/docs/integrations) Callbacks (/docs/integrations/callbacks/) Chat models (/docs/integrations/chat/) Anthropic (/docs/integrations/chat/anthropic) Anthropic Functions (/docs/integrations/chat/anthropic_functions) Anyscale (/docs/integrations/chat/anyscale) Azure (/docs/integrations/chat/azure_chat_openai) AzureML Chat Online Endpoint (/docs/integrations/chat/azureml_chat_endpoint) Bedrock Chat (/docs/integrations/chat/bedrock) ERNIE-Bot Chat (/docs/integrations/chat/ernie) Google Cloud Platform Vertex AI PaLM (/docs/integrations/chat/google_vertex_ai_palm) JinaChat (/docs/integrations/chat/jinachat) Konko (/docs/integrations/chat/konko) üöÖ LiteLLM (/docs/integrations/chat/litellm) Llama API (/docs/integrations/chat/llama_api) Ollama (/docs/integrations/chat/ollama) OpenAI (/docs/integrations/chat/openai) PromptLayer ChatOpenAI (/docs/integrations/chat/promptlayer_chatopenai) Chat loaders (/docs/integrations/chat_loaders/) Document loaders (/docs/integrations/document_loaders/) Document transformers (/docs/integrations/document_transformers/) LLMs (/docs/integrations/llms/) Memory (/docs/integrations/memory/) Retrievers (/docs/integrations/retrievers/) Text embedding models (/docs/integrations/text_embedding/) Agents & Toolkits (/docs/integrations/toolkits/) Tools (/docs/integrations/tools/) Vector stores (/docs/integrations/vectorstores/) Grouped by provider (/docs/integrations/providers/)  (/) Integrations (/docs/integrations) Chat models (/docs/integrations/chat/) LiteLLM (https://github.com/BerriAI/litellm) ChatLiteLLM (https://api.python.langchain.com/en/latest/chat_models/langchain.chat_models.litellm.ChatLiteLLM.html) ChatPromptTemplate (https://api.python.langchain.com/en/latest/prompts/langchain.prompts.chat.ChatPromptTemplate.html) SystemMessagePromptTemplate (https://api.python.langchain.com/en/latest/prompts/langchain.prompts.chat.SystemMessagePromptTemplate.html) AIMessagePromptTemplate (https://api.python.langchain.com/en/latest/prompts/langchain.prompts.chat.AIMessagePromptTemplate.html) HumanMessagePromptTemplate (https://api.python.langchain.com/en/latest/prompts/langchain.prompts.chat.HumanMessagePromptTemplate.html) AIMessage (https://api.python.langchain.com/en/latest/schema/langchain.schema.messages.AIMessage.html) HumanMessage (https://api.python.langchain.com/en/latest/schema/langchain.schema.messages.HumanMessage.html) SystemMessage (https://api.python.langchain.com/en/latest/schema/langchain.schema.messages.SystemMessage.html) ‚Äã (#chatlitellm-also-supports-async-and-streaming-functionality) CallbackManager (https://api.python.langchain.com/en/latest/callbacks/langchain.callbacks.manager.CallbackManager.html) StreamingStdOutCallbackHandler (https://api.python.langchain.com/en/latest/callbacks/langchain.callbacks.streaming_stdout.StreamingStdOutCallbackHandler.html) PreviousKonko (/docs/integrations/chat/konko) NextLlama API (/docs/integrations/chat/llama_api) ChatLiteLLM also supports async and streaming functionality: (#chatlitellm-also-supports-async-and-streaming-functionality) Discord (https://discord.gg/cU2adEyC7w) Twitter (https://twitter.com/LangChainAI) Python (https://github.com/hwchase17/langchain) JS/TS (https://github.com/hwchase17/langchainjs) Homepage (https://langchain.com) Blog (https://blog.langchain.dev)