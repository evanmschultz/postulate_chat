JSONFormer is a library that wraps local HuggingFace pipeline models for structured decoding of a subset of the JSON Schema. It works by filling in the structure tokens and then sampling the content tokens from the model. Warning - this module is still experimental First, let's establish a qualitative baseline by checking the output of the model without structured decoding. That's not so impressive, is it? It didn't follow the JSON format at all! Let's try with the structured decoder. Let's try that again, now providing a the Action input's JSON Schema to the model. Voila! Free of parsing errors. IntegrationsCallbacksChat modelsChat loadersDocument loadersDocument transformersLLMsAI21Aleph AlphaAmazon API GatewayAnyscaleAzure MLAzure OpenAIBananaBasetenBeamBedrockBittensorCerebriumAIChatGLMClarifaiCohereC TransformersCTranslate2DatabricksDeepInfraDeepSparseEden AIFireworksForefrontAIGoogle Vertex AI PaLMGooseAIGPT4AllHugging Face HubHugging Face Local PipelinesHuggingface TextGen InferenceJSONFormerKoboldAI APILlama.cppLLM Caching integrationsManifestMinimaxModalMosaicMLNLP CloudOctoAIOllamaOpaquePromptsOpenAIOpenLLMOpenLMPetalsPipelineAIPredibasePrediction GuardPromptLayer OpenAIRELLMReplicateRunhouseSageMakerEndpointStochasticAINebula (Symbl.ai)TextGenTitan TakeoffTongyi QwenvLLMWriterXorbits Inference (Xinference)MemoryRetrieversText embedding modelsAgents & ToolkitsToolsVector storesGrouped by provider Callbacks Chat models Chat loaders Document loaders Document transformers LLMsAI21Aleph AlphaAmazon API GatewayAnyscaleAzure MLAzure OpenAIBananaBasetenBeamBedrockBittensorCerebriumAIChatGLMClarifaiCohereC TransformersCTranslate2DatabricksDeepInfraDeepSparseEden AIFireworksForefrontAIGoogle Vertex AI PaLMGooseAIGPT4AllHugging Face HubHugging Face Local PipelinesHuggingface TextGen InferenceJSONFormerKoboldAI APILlama.cppLLM Caching integrationsManifestMinimaxModalMosaicMLNLP CloudOctoAIOllamaOpaquePromptsOpenAIOpenLLMOpenLMPetalsPipelineAIPredibasePrediction GuardPromptLayer OpenAIRELLMReplicateRunhouseSageMakerEndpointStochasticAINebula (Symbl.ai)TextGenTitan TakeoffTongyi QwenvLLMWriterXorbits Inference (Xinference) AI21 Aleph Alpha Amazon API Gateway Anyscale Azure ML Azure OpenAI Banana Baseten Beam Bedrock Bittensor CerebriumAI ChatGLM Clarifai Cohere C Transformers CTranslate2 Databricks DeepInfra DeepSparse Eden AI Fireworks ForefrontAI Google Vertex AI PaLM GooseAI GPT4All Hugging Face Hub Hugging Face Local Pipelines Huggingface TextGen Inference JSONFormer KoboldAI API Llama.cpp LLM Caching integrations Manifest Minimax Modal MosaicML NLP Cloud OctoAI Ollama OpaquePrompts OpenAI OpenLLM OpenLM Petals PipelineAI Predibase Prediction Guard PromptLayer OpenAI RELLM Replicate Runhouse SageMakerEndpoint StochasticAI Nebula (Symbl.ai) TextGen Titan Takeoff Tongyi Qwen vLLM Writer Xorbits Inference (Xinference) Memory Retrievers Text embedding models Agents & Toolkits Tools Vector stores Grouped by provider  Integrations LLMs JSONFormer tool HuggingFacePipeline HuggingFace Baseline JSONFormer LLM Wrapper Discord Twitter Python JS/TS Homepage Blog Skip to main content🦜️🔗 LangChainDocsUse casesIntegrationsAPILangSmithJS/TS DocsCTRLKIntegrationsCallbacksChat modelsChat loadersDocument loadersDocument transformersLLMsAI21Aleph AlphaAmazon API GatewayAnyscaleAzure MLAzure OpenAIBananaBasetenBeamBedrockBittensorCerebriumAIChatGLMClarifaiCohereC TransformersCTranslate2DatabricksDeepInfraDeepSparseEden AIFireworksForefrontAIGoogle Vertex AI PaLMGooseAIGPT4AllHugging Face HubHugging Face Local PipelinesHuggingface TextGen InferenceJSONFormerKoboldAI APILlama.cppLLM Caching integrationsManifestMinimaxModalMosaicMLNLP CloudOctoAIOllamaOpaquePromptsOpenAIOpenLLMOpenLMPetalsPipelineAIPredibasePrediction GuardPromptLayer OpenAIRELLMReplicateRunhouseSageMakerEndpointStochasticAINebula (Symbl.ai)TextGenTitan TakeoffTongyi QwenvLLMWriterXorbits Inference (Xinference)MemoryRetrieversText embedding modelsAgents & ToolkitsToolsVector storesGrouped by providerIntegrationsLLMsJSONFormerOn this pageJSONFormerJSONFormer is a library that wraps local HuggingFace pipeline models for structured decoding of a subset of the JSON Schema.It works by filling in the structure tokens and then sampling the content tokens from the model.Warning - this module is still experimentalpip install --upgrade jsonformer > /dev/nullHuggingFace Baseline​First, let's establish a qualitative baseline by checking the output of the model without structured decoding.import logginglogging.basicConfig(level=logging.ERROR)from typing import Optionalfrom langchain.tools import toolimport osimport jsonimport requestsHF_TOKEN = os.environ.get("HUGGINGFACE_API_KEY")@tooldef ask_star_coder(query: str, temperature: float = 1.0, max_new_tokens: float = 250):    """Query the BigCode StarCoder model about coding questions."""    url = "https://api-inference.huggingface.co/models/bigcode/starcoder"    headers = {        "Authorization": f"Bearer {HF_TOKEN}",        "content-type": "application/json",    }    payload = {        "inputs": f"{query}\n\nAnswer:",        "temperature": temperature,        "max_new_tokens": int(max_new_tokens),    }    response = requests.post(url, headers=headers, data=json.dumps(payload))    response.raise_for_status()    return json.loads(response.content.decode("utf-8"))API Reference:toolprompt = """You must respond using JSON format, with a single action and single action input.You may 'ask_star_coder' for help on coding problems.{arg_schema}EXAMPLES----Human: "So what's all this about a GIL?"AI Assistant:{{  "action": "ask_star_coder",  "action_input": {{"query": "What is a GIL?", "temperature": 0.0, "max_new_tokens": 100}}"}}Observation: "The GIL is python's Global Interpreter Lock"Human: "Could you please write a calculator program in LISP?"AI Assistant:{{  "action": "ask_star_coder",  "action_input": {{"query": "Write a calculator program in LISP", "temperature": 0.0, "max_new_tokens": 250}}}}Observation: "(defun add (x y) (+ x y))\n(defun sub (x y) (- x y ))"Human: "What's the difference between an SVM and an LLM?"AI Assistant:{{  "action": "ask_star_coder",  "action_input": {{"query": "What's the difference between SGD and an SVM?", "temperature": 1.0, "max_new_tokens": 250}}}}Observation: "SGD stands for stochastic gradient descent, while an SVM is a Support Vector Machine."BEGIN! Answer the Human's question as best as you are able.------Human: 'What's the difference between an iterator and an iterable?'AI Assistant:""".format(    arg_schema=ask_star_coder.args)from transformers import pipelinefrom langchain.llms import HuggingFacePipelinehf_model = pipeline(    "text-generation", model="cerebras/Cerebras-GPT-590M", max_new_tokens=200)original_model = HuggingFacePipeline(pipeline=hf_model)generated = original_model.predict(prompt, stop=["Observation:", "Human:"])print(generated)API Reference:HuggingFacePipeline    Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.     'What's the difference between an iterator and an iterable?'    That's not so impressive, is it? It didn't follow the JSON format at all! Let's try with the structured decoder.JSONFormer LLM Wrapper​Let's try that again, now providing a the Action input's JSON Schema to the model.decoder_schema = {    "title": "Decoding Schema",    "type": "object",    "properties": {        "action": {"type": "string", "default": ask_star_coder.name},        "action_input": {            "type": "object",            "properties": ask_star_coder.args,        },    },}from langchain_experimental.llms import JsonFormerjson_former = JsonFormer(json_schema=decoder_schema, pipeline=hf_model)results = json_former.predict(prompt, stop=["Observation:", "Human:"])print(results)    {"action": "ask_star_coder", "action_input": {"query": "What's the difference between an iterator and an iter", "temperature": 0.0, "max_new_tokens": 50.0}}Voila! Free of parsing errors.PreviousHuggingface TextGen InferenceNextKoboldAI APIHuggingFace BaselineJSONFormer LLM WrapperCommunityDiscordTwitterGitHubPythonJS/TSMoreHomepageBlogCopyright © 2023 LangChain, Inc. Skip to main content 🦜️🔗 LangChainDocsUse casesIntegrationsAPILangSmithJS/TS DocsCTRLK 🦜️🔗 LangChainDocsUse casesIntegrationsAPI LangSmithJS/TS DocsCTRLK  CTRLK CTRLK  CTRLK   IntegrationsCallbacksChat modelsChat loadersDocument loadersDocument transformersLLMsAI21Aleph AlphaAmazon API GatewayAnyscaleAzure MLAzure OpenAIBananaBasetenBeamBedrockBittensorCerebriumAIChatGLMClarifaiCohereC TransformersCTranslate2DatabricksDeepInfraDeepSparseEden AIFireworksForefrontAIGoogle Vertex AI PaLMGooseAIGPT4AllHugging Face HubHugging Face Local PipelinesHuggingface TextGen InferenceJSONFormerKoboldAI APILlama.cppLLM Caching integrationsManifestMinimaxModalMosaicMLNLP CloudOctoAIOllamaOpaquePromptsOpenAIOpenLLMOpenLMPetalsPipelineAIPredibasePrediction GuardPromptLayer OpenAIRELLMReplicateRunhouseSageMakerEndpointStochasticAINebula (Symbl.ai)TextGenTitan TakeoffTongyi QwenvLLMWriterXorbits Inference (Xinference)MemoryRetrieversText embedding modelsAgents & ToolkitsToolsVector storesGrouped by providerIntegrationsLLMsJSONFormerOn this pageJSONFormerJSONFormer is a library that wraps local HuggingFace pipeline models for structured decoding of a subset of the JSON Schema.It works by filling in the structure tokens and then sampling the content tokens from the model.Warning - this module is still experimentalpip install --upgrade jsonformer > /dev/nullHuggingFace Baseline​First, let's establish a qualitative baseline by checking the output of the model without structured decoding.import logginglogging.basicConfig(level=logging.ERROR)from typing import Optionalfrom langchain.tools import toolimport osimport jsonimport requestsHF_TOKEN = os.environ.get("HUGGINGFACE_API_KEY")@tooldef ask_star_coder(query: str, temperature: float = 1.0, max_new_tokens: float = 250):    """Query the BigCode StarCoder model about coding questions."""    url = "https://api-inference.huggingface.co/models/bigcode/starcoder"    headers = {        "Authorization": f"Bearer {HF_TOKEN}",        "content-type": "application/json",    }    payload = {        "inputs": f"{query}\n\nAnswer:",        "temperature": temperature,        "max_new_tokens": int(max_new_tokens),    }    response = requests.post(url, headers=headers, data=json.dumps(payload))    response.raise_for_status()    return json.loads(response.content.decode("utf-8"))API Reference:toolprompt = """You must respond using JSON format, with a single action and single action input.You may 'ask_star_coder' for help on coding problems.{arg_schema}EXAMPLES----Human: "So what's all this about a GIL?"AI Assistant:{{  "action": "ask_star_coder",  "action_input": {{"query": "What is a GIL?", "temperature": 0.0, "max_new_tokens": 100}}"}}Observation: "The GIL is python's Global Interpreter Lock"Human: "Could you please write a calculator program in LISP?"AI Assistant:{{  "action": "ask_star_coder",  "action_input": {{"query": "Write a calculator program in LISP", "temperature": 0.0, "max_new_tokens": 250}}}}Observation: "(defun add (x y) (+ x y))\n(defun sub (x y) (- x y ))"Human: "What's the difference between an SVM and an LLM?"AI Assistant:{{  "action": "ask_star_coder",  "action_input": {{"query": "What's the difference between SGD and an SVM?", "temperature": 1.0, "max_new_tokens": 250}}}}Observation: "SGD stands for stochastic gradient descent, while an SVM is a Support Vector Machine."BEGIN! Answer the Human's question as best as you are able.------Human: 'What's the difference between an iterator and an iterable?'AI Assistant:""".format(    arg_schema=ask_star_coder.args)from transformers import pipelinefrom langchain.llms import HuggingFacePipelinehf_model = pipeline(    "text-generation", model="cerebras/Cerebras-GPT-590M", max_new_tokens=200)original_model = HuggingFacePipeline(pipeline=hf_model)generated = original_model.predict(prompt, stop=["Observation:", "Human:"])print(generated)API Reference:HuggingFacePipeline    Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.     'What's the difference between an iterator and an iterable?'    That's not so impressive, is it? It didn't follow the JSON format at all! Let's try with the structured decoder.JSONFormer LLM Wrapper​Let's try that again, now providing a the Action input's JSON Schema to the model.decoder_schema = {    "title": "Decoding Schema",    "type": "object",    "properties": {        "action": {"type": "string", "default": ask_star_coder.name},        "action_input": {            "type": "object",            "properties": ask_star_coder.args,        },    },}from langchain_experimental.llms import JsonFormerjson_former = JsonFormer(json_schema=decoder_schema, pipeline=hf_model)results = json_former.predict(prompt, stop=["Observation:", "Human:"])print(results)    {"action": "ask_star_coder", "action_input": {"query": "What's the difference between an iterator and an iter", "temperature": 0.0, "max_new_tokens": 50.0}}Voila! Free of parsing errors.PreviousHuggingface TextGen InferenceNextKoboldAI APIHuggingFace BaselineJSONFormer LLM Wrapper IntegrationsCallbacksChat modelsChat loadersDocument loadersDocument transformersLLMsAI21Aleph AlphaAmazon API GatewayAnyscaleAzure MLAzure OpenAIBananaBasetenBeamBedrockBittensorCerebriumAIChatGLMClarifaiCohereC TransformersCTranslate2DatabricksDeepInfraDeepSparseEden AIFireworksForefrontAIGoogle Vertex AI PaLMGooseAIGPT4AllHugging Face HubHugging Face Local PipelinesHuggingface TextGen InferenceJSONFormerKoboldAI APILlama.cppLLM Caching integrationsManifestMinimaxModalMosaicMLNLP CloudOctoAIOllamaOpaquePromptsOpenAIOpenLLMOpenLMPetalsPipelineAIPredibasePrediction GuardPromptLayer OpenAIRELLMReplicateRunhouseSageMakerEndpointStochasticAINebula (Symbl.ai)TextGenTitan TakeoffTongyi QwenvLLMWriterXorbits Inference (Xinference)MemoryRetrieversText embedding modelsAgents & ToolkitsToolsVector storesGrouped by providerIntegrationsLLMsJSONFormerOn this pageJSONFormerJSONFormer is a library that wraps local HuggingFace pipeline models for structured decoding of a subset of the JSON Schema.It works by filling in the structure tokens and then sampling the content tokens from the model.Warning - this module is still experimentalpip install --upgrade jsonformer > /dev/nullHuggingFace Baseline​First, let's establish a qualitative baseline by checking the output of the model without structured decoding.import logginglogging.basicConfig(level=logging.ERROR)from typing import Optionalfrom langchain.tools import toolimport osimport jsonimport requestsHF_TOKEN = os.environ.get("HUGGINGFACE_API_KEY")@tooldef ask_star_coder(query: str, temperature: float = 1.0, max_new_tokens: float = 250):    """Query the BigCode StarCoder model about coding questions."""    url = "https://api-inference.huggingface.co/models/bigcode/starcoder"    headers = {        "Authorization": f"Bearer {HF_TOKEN}",        "content-type": "application/json",    }    payload = {        "inputs": f"{query}\n\nAnswer:",        "temperature": temperature,        "max_new_tokens": int(max_new_tokens),    }    response = requests.post(url, headers=headers, data=json.dumps(payload))    response.raise_for_status()    return json.loads(response.content.decode("utf-8"))API Reference:toolprompt = """You must respond using JSON format, with a single action and single action input.You may 'ask_star_coder' for help on coding problems.{arg_schema}EXAMPLES----Human: "So what's all this about a GIL?"AI Assistant:{{  "action": "ask_star_coder",  "action_input": {{"query": "What is a GIL?", "temperature": 0.0, "max_new_tokens": 100}}"}}Observation: "The GIL is python's Global Interpreter Lock"Human: "Could you please write a calculator program in LISP?"AI Assistant:{{  "action": "ask_star_coder",  "action_input": {{"query": "Write a calculator program in LISP", "temperature": 0.0, "max_new_tokens": 250}}}}Observation: "(defun add (x y) (+ x y))\n(defun sub (x y) (- x y ))"Human: "What's the difference between an SVM and an LLM?"AI Assistant:{{  "action": "ask_star_coder",  "action_input": {{"query": "What's the difference between SGD and an SVM?", "temperature": 1.0, "max_new_tokens": 250}}}}Observation: "SGD stands for stochastic gradient descent, while an SVM is a Support Vector Machine."BEGIN! Answer the Human's question as best as you are able.------Human: 'What's the difference between an iterator and an iterable?'AI Assistant:""".format(    arg_schema=ask_star_coder.args)from transformers import pipelinefrom langchain.llms import HuggingFacePipelinehf_model = pipeline(    "text-generation", model="cerebras/Cerebras-GPT-590M", max_new_tokens=200)original_model = HuggingFacePipeline(pipeline=hf_model)generated = original_model.predict(prompt, stop=["Observation:", "Human:"])print(generated)API Reference:HuggingFacePipeline    Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.     'What's the difference between an iterator and an iterable?'    That's not so impressive, is it? It didn't follow the JSON format at all! Let's try with the structured decoder.JSONFormer LLM Wrapper​Let's try that again, now providing a the Action input's JSON Schema to the model.decoder_schema = {    "title": "Decoding Schema",    "type": "object",    "properties": {        "action": {"type": "string", "default": ask_star_coder.name},        "action_input": {            "type": "object",            "properties": ask_star_coder.args,        },    },}from langchain_experimental.llms import JsonFormerjson_former = JsonFormer(json_schema=decoder_schema, pipeline=hf_model)results = json_former.predict(prompt, stop=["Observation:", "Human:"])print(results)    {"action": "ask_star_coder", "action_input": {"query": "What's the difference between an iterator and an iter", "temperature": 0.0, "max_new_tokens": 50.0}}Voila! Free of parsing errors.PreviousHuggingface TextGen InferenceNextKoboldAI APIHuggingFace BaselineJSONFormer LLM Wrapper IntegrationsCallbacksChat modelsChat loadersDocument loadersDocument transformersLLMsAI21Aleph AlphaAmazon API GatewayAnyscaleAzure MLAzure OpenAIBananaBasetenBeamBedrockBittensorCerebriumAIChatGLMClarifaiCohereC TransformersCTranslate2DatabricksDeepInfraDeepSparseEden AIFireworksForefrontAIGoogle Vertex AI PaLMGooseAIGPT4AllHugging Face HubHugging Face Local PipelinesHuggingface TextGen InferenceJSONFormerKoboldAI APILlama.cppLLM Caching integrationsManifestMinimaxModalMosaicMLNLP CloudOctoAIOllamaOpaquePromptsOpenAIOpenLLMOpenLMPetalsPipelineAIPredibasePrediction GuardPromptLayer OpenAIRELLMReplicateRunhouseSageMakerEndpointStochasticAINebula (Symbl.ai)TextGenTitan TakeoffTongyi QwenvLLMWriterXorbits Inference (Xinference)MemoryRetrieversText embedding modelsAgents & ToolkitsToolsVector storesGrouped by provider IntegrationsCallbacksChat modelsChat loadersDocument loadersDocument transformersLLMsAI21Aleph AlphaAmazon API GatewayAnyscaleAzure MLAzure OpenAIBananaBasetenBeamBedrockBittensorCerebriumAIChatGLMClarifaiCohereC TransformersCTranslate2DatabricksDeepInfraDeepSparseEden AIFireworksForefrontAIGoogle Vertex AI PaLMGooseAIGPT4AllHugging Face HubHugging Face Local PipelinesHuggingface TextGen InferenceJSONFormerKoboldAI APILlama.cppLLM Caching integrationsManifestMinimaxModalMosaicMLNLP CloudOctoAIOllamaOpaquePromptsOpenAIOpenLLMOpenLMPetalsPipelineAIPredibasePrediction GuardPromptLayer OpenAIRELLMReplicateRunhouseSageMakerEndpointStochasticAINebula (Symbl.ai)TextGenTitan TakeoffTongyi QwenvLLMWriterXorbits Inference (Xinference)MemoryRetrieversText embedding modelsAgents & ToolkitsToolsVector storesGrouped by provider Integrations Callbacks Chat models Chat loaders Document loaders Document transformers LLMs Memory Retrievers Text embedding models Agents & Toolkits Tools Vector stores Grouped by provider IntegrationsLLMsJSONFormerOn this pageJSONFormerJSONFormer is a library that wraps local HuggingFace pipeline models for structured decoding of a subset of the JSON Schema.It works by filling in the structure tokens and then sampling the content tokens from the model.Warning - this module is still experimentalpip install --upgrade jsonformer > /dev/nullHuggingFace Baseline​First, let's establish a qualitative baseline by checking the output of the model without structured decoding.import logginglogging.basicConfig(level=logging.ERROR)from typing import Optionalfrom langchain.tools import toolimport osimport jsonimport requestsHF_TOKEN = os.environ.get("HUGGINGFACE_API_KEY")@tooldef ask_star_coder(query: str, temperature: float = 1.0, max_new_tokens: float = 250):    """Query the BigCode StarCoder model about coding questions."""    url = "https://api-inference.huggingface.co/models/bigcode/starcoder"    headers = {        "Authorization": f"Bearer {HF_TOKEN}",        "content-type": "application/json",    }    payload = {        "inputs": f"{query}\n\nAnswer:",        "temperature": temperature,        "max_new_tokens": int(max_new_tokens),    }    response = requests.post(url, headers=headers, data=json.dumps(payload))    response.raise_for_status()    return json.loads(response.content.decode("utf-8"))API Reference:toolprompt = """You must respond using JSON format, with a single action and single action input.You may 'ask_star_coder' for help on coding problems.{arg_schema}EXAMPLES----Human: "So what's all this about a GIL?"AI Assistant:{{  "action": "ask_star_coder",  "action_input": {{"query": "What is a GIL?", "temperature": 0.0, "max_new_tokens": 100}}"}}Observation: "The GIL is python's Global Interpreter Lock"Human: "Could you please write a calculator program in LISP?"AI Assistant:{{  "action": "ask_star_coder",  "action_input": {{"query": "Write a calculator program in LISP", "temperature": 0.0, "max_new_tokens": 250}}}}Observation: "(defun add (x y) (+ x y))\n(defun sub (x y) (- x y ))"Human: "What's the difference between an SVM and an LLM?"AI Assistant:{{  "action": "ask_star_coder",  "action_input": {{"query": "What's the difference between SGD and an SVM?", "temperature": 1.0, "max_new_tokens": 250}}}}Observation: "SGD stands for stochastic gradient descent, while an SVM is a Support Vector Machine."BEGIN! Answer the Human's question as best as you are able.------Human: 'What's the difference between an iterator and an iterable?'AI Assistant:""".format(    arg_schema=ask_star_coder.args)from transformers import pipelinefrom langchain.llms import HuggingFacePipelinehf_model = pipeline(    "text-generation", model="cerebras/Cerebras-GPT-590M", max_new_tokens=200)original_model = HuggingFacePipeline(pipeline=hf_model)generated = original_model.predict(prompt, stop=["Observation:", "Human:"])print(generated)API Reference:HuggingFacePipeline    Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.     'What's the difference between an iterator and an iterable?'    That's not so impressive, is it? It didn't follow the JSON format at all! Let's try with the structured decoder.JSONFormer LLM Wrapper​Let's try that again, now providing a the Action input's JSON Schema to the model.decoder_schema = {    "title": "Decoding Schema",    "type": "object",    "properties": {        "action": {"type": "string", "default": ask_star_coder.name},        "action_input": {            "type": "object",            "properties": ask_star_coder.args,        },    },}from langchain_experimental.llms import JsonFormerjson_former = JsonFormer(json_schema=decoder_schema, pipeline=hf_model)results = json_former.predict(prompt, stop=["Observation:", "Human:"])print(results)    {"action": "ask_star_coder", "action_input": {"query": "What's the difference between an iterator and an iter", "temperature": 0.0, "max_new_tokens": 50.0}}Voila! Free of parsing errors.PreviousHuggingface TextGen InferenceNextKoboldAI APIHuggingFace BaselineJSONFormer LLM Wrapper IntegrationsLLMsJSONFormerOn this pageJSONFormerJSONFormer is a library that wraps local HuggingFace pipeline models for structured decoding of a subset of the JSON Schema.It works by filling in the structure tokens and then sampling the content tokens from the model.Warning - this module is still experimentalpip install --upgrade jsonformer > /dev/nullHuggingFace Baseline​First, let's establish a qualitative baseline by checking the output of the model without structured decoding.import logginglogging.basicConfig(level=logging.ERROR)from typing import Optionalfrom langchain.tools import toolimport osimport jsonimport requestsHF_TOKEN = os.environ.get("HUGGINGFACE_API_KEY")@tooldef ask_star_coder(query: str, temperature: float = 1.0, max_new_tokens: float = 250):    """Query the BigCode StarCoder model about coding questions."""    url = "https://api-inference.huggingface.co/models/bigcode/starcoder"    headers = {        "Authorization": f"Bearer {HF_TOKEN}",        "content-type": "application/json",    }    payload = {        "inputs": f"{query}\n\nAnswer:",        "temperature": temperature,        "max_new_tokens": int(max_new_tokens),    }    response = requests.post(url, headers=headers, data=json.dumps(payload))    response.raise_for_status()    return json.loads(response.content.decode("utf-8"))API Reference:toolprompt = """You must respond using JSON format, with a single action and single action input.You may 'ask_star_coder' for help on coding problems.{arg_schema}EXAMPLES----Human: "So what's all this about a GIL?"AI Assistant:{{  "action": "ask_star_coder",  "action_input": {{"query": "What is a GIL?", "temperature": 0.0, "max_new_tokens": 100}}"}}Observation: "The GIL is python's Global Interpreter Lock"Human: "Could you please write a calculator program in LISP?"AI Assistant:{{  "action": "ask_star_coder",  "action_input": {{"query": "Write a calculator program in LISP", "temperature": 0.0, "max_new_tokens": 250}}}}Observation: "(defun add (x y) (+ x y))\n(defun sub (x y) (- x y ))"Human: "What's the difference between an SVM and an LLM?"AI Assistant:{{  "action": "ask_star_coder",  "action_input": {{"query": "What's the difference between SGD and an SVM?", "temperature": 1.0, "max_new_tokens": 250}}}}Observation: "SGD stands for stochastic gradient descent, while an SVM is a Support Vector Machine."BEGIN! Answer the Human's question as best as you are able.------Human: 'What's the difference between an iterator and an iterable?'AI Assistant:""".format(    arg_schema=ask_star_coder.args)from transformers import pipelinefrom langchain.llms import HuggingFacePipelinehf_model = pipeline(    "text-generation", model="cerebras/Cerebras-GPT-590M", max_new_tokens=200)original_model = HuggingFacePipeline(pipeline=hf_model)generated = original_model.predict(prompt, stop=["Observation:", "Human:"])print(generated)API Reference:HuggingFacePipeline    Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.     'What's the difference between an iterator and an iterable?'    That's not so impressive, is it? It didn't follow the JSON format at all! Let's try with the structured decoder.JSONFormer LLM Wrapper​Let's try that again, now providing a the Action input's JSON Schema to the model.decoder_schema = {    "title": "Decoding Schema",    "type": "object",    "properties": {        "action": {"type": "string", "default": ask_star_coder.name},        "action_input": {            "type": "object",            "properties": ask_star_coder.args,        },    },}from langchain_experimental.llms import JsonFormerjson_former = JsonFormer(json_schema=decoder_schema, pipeline=hf_model)results = json_former.predict(prompt, stop=["Observation:", "Human:"])print(results)    {"action": "ask_star_coder", "action_input": {"query": "What's the difference between an iterator and an iter", "temperature": 0.0, "max_new_tokens": 50.0}}Voila! Free of parsing errors.PreviousHuggingface TextGen InferenceNextKoboldAI APIHuggingFace BaselineJSONFormer LLM Wrapper IntegrationsLLMsJSONFormerOn this pageJSONFormerJSONFormer is a library that wraps local HuggingFace pipeline models for structured decoding of a subset of the JSON Schema.It works by filling in the structure tokens and then sampling the content tokens from the model.Warning - this module is still experimentalpip install --upgrade jsonformer > /dev/nullHuggingFace Baseline​First, let's establish a qualitative baseline by checking the output of the model without structured decoding.import logginglogging.basicConfig(level=logging.ERROR)from typing import Optionalfrom langchain.tools import toolimport osimport jsonimport requestsHF_TOKEN = os.environ.get("HUGGINGFACE_API_KEY")@tooldef ask_star_coder(query: str, temperature: float = 1.0, max_new_tokens: float = 250):    """Query the BigCode StarCoder model about coding questions."""    url = "https://api-inference.huggingface.co/models/bigcode/starcoder"    headers = {        "Authorization": f"Bearer {HF_TOKEN}",        "content-type": "application/json",    }    payload = {        "inputs": f"{query}\n\nAnswer:",        "temperature": temperature,        "max_new_tokens": int(max_new_tokens),    }    response = requests.post(url, headers=headers, data=json.dumps(payload))    response.raise_for_status()    return json.loads(response.content.decode("utf-8"))API Reference:toolprompt = """You must respond using JSON format, with a single action and single action input.You may 'ask_star_coder' for help on coding problems.{arg_schema}EXAMPLES----Human: "So what's all this about a GIL?"AI Assistant:{{  "action": "ask_star_coder",  "action_input": {{"query": "What is a GIL?", "temperature": 0.0, "max_new_tokens": 100}}"}}Observation: "The GIL is python's Global Interpreter Lock"Human: "Could you please write a calculator program in LISP?"AI Assistant:{{  "action": "ask_star_coder",  "action_input": {{"query": "Write a calculator program in LISP", "temperature": 0.0, "max_new_tokens": 250}}}}Observation: "(defun add (x y) (+ x y))\n(defun sub (x y) (- x y ))"Human: "What's the difference between an SVM and an LLM?"AI Assistant:{{  "action": "ask_star_coder",  "action_input": {{"query": "What's the difference between SGD and an SVM?", "temperature": 1.0, "max_new_tokens": 250}}}}Observation: "SGD stands for stochastic gradient descent, while an SVM is a Support Vector Machine."BEGIN! Answer the Human's question as best as you are able.------Human: 'What's the difference between an iterator and an iterable?'AI Assistant:""".format(    arg_schema=ask_star_coder.args)from transformers import pipelinefrom langchain.llms import HuggingFacePipelinehf_model = pipeline(    "text-generation", model="cerebras/Cerebras-GPT-590M", max_new_tokens=200)original_model = HuggingFacePipeline(pipeline=hf_model)generated = original_model.predict(prompt, stop=["Observation:", "Human:"])print(generated)API Reference:HuggingFacePipeline    Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.     'What's the difference between an iterator and an iterable?'    That's not so impressive, is it? It didn't follow the JSON format at all! Let's try with the structured decoder.JSONFormer LLM Wrapper​Let's try that again, now providing a the Action input's JSON Schema to the model.decoder_schema = {    "title": "Decoding Schema",    "type": "object",    "properties": {        "action": {"type": "string", "default": ask_star_coder.name},        "action_input": {            "type": "object",            "properties": ask_star_coder.args,        },    },}from langchain_experimental.llms import JsonFormerjson_former = JsonFormer(json_schema=decoder_schema, pipeline=hf_model)results = json_former.predict(prompt, stop=["Observation:", "Human:"])print(results)    {"action": "ask_star_coder", "action_input": {"query": "What's the difference between an iterator and an iter", "temperature": 0.0, "max_new_tokens": 50.0}}Voila! Free of parsing errors.PreviousHuggingface TextGen InferenceNextKoboldAI API IntegrationsLLMsJSONFormerOn this pageJSONFormerJSONFormer is a library that wraps local HuggingFace pipeline models for structured decoding of a subset of the JSON Schema.It works by filling in the structure tokens and then sampling the content tokens from the model.Warning - this module is still experimentalpip install --upgrade jsonformer > /dev/nullHuggingFace Baseline​First, let's establish a qualitative baseline by checking the output of the model without structured decoding.import logginglogging.basicConfig(level=logging.ERROR)from typing import Optionalfrom langchain.tools import toolimport osimport jsonimport requestsHF_TOKEN = os.environ.get("HUGGINGFACE_API_KEY")@tooldef ask_star_coder(query: str, temperature: float = 1.0, max_new_tokens: float = 250):    """Query the BigCode StarCoder model about coding questions."""    url = "https://api-inference.huggingface.co/models/bigcode/starcoder"    headers = {        "Authorization": f"Bearer {HF_TOKEN}",        "content-type": "application/json",    }    payload = {        "inputs": f"{query}\n\nAnswer:",        "temperature": temperature,        "max_new_tokens": int(max_new_tokens),    }    response = requests.post(url, headers=headers, data=json.dumps(payload))    response.raise_for_status()    return json.loads(response.content.decode("utf-8"))API Reference:toolprompt = """You must respond using JSON format, with a single action and single action input.You may 'ask_star_coder' for help on coding problems.{arg_schema}EXAMPLES----Human: "So what's all this about a GIL?"AI Assistant:{{  "action": "ask_star_coder",  "action_input": {{"query": "What is a GIL?", "temperature": 0.0, "max_new_tokens": 100}}"}}Observation: "The GIL is python's Global Interpreter Lock"Human: "Could you please write a calculator program in LISP?"AI Assistant:{{  "action": "ask_star_coder",  "action_input": {{"query": "Write a calculator program in LISP", "temperature": 0.0, "max_new_tokens": 250}}}}Observation: "(defun add (x y) (+ x y))\n(defun sub (x y) (- x y ))"Human: "What's the difference between an SVM and an LLM?"AI Assistant:{{  "action": "ask_star_coder",  "action_input": {{"query": "What's the difference between SGD and an SVM?", "temperature": 1.0, "max_new_tokens": 250}}}}Observation: "SGD stands for stochastic gradient descent, while an SVM is a Support Vector Machine."BEGIN! Answer the Human's question as best as you are able.------Human: 'What's the difference between an iterator and an iterable?'AI Assistant:""".format(    arg_schema=ask_star_coder.args)from transformers import pipelinefrom langchain.llms import HuggingFacePipelinehf_model = pipeline(    "text-generation", model="cerebras/Cerebras-GPT-590M", max_new_tokens=200)original_model = HuggingFacePipeline(pipeline=hf_model)generated = original_model.predict(prompt, stop=["Observation:", "Human:"])print(generated)API Reference:HuggingFacePipeline    Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.     'What's the difference between an iterator and an iterable?'    That's not so impressive, is it? It didn't follow the JSON format at all! Let's try with the structured decoder.JSONFormer LLM Wrapper​Let's try that again, now providing a the Action input's JSON Schema to the model.decoder_schema = {    "title": "Decoding Schema",    "type": "object",    "properties": {        "action": {"type": "string", "default": ask_star_coder.name},        "action_input": {            "type": "object",            "properties": ask_star_coder.args,        },    },}from langchain_experimental.llms import JsonFormerjson_former = JsonFormer(json_schema=decoder_schema, pipeline=hf_model)results = json_former.predict(prompt, stop=["Observation:", "Human:"])print(results)    {"action": "ask_star_coder", "action_input": {"query": "What's the difference between an iterator and an iter", "temperature": 0.0, "max_new_tokens": 50.0}}Voila! Free of parsing errors.PreviousHuggingface TextGen InferenceNextKoboldAI API On this page JSONFormerJSONFormer is a library that wraps local HuggingFace pipeline models for structured decoding of a subset of the JSON Schema.It works by filling in the structure tokens and then sampling the content tokens from the model.Warning - this module is still experimentalpip install --upgrade jsonformer > /dev/nullHuggingFace Baseline​First, let's establish a qualitative baseline by checking the output of the model without structured decoding.import logginglogging.basicConfig(level=logging.ERROR)from typing import Optionalfrom langchain.tools import toolimport osimport jsonimport requestsHF_TOKEN = os.environ.get("HUGGINGFACE_API_KEY")@tooldef ask_star_coder(query: str, temperature: float = 1.0, max_new_tokens: float = 250):    """Query the BigCode StarCoder model about coding questions."""    url = "https://api-inference.huggingface.co/models/bigcode/starcoder"    headers = {        "Authorization": f"Bearer {HF_TOKEN}",        "content-type": "application/json",    }    payload = {        "inputs": f"{query}\n\nAnswer:",        "temperature": temperature,        "max_new_tokens": int(max_new_tokens),    }    response = requests.post(url, headers=headers, data=json.dumps(payload))    response.raise_for_status()    return json.loads(response.content.decode("utf-8"))API Reference:toolprompt = """You must respond using JSON format, with a single action and single action input.You may 'ask_star_coder' for help on coding problems.{arg_schema}EXAMPLES----Human: "So what's all this about a GIL?"AI Assistant:{{  "action": "ask_star_coder",  "action_input": {{"query": "What is a GIL?", "temperature": 0.0, "max_new_tokens": 100}}"}}Observation: "The GIL is python's Global Interpreter Lock"Human: "Could you please write a calculator program in LISP?"AI Assistant:{{  "action": "ask_star_coder",  "action_input": {{"query": "Write a calculator program in LISP", "temperature": 0.0, "max_new_tokens": 250}}}}Observation: "(defun add (x y) (+ x y))\n(defun sub (x y) (- x y ))"Human: "What's the difference between an SVM and an LLM?"AI Assistant:{{  "action": "ask_star_coder",  "action_input": {{"query": "What's the difference between SGD and an SVM?", "temperature": 1.0, "max_new_tokens": 250}}}}Observation: "SGD stands for stochastic gradient descent, while an SVM is a Support Vector Machine."BEGIN! Answer the Human's question as best as you are able.------Human: 'What's the difference between an iterator and an iterable?'AI Assistant:""".format(    arg_schema=ask_star_coder.args)from transformers import pipelinefrom langchain.llms import HuggingFacePipelinehf_model = pipeline(    "text-generation", model="cerebras/Cerebras-GPT-590M", max_new_tokens=200)original_model = HuggingFacePipeline(pipeline=hf_model)generated = original_model.predict(prompt, stop=["Observation:", "Human:"])print(generated)API Reference:HuggingFacePipeline    Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.     'What's the difference between an iterator and an iterable?'    That's not so impressive, is it? It didn't follow the JSON format at all! Let's try with the structured decoder.JSONFormer LLM Wrapper​Let's try that again, now providing a the Action input's JSON Schema to the model.decoder_schema = {    "title": "Decoding Schema",    "type": "object",    "properties": {        "action": {"type": "string", "default": ask_star_coder.name},        "action_input": {            "type": "object",            "properties": ask_star_coder.args,        },    },}from langchain_experimental.llms import JsonFormerjson_former = JsonFormer(json_schema=decoder_schema, pipeline=hf_model)results = json_former.predict(prompt, stop=["Observation:", "Human:"])print(results)    {"action": "ask_star_coder", "action_input": {"query": "What's the difference between an iterator and an iter", "temperature": 0.0, "max_new_tokens": 50.0}}Voila! Free of parsing errors. pip install --upgrade jsonformer > /dev/null pip install --upgrade jsonformer > /dev/null  import logginglogging.basicConfig(level=logging.ERROR) import logginglogging.basicConfig(level=logging.ERROR)  from typing import Optionalfrom langchain.tools import toolimport osimport jsonimport requestsHF_TOKEN = os.environ.get("HUGGINGFACE_API_KEY")@tooldef ask_star_coder(query: str, temperature: float = 1.0, max_new_tokens: float = 250):    """Query the BigCode StarCoder model about coding questions."""    url = "https://api-inference.huggingface.co/models/bigcode/starcoder"    headers = {        "Authorization": f"Bearer {HF_TOKEN}",        "content-type": "application/json",    }    payload = {        "inputs": f"{query}\n\nAnswer:",        "temperature": temperature,        "max_new_tokens": int(max_new_tokens),    }    response = requests.post(url, headers=headers, data=json.dumps(payload))    response.raise_for_status()    return json.loads(response.content.decode("utf-8")) from typing import Optionalfrom langchain.tools import toolimport osimport jsonimport requestsHF_TOKEN = os.environ.get("HUGGINGFACE_API_KEY")@tooldef ask_star_coder(query: str, temperature: float = 1.0, max_new_tokens: float = 250):    """Query the BigCode StarCoder model about coding questions."""    url = "https://api-inference.huggingface.co/models/bigcode/starcoder"    headers = {        "Authorization": f"Bearer {HF_TOKEN}",        "content-type": "application/json",    }    payload = {        "inputs": f"{query}\n\nAnswer:",        "temperature": temperature,        "max_new_tokens": int(max_new_tokens),    }    response = requests.post(url, headers=headers, data=json.dumps(payload))    response.raise_for_status()    return json.loads(response.content.decode("utf-8"))  API Reference:tool prompt = """You must respond using JSON format, with a single action and single action input.You may 'ask_star_coder' for help on coding problems.{arg_schema}EXAMPLES----Human: "So what's all this about a GIL?"AI Assistant:{{  "action": "ask_star_coder",  "action_input": {{"query": "What is a GIL?", "temperature": 0.0, "max_new_tokens": 100}}"}}Observation: "The GIL is python's Global Interpreter Lock"Human: "Could you please write a calculator program in LISP?"AI Assistant:{{  "action": "ask_star_coder",  "action_input": {{"query": "Write a calculator program in LISP", "temperature": 0.0, "max_new_tokens": 250}}}}Observation: "(defun add (x y) (+ x y))\n(defun sub (x y) (- x y ))"Human: "What's the difference between an SVM and an LLM?"AI Assistant:{{  "action": "ask_star_coder",  "action_input": {{"query": "What's the difference between SGD and an SVM?", "temperature": 1.0, "max_new_tokens": 250}}}}Observation: "SGD stands for stochastic gradient descent, while an SVM is a Support Vector Machine."BEGIN! Answer the Human's question as best as you are able.------Human: 'What's the difference between an iterator and an iterable?'AI Assistant:""".format(    arg_schema=ask_star_coder.args) prompt = """You must respond using JSON format, with a single action and single action input.You may 'ask_star_coder' for help on coding problems.{arg_schema}EXAMPLES----Human: "So what's all this about a GIL?"AI Assistant:{{  "action": "ask_star_coder",  "action_input": {{"query": "What is a GIL?", "temperature": 0.0, "max_new_tokens": 100}}"}}Observation: "The GIL is python's Global Interpreter Lock"Human: "Could you please write a calculator program in LISP?"AI Assistant:{{  "action": "ask_star_coder",  "action_input": {{"query": "Write a calculator program in LISP", "temperature": 0.0, "max_new_tokens": 250}}}}Observation: "(defun add (x y) (+ x y))\n(defun sub (x y) (- x y ))"Human: "What's the difference between an SVM and an LLM?"AI Assistant:{{  "action": "ask_star_coder",  "action_input": {{"query": "What's the difference between SGD and an SVM?", "temperature": 1.0, "max_new_tokens": 250}}}}Observation: "SGD stands for stochastic gradient descent, while an SVM is a Support Vector Machine."BEGIN! Answer the Human's question as best as you are able.------Human: 'What's the difference between an iterator and an iterable?'AI Assistant:""".format(    arg_schema=ask_star_coder.args)  from transformers import pipelinefrom langchain.llms import HuggingFacePipelinehf_model = pipeline(    "text-generation", model="cerebras/Cerebras-GPT-590M", max_new_tokens=200)original_model = HuggingFacePipeline(pipeline=hf_model)generated = original_model.predict(prompt, stop=["Observation:", "Human:"])print(generated) from transformers import pipelinefrom langchain.llms import HuggingFacePipelinehf_model = pipeline(    "text-generation", model="cerebras/Cerebras-GPT-590M", max_new_tokens=200)original_model = HuggingFacePipeline(pipeline=hf_model)generated = original_model.predict(prompt, stop=["Observation:", "Human:"])print(generated)  API Reference:HuggingFacePipeline     Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.     'What's the difference between an iterator and an iterable?'         Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.     'What's the difference between an iterator and an iterable?'         Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.     'What's the difference between an iterator and an iterable?'      decoder_schema = {    "title": "Decoding Schema",    "type": "object",    "properties": {        "action": {"type": "string", "default": ask_star_coder.name},        "action_input": {            "type": "object",            "properties": ask_star_coder.args,        },    },} decoder_schema = {    "title": "Decoding Schema",    "type": "object",    "properties": {        "action": {"type": "string", "default": ask_star_coder.name},        "action_input": {            "type": "object",            "properties": ask_star_coder.args,        },    },}  from langchain_experimental.llms import JsonFormerjson_former = JsonFormer(json_schema=decoder_schema, pipeline=hf_model) from langchain_experimental.llms import JsonFormerjson_former = JsonFormer(json_schema=decoder_schema, pipeline=hf_model)  results = json_former.predict(prompt, stop=["Observation:", "Human:"])print(results) results = json_former.predict(prompt, stop=["Observation:", "Human:"])print(results)      {"action": "ask_star_coder", "action_input": {"query": "What's the difference between an iterator and an iter", "temperature": 0.0, "max_new_tokens": 50.0}}     {"action": "ask_star_coder", "action_input": {"query": "What's the difference between an iterator and an iter", "temperature": 0.0, "max_new_tokens": 50.0}}     {"action": "ask_star_coder", "action_input": {"query": "What's the difference between an iterator and an iter", "temperature": 0.0, "max_new_tokens": 50.0}}  Previous Huggingface TextGen Inference Next KoboldAI API HuggingFace BaselineJSONFormer LLM Wrapper HuggingFace BaselineJSONFormer LLM Wrapper CommunityDiscordTwitterGitHubPythonJS/TSMoreHomepageBlogCopyright © 2023 LangChain, Inc. CommunityDiscordTwitterGitHubPythonJS/TSMoreHomepageBlog CommunityDiscordTwitter Community GitHubPythonJS/TS GitHub MoreHomepageBlog More Copyright © 2023 LangChain, Inc. Copyright © 2023 LangChain, Inc. Skip to main content (#docusaurus_skipToContent_fallback) 🦜️🔗 LangChain (/) Docs (/docs/get_started/introduction) Use cases (/docs/use_cases) Integrations (/docs/integrations) API (https://api.python.langchain.com) LangSmith (https://smith.langchain.com) JS/TS Docs (https://js.langchain.com/docs)  (https://github.com/hwchase17/langchain) Integrations (/docs/integrations) Callbacks (/docs/integrations/callbacks/) Chat models (/docs/integrations/chat/) Chat loaders (/docs/integrations/chat_loaders/) Document loaders (/docs/integrations/document_loaders/) Document transformers (/docs/integrations/document_transformers/) LLMs (/docs/integrations/llms/) AI21 (/docs/integrations/llms/ai21) Aleph Alpha (/docs/integrations/llms/aleph_alpha) Amazon API Gateway (/docs/integrations/llms/amazon_api_gateway) Anyscale (/docs/integrations/llms/anyscale) Azure ML (/docs/integrations/llms/azure_ml) Azure OpenAI (/docs/integrations/llms/azure_openai) Banana (/docs/integrations/llms/banana) Baseten (/docs/integrations/llms/baseten) Beam (/docs/integrations/llms/beam) Bedrock (/docs/integrations/llms/bedrock) Bittensor (/docs/integrations/llms/bittensor) CerebriumAI (/docs/integrations/llms/cerebriumai) ChatGLM (/docs/integrations/llms/chatglm) Clarifai (/docs/integrations/llms/clarifai) Cohere (/docs/integrations/llms/cohere) C Transformers (/docs/integrations/llms/ctransformers) CTranslate2 (/docs/integrations/llms/ctranslate2) Databricks (/docs/integrations/llms/databricks) DeepInfra (/docs/integrations/llms/deepinfra) DeepSparse (/docs/integrations/llms/deepsparse) Eden AI (/docs/integrations/llms/edenai) Fireworks (/docs/integrations/llms/fireworks) ForefrontAI (/docs/integrations/llms/forefrontai) Google Vertex AI PaLM (/docs/integrations/llms/google_vertex_ai_palm) GooseAI (/docs/integrations/llms/gooseai) GPT4All (/docs/integrations/llms/gpt4all) Hugging Face Hub (/docs/integrations/llms/huggingface_hub) Hugging Face Local Pipelines (/docs/integrations/llms/huggingface_pipelines) Huggingface TextGen Inference (/docs/integrations/llms/huggingface_textgen_inference) JSONFormer (/docs/integrations/llms/jsonformer_experimental) KoboldAI API (/docs/integrations/llms/koboldai) Llama.cpp (/docs/integrations/llms/llamacpp) LLM Caching integrations (/docs/integrations/llms/llm_caching) Manifest (/docs/integrations/llms/manifest) Minimax (/docs/integrations/llms/minimax) Modal (/docs/integrations/llms/modal) MosaicML (/docs/integrations/llms/mosaicml) NLP Cloud (/docs/integrations/llms/nlpcloud) OctoAI (/docs/integrations/llms/octoai) Ollama (/docs/integrations/llms/ollama) OpaquePrompts (/docs/integrations/llms/opaqueprompts) OpenAI (/docs/integrations/llms/openai) OpenLLM (/docs/integrations/llms/openllm) OpenLM (/docs/integrations/llms/openlm) Petals (/docs/integrations/llms/petals) PipelineAI (/docs/integrations/llms/pipelineai) Predibase (/docs/integrations/llms/predibase) Prediction Guard (/docs/integrations/llms/predictionguard) PromptLayer OpenAI (/docs/integrations/llms/promptlayer_openai) RELLM (/docs/integrations/llms/rellm_experimental) Replicate (/docs/integrations/llms/replicate) Runhouse (/docs/integrations/llms/runhouse) SageMakerEndpoint (/docs/integrations/llms/sagemaker) StochasticAI (/docs/integrations/llms/stochasticai) Nebula (Symbl.ai) (/docs/integrations/llms/symblai_nebula) TextGen (/docs/integrations/llms/textgen) Titan Takeoff (/docs/integrations/llms/titan_takeoff) Tongyi Qwen (/docs/integrations/llms/tongyi) vLLM (/docs/integrations/llms/vllm) Writer (/docs/integrations/llms/writer) Xorbits Inference (Xinference) (/docs/integrations/llms/xinference) Memory (/docs/integrations/memory/) Retrievers (/docs/integrations/retrievers/) Text embedding models (/docs/integrations/text_embedding/) Agents & Toolkits (/docs/integrations/toolkits/) Tools (/docs/integrations/tools/) Vector stores (/docs/integrations/vectorstores/) Grouped by provider (/docs/integrations/providers/)  (/) Integrations (/docs/integrations) LLMs (/docs/integrations/llms/) JSONFormer (https://github.com/1rgs/jsonformer) ​ (#huggingface-baseline) tool (https://api.python.langchain.com/en/latest/tools/langchain.tools.base.tool.html) HuggingFacePipeline (https://api.python.langchain.com/en/latest/llms/langchain.llms.huggingface_pipeline.HuggingFacePipeline.html) ​ (#jsonformer-llm-wrapper) PreviousHuggingface TextGen Inference (/docs/integrations/llms/huggingface_textgen_inference) NextKoboldAI API (/docs/integrations/llms/koboldai) HuggingFace Baseline (#huggingface-baseline) JSONFormer LLM Wrapper (#jsonformer-llm-wrapper) Discord (https://discord.gg/cU2adEyC7w) Twitter (https://twitter.com/LangChainAI) Python (https://github.com/hwchase17/langchain) JS/TS (https://github.com/hwchase17/langchainjs) Homepage (https://langchain.com) Blog (https://blog.langchain.dev)