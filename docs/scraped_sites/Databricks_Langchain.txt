The Databricks Lakehouse Platform unifies data, analytics, and AI on one platform. This example notebook shows how to wrap Databricks endpoints as LLMs in LangChain. It supports two endpoint types: Prerequisites: The expected MLflow model signature is: If the model signature is incompatible or you want to insert extra configs, you can set transform_input_fn and transform_output_fn accordingly. Prerequisites: The expected server schema (using JSON schema) is: If the server schema is incompatible or you want to insert extra configs, you can use transform_input_fn and transform_output_fn accordingly. The following is a minimal example for running a driver proxy app to serve an LLM: Once the server is running, you can create a Databricks instance to wrap it as an LLM. IntegrationsCallbacksChat modelsChat loadersDocument loadersDocument transformersLLMsAI21Aleph AlphaAmazon API GatewayAnyscaleAzure MLAzure OpenAIBananaBasetenBeamBedrockBittensorCerebriumAIChatGLMClarifaiCohereC TransformersCTranslate2DatabricksDeepInfraDeepSparseEden AIFireworksForefrontAIGoogle Vertex AI PaLMGooseAIGPT4AllHugging Face HubHugging Face Local PipelinesHuggingface TextGen InferenceJSONFormerKoboldAI APILlama.cppLLM Caching integrationsManifestMinimaxModalMosaicMLNLP CloudOctoAIOllamaOpaquePromptsOpenAIOpenLLMOpenLMPetalsPipelineAIPredibasePrediction GuardPromptLayer OpenAIRELLMReplicateRunhouseSageMakerEndpointStochasticAINebula (Symbl.ai)TextGenTitan TakeoffTongyi QwenvLLMWriterXorbits Inference (Xinference)MemoryRetrieversText embedding modelsAgents & ToolkitsToolsVector storesGrouped by provider Callbacks Chat models Chat loaders Document loaders Document transformers LLMsAI21Aleph AlphaAmazon API GatewayAnyscaleAzure MLAzure OpenAIBananaBasetenBeamBedrockBittensorCerebriumAIChatGLMClarifaiCohereC TransformersCTranslate2DatabricksDeepInfraDeepSparseEden AIFireworksForefrontAIGoogle Vertex AI PaLMGooseAIGPT4AllHugging Face HubHugging Face Local PipelinesHuggingface TextGen InferenceJSONFormerKoboldAI APILlama.cppLLM Caching integrationsManifestMinimaxModalMosaicMLNLP CloudOctoAIOllamaOpaquePromptsOpenAIOpenLLMOpenLMPetalsPipelineAIPredibasePrediction GuardPromptLayer OpenAIRELLMReplicateRunhouseSageMakerEndpointStochasticAINebula (Symbl.ai)TextGenTitan TakeoffTongyi QwenvLLMWriterXorbits Inference (Xinference) AI21 Aleph Alpha Amazon API Gateway Anyscale Azure ML Azure OpenAI Banana Baseten Beam Bedrock Bittensor CerebriumAI ChatGLM Clarifai Cohere C Transformers CTranslate2 Databricks DeepInfra DeepSparse Eden AI Fireworks ForefrontAI Google Vertex AI PaLM GooseAI GPT4All Hugging Face Hub Hugging Face Local Pipelines Huggingface TextGen Inference JSONFormer KoboldAI API Llama.cpp LLM Caching integrations Manifest Minimax Modal MosaicML NLP Cloud OctoAI Ollama OpaquePrompts OpenAI OpenLLM OpenLM Petals PipelineAI Predibase Prediction Guard PromptLayer OpenAI RELLM Replicate Runhouse SageMakerEndpoint StochasticAI Nebula (Symbl.ai) TextGen Titan Takeoff Tongyi Qwen vLLM Writer Xorbits Inference (Xinference) Memory Retrievers Text embedding models Agents & Toolkits Tools Vector stores Grouped by provider  Integrations LLMs Databricks Serving endpoint, recommended for production and development, Cluster driver proxy app, recommended for iteractive development. Databricks An LLM was registered and deployed to a Databricks serving endpoint. You have "Can Query" permission to the endpoint. inputs: [{"name": "prompt", "type": "string"}, {"name": "stop", "type": "list[string]"}] outputs: [{"type": "string"}] An LLM loaded on a Databricks interactive cluster in "single user" or "no isolation shared" mode. A local HTTP server running on the driver node to serve the model at "/" using HTTP POST with JSON input/output. It uses a port number between [3000, 8000] and listens to the driver IP address or simply 0.0.0.0 instead of localhost only. You have "Can Attach To" permission to the cluster. inputs:{"type": "object", "properties": {    "prompt": {"type": "string"},     "stop": {"type": "array", "items": {"type": "string"}}},  "required": ["prompt"]} outputs: {"type": "string"} Wrapping a serving endpoint Wrapping a cluster driver proxy app Discord Twitter Python JS/TS Homepage Blog Skip to main contentü¶úÔ∏èüîó LangChainDocsUse casesIntegrationsAPILangSmithJS/TS DocsCTRLKIntegrationsCallbacksChat modelsChat loadersDocument loadersDocument transformersLLMsAI21Aleph AlphaAmazon API GatewayAnyscaleAzure MLAzure OpenAIBananaBasetenBeamBedrockBittensorCerebriumAIChatGLMClarifaiCohereC TransformersCTranslate2DatabricksDeepInfraDeepSparseEden AIFireworksForefrontAIGoogle Vertex AI PaLMGooseAIGPT4AllHugging Face HubHugging Face Local PipelinesHuggingface TextGen InferenceJSONFormerKoboldAI APILlama.cppLLM Caching integrationsManifestMinimaxModalMosaicMLNLP CloudOctoAIOllamaOpaquePromptsOpenAIOpenLLMOpenLMPetalsPipelineAIPredibasePrediction GuardPromptLayer OpenAIRELLMReplicateRunhouseSageMakerEndpointStochasticAINebula (Symbl.ai)TextGenTitan TakeoffTongyi QwenvLLMWriterXorbits Inference (Xinference)MemoryRetrieversText embedding modelsAgents & ToolkitsToolsVector storesGrouped by providerIntegrationsLLMsDatabricksOn this pageDatabricksThe Databricks Lakehouse Platform unifies data, analytics, and AI on one platform.This example notebook shows how to wrap Databricks endpoints as LLMs in LangChain. It supports two endpoint types:Serving endpoint, recommended for production and development,Cluster driver proxy app, recommended for iteractive development.from langchain.llms import DatabricksAPI Reference:DatabricksWrapping a serving endpoint‚ÄãPrerequisites:An LLM was registered and deployed to a Databricks serving endpoint.You have "Can Query" permission to the endpoint.The expected MLflow model signature is:inputs: [{"name": "prompt", "type": "string"}, {"name": "stop", "type": "list[string]"}]outputs: [{"type": "string"}]If the model signature is incompatible or you want to insert extra configs, you can set transform_input_fn and transform_output_fn accordingly.# If running a Databricks notebook attached to an interactive cluster in "single user"# or "no isolation shared" mode, you only need to specify the endpoint name to create# a `Databricks` instance to query a serving endpoint in the same workspace.llm = Databricks(endpoint_name="dolly")llm("How are you?")    'I am happy to hear that you are in good health and as always, you are appreciated.'llm("How are you?", stop=["."])    'Good'# Otherwise, you can manually specify the Databricks workspace hostname and personal access token# or set `DATABRICKS_HOST` and `DATABRICKS_TOKEN` environment variables, respectively.# See https://docs.databricks.com/dev-tools/auth.html#databricks-personal-access-tokens# We strongly recommend not exposing the API token explicitly inside a notebook.# You can use Databricks secret manager to store your API token securely.# See https://docs.databricks.com/dev-tools/databricks-utils.html#secrets-utility-dbutilssecretsimport osos.environ["DATABRICKS_TOKEN"] = dbutils.secrets.get("myworkspace", "api_token")llm = Databricks(host="myworkspace.cloud.databricks.com", endpoint_name="dolly")llm("How are you?")    'I am fine. Thank you!'# If the serving endpoint accepts extra parameters like `temperature`,# you can set them in `model_kwargs`.llm = Databricks(endpoint_name="dolly", model_kwargs={"temperature": 0.1})llm("How are you?")    'I am fine.'# Use `transform_input_fn` and `transform_output_fn` if the serving endpoint# expects a different input schema and does not return a JSON string,# respectively, or you want to apply a prompt template on top.def transform_input(**request):    full_prompt = f"""{request["prompt"]}    Be Concise.    """    request["prompt"] = full_prompt    return requestllm = Databricks(endpoint_name="dolly", transform_input_fn=transform_input)llm("How are you?")    'I‚Äôm Excellent. You?'Wrapping a cluster driver proxy app‚ÄãPrerequisites:An LLM loaded on a Databricks interactive cluster in "single user" or "no isolation shared" mode.A local HTTP server running on the driver node to serve the model at "/" using HTTP POST with JSON input/output.It uses a port number between [3000, 8000] and listens to the driver IP address or simply 0.0.0.0 instead of localhost only.You have "Can Attach To" permission to the cluster.The expected server schema (using JSON schema) is:inputs:{"type": "object", "properties": {    "prompt": {"type": "string"},     "stop": {"type": "array", "items": {"type": "string"}}},  "required": ["prompt"]}outputs: {"type": "string"}If the server schema is incompatible or you want to insert extra configs, you can use transform_input_fn and transform_output_fn accordingly.The following is a minimal example for running a driver proxy app to serve an LLM:from flask import Flask, request, jsonifyimport torchfrom transformers import pipeline, AutoTokenizer, StoppingCriteriamodel = "databricks/dolly-v2-3b"tokenizer = AutoTokenizer.from_pretrained(model, padding_side="left")dolly = pipeline(model=model, tokenizer=tokenizer, trust_remote_code=True, device_map="auto")device = dolly.deviceclass CheckStop(StoppingCriteria):    def __init__(self, stop=None):        super().__init__()        self.stop = stop or []        self.matched = ""        self.stop_ids = [tokenizer.encode(s, return_tensors='pt').to(device) for s in self.stop]    def __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor, **kwargs):        for i, s in enumerate(self.stop_ids):            if torch.all((s == input_ids[0][-s.shape[1]:])).item():                self.matched = self.stop[i]                return True        return Falsedef llm(prompt, stop=None, **kwargs):  check_stop = CheckStop(stop)  result = dolly(prompt, stopping_criteria=[check_stop], **kwargs)  return result[0]["generated_text"].rstrip(check_stop.matched)app = Flask("dolly")@app.route('/', methods=['POST'])def serve_llm():  resp = llm(**request.json)  return jsonify(resp)app.run(host="0.0.0.0", port="7777")Once the server is running, you can create a Databricks instance to wrap it as an LLM.# If running a Databricks notebook attached to the same cluster that runs the app,# you only need to specify the driver port to create a `Databricks` instance.llm = Databricks(cluster_driver_port="7777")llm("How are you?")    'Hello, thank you for asking. It is wonderful to hear that you are well.'# Otherwise, you can manually specify the cluster ID to use,# as well as Databricks workspace hostname and personal access token.llm = Databricks(cluster_id="0000-000000-xxxxxxxx", cluster_driver_port="7777")llm("How are you?")    'I am well. You?'# If the app accepts extra parameters like `temperature`,# you can set them in `model_kwargs`.llm = Databricks(cluster_driver_port="7777", model_kwargs={"temperature": 0.1})llm("How are you?")    'I am very well. It is a pleasure to meet you.'# Use `transform_input_fn` and `transform_output_fn` if the app# expects a different input schema and does not return a JSON string,# respectively, or you want to apply a prompt template on top.def transform_input(**request):    full_prompt = f"""{request["prompt"]}    Be Concise.    """    request["prompt"] = full_prompt    return requestdef transform_output(response):    return response.upper()llm = Databricks(    cluster_driver_port="7777",    transform_input_fn=transform_input,    transform_output_fn=transform_output,)llm("How are you?")    'I AM DOING GREAT THANK YOU.'PreviousCTranslate2NextDeepInfraWrapping a serving endpointWrapping a cluster driver proxy appCommunityDiscordTwitterGitHubPythonJS/TSMoreHomepageBlogCopyright ¬© 2023 LangChain, Inc. Skip to main content ü¶úÔ∏èüîó LangChainDocsUse casesIntegrationsAPILangSmithJS/TS DocsCTRLK ü¶úÔ∏èüîó LangChainDocsUse casesIntegrationsAPI LangSmithJS/TS DocsCTRLK  CTRLK CTRLK  CTRLK   IntegrationsCallbacksChat modelsChat loadersDocument loadersDocument transformersLLMsAI21Aleph AlphaAmazon API GatewayAnyscaleAzure MLAzure OpenAIBananaBasetenBeamBedrockBittensorCerebriumAIChatGLMClarifaiCohereC TransformersCTranslate2DatabricksDeepInfraDeepSparseEden AIFireworksForefrontAIGoogle Vertex AI PaLMGooseAIGPT4AllHugging Face HubHugging Face Local PipelinesHuggingface TextGen InferenceJSONFormerKoboldAI APILlama.cppLLM Caching integrationsManifestMinimaxModalMosaicMLNLP CloudOctoAIOllamaOpaquePromptsOpenAIOpenLLMOpenLMPetalsPipelineAIPredibasePrediction GuardPromptLayer OpenAIRELLMReplicateRunhouseSageMakerEndpointStochasticAINebula (Symbl.ai)TextGenTitan TakeoffTongyi QwenvLLMWriterXorbits Inference (Xinference)MemoryRetrieversText embedding modelsAgents & ToolkitsToolsVector storesGrouped by providerIntegrationsLLMsDatabricksOn this pageDatabricksThe Databricks Lakehouse Platform unifies data, analytics, and AI on one platform.This example notebook shows how to wrap Databricks endpoints as LLMs in LangChain. It supports two endpoint types:Serving endpoint, recommended for production and development,Cluster driver proxy app, recommended for iteractive development.from langchain.llms import DatabricksAPI Reference:DatabricksWrapping a serving endpoint‚ÄãPrerequisites:An LLM was registered and deployed to a Databricks serving endpoint.You have "Can Query" permission to the endpoint.The expected MLflow model signature is:inputs: [{"name": "prompt", "type": "string"}, {"name": "stop", "type": "list[string]"}]outputs: [{"type": "string"}]If the model signature is incompatible or you want to insert extra configs, you can set transform_input_fn and transform_output_fn accordingly.# If running a Databricks notebook attached to an interactive cluster in "single user"# or "no isolation shared" mode, you only need to specify the endpoint name to create# a `Databricks` instance to query a serving endpoint in the same workspace.llm = Databricks(endpoint_name="dolly")llm("How are you?")    'I am happy to hear that you are in good health and as always, you are appreciated.'llm("How are you?", stop=["."])    'Good'# Otherwise, you can manually specify the Databricks workspace hostname and personal access token# or set `DATABRICKS_HOST` and `DATABRICKS_TOKEN` environment variables, respectively.# See https://docs.databricks.com/dev-tools/auth.html#databricks-personal-access-tokens# We strongly recommend not exposing the API token explicitly inside a notebook.# You can use Databricks secret manager to store your API token securely.# See https://docs.databricks.com/dev-tools/databricks-utils.html#secrets-utility-dbutilssecretsimport osos.environ["DATABRICKS_TOKEN"] = dbutils.secrets.get("myworkspace", "api_token")llm = Databricks(host="myworkspace.cloud.databricks.com", endpoint_name="dolly")llm("How are you?")    'I am fine. Thank you!'# If the serving endpoint accepts extra parameters like `temperature`,# you can set them in `model_kwargs`.llm = Databricks(endpoint_name="dolly", model_kwargs={"temperature": 0.1})llm("How are you?")    'I am fine.'# Use `transform_input_fn` and `transform_output_fn` if the serving endpoint# expects a different input schema and does not return a JSON string,# respectively, or you want to apply a prompt template on top.def transform_input(**request):    full_prompt = f"""{request["prompt"]}    Be Concise.    """    request["prompt"] = full_prompt    return requestllm = Databricks(endpoint_name="dolly", transform_input_fn=transform_input)llm("How are you?")    'I‚Äôm Excellent. You?'Wrapping a cluster driver proxy app‚ÄãPrerequisites:An LLM loaded on a Databricks interactive cluster in "single user" or "no isolation shared" mode.A local HTTP server running on the driver node to serve the model at "/" using HTTP POST with JSON input/output.It uses a port number between [3000, 8000] and listens to the driver IP address or simply 0.0.0.0 instead of localhost only.You have "Can Attach To" permission to the cluster.The expected server schema (using JSON schema) is:inputs:{"type": "object", "properties": {    "prompt": {"type": "string"},     "stop": {"type": "array", "items": {"type": "string"}}},  "required": ["prompt"]}outputs: {"type": "string"}If the server schema is incompatible or you want to insert extra configs, you can use transform_input_fn and transform_output_fn accordingly.The following is a minimal example for running a driver proxy app to serve an LLM:from flask import Flask, request, jsonifyimport torchfrom transformers import pipeline, AutoTokenizer, StoppingCriteriamodel = "databricks/dolly-v2-3b"tokenizer = AutoTokenizer.from_pretrained(model, padding_side="left")dolly = pipeline(model=model, tokenizer=tokenizer, trust_remote_code=True, device_map="auto")device = dolly.deviceclass CheckStop(StoppingCriteria):    def __init__(self, stop=None):        super().__init__()        self.stop = stop or []        self.matched = ""        self.stop_ids = [tokenizer.encode(s, return_tensors='pt').to(device) for s in self.stop]    def __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor, **kwargs):        for i, s in enumerate(self.stop_ids):            if torch.all((s == input_ids[0][-s.shape[1]:])).item():                self.matched = self.stop[i]                return True        return Falsedef llm(prompt, stop=None, **kwargs):  check_stop = CheckStop(stop)  result = dolly(prompt, stopping_criteria=[check_stop], **kwargs)  return result[0]["generated_text"].rstrip(check_stop.matched)app = Flask("dolly")@app.route('/', methods=['POST'])def serve_llm():  resp = llm(**request.json)  return jsonify(resp)app.run(host="0.0.0.0", port="7777")Once the server is running, you can create a Databricks instance to wrap it as an LLM.# If running a Databricks notebook attached to the same cluster that runs the app,# you only need to specify the driver port to create a `Databricks` instance.llm = Databricks(cluster_driver_port="7777")llm("How are you?")    'Hello, thank you for asking. It is wonderful to hear that you are well.'# Otherwise, you can manually specify the cluster ID to use,# as well as Databricks workspace hostname and personal access token.llm = Databricks(cluster_id="0000-000000-xxxxxxxx", cluster_driver_port="7777")llm("How are you?")    'I am well. You?'# If the app accepts extra parameters like `temperature`,# you can set them in `model_kwargs`.llm = Databricks(cluster_driver_port="7777", model_kwargs={"temperature": 0.1})llm("How are you?")    'I am very well. It is a pleasure to meet you.'# Use `transform_input_fn` and `transform_output_fn` if the app# expects a different input schema and does not return a JSON string,# respectively, or you want to apply a prompt template on top.def transform_input(**request):    full_prompt = f"""{request["prompt"]}    Be Concise.    """    request["prompt"] = full_prompt    return requestdef transform_output(response):    return response.upper()llm = Databricks(    cluster_driver_port="7777",    transform_input_fn=transform_input,    transform_output_fn=transform_output,)llm("How are you?")    'I AM DOING GREAT THANK YOU.'PreviousCTranslate2NextDeepInfraWrapping a serving endpointWrapping a cluster driver proxy app IntegrationsCallbacksChat modelsChat loadersDocument loadersDocument transformersLLMsAI21Aleph AlphaAmazon API GatewayAnyscaleAzure MLAzure OpenAIBananaBasetenBeamBedrockBittensorCerebriumAIChatGLMClarifaiCohereC TransformersCTranslate2DatabricksDeepInfraDeepSparseEden AIFireworksForefrontAIGoogle Vertex AI PaLMGooseAIGPT4AllHugging Face HubHugging Face Local PipelinesHuggingface TextGen InferenceJSONFormerKoboldAI APILlama.cppLLM Caching integrationsManifestMinimaxModalMosaicMLNLP CloudOctoAIOllamaOpaquePromptsOpenAIOpenLLMOpenLMPetalsPipelineAIPredibasePrediction GuardPromptLayer OpenAIRELLMReplicateRunhouseSageMakerEndpointStochasticAINebula (Symbl.ai)TextGenTitan TakeoffTongyi QwenvLLMWriterXorbits Inference (Xinference)MemoryRetrieversText embedding modelsAgents & ToolkitsToolsVector storesGrouped by providerIntegrationsLLMsDatabricksOn this pageDatabricksThe Databricks Lakehouse Platform unifies data, analytics, and AI on one platform.This example notebook shows how to wrap Databricks endpoints as LLMs in LangChain. It supports two endpoint types:Serving endpoint, recommended for production and development,Cluster driver proxy app, recommended for iteractive development.from langchain.llms import DatabricksAPI Reference:DatabricksWrapping a serving endpoint‚ÄãPrerequisites:An LLM was registered and deployed to a Databricks serving endpoint.You have "Can Query" permission to the endpoint.The expected MLflow model signature is:inputs: [{"name": "prompt", "type": "string"}, {"name": "stop", "type": "list[string]"}]outputs: [{"type": "string"}]If the model signature is incompatible or you want to insert extra configs, you can set transform_input_fn and transform_output_fn accordingly.# If running a Databricks notebook attached to an interactive cluster in "single user"# or "no isolation shared" mode, you only need to specify the endpoint name to create# a `Databricks` instance to query a serving endpoint in the same workspace.llm = Databricks(endpoint_name="dolly")llm("How are you?")    'I am happy to hear that you are in good health and as always, you are appreciated.'llm("How are you?", stop=["."])    'Good'# Otherwise, you can manually specify the Databricks workspace hostname and personal access token# or set `DATABRICKS_HOST` and `DATABRICKS_TOKEN` environment variables, respectively.# See https://docs.databricks.com/dev-tools/auth.html#databricks-personal-access-tokens# We strongly recommend not exposing the API token explicitly inside a notebook.# You can use Databricks secret manager to store your API token securely.# See https://docs.databricks.com/dev-tools/databricks-utils.html#secrets-utility-dbutilssecretsimport osos.environ["DATABRICKS_TOKEN"] = dbutils.secrets.get("myworkspace", "api_token")llm = Databricks(host="myworkspace.cloud.databricks.com", endpoint_name="dolly")llm("How are you?")    'I am fine. Thank you!'# If the serving endpoint accepts extra parameters like `temperature`,# you can set them in `model_kwargs`.llm = Databricks(endpoint_name="dolly", model_kwargs={"temperature": 0.1})llm("How are you?")    'I am fine.'# Use `transform_input_fn` and `transform_output_fn` if the serving endpoint# expects a different input schema and does not return a JSON string,# respectively, or you want to apply a prompt template on top.def transform_input(**request):    full_prompt = f"""{request["prompt"]}    Be Concise.    """    request["prompt"] = full_prompt    return requestllm = Databricks(endpoint_name="dolly", transform_input_fn=transform_input)llm("How are you?")    'I‚Äôm Excellent. You?'Wrapping a cluster driver proxy app‚ÄãPrerequisites:An LLM loaded on a Databricks interactive cluster in "single user" or "no isolation shared" mode.A local HTTP server running on the driver node to serve the model at "/" using HTTP POST with JSON input/output.It uses a port number between [3000, 8000] and listens to the driver IP address or simply 0.0.0.0 instead of localhost only.You have "Can Attach To" permission to the cluster.The expected server schema (using JSON schema) is:inputs:{"type": "object", "properties": {    "prompt": {"type": "string"},     "stop": {"type": "array", "items": {"type": "string"}}},  "required": ["prompt"]}outputs: {"type": "string"}If the server schema is incompatible or you want to insert extra configs, you can use transform_input_fn and transform_output_fn accordingly.The following is a minimal example for running a driver proxy app to serve an LLM:from flask import Flask, request, jsonifyimport torchfrom transformers import pipeline, AutoTokenizer, StoppingCriteriamodel = "databricks/dolly-v2-3b"tokenizer = AutoTokenizer.from_pretrained(model, padding_side="left")dolly = pipeline(model=model, tokenizer=tokenizer, trust_remote_code=True, device_map="auto")device = dolly.deviceclass CheckStop(StoppingCriteria):    def __init__(self, stop=None):        super().__init__()        self.stop = stop or []        self.matched = ""        self.stop_ids = [tokenizer.encode(s, return_tensors='pt').to(device) for s in self.stop]    def __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor, **kwargs):        for i, s in enumerate(self.stop_ids):            if torch.all((s == input_ids[0][-s.shape[1]:])).item():                self.matched = self.stop[i]                return True        return Falsedef llm(prompt, stop=None, **kwargs):  check_stop = CheckStop(stop)  result = dolly(prompt, stopping_criteria=[check_stop], **kwargs)  return result[0]["generated_text"].rstrip(check_stop.matched)app = Flask("dolly")@app.route('/', methods=['POST'])def serve_llm():  resp = llm(**request.json)  return jsonify(resp)app.run(host="0.0.0.0", port="7777")Once the server is running, you can create a Databricks instance to wrap it as an LLM.# If running a Databricks notebook attached to the same cluster that runs the app,# you only need to specify the driver port to create a `Databricks` instance.llm = Databricks(cluster_driver_port="7777")llm("How are you?")    'Hello, thank you for asking. It is wonderful to hear that you are well.'# Otherwise, you can manually specify the cluster ID to use,# as well as Databricks workspace hostname and personal access token.llm = Databricks(cluster_id="0000-000000-xxxxxxxx", cluster_driver_port="7777")llm("How are you?")    'I am well. You?'# If the app accepts extra parameters like `temperature`,# you can set them in `model_kwargs`.llm = Databricks(cluster_driver_port="7777", model_kwargs={"temperature": 0.1})llm("How are you?")    'I am very well. It is a pleasure to meet you.'# Use `transform_input_fn` and `transform_output_fn` if the app# expects a different input schema and does not return a JSON string,# respectively, or you want to apply a prompt template on top.def transform_input(**request):    full_prompt = f"""{request["prompt"]}    Be Concise.    """    request["prompt"] = full_prompt    return requestdef transform_output(response):    return response.upper()llm = Databricks(    cluster_driver_port="7777",    transform_input_fn=transform_input,    transform_output_fn=transform_output,)llm("How are you?")    'I AM DOING GREAT THANK YOU.'PreviousCTranslate2NextDeepInfraWrapping a serving endpointWrapping a cluster driver proxy app IntegrationsCallbacksChat modelsChat loadersDocument loadersDocument transformersLLMsAI21Aleph AlphaAmazon API GatewayAnyscaleAzure MLAzure OpenAIBananaBasetenBeamBedrockBittensorCerebriumAIChatGLMClarifaiCohereC TransformersCTranslate2DatabricksDeepInfraDeepSparseEden AIFireworksForefrontAIGoogle Vertex AI PaLMGooseAIGPT4AllHugging Face HubHugging Face Local PipelinesHuggingface TextGen InferenceJSONFormerKoboldAI APILlama.cppLLM Caching integrationsManifestMinimaxModalMosaicMLNLP CloudOctoAIOllamaOpaquePromptsOpenAIOpenLLMOpenLMPetalsPipelineAIPredibasePrediction GuardPromptLayer OpenAIRELLMReplicateRunhouseSageMakerEndpointStochasticAINebula (Symbl.ai)TextGenTitan TakeoffTongyi QwenvLLMWriterXorbits Inference (Xinference)MemoryRetrieversText embedding modelsAgents & ToolkitsToolsVector storesGrouped by provider IntegrationsCallbacksChat modelsChat loadersDocument loadersDocument transformersLLMsAI21Aleph AlphaAmazon API GatewayAnyscaleAzure MLAzure OpenAIBananaBasetenBeamBedrockBittensorCerebriumAIChatGLMClarifaiCohereC TransformersCTranslate2DatabricksDeepInfraDeepSparseEden AIFireworksForefrontAIGoogle Vertex AI PaLMGooseAIGPT4AllHugging Face HubHugging Face Local PipelinesHuggingface TextGen InferenceJSONFormerKoboldAI APILlama.cppLLM Caching integrationsManifestMinimaxModalMosaicMLNLP CloudOctoAIOllamaOpaquePromptsOpenAIOpenLLMOpenLMPetalsPipelineAIPredibasePrediction GuardPromptLayer OpenAIRELLMReplicateRunhouseSageMakerEndpointStochasticAINebula (Symbl.ai)TextGenTitan TakeoffTongyi QwenvLLMWriterXorbits Inference (Xinference)MemoryRetrieversText embedding modelsAgents & ToolkitsToolsVector storesGrouped by provider Integrations Callbacks Chat models Chat loaders Document loaders Document transformers LLMs Memory Retrievers Text embedding models Agents & Toolkits Tools Vector stores Grouped by provider IntegrationsLLMsDatabricksOn this pageDatabricksThe Databricks Lakehouse Platform unifies data, analytics, and AI on one platform.This example notebook shows how to wrap Databricks endpoints as LLMs in LangChain. It supports two endpoint types:Serving endpoint, recommended for production and development,Cluster driver proxy app, recommended for iteractive development.from langchain.llms import DatabricksAPI Reference:DatabricksWrapping a serving endpoint‚ÄãPrerequisites:An LLM was registered and deployed to a Databricks serving endpoint.You have "Can Query" permission to the endpoint.The expected MLflow model signature is:inputs: [{"name": "prompt", "type": "string"}, {"name": "stop", "type": "list[string]"}]outputs: [{"type": "string"}]If the model signature is incompatible or you want to insert extra configs, you can set transform_input_fn and transform_output_fn accordingly.# If running a Databricks notebook attached to an interactive cluster in "single user"# or "no isolation shared" mode, you only need to specify the endpoint name to create# a `Databricks` instance to query a serving endpoint in the same workspace.llm = Databricks(endpoint_name="dolly")llm("How are you?")    'I am happy to hear that you are in good health and as always, you are appreciated.'llm("How are you?", stop=["."])    'Good'# Otherwise, you can manually specify the Databricks workspace hostname and personal access token# or set `DATABRICKS_HOST` and `DATABRICKS_TOKEN` environment variables, respectively.# See https://docs.databricks.com/dev-tools/auth.html#databricks-personal-access-tokens# We strongly recommend not exposing the API token explicitly inside a notebook.# You can use Databricks secret manager to store your API token securely.# See https://docs.databricks.com/dev-tools/databricks-utils.html#secrets-utility-dbutilssecretsimport osos.environ["DATABRICKS_TOKEN"] = dbutils.secrets.get("myworkspace", "api_token")llm = Databricks(host="myworkspace.cloud.databricks.com", endpoint_name="dolly")llm("How are you?")    'I am fine. Thank you!'# If the serving endpoint accepts extra parameters like `temperature`,# you can set them in `model_kwargs`.llm = Databricks(endpoint_name="dolly", model_kwargs={"temperature": 0.1})llm("How are you?")    'I am fine.'# Use `transform_input_fn` and `transform_output_fn` if the serving endpoint# expects a different input schema and does not return a JSON string,# respectively, or you want to apply a prompt template on top.def transform_input(**request):    full_prompt = f"""{request["prompt"]}    Be Concise.    """    request["prompt"] = full_prompt    return requestllm = Databricks(endpoint_name="dolly", transform_input_fn=transform_input)llm("How are you?")    'I‚Äôm Excellent. You?'Wrapping a cluster driver proxy app‚ÄãPrerequisites:An LLM loaded on a Databricks interactive cluster in "single user" or "no isolation shared" mode.A local HTTP server running on the driver node to serve the model at "/" using HTTP POST with JSON input/output.It uses a port number between [3000, 8000] and listens to the driver IP address or simply 0.0.0.0 instead of localhost only.You have "Can Attach To" permission to the cluster.The expected server schema (using JSON schema) is:inputs:{"type": "object", "properties": {    "prompt": {"type": "string"},     "stop": {"type": "array", "items": {"type": "string"}}},  "required": ["prompt"]}outputs: {"type": "string"}If the server schema is incompatible or you want to insert extra configs, you can use transform_input_fn and transform_output_fn accordingly.The following is a minimal example for running a driver proxy app to serve an LLM:from flask import Flask, request, jsonifyimport torchfrom transformers import pipeline, AutoTokenizer, StoppingCriteriamodel = "databricks/dolly-v2-3b"tokenizer = AutoTokenizer.from_pretrained(model, padding_side="left")dolly = pipeline(model=model, tokenizer=tokenizer, trust_remote_code=True, device_map="auto")device = dolly.deviceclass CheckStop(StoppingCriteria):    def __init__(self, stop=None):        super().__init__()        self.stop = stop or []        self.matched = ""        self.stop_ids = [tokenizer.encode(s, return_tensors='pt').to(device) for s in self.stop]    def __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor, **kwargs):        for i, s in enumerate(self.stop_ids):            if torch.all((s == input_ids[0][-s.shape[1]:])).item():                self.matched = self.stop[i]                return True        return Falsedef llm(prompt, stop=None, **kwargs):  check_stop = CheckStop(stop)  result = dolly(prompt, stopping_criteria=[check_stop], **kwargs)  return result[0]["generated_text"].rstrip(check_stop.matched)app = Flask("dolly")@app.route('/', methods=['POST'])def serve_llm():  resp = llm(**request.json)  return jsonify(resp)app.run(host="0.0.0.0", port="7777")Once the server is running, you can create a Databricks instance to wrap it as an LLM.# If running a Databricks notebook attached to the same cluster that runs the app,# you only need to specify the driver port to create a `Databricks` instance.llm = Databricks(cluster_driver_port="7777")llm("How are you?")    'Hello, thank you for asking. It is wonderful to hear that you are well.'# Otherwise, you can manually specify the cluster ID to use,# as well as Databricks workspace hostname and personal access token.llm = Databricks(cluster_id="0000-000000-xxxxxxxx", cluster_driver_port="7777")llm("How are you?")    'I am well. You?'# If the app accepts extra parameters like `temperature`,# you can set them in `model_kwargs`.llm = Databricks(cluster_driver_port="7777", model_kwargs={"temperature": 0.1})llm("How are you?")    'I am very well. It is a pleasure to meet you.'# Use `transform_input_fn` and `transform_output_fn` if the app# expects a different input schema and does not return a JSON string,# respectively, or you want to apply a prompt template on top.def transform_input(**request):    full_prompt = f"""{request["prompt"]}    Be Concise.    """    request["prompt"] = full_prompt    return requestdef transform_output(response):    return response.upper()llm = Databricks(    cluster_driver_port="7777",    transform_input_fn=transform_input,    transform_output_fn=transform_output,)llm("How are you?")    'I AM DOING GREAT THANK YOU.'PreviousCTranslate2NextDeepInfraWrapping a serving endpointWrapping a cluster driver proxy app IntegrationsLLMsDatabricksOn this pageDatabricksThe Databricks Lakehouse Platform unifies data, analytics, and AI on one platform.This example notebook shows how to wrap Databricks endpoints as LLMs in LangChain. It supports two endpoint types:Serving endpoint, recommended for production and development,Cluster driver proxy app, recommended for iteractive development.from langchain.llms import DatabricksAPI Reference:DatabricksWrapping a serving endpoint‚ÄãPrerequisites:An LLM was registered and deployed to a Databricks serving endpoint.You have "Can Query" permission to the endpoint.The expected MLflow model signature is:inputs: [{"name": "prompt", "type": "string"}, {"name": "stop", "type": "list[string]"}]outputs: [{"type": "string"}]If the model signature is incompatible or you want to insert extra configs, you can set transform_input_fn and transform_output_fn accordingly.# If running a Databricks notebook attached to an interactive cluster in "single user"# or "no isolation shared" mode, you only need to specify the endpoint name to create# a `Databricks` instance to query a serving endpoint in the same workspace.llm = Databricks(endpoint_name="dolly")llm("How are you?")    'I am happy to hear that you are in good health and as always, you are appreciated.'llm("How are you?", stop=["."])    'Good'# Otherwise, you can manually specify the Databricks workspace hostname and personal access token# or set `DATABRICKS_HOST` and `DATABRICKS_TOKEN` environment variables, respectively.# See https://docs.databricks.com/dev-tools/auth.html#databricks-personal-access-tokens# We strongly recommend not exposing the API token explicitly inside a notebook.# You can use Databricks secret manager to store your API token securely.# See https://docs.databricks.com/dev-tools/databricks-utils.html#secrets-utility-dbutilssecretsimport osos.environ["DATABRICKS_TOKEN"] = dbutils.secrets.get("myworkspace", "api_token")llm = Databricks(host="myworkspace.cloud.databricks.com", endpoint_name="dolly")llm("How are you?")    'I am fine. Thank you!'# If the serving endpoint accepts extra parameters like `temperature`,# you can set them in `model_kwargs`.llm = Databricks(endpoint_name="dolly", model_kwargs={"temperature": 0.1})llm("How are you?")    'I am fine.'# Use `transform_input_fn` and `transform_output_fn` if the serving endpoint# expects a different input schema and does not return a JSON string,# respectively, or you want to apply a prompt template on top.def transform_input(**request):    full_prompt = f"""{request["prompt"]}    Be Concise.    """    request["prompt"] = full_prompt    return requestllm = Databricks(endpoint_name="dolly", transform_input_fn=transform_input)llm("How are you?")    'I‚Äôm Excellent. You?'Wrapping a cluster driver proxy app‚ÄãPrerequisites:An LLM loaded on a Databricks interactive cluster in "single user" or "no isolation shared" mode.A local HTTP server running on the driver node to serve the model at "/" using HTTP POST with JSON input/output.It uses a port number between [3000, 8000] and listens to the driver IP address or simply 0.0.0.0 instead of localhost only.You have "Can Attach To" permission to the cluster.The expected server schema (using JSON schema) is:inputs:{"type": "object", "properties": {    "prompt": {"type": "string"},     "stop": {"type": "array", "items": {"type": "string"}}},  "required": ["prompt"]}outputs: {"type": "string"}If the server schema is incompatible or you want to insert extra configs, you can use transform_input_fn and transform_output_fn accordingly.The following is a minimal example for running a driver proxy app to serve an LLM:from flask import Flask, request, jsonifyimport torchfrom transformers import pipeline, AutoTokenizer, StoppingCriteriamodel = "databricks/dolly-v2-3b"tokenizer = AutoTokenizer.from_pretrained(model, padding_side="left")dolly = pipeline(model=model, tokenizer=tokenizer, trust_remote_code=True, device_map="auto")device = dolly.deviceclass CheckStop(StoppingCriteria):    def __init__(self, stop=None):        super().__init__()        self.stop = stop or []        self.matched = ""        self.stop_ids = [tokenizer.encode(s, return_tensors='pt').to(device) for s in self.stop]    def __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor, **kwargs):        for i, s in enumerate(self.stop_ids):            if torch.all((s == input_ids[0][-s.shape[1]:])).item():                self.matched = self.stop[i]                return True        return Falsedef llm(prompt, stop=None, **kwargs):  check_stop = CheckStop(stop)  result = dolly(prompt, stopping_criteria=[check_stop], **kwargs)  return result[0]["generated_text"].rstrip(check_stop.matched)app = Flask("dolly")@app.route('/', methods=['POST'])def serve_llm():  resp = llm(**request.json)  return jsonify(resp)app.run(host="0.0.0.0", port="7777")Once the server is running, you can create a Databricks instance to wrap it as an LLM.# If running a Databricks notebook attached to the same cluster that runs the app,# you only need to specify the driver port to create a `Databricks` instance.llm = Databricks(cluster_driver_port="7777")llm("How are you?")    'Hello, thank you for asking. It is wonderful to hear that you are well.'# Otherwise, you can manually specify the cluster ID to use,# as well as Databricks workspace hostname and personal access token.llm = Databricks(cluster_id="0000-000000-xxxxxxxx", cluster_driver_port="7777")llm("How are you?")    'I am well. You?'# If the app accepts extra parameters like `temperature`,# you can set them in `model_kwargs`.llm = Databricks(cluster_driver_port="7777", model_kwargs={"temperature": 0.1})llm("How are you?")    'I am very well. It is a pleasure to meet you.'# Use `transform_input_fn` and `transform_output_fn` if the app# expects a different input schema and does not return a JSON string,# respectively, or you want to apply a prompt template on top.def transform_input(**request):    full_prompt = f"""{request["prompt"]}    Be Concise.    """    request["prompt"] = full_prompt    return requestdef transform_output(response):    return response.upper()llm = Databricks(    cluster_driver_port="7777",    transform_input_fn=transform_input,    transform_output_fn=transform_output,)llm("How are you?")    'I AM DOING GREAT THANK YOU.'PreviousCTranslate2NextDeepInfra IntegrationsLLMsDatabricksOn this pageDatabricksThe Databricks Lakehouse Platform unifies data, analytics, and AI on one platform.This example notebook shows how to wrap Databricks endpoints as LLMs in LangChain. It supports two endpoint types:Serving endpoint, recommended for production and development,Cluster driver proxy app, recommended for iteractive development.from langchain.llms import DatabricksAPI Reference:DatabricksWrapping a serving endpoint‚ÄãPrerequisites:An LLM was registered and deployed to a Databricks serving endpoint.You have "Can Query" permission to the endpoint.The expected MLflow model signature is:inputs: [{"name": "prompt", "type": "string"}, {"name": "stop", "type": "list[string]"}]outputs: [{"type": "string"}]If the model signature is incompatible or you want to insert extra configs, you can set transform_input_fn and transform_output_fn accordingly.# If running a Databricks notebook attached to an interactive cluster in "single user"# or "no isolation shared" mode, you only need to specify the endpoint name to create# a `Databricks` instance to query a serving endpoint in the same workspace.llm = Databricks(endpoint_name="dolly")llm("How are you?")    'I am happy to hear that you are in good health and as always, you are appreciated.'llm("How are you?", stop=["."])    'Good'# Otherwise, you can manually specify the Databricks workspace hostname and personal access token# or set `DATABRICKS_HOST` and `DATABRICKS_TOKEN` environment variables, respectively.# See https://docs.databricks.com/dev-tools/auth.html#databricks-personal-access-tokens# We strongly recommend not exposing the API token explicitly inside a notebook.# You can use Databricks secret manager to store your API token securely.# See https://docs.databricks.com/dev-tools/databricks-utils.html#secrets-utility-dbutilssecretsimport osos.environ["DATABRICKS_TOKEN"] = dbutils.secrets.get("myworkspace", "api_token")llm = Databricks(host="myworkspace.cloud.databricks.com", endpoint_name="dolly")llm("How are you?")    'I am fine. Thank you!'# If the serving endpoint accepts extra parameters like `temperature`,# you can set them in `model_kwargs`.llm = Databricks(endpoint_name="dolly", model_kwargs={"temperature": 0.1})llm("How are you?")    'I am fine.'# Use `transform_input_fn` and `transform_output_fn` if the serving endpoint# expects a different input schema and does not return a JSON string,# respectively, or you want to apply a prompt template on top.def transform_input(**request):    full_prompt = f"""{request["prompt"]}    Be Concise.    """    request["prompt"] = full_prompt    return requestllm = Databricks(endpoint_name="dolly", transform_input_fn=transform_input)llm("How are you?")    'I‚Äôm Excellent. You?'Wrapping a cluster driver proxy app‚ÄãPrerequisites:An LLM loaded on a Databricks interactive cluster in "single user" or "no isolation shared" mode.A local HTTP server running on the driver node to serve the model at "/" using HTTP POST with JSON input/output.It uses a port number between [3000, 8000] and listens to the driver IP address or simply 0.0.0.0 instead of localhost only.You have "Can Attach To" permission to the cluster.The expected server schema (using JSON schema) is:inputs:{"type": "object", "properties": {    "prompt": {"type": "string"},     "stop": {"type": "array", "items": {"type": "string"}}},  "required": ["prompt"]}outputs: {"type": "string"}If the server schema is incompatible or you want to insert extra configs, you can use transform_input_fn and transform_output_fn accordingly.The following is a minimal example for running a driver proxy app to serve an LLM:from flask import Flask, request, jsonifyimport torchfrom transformers import pipeline, AutoTokenizer, StoppingCriteriamodel = "databricks/dolly-v2-3b"tokenizer = AutoTokenizer.from_pretrained(model, padding_side="left")dolly = pipeline(model=model, tokenizer=tokenizer, trust_remote_code=True, device_map="auto")device = dolly.deviceclass CheckStop(StoppingCriteria):    def __init__(self, stop=None):        super().__init__()        self.stop = stop or []        self.matched = ""        self.stop_ids = [tokenizer.encode(s, return_tensors='pt').to(device) for s in self.stop]    def __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor, **kwargs):        for i, s in enumerate(self.stop_ids):            if torch.all((s == input_ids[0][-s.shape[1]:])).item():                self.matched = self.stop[i]                return True        return Falsedef llm(prompt, stop=None, **kwargs):  check_stop = CheckStop(stop)  result = dolly(prompt, stopping_criteria=[check_stop], **kwargs)  return result[0]["generated_text"].rstrip(check_stop.matched)app = Flask("dolly")@app.route('/', methods=['POST'])def serve_llm():  resp = llm(**request.json)  return jsonify(resp)app.run(host="0.0.0.0", port="7777")Once the server is running, you can create a Databricks instance to wrap it as an LLM.# If running a Databricks notebook attached to the same cluster that runs the app,# you only need to specify the driver port to create a `Databricks` instance.llm = Databricks(cluster_driver_port="7777")llm("How are you?")    'Hello, thank you for asking. It is wonderful to hear that you are well.'# Otherwise, you can manually specify the cluster ID to use,# as well as Databricks workspace hostname and personal access token.llm = Databricks(cluster_id="0000-000000-xxxxxxxx", cluster_driver_port="7777")llm("How are you?")    'I am well. You?'# If the app accepts extra parameters like `temperature`,# you can set them in `model_kwargs`.llm = Databricks(cluster_driver_port="7777", model_kwargs={"temperature": 0.1})llm("How are you?")    'I am very well. It is a pleasure to meet you.'# Use `transform_input_fn` and `transform_output_fn` if the app# expects a different input schema and does not return a JSON string,# respectively, or you want to apply a prompt template on top.def transform_input(**request):    full_prompt = f"""{request["prompt"]}    Be Concise.    """    request["prompt"] = full_prompt    return requestdef transform_output(response):    return response.upper()llm = Databricks(    cluster_driver_port="7777",    transform_input_fn=transform_input,    transform_output_fn=transform_output,)llm("How are you?")    'I AM DOING GREAT THANK YOU.'PreviousCTranslate2NextDeepInfra On this page DatabricksThe Databricks Lakehouse Platform unifies data, analytics, and AI on one platform.This example notebook shows how to wrap Databricks endpoints as LLMs in LangChain. It supports two endpoint types:Serving endpoint, recommended for production and development,Cluster driver proxy app, recommended for iteractive development.from langchain.llms import DatabricksAPI Reference:DatabricksWrapping a serving endpoint‚ÄãPrerequisites:An LLM was registered and deployed to a Databricks serving endpoint.You have "Can Query" permission to the endpoint.The expected MLflow model signature is:inputs: [{"name": "prompt", "type": "string"}, {"name": "stop", "type": "list[string]"}]outputs: [{"type": "string"}]If the model signature is incompatible or you want to insert extra configs, you can set transform_input_fn and transform_output_fn accordingly.# If running a Databricks notebook attached to an interactive cluster in "single user"# or "no isolation shared" mode, you only need to specify the endpoint name to create# a `Databricks` instance to query a serving endpoint in the same workspace.llm = Databricks(endpoint_name="dolly")llm("How are you?")    'I am happy to hear that you are in good health and as always, you are appreciated.'llm("How are you?", stop=["."])    'Good'# Otherwise, you can manually specify the Databricks workspace hostname and personal access token# or set `DATABRICKS_HOST` and `DATABRICKS_TOKEN` environment variables, respectively.# See https://docs.databricks.com/dev-tools/auth.html#databricks-personal-access-tokens# We strongly recommend not exposing the API token explicitly inside a notebook.# You can use Databricks secret manager to store your API token securely.# See https://docs.databricks.com/dev-tools/databricks-utils.html#secrets-utility-dbutilssecretsimport osos.environ["DATABRICKS_TOKEN"] = dbutils.secrets.get("myworkspace", "api_token")llm = Databricks(host="myworkspace.cloud.databricks.com", endpoint_name="dolly")llm("How are you?")    'I am fine. Thank you!'# If the serving endpoint accepts extra parameters like `temperature`,# you can set them in `model_kwargs`.llm = Databricks(endpoint_name="dolly", model_kwargs={"temperature": 0.1})llm("How are you?")    'I am fine.'# Use `transform_input_fn` and `transform_output_fn` if the serving endpoint# expects a different input schema and does not return a JSON string,# respectively, or you want to apply a prompt template on top.def transform_input(**request):    full_prompt = f"""{request["prompt"]}    Be Concise.    """    request["prompt"] = full_prompt    return requestllm = Databricks(endpoint_name="dolly", transform_input_fn=transform_input)llm("How are you?")    'I‚Äôm Excellent. You?'Wrapping a cluster driver proxy app‚ÄãPrerequisites:An LLM loaded on a Databricks interactive cluster in "single user" or "no isolation shared" mode.A local HTTP server running on the driver node to serve the model at "/" using HTTP POST with JSON input/output.It uses a port number between [3000, 8000] and listens to the driver IP address or simply 0.0.0.0 instead of localhost only.You have "Can Attach To" permission to the cluster.The expected server schema (using JSON schema) is:inputs:{"type": "object", "properties": {    "prompt": {"type": "string"},     "stop": {"type": "array", "items": {"type": "string"}}},  "required": ["prompt"]}outputs: {"type": "string"}If the server schema is incompatible or you want to insert extra configs, you can use transform_input_fn and transform_output_fn accordingly.The following is a minimal example for running a driver proxy app to serve an LLM:from flask import Flask, request, jsonifyimport torchfrom transformers import pipeline, AutoTokenizer, StoppingCriteriamodel = "databricks/dolly-v2-3b"tokenizer = AutoTokenizer.from_pretrained(model, padding_side="left")dolly = pipeline(model=model, tokenizer=tokenizer, trust_remote_code=True, device_map="auto")device = dolly.deviceclass CheckStop(StoppingCriteria):    def __init__(self, stop=None):        super().__init__()        self.stop = stop or []        self.matched = ""        self.stop_ids = [tokenizer.encode(s, return_tensors='pt').to(device) for s in self.stop]    def __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor, **kwargs):        for i, s in enumerate(self.stop_ids):            if torch.all((s == input_ids[0][-s.shape[1]:])).item():                self.matched = self.stop[i]                return True        return Falsedef llm(prompt, stop=None, **kwargs):  check_stop = CheckStop(stop)  result = dolly(prompt, stopping_criteria=[check_stop], **kwargs)  return result[0]["generated_text"].rstrip(check_stop.matched)app = Flask("dolly")@app.route('/', methods=['POST'])def serve_llm():  resp = llm(**request.json)  return jsonify(resp)app.run(host="0.0.0.0", port="7777")Once the server is running, you can create a Databricks instance to wrap it as an LLM.# If running a Databricks notebook attached to the same cluster that runs the app,# you only need to specify the driver port to create a `Databricks` instance.llm = Databricks(cluster_driver_port="7777")llm("How are you?")    'Hello, thank you for asking. It is wonderful to hear that you are well.'# Otherwise, you can manually specify the cluster ID to use,# as well as Databricks workspace hostname and personal access token.llm = Databricks(cluster_id="0000-000000-xxxxxxxx", cluster_driver_port="7777")llm("How are you?")    'I am well. You?'# If the app accepts extra parameters like `temperature`,# you can set them in `model_kwargs`.llm = Databricks(cluster_driver_port="7777", model_kwargs={"temperature": 0.1})llm("How are you?")    'I am very well. It is a pleasure to meet you.'# Use `transform_input_fn` and `transform_output_fn` if the app# expects a different input schema and does not return a JSON string,# respectively, or you want to apply a prompt template on top.def transform_input(**request):    full_prompt = f"""{request["prompt"]}    Be Concise.    """    request["prompt"] = full_prompt    return requestdef transform_output(response):    return response.upper()llm = Databricks(    cluster_driver_port="7777",    transform_input_fn=transform_input,    transform_output_fn=transform_output,)llm("How are you?")    'I AM DOING GREAT THANK YOU.' from langchain.llms import Databricks from langchain.llms import Databricks  API Reference:Databricks # If running a Databricks notebook attached to an interactive cluster in "single user"# or "no isolation shared" mode, you only need to specify the endpoint name to create# a `Databricks` instance to query a serving endpoint in the same workspace.llm = Databricks(endpoint_name="dolly")llm("How are you?") # If running a Databricks notebook attached to an interactive cluster in "single user"# or "no isolation shared" mode, you only need to specify the endpoint name to create# a `Databricks` instance to query a serving endpoint in the same workspace.llm = Databricks(endpoint_name="dolly")llm("How are you?")      'I am happy to hear that you are in good health and as always, you are appreciated.'     'I am happy to hear that you are in good health and as always, you are appreciated.'     'I am happy to hear that you are in good health and as always, you are appreciated.'  llm("How are you?", stop=["."]) llm("How are you?", stop=["."])      'Good'     'Good'     'Good'  # Otherwise, you can manually specify the Databricks workspace hostname and personal access token# or set `DATABRICKS_HOST` and `DATABRICKS_TOKEN` environment variables, respectively.# See https://docs.databricks.com/dev-tools/auth.html#databricks-personal-access-tokens# We strongly recommend not exposing the API token explicitly inside a notebook.# You can use Databricks secret manager to store your API token securely.# See https://docs.databricks.com/dev-tools/databricks-utils.html#secrets-utility-dbutilssecretsimport osos.environ["DATABRICKS_TOKEN"] = dbutils.secrets.get("myworkspace", "api_token")llm = Databricks(host="myworkspace.cloud.databricks.com", endpoint_name="dolly")llm("How are you?") # Otherwise, you can manually specify the Databricks workspace hostname and personal access token# or set `DATABRICKS_HOST` and `DATABRICKS_TOKEN` environment variables, respectively.# See https://docs.databricks.com/dev-tools/auth.html#databricks-personal-access-tokens# We strongly recommend not exposing the API token explicitly inside a notebook.# You can use Databricks secret manager to store your API token securely.# See https://docs.databricks.com/dev-tools/databricks-utils.html#secrets-utility-dbutilssecretsimport osos.environ["DATABRICKS_TOKEN"] = dbutils.secrets.get("myworkspace", "api_token")llm = Databricks(host="myworkspace.cloud.databricks.com", endpoint_name="dolly")llm("How are you?")      'I am fine. Thank you!'     'I am fine. Thank you!'     'I am fine. Thank you!'  # If the serving endpoint accepts extra parameters like `temperature`,# you can set them in `model_kwargs`.llm = Databricks(endpoint_name="dolly", model_kwargs={"temperature": 0.1})llm("How are you?") # If the serving endpoint accepts extra parameters like `temperature`,# you can set them in `model_kwargs`.llm = Databricks(endpoint_name="dolly", model_kwargs={"temperature": 0.1})llm("How are you?")      'I am fine.'     'I am fine.'     'I am fine.'  # Use `transform_input_fn` and `transform_output_fn` if the serving endpoint# expects a different input schema and does not return a JSON string,# respectively, or you want to apply a prompt template on top.def transform_input(**request):    full_prompt = f"""{request["prompt"]}    Be Concise.    """    request["prompt"] = full_prompt    return requestllm = Databricks(endpoint_name="dolly", transform_input_fn=transform_input)llm("How are you?") # Use `transform_input_fn` and `transform_output_fn` if the serving endpoint# expects a different input schema and does not return a JSON string,# respectively, or you want to apply a prompt template on top.def transform_input(**request):    full_prompt = f"""{request["prompt"]}    Be Concise.    """    request["prompt"] = full_prompt    return requestllm = Databricks(endpoint_name="dolly", transform_input_fn=transform_input)llm("How are you?")      'I‚Äôm Excellent. You?'     'I‚Äôm Excellent. You?'     'I‚Äôm Excellent. You?'  {"type": "object", "properties": {    "prompt": {"type": "string"},     "stop": {"type": "array", "items": {"type": "string"}}},  "required": ["prompt"]} {"type": "object", "properties": {    "prompt": {"type": "string"},     "stop": {"type": "array", "items": {"type": "string"}}},  "required": ["prompt"]}  from flask import Flask, request, jsonifyimport torchfrom transformers import pipeline, AutoTokenizer, StoppingCriteriamodel = "databricks/dolly-v2-3b"tokenizer = AutoTokenizer.from_pretrained(model, padding_side="left")dolly = pipeline(model=model, tokenizer=tokenizer, trust_remote_code=True, device_map="auto")device = dolly.deviceclass CheckStop(StoppingCriteria):    def __init__(self, stop=None):        super().__init__()        self.stop = stop or []        self.matched = ""        self.stop_ids = [tokenizer.encode(s, return_tensors='pt').to(device) for s in self.stop]    def __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor, **kwargs):        for i, s in enumerate(self.stop_ids):            if torch.all((s == input_ids[0][-s.shape[1]:])).item():                self.matched = self.stop[i]                return True        return Falsedef llm(prompt, stop=None, **kwargs):  check_stop = CheckStop(stop)  result = dolly(prompt, stopping_criteria=[check_stop], **kwargs)  return result[0]["generated_text"].rstrip(check_stop.matched)app = Flask("dolly")@app.route('/', methods=['POST'])def serve_llm():  resp = llm(**request.json)  return jsonify(resp)app.run(host="0.0.0.0", port="7777") from flask import Flask, request, jsonifyimport torchfrom transformers import pipeline, AutoTokenizer, StoppingCriteriamodel = "databricks/dolly-v2-3b"tokenizer = AutoTokenizer.from_pretrained(model, padding_side="left")dolly = pipeline(model=model, tokenizer=tokenizer, trust_remote_code=True, device_map="auto")device = dolly.deviceclass CheckStop(StoppingCriteria):    def __init__(self, stop=None):        super().__init__()        self.stop = stop or []        self.matched = ""        self.stop_ids = [tokenizer.encode(s, return_tensors='pt').to(device) for s in self.stop]    def __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor, **kwargs):        for i, s in enumerate(self.stop_ids):            if torch.all((s == input_ids[0][-s.shape[1]:])).item():                self.matched = self.stop[i]                return True        return Falsedef llm(prompt, stop=None, **kwargs):  check_stop = CheckStop(stop)  result = dolly(prompt, stopping_criteria=[check_stop], **kwargs)  return result[0]["generated_text"].rstrip(check_stop.matched)app = Flask("dolly")@app.route('/', methods=['POST'])def serve_llm():  resp = llm(**request.json)  return jsonify(resp)app.run(host="0.0.0.0", port="7777")  # If running a Databricks notebook attached to the same cluster that runs the app,# you only need to specify the driver port to create a `Databricks` instance.llm = Databricks(cluster_driver_port="7777")llm("How are you?") # If running a Databricks notebook attached to the same cluster that runs the app,# you only need to specify the driver port to create a `Databricks` instance.llm = Databricks(cluster_driver_port="7777")llm("How are you?")      'Hello, thank you for asking. It is wonderful to hear that you are well.'     'Hello, thank you for asking. It is wonderful to hear that you are well.'     'Hello, thank you for asking. It is wonderful to hear that you are well.'  # Otherwise, you can manually specify the cluster ID to use,# as well as Databricks workspace hostname and personal access token.llm = Databricks(cluster_id="0000-000000-xxxxxxxx", cluster_driver_port="7777")llm("How are you?") # Otherwise, you can manually specify the cluster ID to use,# as well as Databricks workspace hostname and personal access token.llm = Databricks(cluster_id="0000-000000-xxxxxxxx", cluster_driver_port="7777")llm("How are you?")      'I am well. You?'     'I am well. You?'     'I am well. You?'  # If the app accepts extra parameters like `temperature`,# you can set them in `model_kwargs`.llm = Databricks(cluster_driver_port="7777", model_kwargs={"temperature": 0.1})llm("How are you?") # If the app accepts extra parameters like `temperature`,# you can set them in `model_kwargs`.llm = Databricks(cluster_driver_port="7777", model_kwargs={"temperature": 0.1})llm("How are you?")      'I am very well. It is a pleasure to meet you.'     'I am very well. It is a pleasure to meet you.'     'I am very well. It is a pleasure to meet you.'  # Use `transform_input_fn` and `transform_output_fn` if the app# expects a different input schema and does not return a JSON string,# respectively, or you want to apply a prompt template on top.def transform_input(**request):    full_prompt = f"""{request["prompt"]}    Be Concise.    """    request["prompt"] = full_prompt    return requestdef transform_output(response):    return response.upper()llm = Databricks(    cluster_driver_port="7777",    transform_input_fn=transform_input,    transform_output_fn=transform_output,)llm("How are you?") # Use `transform_input_fn` and `transform_output_fn` if the app# expects a different input schema and does not return a JSON string,# respectively, or you want to apply a prompt template on top.def transform_input(**request):    full_prompt = f"""{request["prompt"]}    Be Concise.    """    request["prompt"] = full_prompt    return requestdef transform_output(response):    return response.upper()llm = Databricks(    cluster_driver_port="7777",    transform_input_fn=transform_input,    transform_output_fn=transform_output,)llm("How are you?")      'I AM DOING GREAT THANK YOU.'     'I AM DOING GREAT THANK YOU.'     'I AM DOING GREAT THANK YOU.'  Previous CTranslate2 Next DeepInfra Wrapping a serving endpointWrapping a cluster driver proxy app Wrapping a serving endpointWrapping a cluster driver proxy app CommunityDiscordTwitterGitHubPythonJS/TSMoreHomepageBlogCopyright ¬© 2023 LangChain, Inc. CommunityDiscordTwitterGitHubPythonJS/TSMoreHomepageBlog CommunityDiscordTwitter Community GitHubPythonJS/TS GitHub MoreHomepageBlog More Copyright ¬© 2023 LangChain, Inc. Copyright ¬© 2023 LangChain, Inc. Skip to main content (#docusaurus_skipToContent_fallback) ü¶úÔ∏èüîó LangChain (/) Docs (/docs/get_started/introduction) Use cases (/docs/use_cases) Integrations (/docs/integrations) API (https://api.python.langchain.com) LangSmith (https://smith.langchain.com) JS/TS Docs (https://js.langchain.com/docs)  (https://github.com/hwchase17/langchain) Integrations (/docs/integrations) Callbacks (/docs/integrations/callbacks/) Chat models (/docs/integrations/chat/) Chat loaders (/docs/integrations/chat_loaders/) Document loaders (/docs/integrations/document_loaders/) Document transformers (/docs/integrations/document_transformers/) LLMs (/docs/integrations/llms/) AI21 (/docs/integrations/llms/ai21) Aleph Alpha (/docs/integrations/llms/aleph_alpha) Amazon API Gateway (/docs/integrations/llms/amazon_api_gateway) Anyscale (/docs/integrations/llms/anyscale) Azure ML (/docs/integrations/llms/azure_ml) Azure OpenAI (/docs/integrations/llms/azure_openai) Banana (/docs/integrations/llms/banana) Baseten (/docs/integrations/llms/baseten) Beam (/docs/integrations/llms/beam) Bedrock (/docs/integrations/llms/bedrock) Bittensor (/docs/integrations/llms/bittensor) CerebriumAI (/docs/integrations/llms/cerebriumai) ChatGLM (/docs/integrations/llms/chatglm) Clarifai (/docs/integrations/llms/clarifai) Cohere (/docs/integrations/llms/cohere) C Transformers (/docs/integrations/llms/ctransformers) CTranslate2 (/docs/integrations/llms/ctranslate2) Databricks (/docs/integrations/llms/databricks) DeepInfra (/docs/integrations/llms/deepinfra) DeepSparse (/docs/integrations/llms/deepsparse) Eden AI (/docs/integrations/llms/edenai) Fireworks (/docs/integrations/llms/fireworks) ForefrontAI (/docs/integrations/llms/forefrontai) Google Vertex AI PaLM (/docs/integrations/llms/google_vertex_ai_palm) GooseAI (/docs/integrations/llms/gooseai) GPT4All (/docs/integrations/llms/gpt4all) Hugging Face Hub (/docs/integrations/llms/huggingface_hub) Hugging Face Local Pipelines (/docs/integrations/llms/huggingface_pipelines) Huggingface TextGen Inference (/docs/integrations/llms/huggingface_textgen_inference) JSONFormer (/docs/integrations/llms/jsonformer_experimental) KoboldAI API (/docs/integrations/llms/koboldai) Llama.cpp (/docs/integrations/llms/llamacpp) LLM Caching integrations (/docs/integrations/llms/llm_caching) Manifest (/docs/integrations/llms/manifest) Minimax (/docs/integrations/llms/minimax) Modal (/docs/integrations/llms/modal) MosaicML (/docs/integrations/llms/mosaicml) NLP Cloud (/docs/integrations/llms/nlpcloud) OctoAI (/docs/integrations/llms/octoai) Ollama (/docs/integrations/llms/ollama) OpaquePrompts (/docs/integrations/llms/opaqueprompts) OpenAI (/docs/integrations/llms/openai) OpenLLM (/docs/integrations/llms/openllm) OpenLM (/docs/integrations/llms/openlm) Petals (/docs/integrations/llms/petals) PipelineAI (/docs/integrations/llms/pipelineai) Predibase (/docs/integrations/llms/predibase) Prediction Guard (/docs/integrations/llms/predictionguard) PromptLayer OpenAI (/docs/integrations/llms/promptlayer_openai) RELLM (/docs/integrations/llms/rellm_experimental) Replicate (/docs/integrations/llms/replicate) Runhouse (/docs/integrations/llms/runhouse) SageMakerEndpoint (/docs/integrations/llms/sagemaker) StochasticAI (/docs/integrations/llms/stochasticai) Nebula (Symbl.ai) (/docs/integrations/llms/symblai_nebula) TextGen (/docs/integrations/llms/textgen) Titan Takeoff (/docs/integrations/llms/titan_takeoff) Tongyi Qwen (/docs/integrations/llms/tongyi) vLLM (/docs/integrations/llms/vllm) Writer (/docs/integrations/llms/writer) Xorbits Inference (Xinference) (/docs/integrations/llms/xinference) Memory (/docs/integrations/memory/) Retrievers (/docs/integrations/retrievers/) Text embedding models (/docs/integrations/text_embedding/) Agents & Toolkits (/docs/integrations/toolkits/) Tools (/docs/integrations/tools/) Vector stores (/docs/integrations/vectorstores/) Grouped by provider (/docs/integrations/providers/)  (/) Integrations (/docs/integrations) LLMs (/docs/integrations/llms/) Databricks (https://www.databricks.com/) Databricks (https://api.python.langchain.com/en/latest/llms/langchain.llms.databricks.Databricks.html) ‚Äã (#wrapping-a-serving-endpoint) a Databricks serving endpoint (https://docs.databricks.com/machine-learning/model-serving/index.html) "Can Query" permission (https://docs.databricks.com/security/auth-authz/access-control/serving-endpoint-acl.html) ‚Äã (#wrapping-a-cluster-driver-proxy-app) PreviousCTranslate2 (/docs/integrations/llms/ctranslate2) NextDeepInfra (/docs/integrations/llms/deepinfra) Wrapping a serving endpoint (#wrapping-a-serving-endpoint) Wrapping a cluster driver proxy app (#wrapping-a-cluster-driver-proxy-app) Discord (https://discord.gg/cU2adEyC7w) Twitter (https://twitter.com/LangChainAI) Python (https://github.com/hwchase17/langchain) JS/TS (https://github.com/hwchase17/langchainjs) Homepage (https://langchain.com) Blog (https://blog.langchain.dev)