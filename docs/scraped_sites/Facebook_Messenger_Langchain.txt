This notebook shows how to load data from Facebook in a format you can finetune on. The overall steps are: Once this has been done, you can fine-tune your model. To do so you would complete the following steps: Let's begin. To download your own messenger data, following instructions here. IMPORTANT - make sure to download them in JSON format (not HTML). We are hosting an example dump at this google drive link that we will use in this walkthrough. We have 2 different FacebookMessengerChatLoader classes, one for an entire directory of chats, and one to load individual files. We Calling load() returns all the chat messages we could extract as human messages. When conversing with chat bots, conversations typically follow a more strict alternating dialogue pattern relative to real conversations.  You can choose to merge message "runs" (consecutive messages from the same sender) and select a sender to represent the "AI". The fine-tuned LLM will learn to generate these AI messages. OpenAI currently requires at least 10 training examples for a fine-tuning job, though they recommend between 50-100 for most tasks. Since we only have 9 chat sessions, we can subdivide them (optionally with some overlap) so that each training example is comprised of a portion of a whole conversation. Facebook chat sessions (1 per person) often span multiple days and conversations, so the long-range dependencies may not be that important to model anyhow. It's time to fine-tune the model. Make sure you have openai installed and have set your OPENAI_API_KEY appropriately With the file ready, it's time to kick off a training job. Grab a cup of tea while your model is being prepared. This may take some time! You can use the resulting model ID directly the ChatOpenAI model class. IntegrationsCallbacksChat modelsChat loadersDiscordFacebook MessengerGMailiMessageSlackTelegramTwitter (via Apify)WhatsAppDocument loadersDocument transformersLLMsMemoryRetrieversText embedding modelsAgents & ToolkitsToolsVector storesGrouped by provider Callbacks Chat models Chat loadersDiscordFacebook MessengerGMailiMessageSlackTelegramTwitter (via Apify)WhatsApp Discord Facebook Messenger GMail iMessage Slack Telegram Twitter (via Apify) WhatsApp Document loaders Document transformers LLMs Memory Retrievers Text embedding models Agents & Toolkits Tools Vector stores Grouped by provider  Integrations Chat loaders Facebook Messenger Download your messenger data to disk. Create the Chat Loader and call loader.load() (or loader.lazy_load()) to perform the conversion. Optionally use merge_chat_runs to combine message from the same sender in sequence, and/or map_ai_messages to convert messages from the specified sender to the "AIMessage" class. Once you've done this, call convert_messages_for_finetuning to prepare your data for fine-tuning. Upload your messages to OpenAI and run a fine-tuning job. Use the resulting model in your LangChain app! SingleFileFacebookMessengerChatLoader FolderFacebookMessengerChatLoader merge_chat_runs map_ai_messages convert_messages_for_finetuning ChatOpenAI ChatPromptTemplate StrOutputParser 1. Download Data 2. Create Chat Loader 3. Prepare for fine-tuning 4. Fine-tune the model 5. Use in LangChain Discord Twitter Python JS/TS Homepage Blog Skip to main content🦜️🔗 LangChainDocsUse casesIntegrationsAPILangSmithJS/TS DocsCTRLKIntegrationsCallbacksChat modelsChat loadersDiscordFacebook MessengerGMailiMessageSlackTelegramTwitter (via Apify)WhatsAppDocument loadersDocument transformersLLMsMemoryRetrieversText embedding modelsAgents & ToolkitsToolsVector storesGrouped by providerIntegrationsChat loadersFacebook MessengerOn this pageFacebook MessengerThis notebook shows how to load data from Facebook in a format you can finetune on. The overall steps are:Download your messenger data to disk.Create the Chat Loader and call loader.load() (or loader.lazy_load()) to perform the conversion.Optionally use merge_chat_runs to combine message from the same sender in sequence, and/or map_ai_messages to convert messages from the specified sender to the "AIMessage" class. Once you've done this, call convert_messages_for_finetuning to prepare your data for fine-tuning.Once this has been done, you can fine-tune your model. To do so you would complete the following steps:Upload your messages to OpenAI and run a fine-tuning job.Use the resulting model in your LangChain app!Let's begin.1. Download Data​To download your own messenger data, following instructions here. IMPORTANT - make sure to download them in JSON format (not HTML).We are hosting an example dump at this google drive link that we will use in this walkthrough.# This uses some example dataimport requestsimport zipfiledef download_and_unzip(url: str, output_path: str = 'file.zip') -> None:    file_id = url.split('/')[-2]    download_url = f'https://drive.google.com/uc?export=download&id={file_id}'    response = requests.get(download_url)    if response.status_code != 200:        print('Failed to download the file.')        return    with open(output_path, 'wb') as file:        file.write(response.content)        print(f'File {output_path} downloaded.')    with zipfile.ZipFile(output_path, 'r') as zip_ref:        zip_ref.extractall()        print(f'File {output_path} has been unzipped.')# URL of the file to downloadurl = 'https://drive.google.com/file/d/1rh1s1o2i7B-Sk1v9o8KNgivLVGwJ-osV/view?usp=sharing'# Download and unzipdownload_and_unzip(url)    File file.zip downloaded.    File file.zip has been unzipped.2. Create Chat Loader​We have 2 different FacebookMessengerChatLoader classes, one for an entire directory of chats, and one to load individual files. Wedirectory_path = "./hogwarts"from langchain.chat_loaders.facebook_messenger import (    SingleFileFacebookMessengerChatLoader,    FolderFacebookMessengerChatLoader,)API Reference:SingleFileFacebookMessengerChatLoaderFolderFacebookMessengerChatLoaderloader = SingleFileFacebookMessengerChatLoader(    path="./hogwarts/inbox/HermioneGranger/messages_Hermione_Granger.json",)chat_session = loader.load()[0]chat_session["messages"][:3]    [HumanMessage(content="Hi Hermione! How's your summer going so far?", additional_kwargs={'sender': 'Harry Potter'}, example=False),     HumanMessage(content="Harry! Lovely to hear from you. My summer is going well, though I do miss everyone. I'm spending most of my time going through my books and researching fascinating new topics. How about you?", additional_kwargs={'sender': 'Hermione Granger'}, example=False),     HumanMessage(content="I miss you all too. The Dursleys are being their usual unpleasant selves but I'm getting by. At least I can practice some spells in my room without them knowing. Let me know if you find anything good in your researching!", additional_kwargs={'sender': 'Harry Potter'}, example=False)]loader = FolderFacebookMessengerChatLoader(    path="./hogwarts",)chat_sessions = loader.load()len(chat_sessions)    93. Prepare for fine-tuning​Calling load() returns all the chat messages we could extract as human messages. When conversing with chat bots, conversations typically follow a more strict alternating dialogue pattern relative to real conversations. You can choose to merge message "runs" (consecutive messages from the same sender) and select a sender to represent the "AI". The fine-tuned LLM will learn to generate these AI messages.from langchain.chat_loaders.utils import (    merge_chat_runs,    map_ai_messages,)API Reference:merge_chat_runsmap_ai_messagesmerged_sessions = merge_chat_runs(chat_sessions)alternating_sessions = list(map_ai_messages(merged_sessions, "Harry Potter"))# Now all of Harry Potter's messages will take the AI message class# which maps to the 'assistant' role in OpenAI's training formatalternating_sessions[0]['messages'][:3]    [AIMessage(content="Professor Snape, I was hoping I could speak with you for a moment about something that's been concerning me lately.", additional_kwargs={'sender': 'Harry Potter'}, example=False),     HumanMessage(content="What is it, Potter? I'm quite busy at the moment.", additional_kwargs={'sender': 'Severus Snape'}, example=False),     AIMessage(content="I apologize for the interruption, sir. I'll be brief. I've noticed some strange activity around the school grounds at night. I saw a cloaked figure lurking near the Forbidden Forest last night. I'm worried someone may be plotting something sinister.", additional_kwargs={'sender': 'Harry Potter'}, example=False)]Now we can convert to OpenAI format dictionaries​from langchain.adapters.openai import convert_messages_for_finetuningAPI Reference:convert_messages_for_finetuningtraining_data = convert_messages_for_finetuning(alternating_sessions)print(f"Prepared {len(training_data)} dialogues for training")    Prepared 9 dialogues for trainingtraining_data[0][:3]    [{'role': 'assistant',      'content': "Professor Snape, I was hoping I could speak with you for a moment about something that's been concerning me lately."},     {'role': 'user',      'content': "What is it, Potter? I'm quite busy at the moment."},     {'role': 'assistant',      'content': "I apologize for the interruption, sir. I'll be brief. I've noticed some strange activity around the school grounds at night. I saw a cloaked figure lurking near the Forbidden Forest last night. I'm worried someone may be plotting something sinister."}]OpenAI currently requires at least 10 training examples for a fine-tuning job, though they recommend between 50-100 for most tasks. Since we only have 9 chat sessions, we can subdivide them (optionally with some overlap) so that each training example is comprised of a portion of a whole conversation.Facebook chat sessions (1 per person) often span multiple days and conversations, so the long-range dependencies may not be that important to model anyhow.# Our chat is alternating, we will make each datapoint a group of 8 messages,# with 2 messages overlappingchunk_size = 8overlap = 2training_examples = [    conversation_messages[i: i + chunk_size]     for conversation_messages in training_data    for i in range(        0, len(conversation_messages) - chunk_size + 1,         chunk_size - overlap)]len(training_examples)    1004. Fine-tune the model​It's time to fine-tune the model. Make sure you have openai installed and have set your OPENAI_API_KEY appropriately# %pip install -U openai --quietimport jsonfrom io import BytesIOimport timeimport openai# We will write the jsonl file in memorymy_file = BytesIO()for m in training_examples:    my_file.write((json.dumps({"messages": m}) + "\n").encode('utf-8'))my_file.seek(0)training_file = openai.File.create(  file=my_file,  purpose='fine-tune')# OpenAI audits each training file for compliance reasons.# This make take a few minutesstatus = openai.File.retrieve(training_file.id).statusstart_time = time.time()while status != "processed":    print(f"Status=[{status}]... {time.time() - start_time:.2f}s", end="\r", flush=True)    time.sleep(5)    status = openai.File.retrieve(training_file.id).statusprint(f"File {training_file.id} ready after {time.time() - start_time:.2f} seconds.")    File file-zCyNBeg4snpbBL7VkvsuhCz8 ready afer 30.55 seconds.With the file ready, it's time to kick off a training job.job = openai.FineTuningJob.create(    training_file=training_file.id,    model="gpt-3.5-turbo",)Grab a cup of tea while your model is being prepared. This may take some time!status = openai.FineTuningJob.retrieve(job.id).statusstart_time = time.time()while status != "succeeded":    print(f"Status=[{status}]... {time.time() - start_time:.2f}s", end="\r", flush=True)    time.sleep(5)    job = openai.FineTuningJob.retrieve(job.id)    status = job.status    Status=[running]... 908.87sprint(job.fine_tuned_model)    ft:gpt-3.5-turbo-0613:personal::7rDwkaOq5. Use in LangChain​You can use the resulting model ID directly the ChatOpenAI model class.from langchain.chat_models import ChatOpenAImodel = ChatOpenAI(    model=job.fine_tuned_model,    temperature=1,)API Reference:ChatOpenAIfrom langchain.prompts import ChatPromptTemplatefrom langchain.schema.output_parser import StrOutputParserprompt = ChatPromptTemplate.from_messages(    [        ("human", "{input}"),    ])chain = prompt | model | StrOutputParser()API Reference:ChatPromptTemplateStrOutputParserfor tok in chain.stream({"input": "What classes are you taking?"}):    print(tok, end="", flush=True)    The usual - Potions, Transfiguration, Defense Against the Dark Arts. What about you?PreviousDiscordNextGMail1. Download Data2. Create Chat Loader3. Prepare for fine-tuning4. Fine-tune the model5. Use in LangChainCommunityDiscordTwitterGitHubPythonJS/TSMoreHomepageBlogCopyright © 2023 LangChain, Inc. Skip to main content 🦜️🔗 LangChainDocsUse casesIntegrationsAPILangSmithJS/TS DocsCTRLK 🦜️🔗 LangChainDocsUse casesIntegrationsAPI LangSmithJS/TS DocsCTRLK  CTRLK CTRLK  CTRLK   IntegrationsCallbacksChat modelsChat loadersDiscordFacebook MessengerGMailiMessageSlackTelegramTwitter (via Apify)WhatsAppDocument loadersDocument transformersLLMsMemoryRetrieversText embedding modelsAgents & ToolkitsToolsVector storesGrouped by providerIntegrationsChat loadersFacebook MessengerOn this pageFacebook MessengerThis notebook shows how to load data from Facebook in a format you can finetune on. The overall steps are:Download your messenger data to disk.Create the Chat Loader and call loader.load() (or loader.lazy_load()) to perform the conversion.Optionally use merge_chat_runs to combine message from the same sender in sequence, and/or map_ai_messages to convert messages from the specified sender to the "AIMessage" class. Once you've done this, call convert_messages_for_finetuning to prepare your data for fine-tuning.Once this has been done, you can fine-tune your model. To do so you would complete the following steps:Upload your messages to OpenAI and run a fine-tuning job.Use the resulting model in your LangChain app!Let's begin.1. Download Data​To download your own messenger data, following instructions here. IMPORTANT - make sure to download them in JSON format (not HTML).We are hosting an example dump at this google drive link that we will use in this walkthrough.# This uses some example dataimport requestsimport zipfiledef download_and_unzip(url: str, output_path: str = 'file.zip') -> None:    file_id = url.split('/')[-2]    download_url = f'https://drive.google.com/uc?export=download&id={file_id}'    response = requests.get(download_url)    if response.status_code != 200:        print('Failed to download the file.')        return    with open(output_path, 'wb') as file:        file.write(response.content)        print(f'File {output_path} downloaded.')    with zipfile.ZipFile(output_path, 'r') as zip_ref:        zip_ref.extractall()        print(f'File {output_path} has been unzipped.')# URL of the file to downloadurl = 'https://drive.google.com/file/d/1rh1s1o2i7B-Sk1v9o8KNgivLVGwJ-osV/view?usp=sharing'# Download and unzipdownload_and_unzip(url)    File file.zip downloaded.    File file.zip has been unzipped.2. Create Chat Loader​We have 2 different FacebookMessengerChatLoader classes, one for an entire directory of chats, and one to load individual files. Wedirectory_path = "./hogwarts"from langchain.chat_loaders.facebook_messenger import (    SingleFileFacebookMessengerChatLoader,    FolderFacebookMessengerChatLoader,)API Reference:SingleFileFacebookMessengerChatLoaderFolderFacebookMessengerChatLoaderloader = SingleFileFacebookMessengerChatLoader(    path="./hogwarts/inbox/HermioneGranger/messages_Hermione_Granger.json",)chat_session = loader.load()[0]chat_session["messages"][:3]    [HumanMessage(content="Hi Hermione! How's your summer going so far?", additional_kwargs={'sender': 'Harry Potter'}, example=False),     HumanMessage(content="Harry! Lovely to hear from you. My summer is going well, though I do miss everyone. I'm spending most of my time going through my books and researching fascinating new topics. How about you?", additional_kwargs={'sender': 'Hermione Granger'}, example=False),     HumanMessage(content="I miss you all too. The Dursleys are being their usual unpleasant selves but I'm getting by. At least I can practice some spells in my room without them knowing. Let me know if you find anything good in your researching!", additional_kwargs={'sender': 'Harry Potter'}, example=False)]loader = FolderFacebookMessengerChatLoader(    path="./hogwarts",)chat_sessions = loader.load()len(chat_sessions)    93. Prepare for fine-tuning​Calling load() returns all the chat messages we could extract as human messages. When conversing with chat bots, conversations typically follow a more strict alternating dialogue pattern relative to real conversations. You can choose to merge message "runs" (consecutive messages from the same sender) and select a sender to represent the "AI". The fine-tuned LLM will learn to generate these AI messages.from langchain.chat_loaders.utils import (    merge_chat_runs,    map_ai_messages,)API Reference:merge_chat_runsmap_ai_messagesmerged_sessions = merge_chat_runs(chat_sessions)alternating_sessions = list(map_ai_messages(merged_sessions, "Harry Potter"))# Now all of Harry Potter's messages will take the AI message class# which maps to the 'assistant' role in OpenAI's training formatalternating_sessions[0]['messages'][:3]    [AIMessage(content="Professor Snape, I was hoping I could speak with you for a moment about something that's been concerning me lately.", additional_kwargs={'sender': 'Harry Potter'}, example=False),     HumanMessage(content="What is it, Potter? I'm quite busy at the moment.", additional_kwargs={'sender': 'Severus Snape'}, example=False),     AIMessage(content="I apologize for the interruption, sir. I'll be brief. I've noticed some strange activity around the school grounds at night. I saw a cloaked figure lurking near the Forbidden Forest last night. I'm worried someone may be plotting something sinister.", additional_kwargs={'sender': 'Harry Potter'}, example=False)]Now we can convert to OpenAI format dictionaries​from langchain.adapters.openai import convert_messages_for_finetuningAPI Reference:convert_messages_for_finetuningtraining_data = convert_messages_for_finetuning(alternating_sessions)print(f"Prepared {len(training_data)} dialogues for training")    Prepared 9 dialogues for trainingtraining_data[0][:3]    [{'role': 'assistant',      'content': "Professor Snape, I was hoping I could speak with you for a moment about something that's been concerning me lately."},     {'role': 'user',      'content': "What is it, Potter? I'm quite busy at the moment."},     {'role': 'assistant',      'content': "I apologize for the interruption, sir. I'll be brief. I've noticed some strange activity around the school grounds at night. I saw a cloaked figure lurking near the Forbidden Forest last night. I'm worried someone may be plotting something sinister."}]OpenAI currently requires at least 10 training examples for a fine-tuning job, though they recommend between 50-100 for most tasks. Since we only have 9 chat sessions, we can subdivide them (optionally with some overlap) so that each training example is comprised of a portion of a whole conversation.Facebook chat sessions (1 per person) often span multiple days and conversations, and have set your OPENAI_API_KEY appropriately# %pip install -U openai --quietimport jsonfrom io import BytesIOimport timeimport openai# We will write the jsonl file in memorymy_file = BytesIO()for m in training_examples:    my_file.write((json.dumps({"messages": m}) + "\n").encode('utf-8'))my_file.seek(0)training_file = openai.File.create(  file=my_file,  purpose='fine-tune')# OpenAI audits each training file for compliance reasons.# This make take a few minutesstatus = openai.File.retrieve(training_file.id).statusstart_time = time.time()while status != "processed":    print(f"Status=[{status}]... {time.time() - start_time:.2f}s", end="\r", flush=True)    time.sleep(5)    status = openai.File.retrieve(training_file.id).statusprint(f"File {training_file.id} ready after {time.time() - start_time:.2f} seconds.")    File file-zCyNBeg4snpbBL7VkvsuhCz8 ready afer 30.55 seconds.With the file ready, it's time to kick off a training job.job = openai.FineTuningJob.create(    training_file=training_file.id,    model="gpt-3.5-turbo",)Grab a cup of tea while your model is being prepared. This may take some time!status = openai.FineTuningJob.retrieve(job.id).statusstart_time = time.time()while status != "succeeded":    print(f"Status=[{status}]... {time.time() - start_time:.2f}s", end="\r", flush=True)    time.sleep(5)    job = openai.FineTuningJob.retrieve(job.id)    status = job.status    Status=[running]... 908.87sprint(job.fine_tuned_model)    ft:gpt-3.5-turbo-0613:personal::7rDwkaOq5. Use in LangChain​You can use the resulting model ID directly the ChatOpenAI model class.from langchain.chat_models import ChatOpenAImodel = ChatOpenAI(    model=job.fine_tuned_model,    temperature=1,)API Reference:ChatOpenAIfrom langchain.prompts import ChatPromptTemplatefrom langchain.schema.output_parser import StrOutputParserprompt = ChatPromptTemplate.from_messages(    [        ("human", "{input}"),    ])chain = prompt | model | StrOutputParser()API Reference:ChatPromptTemplateStrOutputParserfor tok in chain.stream({"input": "What classes are you taking?"}):    print(tok, end="", flush=True)    The usual - Potions, Transfiguration, Defense Against the Dark Arts. What about you?PreviousDiscordNextGMail1. Download Data2. Create Chat Loader3. Prepare for fine-tuning4. Fine-tune the model5. Use in LangChain IntegrationsCallbacksChat modelsChat loadersDiscordFacebook MessengerGMailiMessageSlackTelegramTwitter (via Apify)WhatsAppDocument loadersDocument transformersLLMsMemoryRetrieversText embedding modelsAgents & ToolkitsToolsVector storesGrouped by providerIntegrationsChat loadersFacebook MessengerOn this pageFacebook MessengerThis notebook shows how to load data from Facebook in a format you can finetune on. The overall steps are:Download your messenger data to disk.Create the Chat Loader and call loader.load() (or loader.lazy_load()) to perform the conversion.Optionally use merge_chat_runs to combine message from the same sender in sequence, and/or map_ai_messages to convert messages from the specified sender to the "AIMessage" class. Once you've done this, call convert_messages_for_finetuning to prepare your data for fine-tuning.Once this has been done, you can fine-tune your model. To do so you would complete the following steps:Upload your messages to OpenAI and run a fine-tuning job.Use the resulting model in your LangChain app!Let's begin.1. Download Data​To download your own messenger data, following instructions here. IMPORTANT - make sure to download them in JSON format (not HTML).We are hosting an example dump at this google drive link that we will use in this walkthrough.# This uses some example dataimport requestsimport zipfiledef download_and_unzip(url: str, output_path: str = 'file.zip') -> None:    file_id = url.split('/')[-2]    download_url = f'https://drive.google.com/uc?export=download&id={file_id}'    response = requests.get(download_url)    if response.status_code != 200:        print('Failed to download the file.')        return    with open(output_path, 'wb') as file:        file.write(response.content)        print(f'File {output_path} downloaded.')    with zipfile.ZipFile(output_path, 'r') as zip_ref:        zip_ref.extractall()        print(f'File {output_path} has been unzipped.')# URL of the file to downloadurl = 'https://drive.google.com/file/d/1rh1s1o2i7B-Sk1v9o8KNgivLVGwJ-osV/view?usp=sharing'# Download and unzipdownload_and_unzip(url)    File file.zip downloaded.    File file.zip has been unzipped.2. Create Chat Loader​We have 2 different FacebookMessengerChatLoader classes, one for an entire directory of chats, and one to load individual files. Wedirectory_path = "./hogwarts"from langchain.chat_loaders.facebook_messenger import (    SingleFileFacebookMessengerChatLoader,    FolderFacebookMessengerChatLoader,)API Reference:SingleFileFacebookMessengerChatLoaderFolderFacebookMessengerChatLoaderloader = SingleFileFacebookMessengerChatLoader(    path="./hogwarts/inbox/HermioneGranger/messages_Hermione_Granger.json",)chat_session = loader.load()[0]chat_session["messages"][:3]    [HumanMessage(content="Hi Hermione! How's your summer going so far?", additional_kwargs={'sender': 'Harry Potter'}, example=False),     HumanMessage(content="Harry! Lovely to hear from you. My summer is going well, though I do miss everyone. I'm spending most of my time going through my books and researching fascinating new topics. How about you?", additional_kwargs={'sender': 'Hermione Granger'}, example=False),     HumanMessage(content="I miss you all too. The Dursleys are being their usual unpleasant selves but I'm getting by. At least I can practice some spells in my room without them knowing. Let me know if you find anything good in your researching!", additional_kwargs={'sender': 'Harry Potter'}, example=False)]loader = FolderFacebookMessengerChatLoader(    path="./hogwarts",)chat_sessions = loader.load()len(chat_sessions)    93. Prepare for fine-tuning​Calling load() returns all the chat messages we could extract as human messages. When conversing with chat bots, conversations typically follow a more strict alternating dialogue pattern relative to real conversations. You can choose to merge message "runs" (consecutive messages from the same sender) and select a sender to represent the "AI". The fine-tuned LLM will learn to generate these AI messages.from langchain.chat_loaders.utils import (    merge_chat_runs,    map_ai_messages,)API Reference:merge_chat_runsmap_ai_messagesmerged_sessions = merge_chat_runs(chat_sessions)alternating_sessions = list(map_ai_messages(merged_sessions, "Harry Potter"))# Now all of Harry Potter's messages will take the AI message class# which maps to the 'assistant' role in OpenAI's training formatalternating_sessions[0]['messages'][:3]    [AIMessage(content="Professor Snape, I was hoping I could speak with you for a moment about something that's been concerning me lately.", additional_kwargs={'sender': 'Harry Potter'}, example=False),     HumanMessage(content="What is it, Potter? I'm quite busy at the moment.", additional_kwargs={'sender': 'Severus Snape'}, example=False),     AIMessage(content="I apologize for the interruption, sir. I'll be brief. I've noticed some strange activity around the school grounds at night. I saw a cloaked figure lurking near the Forbidden Forest last night. I'm worried someone may be plotting something sinister.", additional_kwargs={'sender': 'Harry Potter'}, example=False)]Now we can convert to OpenAI format dictionaries​from langchain.adapters.openai import convert_messages_for_finetuningAPI Reference:convert_messages_for_finetuningtraining_data = convert_messages_for_finetuning(alternating_sessions)print(f"Prepared {len(training_data)} dialogues for training")    Prepared 9 dialogues for trainingtraining_data[0][:3]    [{'role': 'assistant',      'content': "Professor Snape, I was hoping I could speak with you for a moment about something that's been concerning me lately."},     {'role': 'user',      'content': "What is it, Potter? I'm quite busy at the moment."},     {'role': 'assistant',      'content': "I apologize for the interruption, sir. I'll be brief. I've noticed some strange activity around the school grounds at night. I saw a cloaked figure lurking near the Forbidden Forest last night. I'm worried someone may be plotting something sinister."}]OpenAI currently requires at least 10 training examples for a fine-tuning job, though they recommend between 50-100 for most tasks. Since we only have 9 chat sessions, we can subdivide them (optionally with some overlap) so that each training example is comprised of a portion of a whole conversation.Facebook chat sessions (1 per person) often span multiple days and conversations, and have set your OPENAI_API_KEY appropriately# %pip install -U openai --quietimport jsonfrom io import BytesIOimport timeimport openai# We will write the jsonl file in memorymy_file = BytesIO()for m in training_examples:    my_file.write((json.dumps({"messages": m}) + "\n").encode('utf-8'))my_file.seek(0)training_file = openai.File.create(  file=my_file,  purpose='fine-tune')# OpenAI audits each training file for compliance reasons.# This make take a few minutesstatus = openai.File.retrieve(training_file.id).statusstart_time = time.time()while status != "processed":    print(f"Status=[{status}]... {time.time() - start_time:.2f}s", end="\r", flush=True)    time.sleep(5)    status = openai.File.retrieve(training_file.id).statusprint(f"File {training_file.id} ready after {time.time() - start_time:.2f} seconds.")    File file-zCyNBeg4snpbBL7VkvsuhCz8 ready afer 30.55 seconds.With the file ready, it's time to kick off a training job.job = openai.FineTuningJob.create(    training_file=training_file.id,    model="gpt-3.5-turbo",)Grab a cup of tea while your model is being prepared. This may take some time!status = openai.FineTuningJob.retrieve(job.id).statusstart_time = time.time()while status != "succeeded":    print(f"Status=[{status}]... {time.time() - start_time:.2f}s", end="\r", flush=True)    time.sleep(5)    job = openai.FineTuningJob.retrieve(job.id)    status = job.status    Status=[running]... 908.87sprint(job.fine_tuned_model)    ft:gpt-3.5-turbo-0613:personal::7rDwkaOq5. Use in LangChain​You can use the resulting model ID directly the ChatOpenAI model class.from langchain.chat_models import ChatOpenAImodel = ChatOpenAI(    model=job.fine_tuned_model,    temperature=1,)API Reference:ChatOpenAIfrom langchain.prompts import ChatPromptTemplatefrom langchain.schema.output_parser import StrOutputParserprompt = ChatPromptTemplate.from_messages(    [        ("human", "{input}"),    ])chain = prompt | model | StrOutputParser()API Reference:ChatPromptTemplateStrOutputParserfor tok in chain.stream({"input": "What classes are you taking?"}):    print(tok, end="", flush=True)    The usual - Potions, Transfiguration, Defense Against the Dark Arts. What about you?PreviousDiscordNextGMail1. Download Data2. Create Chat Loader3. Prepare for fine-tuning4. Fine-tune the model5. Use in LangChain IntegrationsCallbacksChat modelsChat loadersDiscordFacebook MessengerGMailiMessageSlackTelegramTwitter (via Apify)WhatsAppDocument loadersDocument transformersLLMsMemoryRetrieversText embedding modelsAgents & ToolkitsToolsVector storesGrouped by provider IntegrationsCallbacksChat modelsChat loadersDiscordFacebook MessengerGMailiMessageSlackTelegramTwitter (via Apify)WhatsAppDocument loadersDocument transformersLLMsMemoryRetrieversText embedding modelsAgents & ToolkitsToolsVector storesGrouped by provider Integrations Callbacks Chat models Chat loaders Document loaders Document transformers LLMs Memory Retrievers Text embedding models Agents & Toolkits Tools Vector stores Grouped by provider IntegrationsChat loadersFacebook MessengerOn this pageFacebook MessengerThis notebook shows how to load data from Facebook in a format you can finetune on. The overall steps are:Download your messenger data to disk.Create the Chat Loader and call loader.load() (or loader.lazy_load()) to perform the conversion.Optionally use merge_chat_runs to combine message from the same sender in sequence, and/or map_ai_messages to convert messages from the specified sender to the "AIMessage" class. Once you've done this, call convert_messages_for_finetuning to prepare your data for fine-tuning.Once this has been done, you can fine-tune your model. To do so you would complete the following steps:Upload your messages to OpenAI and run a fine-tuning job.Use the resulting model in your LangChain app!Let's begin.1. Download Data​To download your own messenger data, following instructions here. IMPORTANT - make sure to download them in JSON format (not HTML).We are hosting an example dump at this google drive link that we will use in this walkthrough.# This uses some example dataimport requestsimport zipfiledef download_and_unzip(url: str, output_path: str = 'file.zip') -> None:    file_id = url.split('/')[-2]    download_url = f'https://drive.google.com/uc?export=download&id={file_id}'    response = requests.get(download_url)    if response.status_code != 200:        print('Failed to download the file.')        return    with open(output_path, 'wb') as file:        file.write(response.content)        print(f'File {output_path} downloaded.')    with zipfile.ZipFile(output_path, 'r') as zip_ref:        zip_ref.extractall()        print(f'File {output_path} has been unzipped.')# URL of the file to downloadurl = 'https://drive.google.com/file/d/1rh1s1o2i7B-Sk1v9o8KNgivLVGwJ-osV/view?usp=sharing'# Download and unzipdownload_and_unzip(url)    File file.zip downloaded.    File file.zip has been unzipped.2. Create Chat Loader​We have 2 different FacebookMessengerChatLoader classes, one for an entire directory of chats, and one to load individual files. Wedirectory_path = "./hogwarts"from langchain.chat_loaders.facebook_messenger import (    SingleFileFacebookMessengerChatLoader,    FolderFacebookMessengerChatLoader,)API Reference:SingleFileFacebookMessengerChatLoaderFolderFacebookMessengerChatLoaderloader = SingleFileFacebookMessengerChatLoader(    path="./hogwarts/inbox/HermioneGranger/messages_Hermione_Granger.json",)chat_session = loader.load()[0]chat_session["messages"][:3]    [HumanMessage(content="Hi Hermione! How's your summer going so far?", additional_kwargs={'sender': 'Harry Potter'}, example=False),     HumanMessage(content="Harry! Lovely to hear from you. My summer is going well, though I do miss everyone. I'm spending most of my time going through my books and researching fascinating new topics. How about you?", additional_kwargs={'sender': 'Hermione Granger'}, example=False),     HumanMessage(content="I miss you all too. The Dursleys are being their usual unpleasant selves but I'm getting by. At least I can practice some spells in my room without them knowing. Let me know if you find anything good in your researching!", additional_kwargs={'sender': 'Harry Potter'}, example=False)]loader = FolderFacebookMessengerChatLoader(    path="./hogwarts",)chat_sessions = loader.load()len(chat_sessions)    93. Prepare for fine-tuning​Calling load() returns all the chat messages we could extract as human messages. When conversing with chat bots, conversations typically follow a more strict alternating dialogue pattern relative to real conversations. You can choose to merge message "runs" (consecutive messages from the same sender) and select a sender to represent the "AI". The fine-tuned LLM will learn to generate these AI messages.from langchain.chat_loaders.utils import (    merge_chat_runs,    map_ai_messages,)API Reference:merge_chat_runsmap_ai_messagesmerged_sessions = merge_chat_runs(chat_sessions)alternating_sessions = list(map_ai_messages(merged_sessions, "Harry Potter"))# Now all of Harry Potter's messages will take the AI message class# which maps to the 'assistant' role in OpenAI's training formatalternating_sessions[0]['messages'][:3]    [AIMessage(content="Professor Snape, I was hoping I could speak with you for a moment about something that's been concerning me lately.", additional_kwargs={'sender': 'Harry Potter'}, example=False),     HumanMessage(content="What is it, Potter? I'm quite busy at the moment.", additional_kwargs={'sender': 'Severus Snape'}, example=False),     AIMessage(content="I apologize for the interruption, sir. I'll be brief. I've noticed some strange activity around the school grounds at night. I saw a cloaked figure lurking near the Forbidden Forest last night. I'm worried someone may be plotting something sinister.", additional_kwargs={'sender': 'Harry Potter'}, example=False)]Now we can convert to OpenAI format dictionaries​from langchain.adapters.openai import convert_messages_for_finetuningAPI Reference:convert_messages_for_finetuningtraining_data = convert_messages_for_finetuning(alternating_sessions)print(f"Prepared {len(training_data)} dialogues for training")    Prepared 9 dialogues for trainingtraining_data[0][:3]    [{'role': 'assistant',      'content': "Professor Snape, I was hoping I could speak with you for a moment about something that's been concerning me lately."},     {'role': 'user',      'content': "What is it, Potter? I'm quite busy at the moment."},     {'role': 'assistant',      'content': "I apologize for the interruption, sir. I'll be brief. I've noticed some strange activity around the school grounds at night. I saw a cloaked figure lurking near the Forbidden Forest last night. I'm worried someone may be plotting something sinister."}]OpenAI currently requires at least 10 training examples for a fine-tuning job, though they recommend between 50-100 for most tasks. Since we only have 9 chat sessions, we can subdivide them (optionally with some overlap) so that each training example is comprised of a portion of a whole conversation.Facebook chat sessions (1 per person) often span multiple days and conversations, and have set your OPENAI_API_KEY appropriately# %pip install -U openai --quietimport jsonfrom io import BytesIOimport timeimport openai# We will write the jsonl file in memorymy_file = BytesIO()for m in training_examples:    my_file.write((json.dumps({"messages": m}) + "\n").encode('utf-8'))my_file.seek(0)training_file = openai.File.create(  file=my_file,  purpose='fine-tune')# OpenAI audits each training file for compliance reasons.# This make take a few minutesstatus = openai.File.retrieve(training_file.id).statusstart_time = time.time()while status != "processed":    print(f"Status=[{status}]... {time.time() - start_time:.2f}s", end="\r", flush=True)    time.sleep(5)    status = openai.File.retrieve(training_file.id).statusprint(f"File {training_file.id} ready after {time.time() - start_time:.2f} seconds.")    File file-zCyNBeg4snpbBL7VkvsuhCz8 ready afer 30.55 seconds.With the file ready, it's time to kick off a training job.job = openai.FineTuningJob.create(    training_file=training_file.id,    model="gpt-3.5-turbo",)Grab a cup of tea while your model is being prepared. This may take some time!status = openai.FineTuningJob.retrieve(job.id).statusstart_time = time.time()while status != "succeeded":    print(f"Status=[{status}]... {time.time() - start_time:.2f}s", end="\r", flush=True)    time.sleep(5)    job = openai.FineTuningJob.retrieve(job.id)    status = job.status    Status=[running]... 908.87sprint(job.fine_tuned_model)    ft:gpt-3.5-turbo-0613:personal::7rDwkaOq5. Use in LangChain​You can use the resulting model ID directly the ChatOpenAI model class.from langchain.chat_models import ChatOpenAImodel = ChatOpenAI(    model=job.fine_tuned_model,    temperature=1,)API Reference:ChatOpenAIfrom langchain.prompts import ChatPromptTemplatefrom langchain.schema.output_parser import StrOutputParserprompt = ChatPromptTemplate.from_messages(    [        ("human", "{input}"),    ])chain = prompt | model | StrOutputParser()API Reference:ChatPromptTemplateStrOutputParserfor tok in chain.stream({"input": "What classes are you taking?"}):    print(tok, end="", flush=True)    The usual - Potions, Transfiguration, Defense Against the Dark Arts. What about you?PreviousDiscordNextGMail1. Download Data2. Create Chat Loader3. Prepare for fine-tuning4. Fine-tune the model5. Use in LangChain IntegrationsChat loadersFacebook MessengerOn this pageFacebook MessengerThis notebook shows how to load data from Facebook in a format you can finetune on. The overall steps are:Download your messenger data to disk.Create the Chat Loader and call loader.load() (or loader.lazy_load()) to perform the conversion.Optionally use merge_chat_runs to combine message from the same sender in sequence, and/or map_ai_messages to convert messages from the specified sender to the "AIMessage" class. Once you've done this, call convert_messages_for_finetuning to prepare your data for fine-tuning.Once this has been done, you can fine-tune your model. To do so you would complete the following steps:Upload your messages to OpenAI and run a fine-tuning job.Use the resulting model in your LangChain app!Let's begin.1. Download Data​To download your own messenger data, following instructions here. IMPORTANT - make sure to download them in JSON format (not HTML).We are hosting an example dump at this google drive link that we will use in this walkthrough.# This uses some example dataimport requestsimport zipfiledef download_and_unzip(url: str, output_path: str = 'file.zip') -> None:    file_id = url.split('/')[-2]    download_url = f'https://drive.google.com/uc?export=download&id={file_id}'    response = requests.get(download_url)    if response.status_code != 200:        print('Failed to download the file.')        return    with open(output_path, 'wb') as file:        file.write(response.content)        print(f'File {output_path} downloaded.')    with zipfile.ZipFile(output_path, 'r') as zip_ref:        zip_ref.extractall()        print(f'File {output_path} has been unzipped.')# URL of the file to downloadurl = 'https://drive.google.com/file/d/1rh1s1o2i7B-Sk1v9o8KNgivLVGwJ-osV/view?usp=sharing'# Download and unzipdownload_and_unzip(url)    File file.zip downloaded.    File file.zip has been unzipped.2. Create Chat Loader​We have 2 different FacebookMessengerChatLoader classes, one for an entire directory of chats, and one to load individual files. Wedirectory_path = "./hogwarts"from langchain.chat_loaders.facebook_messenger import (    SingleFileFacebookMessengerChatLoader,    FolderFacebookMessengerChatLoader,)API Reference:SingleFileFacebookMessengerChatLoaderFolderFacebookMessengerChatLoaderloader = SingleFileFacebookMessengerChatLoader(    path="./hogwarts/inbox/HermioneGranger/messages_Hermione_Granger.json",)chat_session = loader.load()[0]chat_session["messages"][:3]    [HumanMessage(content="Hi Hermione! How's your summer going so far?", additional_kwargs={'sender': 'Harry Potter'}, example=False),     HumanMessage(content="Harry! Lovely to hear from you. My summer is going well, though I do miss everyone. I'm spending most of my time going through my books and researching fascinating new topics. How about you?", additional_kwargs={'sender': 'Hermione Granger'}, example=False),     HumanMessage(content="I miss you all too. The Dursleys are being their usual unpleasant selves but I'm getting by. At least I can practice some spells in my room without them knowing. Let me know if you find anything good in your researching!", additional_kwargs={'sender': 'Harry Potter'}, example=False)]loader = FolderFacebookMessengerChatLoader(    path="./hogwarts",)chat_sessions = loader.load()len(chat_sessions)    93. Prepare for fine-tuning​Calling load() returns all the chat messages we could extract as human messages. When conversing with chat bots, conversations typically follow a more strict alternating dialogue pattern relative to real conversations. You can choose to merge message "runs" (consecutive messages from the same sender) and select a sender to represent the "AI". The fine-tuned LLM will learn to generate these AI messages.from langchain.chat_loaders.utils import (    merge_chat_runs,    map_ai_messages,)API Reference:merge_chat_runsmap_ai_messagesmerged_sessions = merge_chat_runs(chat_sessions)alternating_sessions = list(map_ai_messages(merged_sessions, "Harry Potter"))# Now all of Harry Potter's messages will take the AI message class# which maps to the 'assistant' role in OpenAI's training formatalternating_sessions[0]['messages'][:3]    [AIMessage(content="Professor Snape, I was hoping I could speak with you for a moment about something that's been concerning me lately.", additional_kwargs={'sender': 'Harry Potter'}, example=False),     HumanMessage(content="What is it, Potter? I'm quite busy at the moment.", additional_kwargs={'sender': 'Severus Snape'}, example=False),     AIMessage(content="I apologize for the interruption, sir. I'll be brief. I've noticed some strange activity around the school grounds at night. I saw a cloaked figure lurking near the Forbidden Forest last night. I'm worried someone may be plotting something sinister.", additional_kwargs={'sender': 'Harry Potter'}, example=False)]Now we can convert to OpenAI format dictionaries​from langchain.adapters.openai import convert_messages_for_finetuningAPI Reference:convert_messages_for_finetuningtraining_data = convert_messages_for_finetuning(alternating_sessions)print(f"Prepared {len(training_data)} dialogues for training")    Prepared 9 dialogues for trainingtraining_data[0][:3]    [{'role': 'assistant',      'content': "Professor Snape, I was hoping I could speak with you for a moment about something that's been concerning me lately."},     {'role': 'user',      'content': "What is it, Potter? I'm quite busy at the moment."},     {'role': 'assistant',      'content': "I apologize for the interruption, sir. I'll be brief. I've noticed some strange activity around the school grounds at night. I saw a cloaked figure lurking near the Forbidden Forest last night. I'm worried someone may be plotting something sinister."}]OpenAI currently requires at least 10 training examples for a fine-tuning job, though they recommend between 50-100 for most tasks. Since we only have 9 chat sessions, we can subdivide them (optionally with some overlap) so that each training example is comprised of a portion of a whole conversation.Facebook chat sessions (1 per person) often span multiple days and conversations, and have set your OPENAI_API_KEY appropriately# %pip install -U openai --quietimport jsonfrom io import BytesIOimport timeimport openai# We will write the jsonl file in memorymy_file = BytesIO()for m in training_examples:    my_file.write((json.dumps({"messages": m}) + "\n").encode('utf-8'))my_file.seek(0)training_file = openai.File.create(  file=my_file,  purpose='fine-tune')# OpenAI audits each training file for compliance reasons.# This make take a few minutesstatus = openai.File.retrieve(training_file.id).statusstart_time = time.time()while status != "processed":    print(f"Status=[{status}]... {time.time() - start_time:.2f}s", end="\r", flush=True)    time.sleep(5)    status = openai.File.retrieve(training_file.id).statusprint(f"File {training_file.id} ready after {time.time() - start_time:.2f} seconds.")    File file-zCyNBeg4snpbBL7VkvsuhCz8 ready afer 30.55 seconds.With the file ready, it's time to kick off a training job.job = openai.FineTuningJob.create(    training_file=training_file.id,    model="gpt-3.5-turbo",)Grab a cup of tea while your model is being prepared. This may take some time!status = openai.FineTuningJob.retrieve(job.id).statusstart_time = time.time()while status != "succeeded":    print(f"Status=[{status}]... {time.time() - start_time:.2f}s", end="\r", flush=True)    time.sleep(5)    job = openai.FineTuningJob.retrieve(job.id)    status = job.status    Status=[running]... 908.87sprint(job.fine_tuned_model)    ft:gpt-3.5-turbo-0613:personal::7rDwkaOq5. Use in LangChain​You can use the resulting model ID directly the ChatOpenAI model class.from langchain.chat_models import ChatOpenAImodel = ChatOpenAI(    model=job.fine_tuned_model,    temperature=1,)API Reference:ChatOpenAIfrom langchain.prompts import ChatPromptTemplatefrom langchain.schema.output_parser import StrOutputParserprompt = ChatPromptTemplate.from_messages(    [        ("human", "{input}"),    ])chain = prompt | model | StrOutputParser()API Reference:ChatPromptTemplateStrOutputParserfor tok in chain.stream({"input": "What classes are you taking?"}):    print(tok, end="", flush=True)    The usual - Potions, Transfiguration, Defense Against the Dark Arts. What about you?PreviousDiscordNextGMail IntegrationsChat loadersFacebook MessengerOn this pageFacebook MessengerThis notebook shows how to load data from Facebook in a format you can finetune on. The overall steps are:Download your messenger data to disk.Create the Chat Loader and call loader.load() (or loader.lazy_load()) to perform the conversion.Optionally use merge_chat_runs to combine message from the same sender in sequence, and/or map_ai_messages to convert messages from the specified sender to the "AIMessage" class. Once you've done this, call convert_messages_for_finetuning to prepare your data for fine-tuning.Once this has been done, you can fine-tune your model. To do so you would complete the following steps:Upload your messages to OpenAI and run a fine-tuning job.Use the resulting model in your LangChain app!Let's begin.1. Download Data​To download your own messenger data, following instructions here. IMPORTANT - make sure to download them in JSON format (not HTML).We are hosting an example dump at this google drive link that we will use in this walkthrough.# This uses some example dataimport requestsimport zipfiledef download_and_unzip(url: str, output_path: str = 'file.zip') -> None:    file_id = url.split('/')[-2]    download_url = f'https://drive.google.com/uc?export=download&id={file_id}'    response = requests.get(download_url)    if response.status_code != 200:        print('Failed to download the file.')        return    with open(output_path, 'wb') as file:        file.write(response.content)        print(f'File {output_path} downloaded.')    with zipfile.ZipFile(output_path, 'r') as zip_ref:        zip_ref.extractall()        print(f'File {output_path} has been unzipped.')# URL of the file to downloadurl = 'https://drive.google.com/file/d/1rh1s1o2i7B-Sk1v9o8KNgivLVGwJ-osV/view?usp=sharing'# Download and unzipdownload_and_unzip(url)    File file.zip downloaded.    File file.zip has been unzipped.2. Create Chat Loader​We have 2 different FacebookMessengerChatLoader classes, one for an entire directory of chats, and one to load individual files. Wedirectory_path = "./hogwarts"from langchain.chat_loaders.facebook_messenger import (    SingleFileFacebookMessengerChatLoader,    FolderFacebookMessengerChatLoader,)API Reference:SingleFileFacebookMessengerChatLoaderFolderFacebookMessengerChatLoaderloader = SingleFileFacebookMessengerChatLoader(    path="./hogwarts/inbox/HermioneGranger/messages_Hermione_Granger.json",)chat_session = loader.load()[0]chat_session["messages"][:3]    [HumanMessage(content="Hi Hermione! How's your summer going so far?", additional_kwargs={'sender': 'Harry Potter'}, example=False),     HumanMessage(content="Harry! Lovely to hear from you. My summer is going well, though I do miss everyone. I'm spending most of my time going through my books and researching fascinating new topics. How about you?", additional_kwargs={'sender': 'Hermione Granger'}, example=False),     HumanMessage(content="I miss you all too. The Dursleys are being their usual unpleasant selves but I'm getting by. At least I can practice some spells in my room without them knowing. Let me know if you find anything good in your researching!", additional_kwargs={'sender': 'Harry Potter'}, example=False)]loader = FolderFacebookMessengerChatLoader(    path="./hogwarts",)chat_sessions = loader.load()len(chat_sessions)    93. Prepare for fine-tuning​Calling load() returns all the chat messages we could extract as human messages. When conversing with chat bots, conversations typically follow a more strict alternating dialogue pattern relative to real conversations. You can choose to merge message "runs" (consecutive messages from the same sender) and select a sender to represent the "AI". The fine-tuned LLM will learn to generate these AI messages.from langchain.chat_loaders.utils import (    merge_chat_runs,    map_ai_messages,)API Reference:merge_chat_runsmap_ai_messagesmerged_sessions = merge_chat_runs(chat_sessions)alternating_sessions = list(map_ai_messages(merged_sessions, "Harry Potter"))# Now all of Harry Potter's messages will take the AI message class# which maps to the 'assistant' role in OpenAI's training formatalternating_sessions[0]['messages'][:3]    [AIMessage(content="Professor Snape, I was hoping I could speak with you for a moment about something that's been concerning me lately.", additional_kwargs={'sender': 'Harry Potter'}, example=False),     HumanMessage(content="What is it, Potter? I'm quite busy at the moment.", additional_kwargs={'sender': 'Severus Snape'}, example=False),     AIMessage(content="I apologize for the interruption, sir. I'll be brief. I've noticed some strange activity around the school grounds at night. I saw a cloaked figure lurking near the Forbidden Forest last night. I'm worried someone may be plotting something sinister.", additional_kwargs={'sender': 'Harry Potter'}, example=False)]Now we can convert to OpenAI format dictionaries​from langchain.adapters.openai import convert_messages_for_finetuningAPI Reference:convert_messages_for_finetuningtraining_data = convert_messages_for_finetuning(alternating_sessions)print(f"Prepared {len(training_data)} dialogues for training")    Prepared 9 dialogues for trainingtraining_data[0][:3]    [{'role': 'assistant',      'content': "Professor Snape, I was hoping I could speak with you for a moment about something that's been concerning me lately."},     {'role': 'user',      'content': "What is it, Potter? I'm quite busy at the moment."},     {'role': 'assistant',      'content': "I apologize for the interruption, sir. I'll be brief. I've noticed some strange activity around the school grounds at night. I saw a cloaked figure lurking near the Forbidden Forest last night. I'm worried someone may be plotting something sinister."}]OpenAI currently requires at least 10 training examples for a fine-tuning job, though they recommend between 50-100 for most tasks. Since we only have 9 chat sessions, we can subdivide them (optionally with some overlap) so that each training example is comprised of a portion of a whole conversation.Facebook chat sessions (1 per person) often span multiple days and conversations, and have set your OPENAI_API_KEY appropriately# %pip install -U openai --quietimport jsonfrom io import BytesIOimport timeimport openai# We will write the jsonl file in memorymy_file = BytesIO()for m in training_examples:    my_file.write((json.dumps({"messages": m}) + "\n").encode('utf-8'))my_file.seek(0)training_file = openai.File.create(  file=my_file,  purpose='fine-tune')# OpenAI audits each training file for compliance reasons.# This make take a few minutesstatus = openai.File.retrieve(training_file.id).statusstart_time = time.time()while status != "processed":    print(f"Status=[{status}]... {time.time() - start_time:.2f}s", end="\r", flush=True)    time.sleep(5)    status = openai.File.retrieve(training_file.id).statusprint(f"File {training_file.id} ready after {time.time() - start_time:.2f} seconds.")    File file-zCyNBeg4snpbBL7VkvsuhCz8 ready afer 30.55 seconds.With the file ready, it's time to kick off a training job.job = openai.FineTuningJob.create(    training_file=training_file.id,    model="gpt-3.5-turbo",)Grab a cup of tea while your model is being prepared. This may take some time!status = openai.FineTuningJob.retrieve(job.id).statusstart_time = time.time()while status != "succeeded":    print(f"Status=[{status}]... {time.time() - start_time:.2f}s", end="\r", flush=True)    time.sleep(5)    job = openai.FineTuningJob.retrieve(job.id)    status = job.status    Status=[running]... 908.87sprint(job.fine_tuned_model)    ft:gpt-3.5-turbo-0613:personal::7rDwkaOq5. Use in LangChain​You can use the resulting model ID directly the ChatOpenAI model class.from langchain.chat_models import ChatOpenAImodel = ChatOpenAI(    model=job.fine_tuned_model,    temperature=1,)API Reference:ChatOpenAIfrom langchain.prompts import ChatPromptTemplatefrom langchain.schema.output_parser import StrOutputParserprompt = ChatPromptTemplate.from_messages(    [        ("human", "{input}"),    ])chain = prompt | model | StrOutputParser()API Reference:ChatPromptTemplateStrOutputParserfor tok in chain.stream({"input": "What classes are you taking?"}):    print(tok, end="", flush=True)    The usual - Potions, Transfiguration, Defense Against the Dark Arts. What about you?PreviousDiscordNextGMail On this page Facebook MessengerThis notebook shows how to load data from Facebook in a format you can finetune on. The overall steps are:Download your messenger data to disk.Create the Chat Loader and call loader.load() (or loader.lazy_load()) to perform the conversion.Optionally use merge_chat_runs to combine message from the same sender in sequence, and/or map_ai_messages to convert messages from the specified sender to the "AIMessage" class. Once you've done this, call convert_messages_for_finetuning to prepare your data for fine-tuning.Once this has been done, you can fine-tune your model. To do so you would complete the following steps:Upload your messages to OpenAI and run a fine-tuning job.Use the resulting model in your LangChain app!Let's begin.1. Download Data​To download your own messenger data, following instructions here. IMPORTANT - make sure to download them in JSON format (not HTML).We are hosting an example dump at this google drive link that we will use in this walkthrough.# This uses some example dataimport requestsimport zipfiledef download_and_unzip(url: str, output_path: str = 'file.zip') -> None:    file_id = url.split('/')[-2]    download_url = f'https://drive.google.com/uc?export=download&id={file_id}'    response = requests.get(download_url)    if response.status_code != 200:        print('Failed to download the file.')        return    with open(output_path, 'wb') as file:        file.write(response.content)        print(f'File {output_path} downloaded.')    with zipfile.ZipFile(output_path, 'r') as zip_ref:        zip_ref.extractall()        print(f'File {output_path} has been unzipped.')# URL of the file to downloadurl = 'https://drive.google.com/file/d/1rh1s1o2i7B-Sk1v9o8KNgivLVGwJ-osV/view?usp=sharing'# Download and unzipdownload_and_unzip(url)    File file.zip downloaded.    File file.zip has been unzipped.2. Create Chat Loader​We have 2 different FacebookMessengerChatLoader classes, one for an entire directory of chats, and one to load individual files. Wedirectory_path = "./hogwarts"from langchain.chat_loaders.facebook_messenger import (    SingleFileFacebookMessengerChatLoader,    FolderFacebookMessengerChatLoader,)API Reference:SingleFileFacebookMessengerChatLoaderFolderFacebookMessengerChatLoaderloader = SingleFileFacebookMessengerChatLoader(    path="./hogwarts/inbox/HermioneGranger/messages_Hermione_Granger.json",)chat_session = loader.load()[0]chat_session["messages"][:3]    [HumanMessage(content="Hi Hermione! How's your summer going so far?", additional_kwargs={'sender': 'Harry Potter'}, example=False),     HumanMessage(content="Harry! Lovely to hear from you. My summer is going well, though I do miss everyone. I'm spending most of my time going through my books and researching fascinating new topics. How about you?", additional_kwargs={'sender': 'Hermione Granger'}, example=False),     HumanMessage(content="I miss you all too. The Dursleys are being their usual unpleasant selves but I'm getting by. At least I can practice some spells in my room without them knowing. Let me know if you find anything good in your researching!", additional_kwargs={'sender': 'Harry Potter'}, example=False)]loader = FolderFacebookMessengerChatLoader(    path="./hogwarts",)chat_sessions = loader.load()len(chat_sessions)    93. Prepare for fine-tuning​Calling load() returns all the chat messages we could extract as human messages. When conversing with chat bots, conversations typically follow a more strict alternating dialogue pattern relative to real conversations. You can choose to merge message "runs" (consecutive messages from the same sender) and select a sender to represent the "AI". The fine-tuned LLM will learn to generate these AI messages.from langchain.chat_loaders.utils import (    merge_chat_runs,    map_ai_messages,)API Reference:merge_chat_runsmap_ai_messagesmerged_sessions = merge_chat_runs(chat_sessions)alternating_sessions = list(map_ai_messages(merged_sessions, "Harry Potter"))# Now all of Harry Potter's messages will take the AI message class# which maps to the 'assistant' role in OpenAI's training formatalternating_sessions[0]['messages'][:3]    [AIMessage(content="Professor Snape, I was hoping I could speak with you for a moment about something that's been concerning me lately.", additional_kwargs={'sender': 'Harry Potter'}, example=False),     HumanMessage(content="What is it, Potter? I'm quite busy at the moment.", additional_kwargs={'sender': 'Severus Snape'}, example=False),     AIMessage(content="I apologize for the interruption, sir. I'll be brief. I've noticed some strange activity around the school grounds at night. I saw a cloaked figure lurking near the Forbidden Forest last night. I'm worried someone may be plotting something sinister.", additional_kwargs={'sender': 'Harry Potter'}, example=False)]Now we can convert to OpenAI format dictionaries​from langchain.adapters.openai import convert_messages_for_finetuningAPI Reference:convert_messages_for_finetuningtraining_data = convert_messages_for_finetuning(alternating_sessions)print(f"Prepared {len(training_data)} dialogues for training")    Prepared 9 dialogues for trainingtraining_data[0][:3]    [{'role': 'assistant',      'content': "Professor Snape, I was hoping I could speak with you for a moment about something that's been concerning me lately."},     {'role': 'user',      'content': "What is it, Potter? I'm quite busy at the moment."},     {'role': 'assistant',      'content': "I apologize for the interruption, sir. I'll be brief. I've noticed some strange activity around the school grounds at night. I saw a cloaked figure lurking near the Forbidden Forest last night. I'm worried someone may be plotting something sinister."}]OpenAI currently requires at least 10 training examples for a fine-tuning job, though they recommend between 50-100 for most tasks. Since we only have 9 chat sessions, we can subdivide them (optionally with some overlap) so that each training example is comprised of a portion of a whole conversation.Facebook chat sessions (1 per person) often span multiple days and conversations, and have set your OPENAI_API_KEY appropriately# %pip install -U openai --quietimport jsonfrom io import BytesIOimport timeimport openai# We will write the jsonl file in memorymy_file = BytesIO()for m in training_examples:    my_file.write((json.dumps({"messages": m}) + "\n").encode('utf-8'))my_file.seek(0)training_file = openai.File.create(  file=my_file,  purpose='fine-tune')# OpenAI audits each training file for compliance reasons.# This make take a few minutesstatus = openai.File.retrieve(training_file.id).statusstart_time = time.time()while status != "processed":    print(f"Status=[{status}]... {time.time() - start_time:.2f}s", end="\r", flush=True)    time.sleep(5)    status = openai.File.retrieve(training_file.id).statusprint(f"File {training_file.id} ready after {time.time() - start_time:.2f} seconds.")    File file-zCyNBeg4snpbBL7VkvsuhCz8 ready afer 30.55 seconds.With the file ready, it's time to kick off a training job.job = openai.FineTuningJob.create(    training_file=training_file.id,    model="gpt-3.5-turbo",)Grab a cup of tea while your model is being prepared. This may take some time!status = openai.FineTuningJob.retrieve(job.id).statusstart_time = time.time()while status != "succeeded":    print(f"Status=[{status}]... {time.time() - start_time:.2f}s", end="\r", flush=True)    time.sleep(5)    job = openai.FineTuningJob.retrieve(job.id)    status = job.status    Status=[running]... 908.87sprint(job.fine_tuned_model)    ft:gpt-3.5-turbo-0613:personal::7rDwkaOq5. Use in LangChain​You can use the resulting model ID directly the ChatOpenAI model class.from langchain.chat_models import ChatOpenAImodel = ChatOpenAI(    model=job.fine_tuned_model,    temperature=1,)API Reference:ChatOpenAIfrom langchain.prompts import ChatPromptTemplatefrom langchain.schema.output_parser import StrOutputParserprompt = ChatPromptTemplate.from_messages(    [        ("human", "{input}"),    ])chain = prompt | model | StrOutputParser()API Reference:ChatPromptTemplateStrOutputParserfor tok in chain.stream({"input": "What classes are you taking?"}):    print(tok, end="", flush=True)    The usual - Potions, Transfiguration, Defense Against the Dark Arts. What about you? # This uses some example dataimport requestsimport zipfiledef download_and_unzip(url: str, output_path: str = 'file.zip') -> None:    file_id = url.split('/')[-2]    download_url = f'https://drive.google.com/uc?export=download&id={file_id}'    response = requests.get(download_url)    if response.status_code != 200:        print('Failed to download the file.')        return    with open(output_path, 'wb') as file:        file.write(response.content)        print(f'File {output_path} downloaded.')    with zipfile.ZipFile(output_path, 'r') as zip_ref:        zip_ref.extractall()        print(f'File {output_path} has been unzipped.')# URL of the file to downloadurl = 'https://drive.google.com/file/d/1rh1s1o2i7B-Sk1v9o8KNgivLVGwJ-osV/view?usp=sharing'# Download and unzipdownload_and_unzip(url) # This uses some example dataimport requestsimport zipfiledef download_and_unzip(url: str, output_path: str = 'file.zip') -> None:    file_id = url.split('/')[-2]    download_url = f'https://drive.google.com/uc?export=download&id={file_id}'    response = requests.get(download_url)    if response.status_code != 200:        print('Failed to download the file.')        return    with open(output_path, 'wb') as file:        file.write(response.content)        print(f'File {output_path} downloaded.')    with zipfile.ZipFile(output_path, 'r') as zip_ref:        zip_ref.extractall()        print(f'File {output_path} has been unzipped.')# URL of the file to downloadurl = 'https://drive.google.com/file/d/1rh1s1o2i7B-Sk1v9o8KNgivLVGwJ-osV/view?usp=sharing'# Download and unzipdownload_and_unzip(url)      File file.zip downloaded.    File file.zip has been unzipped.     File file.zip downloaded.    File file.zip has been unzipped.     File file.zip downloaded.    File file.zip has been unzipped.  directory_path = "./hogwarts" directory_path = "./hogwarts"  from langchain.chat_loaders.facebook_messenger import (    SingleFileFacebookMessengerChatLoader,    FolderFacebookMessengerChatLoader,) from langchain.chat_loaders.facebook_messenger import (    SingleFileFacebookMessengerChatLoader,    FolderFacebookMessengerChatLoader,)  API Reference:SingleFileFacebookMessengerChatLoaderFolderFacebookMessengerChatLoader loader = SingleFileFacebookMessengerChatLoader(    path="./hogwarts/inbox/HermioneGranger/messages_Hermione_Granger.json",) loader = SingleFileFacebookMessengerChatLoader(    path="./hogwarts/inbox/HermioneGranger/messages_Hermione_Granger.json",)  chat_session = loader.load()[0]chat_session["messages"][:3] chat_session = loader.load()[0]chat_session["messages"][:3]      [HumanMessage(content="Hi Hermione! How's your summer going so far?", additional_kwargs={'sender': 'Harry Potter'}, example=False),     HumanMessage(content="Harry! Lovely to hear from you. My summer is going well, though I do miss everyone. I'm spending most of my time going through my books and researching fascinating new topics. How about you?", additional_kwargs={'sender': 'Hermione Granger'}, example=False),     HumanMessage(content="I miss you all too. The Dursleys are being their usual unpleasant selves but I'm getting by. At least I can practice some spells in my room without them knowing. Let me know if you find anything good in your researching!", additional_kwargs={'sender': 'Harry Potter'}, example=False)]     [HumanMessage(content="Hi Hermione! How's your summer going so far?", additional_kwargs={'sender': 'Harry Potter'}, example=False),     HumanMessage(content="Harry! Lovely to hear from you. My summer is going well, though I do miss everyone. I'm spending most of my time going through my books and researching fascinating new topics. How about you?", additional_kwargs={'sender': 'Hermione Granger'}, example=False),     HumanMessage(content="I miss you all too. The Dursleys are being their usual unpleasant selves but I'm getting by. At least I can practice some spells in my room without them knowing. Let me know if you find anything good in your researching!", additional_kwargs={'sender': 'Harry Potter'}, example=False)]     [HumanMessage(content="Hi Hermione! How's your summer going so far?", additional_kwargs={'sender': 'Harry Potter'}, example=False),     HumanMessage(content="Harry! Lovely to hear from you. My summer is going well, though I do miss everyone. I'm spending most of my time going through my books and researching fascinating new topics. How about you?", additional_kwargs={'sender': 'Hermione Granger'}, example=False),     HumanMessage(content="I miss you all too. The Dursleys are being their usual unpleasant selves but I'm getting by. At least I can practice some spells in my room without them knowing. Let me know if you find anything good in your researching!", additional_kwargs={'sender': 'Harry Potter'}, example=False)]  loader = FolderFacebookMessengerChatLoader(    path="./hogwarts",) loader = FolderFacebookMessengerChatLoader(    path="./hogwarts",)  chat_sessions = loader.load()len(chat_sessions) chat_sessions = loader.load()len(chat_sessions)      9     9     9  from langchain.chat_loaders.utils import (    merge_chat_runs,    map_ai_messages,) from langchain.chat_loaders.utils import (    merge_chat_runs,    map_ai_messages,)  API Reference:merge_chat_runsmap_ai_messages merged_sessions = merge_chat_runs(chat_sessions)alternating_sessions = list(map_ai_messages(merged_sessions, "Harry Potter")) merged_sessions = merge_chat_runs(chat_sessions)alternating_sessions = list(map_ai_messages(merged_sessions, "Harry Potter"))  # Now all of Harry Potter's messages will take the AI message class# which maps to the 'assistant' role in OpenAI's training formatalternating_sessions[0]['messages'][:3] # Now all of Harry Potter's messages will take the AI message class# which maps to the 'assistant' role in OpenAI's training formatalternating_sessions[0]['messages'][:3]      [AIMessage(content="Professor Snape, I was hoping I could speak with you for a moment about something that's been concerning me lately.", additional_kwargs={'sender': 'Harry Potter'}, example=False),     HumanMessage(content="What is it, Potter? I'm quite busy at the moment.", additional_kwargs={'sender': 'Severus Snape'}, example=False),     AIMessage(content="I apologize for the interruption, sir. I'll be brief. I've noticed some strange activity around the school grounds at night. I saw a cloaked figure lurking near the Forbidden Forest last night. I'm worried someone may be plotting something sinister.", additional_kwargs={'sender': 'Harry Potter'}, example=False)]     [AIMessage(content="Professor Snape, I was hoping I could speak with you for a moment about something that's been concerning me lately.", additional_kwargs={'sender': 'Harry Potter'}, example=False),     HumanMessage(content="What is it, Potter? I'm quite busy at the moment.", additional_kwargs={'sender': 'Severus Snape'}, example=False),     AIMessage(content="I apologize for the interruption, sir. I'll be brief. I've noticed some strange activity around the school grounds at night. I saw a cloaked figure lurking near the Forbidden Forest last night. I'm worried someone may be plotting something sinister.", additional_kwargs={'sender': 'Harry Potter'}, example=False)]     [AIMessage(content="Professor Snape, I was hoping I could speak with you for a moment about something that's been concerning me lately.", additional_kwargs={'sender': 'Harry Potter'}, example=False),     HumanMessage(content="What is it, Potter? I'm quite busy at the moment.", additional_kwargs={'sender': 'Severus Snape'}, example=False),     AIMessage(content="I apologize for the interruption, sir. I'll be brief. I've noticed some strange activity around the school grounds at night. I saw a cloaked figure lurking near the Forbidden Forest last night. I'm worried someone may be plotting something sinister.", additional_kwargs={'sender': 'Harry Potter'}, example=False)]  from langchain.adapters.openai import convert_messages_for_finetuning from langchain.adapters.openai import convert_messages_for_finetuning  API Reference:convert_messages_for_finetuning training_data = convert_messages_for_finetuning(alternating_sessions)print(f"Prepared {len(training_data)} dialogues for training") training_data = convert_messages_for_finetuning(alternating_sessions)print(f"Prepared {len(training_data)} dialogues for training")      Prepared 9 dialogues for training     Prepared 9 dialogues for training     Prepared 9 dialogues for training  training_data[0][:3] training_data[0][:3]      [{'role': 'assistant',      'content': "Professor Snape, I was hoping I could speak with you for a moment about something that's been concerning me lately."},     {'role': 'user',      'content': "What is it, Potter? I'm quite busy at the moment."},     {'role': 'assistant',      'content': "I apologize for the interruption, sir. I'll be brief. I've noticed some strange activity around the school grounds at night. I saw a cloaked figure lurking near the Forbidden Forest last night. I'm worried someone may be plotting something sinister."}]     [{'role': 'assistant',      'content': "Professor Snape, I was hoping I could speak with you for a moment about something that's been concerning me lately."},     {'role': 'user',      'content': "What is it, Potter? I'm quite busy at the moment."},     {'role': 'assistant',      'content': "I apologize for the interruption, sir. I'll be brief. I've noticed some strange activity around the school grounds at night. I saw a cloaked figure lurking near the Forbidden Forest last night. I'm worried someone may be plotting something sinister."}]     [{'role': 'assistant',      'content': "Professor Snape, I was hoping I could speak with you for a moment about something that's been concerning me lately."},     {'role': 'user',      'content': "What is it, Potter? I'm quite busy at the moment."},     {'role': 'assistant',      'content': "I apologize for the interruption, sir. I'll be brief. I've noticed some strange activity around the school grounds at night. I saw a cloaked figure lurking near the Forbidden Forest last night. I'm worried someone may be plotting something sinister."}]  # Our chat is alternating, we will make each datapoint a group of 8 messages,# with 2 messages overlappingchunk_size = 8overlap = 2training_examples = [    conversation_messages[i: i + chunk_size]     for conversation_messages in training_data    for i in range(        0, len(conversation_messages) - chunk_size + 1,         chunk_size - overlap)]len(training_examples) # Our chat is alternating, we will make each datapoint a group of 8 messages,# with 2 messages overlappingchunk_size = 8overlap = 2training_examples = [    conversation_messages[i: i + chunk_size]     for conversation_messages in training_data    for i in range(        0, len(conversation_messages) - chunk_size + 1,         chunk_size - overlap)]len(training_examples)      100     100     100  # %pip install -U openai --quiet # %pip install -U openai --quiet  import jsonfrom io import BytesIOimport timeimport openai# We will write the jsonl file in memorymy_file = BytesIO()for m in training_examples:    my_file.write((json.dumps({"messages": m}) + "\n").encode('utf-8'))my_file.seek(0)training_file = openai.File.create(  file=my_file,  purpose='fine-tune')# OpenAI audits each training file for compliance reasons.# This make take a few minutesstatus = openai.File.retrieve(training_file.id).statusstart_time = time.time()while status != "processed":    print(f"Status=[{status}]... {time.time() - start_time:.2f}s", end="\r", flush=True)    time.sleep(5)    status = openai.File.retrieve(training_file.id).statusprint(f"File {training_file.id} ready after {time.time() - start_time:.2f} seconds.") import jsonfrom io import BytesIOimport timeimport openai# We will write the jsonl file in memorymy_file = BytesIO()for m in training_examples:    my_file.write((json.dumps({"messages": m}) + "\n").encode('utf-8'))my_file.seek(0)training_file = openai.File.create(  file=my_file,  purpose='fine-tune')# OpenAI audits each training file for compliance reasons.# This make take a few minutesstatus = openai.File.retrieve(training_file.id).statusstart_time = time.time()while status != "processed":    print(f"Status=[{status}]... {time.time() - start_time:.2f}s", end="\r", flush=True)    time.sleep(5)    status = openai.File.retrieve(training_file.id).statusprint(f"File {training_file.id} ready after {time.time() - start_time:.2f} seconds.")      File file-zCyNBeg4snpbBL7VkvsuhCz8 ready afer 30.55 seconds.     File file-zCyNBeg4snpbBL7VkvsuhCz8 ready afer 30.55 seconds.     File file-zCyNBeg4snpbBL7VkvsuhCz8 ready afer 30.55 seconds.  job = openai.FineTuningJob.create(    training_file=training_file.id,    model="gpt-3.5-turbo",) job = openai.FineTuningJob.create(    training_file=training_file.id,    model="gpt-3.5-turbo",)  status = openai.FineTuningJob.retrieve(job.id).statusstart_time = time.time()while status != "succeeded":    print(f"Status=[{status}]... {time.time() - start_time:.2f}s", end="\r", flush=True)    time.sleep(5)    job = openai.FineTuningJob.retrieve(job.id)    status = job.status status = openai.FineTuningJob.retrieve(job.id).statusstart_time = time.time()while status != "succeeded":    print(f"Status=[{status}]... {time.time() - start_time:.2f}s", end="\r", flush=True)    time.sleep(5)    job = openai.FineTuningJob.retrieve(job.id)    status = job.status      Status=[running]... 908.87s     Status=[running]... 908.87s     Status=[running]... 908.87s  print(job.fine_tuned_model) print(job.fine_tuned_model)      ft:gpt-3.5-turbo-0613:personal::7rDwkaOq     ft:gpt-3.5-turbo-0613:personal::7rDwkaOq     ft:gpt-3.5-turbo-0613:personal::7rDwkaOq  from langchain.chat_models import ChatOpenAImodel = ChatOpenAI(    model=job.fine_tuned_model,    temperature=1,) from langchain.chat_models import ChatOpenAImodel = ChatOpenAI(    model=job.fine_tuned_model,    temperature=1,)  API Reference:ChatOpenAI from langchain.prompts import ChatPromptTemplatefrom langchain.schema.output_parser import StrOutputParserprompt = ChatPromptTemplate.from_messages(    [        ("human", "{input}"),    ])chain = prompt | model | StrOutputParser() from langchain.prompts import ChatPromptTemplatefrom langchain.schema.output_parser import StrOutputParserprompt = ChatPromptTemplate.from_messages(    [        ("human", "{input}"),    ])chain = prompt | model | StrOutputParser()  API Reference:ChatPromptTemplateStrOutputParser for tok in chain.stream({"input": "What classes are you taking?"}):    print(tok, end="", flush=True) for tok in chain.stream({"input": "What classes are you taking?"}):    print(tok, end="", flush=True)      The usual - Potions, Transfiguration, Defense Against the Dark Arts. What about you?     The usual - Potions, Transfiguration, Defense Against the Dark Arts. What about you?     The usual - Potions, Transfiguration, Defense Against the Dark Arts. What about you?  Previous Discord Next GMail 1. Download Data2. Create Chat Loader3. Prepare for fine-tuning4. Fine-tune the model5. Use in LangChain 1. Download Data2. Create Chat Loader3. Prepare for fine-tuning4. Fine-tune the model5. Use in LangChain CommunityDiscordTwitterGitHubPythonJS/TSMoreHomepageBlogCopyright © 2023 LangChain, Inc. CommunityDiscordTwitterGitHubPythonJS/TSMoreHomepageBlog CommunityDiscordTwitter Community GitHubPythonJS/TS GitHub MoreHomepageBlog More Copyright © 2023 LangChain, Inc. Copyright © 2023 LangChain, Inc. Skip to main content (#docusaurus_skipToContent_fallback) 🦜️🔗 LangChain (/) Docs (/docs/get_started/introduction) Use cases (/docs/use_cases) Integrations (/docs/integrations) API (https://api.python.langchain.com) LangSmith (https://smith.langchain.com) JS/TS Docs (https://js.langchain.com/docs)  (https://github.com/hwchase17/langchain) Integrations (/docs/integrations) Callbacks (/docs/integrations/callbacks/) Chat models (/docs/integrations/chat/) Chat loaders (/docs/integrations/chat_loaders/) Discord (/docs/integrations/chat_loaders/discord) Facebook Messenger (/docs/integrations/chat_loaders/facebook) GMail (/docs/integrations/chat_loaders/gmail) iMessage (/docs/integrations/chat_loaders/imessage) Slack (/docs/integrations/chat_loaders/slack) Telegram (/docs/integrations/chat_loaders/telegram) Twitter (via Apify) (/docs/integrations/chat_loaders/twitter) WhatsApp (/docs/integrations/chat_loaders/whatsapp) Document loaders (/docs/integrations/document_loaders/) Document transformers (/docs/integrations/document_transformers/) LLMs (/docs/integrations/llms/) Memory (/docs/integrations/memory/) Retrievers (/docs/integrations/retrievers/) Text embedding models (/docs/integrations/text_embedding/) Agents & Toolkits (/docs/integrations/toolkits/) Tools (/docs/integrations/tools/) Vector stores (/docs/integrations/vectorstores/) Grouped by provider (/docs/integrations/providers/)  (/) Integrations (/docs/integrations) Chat loaders (/docs/integrations/chat_loaders/) ​ (#1-download-data) here (https://www.zapptales.com/en/download-facebook-messenger-chat-history-how-to/) this google drive link (https://drive.google.com/file/d/1rh1s1o2i7B-Sk1v9o8KNgivLVGwJ-osV/view?usp=sharing) ​ (#2-create-chat-loader) SingleFileFacebookMessengerChatLoader (https://api.python.langchain.com/en/latest/chat_loaders/langchain.chat_loaders.facebook_messenger.SingleFileFacebookMessengerChatLoader.html) FolderFacebookMessengerChatLoader (https://api.python.langchain.com/en/latest/chat_loaders/langchain.chat_loaders.facebook_messenger.FolderFacebookMessengerChatLoader.html) ​ (#3-prepare-for-fine-tuning) merge_chat_runs (https://api.python.langchain.com/en/latest/chat_loaders/langchain.chat_loaders.utils.merge_chat_runs.html) map_ai_messages (https://api.python.langchain.com/en/latest/chat_loaders/langchain.chat_loaders.utils.map_ai_messages.html) ​ (#now-we-can-convert-to-openai-format-dictionaries) convert_messages_for_finetuning (https://api.python.langchain.com/en/latest/adapters/langchain.adapters.openai.convert_messages_for_finetuning.html) ​ (#4-fine-tune-the-model) ​ (#5-use-in-langchain) ChatOpenAI (https://api.python.langchain.com/en/latest/chat_models/langchain.chat_models.openai.ChatOpenAI.html) ChatPromptTemplate (https://api.python.langchain.com/en/latest/prompts/langchain.prompts.chat.ChatPromptTemplate.html) StrOutputParser (https://api.python.langchain.com/en/latest/schema/langchain.schema.output_parser.StrOutputParser.html) PreviousDiscord (/docs/integrations/chat_loaders/discord) NextGMail (/docs/integrations/chat_loaders/gmail) 1. Download Data (#1-download-data) 2. Create Chat Loader (#2-create-chat-loader) 3. Prepare for fine-tuning (#3-prepare-for-fine-tuning) 4. Fine-tune the model (#4-fine-tune-the-model) 5. Use in LangChain (#5-use-in-langchain) Discord (https://discord.gg/cU2adEyC7w) Twitter (https://twitter.com/LangChainAI) Python (https://github.com/hwchase17/langchain) JS/TS (https://github.com/hwchase17/langchainjs) Homepage (https://langchain.com) Blog (https://blog.langchain.dev)