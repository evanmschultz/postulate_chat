Let's load the SelfHostedEmbeddings, SelfHostedHuggingFaceEmbeddings, and SelfHostedHuggingFaceInstructEmbeddings classes. And similarly for SelfHostedHuggingFaceInstructEmbeddings: Now let's load an embedding model with a custom load function: IntegrationsCallbacksChat modelsChat loadersDocument loadersDocument transformersLLMsMemoryRetrieversText embedding modelsAleph AlphaAwaDBAzureOpenAIBedrockBGE on Hugging FaceClarifaiCohereDashScopeDeepInfraEDEN AIElasticsearchEmbaasERNIE Embedding-V1Fake EmbeddingsGoogle Vertex AI PaLMGPT4AllHugging FaceInstructEmbeddingsJinaLlama-cppLocalAIMiniMaxModelScopeMosaicMLNLP CloudOpenAISageMakerSelf HostedSentence TransformersSpaCyTensorflowHubXorbits inference (Xinference)Agents & ToolkitsToolsVector storesGrouped by provider Callbacks Chat models Chat loaders Document loaders Document transformers LLMs Memory Retrievers Text embedding modelsAleph AlphaAwaDBAzureOpenAIBedrockBGE on Hugging FaceClarifaiCohereDashScopeDeepInfraEDEN AIElasticsearchEmbaasERNIE Embedding-V1Fake EmbeddingsGoogle Vertex AI PaLMGPT4AllHugging FaceInstructEmbeddingsJinaLlama-cppLocalAIMiniMaxModelScopeMosaicMLNLP CloudOpenAISageMakerSelf HostedSentence TransformersSpaCyTensorflowHubXorbits inference (Xinference) Aleph Alpha AwaDB AzureOpenAI Bedrock BGE on Hugging Face Clarifai Cohere DashScope DeepInfra EDEN AI Elasticsearch Embaas ERNIE Embedding-V1 Fake Embeddings Google Vertex AI PaLM GPT4All Hugging Face InstructEmbeddings Jina Llama-cpp LocalAI MiniMax ModelScope MosaicML NLP Cloud OpenAI SageMaker Self Hosted Sentence Transformers SpaCy TensorflowHub Xorbits inference (Xinference) Agents & Toolkits Tools Vector stores Grouped by provider  Integrations Text embedding models Self Hosted SelfHostedEmbeddings SelfHostedHuggingFaceEmbeddings SelfHostedHuggingFaceInstructEmbeddings Discord Twitter Python JS/TS Homepage Blog Skip to main contentü¶úÔ∏èüîó LangChainDocsUse casesIntegrationsAPILangSmithJS/TS DocsCTRLKIntegrationsCallbacksChat modelsChat loadersDocument loadersDocument transformersLLMsMemoryRetrieversText embedding modelsAleph AlphaAwaDBAzureOpenAIBedrockBGE on Hugging FaceClarifaiCohereDashScopeDeepInfraEDEN AIElasticsearchEmbaasERNIE Embedding-V1Fake EmbeddingsGoogle Vertex AI PaLMGPT4AllHugging FaceInstructEmbeddingsJinaLlama-cppLocalAIMiniMaxModelScopeMosaicMLNLP CloudOpenAISageMakerSelf HostedSentence TransformersSpaCyTensorflowHubXorbits inference (Xinference)Agents & ToolkitsToolsVector storesGrouped by providerIntegrationsText embedding modelsSelf HostedSelf HostedLet's load the SelfHostedEmbeddings, SelfHostedHuggingFaceEmbeddings, and SelfHostedHuggingFaceInstructEmbeddings classes.from langchain.embeddings import (    SelfHostedEmbeddings,    SelfHostedHuggingFaceEmbeddings,    SelfHostedHuggingFaceInstructEmbeddings,)import runhouse as rhAPI Reference:SelfHostedEmbeddingsSelfHostedHuggingFaceEmbeddingsSelfHostedHuggingFaceInstructEmbeddings# For an on-demand A100 with GCP, Azure, or Lambdagpu = rh.cluster(name="rh-a10x", instance_type="A100:1", use_spot=False)# For an on-demand A10G with AWS (no single A100s on AWS)# gpu = rh.cluster(name='rh-a10x', instance_type='g5.2xlarge', provider='aws')# For an existing cluster# gpu = rh.cluster(ips=['<ip of the cluster>'],#                  ssh_creds={'ssh_user': '...', 'ssh_private_key':'<path_to_key>'},#                  name='my-cluster')embeddings = SelfHostedHuggingFaceEmbeddings(hardware=gpu)text = "This is a test document."query_result = embeddings.embed_query(text)And similarly for SelfHostedHuggingFaceInstructEmbeddings:embeddings = SelfHostedHuggingFaceInstructEmbeddings(hardware=gpu)Now let's load an embedding model with a custom load function:def get_pipeline():    from transformers import (        AutoModelForCausalLM,        AutoTokenizer,        pipeline,    )  # Must be inside the function in notebooks    model_id = "facebook/bart-base"    tokenizer = AutoTokenizer.from_pretrained(model_id)    model = AutoModelForCausalLM.from_pretrained(model_id)    return pipeline("feature-extraction", model=model, tokenizer=tokenizer)def inference_fn(pipeline, prompt):    # Return last hidden state of the model    if isinstance(prompt, list):        return [emb[0][-1] for emb in pipeline(prompt)]    return pipeline(prompt)[0][-1]embeddings = SelfHostedEmbeddings(    model_load_fn=get_pipeline,    hardware=gpu,    model_reqs=["./", "torch", "transformers"],    inference_fn=inference_fn,)query_result = embeddings.embed_query(text)PreviousSageMakerNextSentence TransformersCommunityDiscordTwitterGitHubPythonJS/TSMoreHomepageBlogCopyright ¬© 2023 LangChain, Inc. Skip to main content ü¶úÔ∏èüîó LangChainDocsUse casesIntegrationsAPILangSmithJS/TS DocsCTRLK ü¶úÔ∏èüîó LangChainDocsUse casesIntegrationsAPI LangSmithJS/TS DocsCTRLK  CTRLK CTRLK  CTRLK   IntegrationsCallbacksChat modelsChat loadersDocument loadersDocument transformersLLMsMemoryRetrieversText embedding modelsAleph AlphaAwaDBAzureOpenAIBedrockBGE on Hugging FaceClarifaiCohereDashScopeDeepInfraEDEN AIElasticsearchEmbaasERNIE Embedding-V1Fake EmbeddingsGoogle Vertex AI PaLMGPT4AllHugging FaceInstructEmbeddingsJinaLlama-cppLocalAIMiniMaxModelScopeMosaicMLNLP CloudOpenAISageMakerSelf HostedSentence TransformersSpaCyTensorflowHubXorbits inference (Xinference)Agents & ToolkitsToolsVector storesGrouped by providerIntegrationsText embedding modelsSelf HostedSelf HostedLet's load the SelfHostedEmbeddings, SelfHostedHuggingFaceEmbeddings, and SelfHostedHuggingFaceInstructEmbeddings classes.from langchain.embeddings import (    SelfHostedEmbeddings,    SelfHostedHuggingFaceEmbeddings,    SelfHostedHuggingFaceInstructEmbeddings,)import runhouse as rhAPI Reference:SelfHostedEmbeddingsSelfHostedHuggingFaceEmbeddingsSelfHostedHuggingFaceInstructEmbeddings# For an on-demand A100 with GCP, Azure, or Lambdagpu = rh.cluster(name="rh-a10x", instance_type="A100:1", use_spot=False)# For an on-demand A10G with AWS (no single A100s on AWS)# gpu = rh.cluster(name='rh-a10x', instance_type='g5.2xlarge', provider='aws')# For an existing cluster# gpu = rh.cluster(ips=['<ip of the cluster>'],#                  ssh_creds={'ssh_user': '...', 'ssh_private_key':'<path_to_key>'},#                  name='my-cluster')embeddings = SelfHostedHuggingFaceEmbeddings(hardware=gpu)text = "This is a test document."query_result = embeddings.embed_query(text)And similarly for SelfHostedHuggingFaceInstructEmbeddings:embeddings = SelfHostedHuggingFaceInstructEmbeddings(hardware=gpu)Now let's load an embedding model with a custom load function:def get_pipeline():    from transformers import (        AutoModelForCausalLM,        AutoTokenizer,        pipeline,    )  # Must be inside the function in notebooks    model_id = "facebook/bart-base"    tokenizer = AutoTokenizer.from_pretrained(model_id)    model = AutoModelForCausalLM.from_pretrained(model_id)    return pipeline("feature-extraction", model=model, tokenizer=tokenizer)def inference_fn(pipeline, prompt):    # Return last hidden state of the model    if isinstance(prompt, list):        return [emb[0][-1] for emb in pipeline(prompt)]    return pipeline(prompt)[0][-1]embeddings = SelfHostedEmbeddings(    model_load_fn=get_pipeline,    hardware=gpu,    model_reqs=["./", "torch", "transformers"],    inference_fn=inference_fn,)query_result = embeddings.embed_query(text)PreviousSageMakerNextSentence Transformers IntegrationsCallbacksChat modelsChat loadersDocument loadersDocument transformersLLMsMemoryRetrieversText embedding modelsAleph AlphaAwaDBAzureOpenAIBedrockBGE on Hugging FaceClarifaiCohereDashScopeDeepInfraEDEN AIElasticsearchEmbaasERNIE Embedding-V1Fake EmbeddingsGoogle Vertex AI PaLMGPT4AllHugging FaceInstructEmbeddingsJinaLlama-cppLocalAIMiniMaxModelScopeMosaicMLNLP CloudOpenAISageMakerSelf HostedSentence TransformersSpaCyTensorflowHubXorbits inference (Xinference)Agents & ToolkitsToolsVector storesGrouped by providerIntegrationsText embedding modelsSelf HostedSelf HostedLet's load the SelfHostedEmbeddings, SelfHostedHuggingFaceEmbeddings, and SelfHostedHuggingFaceInstructEmbeddings classes.from langchain.embeddings import (    SelfHostedEmbeddings,    SelfHostedHuggingFaceEmbeddings,    SelfHostedHuggingFaceInstructEmbeddings,)import runhouse as rhAPI Reference:SelfHostedEmbeddingsSelfHostedHuggingFaceEmbeddingsSelfHostedHuggingFaceInstructEmbeddings# For an on-demand A100 with GCP, Azure, or Lambdagpu = rh.cluster(name="rh-a10x", instance_type="A100:1", use_spot=False)# For an on-demand A10G with AWS (no single A100s on AWS)# gpu = rh.cluster(name='rh-a10x', instance_type='g5.2xlarge', provider='aws')# For an existing cluster# gpu = rh.cluster(ips=['<ip of the cluster>'],#                  ssh_creds={'ssh_user': '...', 'ssh_private_key':'<path_to_key>'},#                  name='my-cluster')embeddings = SelfHostedHuggingFaceEmbeddings(hardware=gpu)text = "This is a test document."query_result = embeddings.embed_query(text)And similarly for SelfHostedHuggingFaceInstructEmbeddings:embeddings = SelfHostedHuggingFaceInstructEmbeddings(hardware=gpu)Now let's load an embedding model with a custom load function:def get_pipeline():    from transformers import (        AutoModelForCausalLM,        AutoTokenizer,        pipeline,    )  # Must be inside the function in notebooks    model_id = "facebook/bart-base"    tokenizer = AutoTokenizer.from_pretrained(model_id)    model = AutoModelForCausalLM.from_pretrained(model_id)    return pipeline("feature-extraction", model=model, tokenizer=tokenizer)def inference_fn(pipeline, prompt):    # Return last hidden state of the model    if isinstance(prompt, list):        return [emb[0][-1] for emb in pipeline(prompt)]    return pipeline(prompt)[0][-1]embeddings = SelfHostedEmbeddings(    model_load_fn=get_pipeline,    hardware=gpu,    model_reqs=["./", "torch", "transformers"],    inference_fn=inference_fn,)query_result = embeddings.embed_query(text)PreviousSageMakerNextSentence Transformers IntegrationsCallbacksChat modelsChat loadersDocument loadersDocument transformersLLMsMemoryRetrieversText embedding modelsAleph AlphaAwaDBAzureOpenAIBedrockBGE on Hugging FaceClarifaiCohereDashScopeDeepInfraEDEN AIElasticsearchEmbaasERNIE Embedding-V1Fake EmbeddingsGoogle Vertex AI PaLMGPT4AllHugging FaceInstructEmbeddingsJinaLlama-cppLocalAIMiniMaxModelScopeMosaicMLNLP CloudOpenAISageMakerSelf HostedSentence TransformersSpaCyTensorflowHubXorbits inference (Xinference)Agents & ToolkitsToolsVector storesGrouped by provider IntegrationsCallbacksChat modelsChat loadersDocument loadersDocument transformersLLMsMemoryRetrieversText embedding modelsAleph AlphaAwaDBAzureOpenAIBedrockBGE on Hugging FaceClarifaiCohereDashScopeDeepInfraEDEN AIElasticsearchEmbaasERNIE Embedding-V1Fake EmbeddingsGoogle Vertex AI PaLMGPT4AllHugging FaceInstructEmbeddingsJinaLlama-cppLocalAIMiniMaxModelScopeMosaicMLNLP CloudOpenAISageMakerSelf HostedSentence TransformersSpaCyTensorflowHubXorbits inference (Xinference)Agents & ToolkitsToolsVector storesGrouped by provider Integrations Callbacks Chat models Chat loaders Document loaders Document transformers LLMs Memory Retrievers Text embedding models Agents & Toolkits Tools Vector stores Grouped by provider IntegrationsText embedding modelsSelf HostedSelf HostedLet's load the SelfHostedEmbeddings, SelfHostedHuggingFaceEmbeddings, and SelfHostedHuggingFaceInstructEmbeddings classes.from langchain.embeddings import (    SelfHostedEmbeddings,    SelfHostedHuggingFaceEmbeddings,    SelfHostedHuggingFaceInstructEmbeddings,)import runhouse as rhAPI Reference:SelfHostedEmbeddingsSelfHostedHuggingFaceEmbeddingsSelfHostedHuggingFaceInstructEmbeddings# For an on-demand A100 with GCP, Azure, or Lambdagpu = rh.cluster(name="rh-a10x", instance_type="A100:1", use_spot=False)# For an on-demand A10G with AWS (no single A100s on AWS)# gpu = rh.cluster(name='rh-a10x', instance_type='g5.2xlarge', provider='aws')# For an existing cluster# gpu = rh.cluster(ips=['<ip of the cluster>'],#                  ssh_creds={'ssh_user': '...', 'ssh_private_key':'<path_to_key>'},#                  name='my-cluster')embeddings = SelfHostedHuggingFaceEmbeddings(hardware=gpu)text = "This is a test document."query_result = embeddings.embed_query(text)And similarly for SelfHostedHuggingFaceInstructEmbeddings:embeddings = SelfHostedHuggingFaceInstructEmbeddings(hardware=gpu)Now let's load an embedding model with a custom load function:def get_pipeline():    from transformers import (        AutoModelForCausalLM,        AutoTokenizer,        pipeline,    )  # Must be inside the function in notebooks    model_id = "facebook/bart-base"    tokenizer = AutoTokenizer.from_pretrained(model_id)    model = AutoModelForCausalLM.from_pretrained(model_id)    return pipeline("feature-extraction", model=model, tokenizer=tokenizer)def inference_fn(pipeline, prompt):    # Return last hidden state of the model    if isinstance(prompt, list):        return [emb[0][-1] for emb in pipeline(prompt)]    return pipeline(prompt)[0][-1]embeddings = SelfHostedEmbeddings(    model_load_fn=get_pipeline,    hardware=gpu,    model_reqs=["./", "torch", "transformers"],    inference_fn=inference_fn,)query_result = embeddings.embed_query(text)PreviousSageMakerNextSentence Transformers IntegrationsText embedding modelsSelf HostedSelf HostedLet's load the SelfHostedEmbeddings, SelfHostedHuggingFaceEmbeddings, and SelfHostedHuggingFaceInstructEmbeddings classes.from langchain.embeddings import (    SelfHostedEmbeddings,    SelfHostedHuggingFaceEmbeddings,    SelfHostedHuggingFaceInstructEmbeddings,)import runhouse as rhAPI Reference:SelfHostedEmbeddingsSelfHostedHuggingFaceEmbeddingsSelfHostedHuggingFaceInstructEmbeddings# For an on-demand A100 with GCP, Azure, or Lambdagpu = rh.cluster(name="rh-a10x", instance_type="A100:1", use_spot=False)# For an on-demand A10G with AWS (no single A100s on AWS)# gpu = rh.cluster(name='rh-a10x', instance_type='g5.2xlarge', provider='aws')# For an existing cluster# gpu = rh.cluster(ips=['<ip of the cluster>'],#                  ssh_creds={'ssh_user': '...', 'ssh_private_key':'<path_to_key>'},#                  name='my-cluster')embeddings = SelfHostedHuggingFaceEmbeddings(hardware=gpu)text = "This is a test document."query_result = embeddings.embed_query(text)And similarly for SelfHostedHuggingFaceInstructEmbeddings:embeddings = SelfHostedHuggingFaceInstructEmbeddings(hardware=gpu)Now let's load an embedding model with a custom load function:def get_pipeline():    from transformers import (        AutoModelForCausalLM,        AutoTokenizer,        pipeline,    )  # Must be inside the function in notebooks    model_id = "facebook/bart-base"    tokenizer = AutoTokenizer.from_pretrained(model_id)    model = AutoModelForCausalLM.from_pretrained(model_id)    return pipeline("feature-extraction", model=model, tokenizer=tokenizer)def inference_fn(pipeline, prompt):    # Return last hidden state of the model    if isinstance(prompt, list):        return [emb[0][-1] for emb in pipeline(prompt)]    return pipeline(prompt)[0][-1]embeddings = SelfHostedEmbeddings(    model_load_fn=get_pipeline,    hardware=gpu,    model_reqs=["./", "torch", "transformers"],    inference_fn=inference_fn,)query_result = embeddings.embed_query(text)PreviousSageMakerNextSentence Transformers IntegrationsText embedding modelsSelf HostedSelf HostedLet's load the SelfHostedEmbeddings, SelfHostedHuggingFaceEmbeddings, and SelfHostedHuggingFaceInstructEmbeddings classes.from langchain.embeddings import (    SelfHostedEmbeddings,    SelfHostedHuggingFaceEmbeddings,    SelfHostedHuggingFaceInstructEmbeddings,)import runhouse as rhAPI Reference:SelfHostedEmbeddingsSelfHostedHuggingFaceEmbeddingsSelfHostedHuggingFaceInstructEmbeddings# For an on-demand A100 with GCP, Azure, or Lambdagpu = rh.cluster(name="rh-a10x", instance_type="A100:1", use_spot=False)# For an on-demand A10G with AWS (no single A100s on AWS)# gpu = rh.cluster(name='rh-a10x', instance_type='g5.2xlarge', provider='aws')# For an existing cluster# gpu = rh.cluster(ips=['<ip of the cluster>'],#                  ssh_creds={'ssh_user': '...', 'ssh_private_key':'<path_to_key>'},#                  name='my-cluster')embeddings = SelfHostedHuggingFaceEmbeddings(hardware=gpu)text = "This is a test document."query_result = embeddings.embed_query(text)And similarly for SelfHostedHuggingFaceInstructEmbeddings:embeddings = SelfHostedHuggingFaceInstructEmbeddings(hardware=gpu)Now let's load an embedding model with a custom load function:def get_pipeline():    from transformers import (        AutoModelForCausalLM,        AutoTokenizer,        pipeline,    )  # Must be inside the function in notebooks    model_id = "facebook/bart-base"    tokenizer = AutoTokenizer.from_pretrained(model_id)    model = AutoModelForCausalLM.from_pretrained(model_id)    return pipeline("feature-extraction", model=model, tokenizer=tokenizer)def inference_fn(pipeline, prompt):    # Return last hidden state of the model    if isinstance(prompt, list):        return [emb[0][-1] for emb in pipeline(prompt)]    return pipeline(prompt)[0][-1]embeddings = SelfHostedEmbeddings(    model_load_fn=get_pipeline,    hardware=gpu,    model_reqs=["./", "torch", "transformers"],    inference_fn=inference_fn,)query_result = embeddings.embed_query(text)PreviousSageMakerNextSentence Transformers IntegrationsText embedding modelsSelf HostedSelf HostedLet's load the SelfHostedEmbeddings, SelfHostedHuggingFaceEmbeddings, and SelfHostedHuggingFaceInstructEmbeddings classes.from langchain.embeddings import (    SelfHostedEmbeddings,    SelfHostedHuggingFaceEmbeddings,    SelfHostedHuggingFaceInstructEmbeddings,)import runhouse as rhAPI Reference:SelfHostedEmbeddingsSelfHostedHuggingFaceEmbeddingsSelfHostedHuggingFaceInstructEmbeddings# For an on-demand A100 with GCP, Azure, or Lambdagpu = rh.cluster(name="rh-a10x", instance_type="A100:1", use_spot=False)# For an on-demand A10G with AWS (no single A100s on AWS)# gpu = rh.cluster(name='rh-a10x', instance_type='g5.2xlarge', provider='aws')# For an existing cluster# gpu = rh.cluster(ips=['<ip of the cluster>'],#                  ssh_creds={'ssh_user': '...', 'ssh_private_key':'<path_to_key>'},#                  name='my-cluster')embeddings = SelfHostedHuggingFaceEmbeddings(hardware=gpu)text = "This is a test document."query_result = embeddings.embed_query(text)And similarly for SelfHostedHuggingFaceInstructEmbeddings:embeddings = SelfHostedHuggingFaceInstructEmbeddings(hardware=gpu)Now let's load an embedding model with a custom load function:def get_pipeline():    from transformers import (        AutoModelForCausalLM,        AutoTokenizer,        pipeline,    )  # Must be inside the function in notebooks    model_id = "facebook/bart-base"    tokenizer = AutoTokenizer.from_pretrained(model_id)    model = AutoModelForCausalLM.from_pretrained(model_id)    return pipeline("feature-extraction", model=model, tokenizer=tokenizer)def inference_fn(pipeline, prompt):    # Return last hidden state of the model    if isinstance(prompt, list):        return [emb[0][-1] for emb in pipeline(prompt)]    return pipeline(prompt)[0][-1]embeddings = SelfHostedEmbeddings(    model_load_fn=get_pipeline,    hardware=gpu,    model_reqs=["./", "torch", "transformers"],    inference_fn=inference_fn,)query_result = embeddings.embed_query(text)PreviousSageMakerNextSentence Transformers Self HostedLet's load the SelfHostedEmbeddings, SelfHostedHuggingFaceEmbeddings, and SelfHostedHuggingFaceInstructEmbeddings classes.from langchain.embeddings import (    SelfHostedEmbeddings,    SelfHostedHuggingFaceEmbeddings,    SelfHostedHuggingFaceInstructEmbeddings,)import runhouse as rhAPI Reference:SelfHostedEmbeddingsSelfHostedHuggingFaceEmbeddingsSelfHostedHuggingFaceInstructEmbeddings# For an on-demand A100 with GCP, Azure, or Lambdagpu = rh.cluster(name="rh-a10x", instance_type="A100:1", use_spot=False)# For an on-demand A10G with AWS (no single A100s on AWS)# gpu = rh.cluster(name='rh-a10x', instance_type='g5.2xlarge', provider='aws')# For an existing cluster# gpu = rh.cluster(ips=['<ip of the cluster>'],#                  ssh_creds={'ssh_user': '...', 'ssh_private_key':'<path_to_key>'},#                  name='my-cluster')embeddings = SelfHostedHuggingFaceEmbeddings(hardware=gpu)text = "This is a test document."query_result = embeddings.embed_query(text)And similarly for SelfHostedHuggingFaceInstructEmbeddings:embeddings = SelfHostedHuggingFaceInstructEmbeddings(hardware=gpu)Now let's load an embedding model with a custom load function:def get_pipeline():    from transformers import (        AutoModelForCausalLM,        AutoTokenizer,        pipeline,    )  # Must be inside the function in notebooks    model_id = "facebook/bart-base"    tokenizer = AutoTokenizer.from_pretrained(model_id)    model = AutoModelForCausalLM.from_pretrained(model_id)    return pipeline("feature-extraction", model=model, tokenizer=tokenizer)def inference_fn(pipeline, prompt):    # Return last hidden state of the model    if isinstance(prompt, list):        return [emb[0][-1] for emb in pipeline(prompt)]    return pipeline(prompt)[0][-1]embeddings = SelfHostedEmbeddings(    model_load_fn=get_pipeline,    hardware=gpu,    model_reqs=["./", "torch", "transformers"],    inference_fn=inference_fn,)query_result = embeddings.embed_query(text) from langchain.embeddings import (    SelfHostedEmbeddings,    SelfHostedHuggingFaceEmbeddings,    SelfHostedHuggingFaceInstructEmbeddings,)import runhouse as rh from langchain.embeddings import (    SelfHostedEmbeddings,    SelfHostedHuggingFaceEmbeddings,    SelfHostedHuggingFaceInstructEmbeddings,)import runhouse as rh  API Reference:SelfHostedEmbeddingsSelfHostedHuggingFaceEmbeddingsSelfHostedHuggingFaceInstructEmbeddings # For an on-demand A100 with GCP, Azure, or Lambdagpu = rh.cluster(name="rh-a10x", instance_type="A100:1", use_spot=False)# For an on-demand A10G with AWS (no single A100s on AWS)# gpu = rh.cluster(name='rh-a10x', instance_type='g5.2xlarge', provider='aws')# For an existing cluster# gpu = rh.cluster(ips=['<ip of the cluster>'],#                  ssh_creds={'ssh_user': '...', 'ssh_private_key':'<path_to_key>'},#                  name='my-cluster') # For an on-demand A100 with GCP, Azure, or Lambdagpu = rh.cluster(name="rh-a10x", instance_type="A100:1", use_spot=False)# For an on-demand A10G with AWS (no single A100s on AWS)# gpu = rh.cluster(name='rh-a10x', instance_type='g5.2xlarge', provider='aws')# For an existing cluster# gpu = rh.cluster(ips=['<ip of the cluster>'],#                  ssh_creds={'ssh_user': '...', 'ssh_private_key':'<path_to_key>'},#                  name='my-cluster')  embeddings = SelfHostedHuggingFaceEmbeddings(hardware=gpu) embeddings = SelfHostedHuggingFaceEmbeddings(hardware=gpu)  text = "This is a test document." text = "This is a test document."  query_result = embeddings.embed_query(text) query_result = embeddings.embed_query(text)  embeddings = SelfHostedHuggingFaceInstructEmbeddings(hardware=gpu) embeddings = SelfHostedHuggingFaceInstructEmbeddings(hardware=gpu)  def get_pipeline():    from transformers import (        AutoModelForCausalLM,        AutoTokenizer,        pipeline,    )  # Must be inside the function in notebooks    model_id = "facebook/bart-base"    tokenizer = AutoTokenizer.from_pretrained(model_id)    model = AutoModelForCausalLM.from_pretrained(model_id)    return pipeline("feature-extraction", model=model, tokenizer=tokenizer)def inference_fn(pipeline, prompt):    # Return last hidden state of the model    if isinstance(prompt, list):        return [emb[0][-1] for emb in pipeline(prompt)]    return pipeline(prompt)[0][-1] def get_pipeline():    from transformers import (        AutoModelForCausalLM,        AutoTokenizer,        pipeline,    )  # Must be inside the function in notebooks    model_id = "facebook/bart-base"    tokenizer = AutoTokenizer.from_pretrained(model_id)    model = AutoModelForCausalLM.from_pretrained(model_id)    return pipeline("feature-extraction", model=model, tokenizer=tokenizer)def inference_fn(pipeline, prompt):    # Return last hidden state of the model    if isinstance(prompt, list):        return [emb[0][-1] for emb in pipeline(prompt)]    return pipeline(prompt)[0][-1]  embeddings = SelfHostedEmbeddings(    model_load_fn=get_pipeline,    hardware=gpu,    model_reqs=["./", "torch", "transformers"],    inference_fn=inference_fn,) embeddings = SelfHostedEmbeddings(    model_load_fn=get_pipeline,    hardware=gpu,    model_reqs=["./", "torch", "transformers"],    inference_fn=inference_fn,)  query_result = embeddings.embed_query(text) query_result = embeddings.embed_query(text)  Previous SageMaker Next Sentence Transformers CommunityDiscordTwitterGitHubPythonJS/TSMoreHomepageBlogCopyright ¬© 2023 LangChain, Inc. CommunityDiscordTwitterGitHubPythonJS/TSMoreHomepageBlog CommunityDiscordTwitter Community GitHubPythonJS/TS GitHub MoreHomepageBlog More Copyright ¬© 2023 LangChain, Inc. Copyright ¬© 2023 LangChain, Inc. Skip to main content (#docusaurus_skipToContent_fallback) ü¶úÔ∏èüîó LangChain (/) Docs (/docs/get_started/introduction) Use cases (/docs/use_cases) Integrations (/docs/integrations) API (https://api.python.langchain.com) LangSmith (https://smith.langchain.com) JS/TS Docs (https://js.langchain.com/docs)  (https://github.com/hwchase17/langchain) Integrations (/docs/integrations) Callbacks (/docs/integrations/callbacks/) Chat models (/docs/integrations/chat/) Chat loaders (/docs/integrations/chat_loaders/) Document loaders (/docs/integrations/document_loaders/) Document transformers (/docs/integrations/document_transformers/) LLMs (/docs/integrations/llms/) Memory (/docs/integrations/memory/) Retrievers (/docs/integrations/retrievers/) Text embedding models (/docs/integrations/text_embedding/) Aleph Alpha (/docs/integrations/text_embedding/aleph_alpha) AwaDB (/docs/integrations/text_embedding/awadb) AzureOpenAI (/docs/integrations/text_embedding/azureopenai) Bedrock (/docs/integrations/text_embedding/bedrock) BGE on Hugging Face (/docs/integrations/text_embedding/bge_huggingface) Clarifai (/docs/integrations/text_embedding/clarifai) Cohere (/docs/integrations/text_embedding/cohere) DashScope (/docs/integrations/text_embedding/dashscope) DeepInfra (/docs/integrations/text_embedding/deepinfra) EDEN AI (/docs/integrations/text_embedding/edenai) Elasticsearch (/docs/integrations/text_embedding/elasticsearch) Embaas (/docs/integrations/text_embedding/embaas) ERNIE Embedding-V1 (/docs/integrations/text_embedding/ernie) Fake Embeddings (/docs/integrations/text_embedding/fake) Google Vertex AI PaLM (/docs/integrations/text_embedding/google_vertex_ai_palm) GPT4All (/docs/integrations/text_embedding/gpt4all) Hugging Face (/docs/integrations/text_embedding/huggingfacehub) InstructEmbeddings (/docs/integrations/text_embedding/instruct_embeddings) Jina (/docs/integrations/text_embedding/jina) Llama-cpp (/docs/integrations/text_embedding/llamacpp) LocalAI (/docs/integrations/text_embedding/localai) MiniMax (/docs/integrations/text_embedding/minimax) ModelScope (/docs/integrations/text_embedding/modelscope_hub) MosaicML (/docs/integrations/text_embedding/mosaicml) NLP Cloud (/docs/integrations/text_embedding/nlp_cloud) OpenAI (/docs/integrations/text_embedding/openai) SageMaker (/docs/integrations/text_embedding/sagemaker-endpoint) Self Hosted (/docs/integrations/text_embedding/self-hosted) Sentence Transformers (/docs/integrations/text_embedding/sentence_transformers) SpaCy (/docs/integrations/text_embedding/spacy_embedding) TensorflowHub (/docs/integrations/text_embedding/tensorflowhub) Xorbits inference (Xinference) (/docs/integrations/text_embedding/xinference) Agents & Toolkits (/docs/integrations/toolkits/) Tools (/docs/integrations/tools/) Vector stores (/docs/integrations/vectorstores/) Grouped by provider (/docs/integrations/providers/)  (/) Integrations (/docs/integrations) Text embedding models (/docs/integrations/text_embedding/) SelfHostedEmbeddings (https://api.python.langchain.com/en/latest/embeddings/langchain.embeddings.self_hosted.SelfHostedEmbeddings.html) SelfHostedHuggingFaceEmbeddings (https://api.python.langchain.com/en/latest/embeddings/langchain.embeddings.self_hosted_hugging_face.SelfHostedHuggingFaceEmbeddings.html) SelfHostedHuggingFaceInstructEmbeddings (https://api.python.langchain.com/en/latest/embeddings/langchain.embeddings.self_hosted_hugging_face.SelfHostedHuggingFaceInstructEmbeddings.html) PreviousSageMaker (/docs/integrations/text_embedding/sagemaker-endpoint) NextSentence Transformers (/docs/integrations/text_embedding/sentence_transformers) Discord (https://discord.gg/cU2adEyC7w) Twitter (https://twitter.com/LangChainAI) Python (https://github.com/hwchase17/langchain) JS/TS (https://github.com/hwchase17/langchainjs) Homepage (https://langchain.com) Blog (https://blog.langchain.dev)