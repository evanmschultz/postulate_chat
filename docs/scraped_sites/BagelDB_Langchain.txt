BagelDB (Open Vector Database for AI), is like GitHub for AI data. It is a collaborative platform where users can create, share, and manage vector datasets. It can support private projects for independent developers, internal collaborations for enterprises, and public contributions for data DAOs. IntegrationsCallbacksChat modelsChat loadersDocument loadersDocument transformersLLMsMemoryRetrieversText embedding modelsAgents & ToolkitsToolsVector storesActiveloop Deep LakeAlibaba Cloud OpenSearchAnalyticDBAnnoyAtlasAwaDBAzure Cognitive SearchBagelDBCassandraChromaClickHouseDashVectorDingoDocArray HnswSearchDocArray InMemorySearchElasticsearchEpsillaFaissHologresLanceDBMarqoGoogle Vertex AI MatchingEngineMeilisearchMilvusMongoDB AtlasMyScaleNeo4j Vector IndexNucliaDBOpenSearchPostgres EmbeddingPGVectorPineconeQdrantRedisRocksetScaNNSingleStoreDBscikit-learnsqlite-vssStarRocksSupabase (Postgres)TairTencent Cloud VectorDBTigrisTypesenseUSearchvearchVectaraWeaviateXataZepZillizGrouped by provider Callbacks Chat models Chat loaders Document loaders Document transformers LLMs Memory Retrievers Text embedding models Agents & Toolkits Tools Vector storesActiveloop Deep LakeAlibaba Cloud OpenSearchAnalyticDBAnnoyAtlasAwaDBAzure Cognitive SearchBagelDBCassandraChromaClickHouseDashVectorDingoDocArray HnswSearchDocArray InMemorySearchElasticsearchEpsillaFaissHologresLanceDBMarqoGoogle Vertex AI MatchingEngineMeilisearchMilvusMongoDB AtlasMyScaleNeo4j Vector IndexNucliaDBOpenSearchPostgres EmbeddingPGVectorPineconeQdrantRedisRocksetScaNNSingleStoreDBscikit-learnsqlite-vssStarRocksSupabase (Postgres)TairTencent Cloud VectorDBTigrisTypesenseUSearchvearchVectaraWeaviateXataZepZilliz Activeloop Deep Lake Alibaba Cloud OpenSearch AnalyticDB Annoy Atlas AwaDB Azure Cognitive Search BagelDB Cassandra Chroma ClickHouse DashVector Dingo DocArray HnswSearch DocArray InMemorySearch Elasticsearch Epsilla Faiss Hologres LanceDB Marqo Google Vertex AI MatchingEngine Meilisearch Milvus MongoDB Atlas MyScale Neo4j Vector Index NucliaDB OpenSearch Postgres Embedding PGVector Pinecone Qdrant Redis Rockset ScaNN SingleStoreDB scikit-learn sqlite-vss StarRocks Supabase (Postgres) Tair Tencent Cloud VectorDB Tigris Typesense USearch vearch Vectara Weaviate Xata Zep Zilliz Grouped by provider  Integrations Vector stores BagelDB Bagel TextLoader CharacterTextSplitter Installation and Setup Create VectorStore from texts Create VectorStore from docs Get all text/doc from Cluster Create cluster with metadata & filter using metadata Discord Twitter Python JS/TS Homepage Blog Skip to main contentü¶úÔ∏èüîó LangChainDocsUse casesIntegrationsAPILangSmithJS/TS DocsCTRLKIntegrationsCallbacksChat modelsChat loadersDocument loadersDocument transformersLLMsMemoryRetrieversText embedding modelsAgents & ToolkitsToolsVector storesActiveloop Deep LakeAlibaba Cloud OpenSearchAnalyticDBAnnoyAtlasAwaDBAzure Cognitive SearchBagelDBCassandraChromaClickHouseDashVectorDingoDocArray HnswSearchDocArray InMemorySearchElasticsearchEpsillaFaissHologresLanceDBMarqoGoogle Vertex AI MatchingEngineMeilisearchMilvusMongoDB AtlasMyScaleNeo4j Vector IndexNucliaDBOpenSearchPostgres EmbeddingPGVectorPineconeQdrantRedisRocksetScaNNSingleStoreDBscikit-learnsqlite-vssStarRocksSupabase (Postgres)TairTencent Cloud VectorDBTigrisTypesenseUSearchvearchVectaraWeaviateXataZepZillizGrouped by providerIntegrationsVector storesBagelDBOn this pageBagelDBBagelDB (Open Vector Database for AI), is like GitHub for AI data. internal collaborations for enterprises, and public contributions for data DAOs.Installation and Setup‚Äãpip install betabageldbCreate VectorStore from texts‚Äãfrom langchain.vectorstores import Bageltexts = ["hello bagel", "hello langchain", "I love salad", "my car", "a dog"]# create cluster and add textscluster = Bagel.from_texts(cluster_name="testing", texts=texts)API Reference:Bagel# similarity searchcluster.similarity_search("bagel", k=3)    [Document(page_content='hello bagel', metadata={}),     Document(page_content='my car', metadata={}),     Document(page_content='I love salad', metadata={})]# the score is a distance metric, so lower is bettercluster.similarity_search_with_score("bagel", k=3)    [(Document(page_content='hello bagel', metadata={}), 0.27392977476119995),     (Document(page_content='my car', metadata={}), 1.4783176183700562),     (Document(page_content='I love salad', metadata={}), 1.5342965126037598)]# delete the clustercluster.delete_cluster()Create VectorStore from docs‚Äãfrom langchain.document_loaders import TextLoaderfrom langchain.text_splitter import CharacterTextSplitterloader = TextLoader("../../../state_of_the_union.txt")documents = loader.load()text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)docs = text_splitter.split_documents(documents)[:10]API Reference:TextLoaderCharacterTextSplitter# create cluster with docscluster = Bagel.from_documents(cluster_name="testing_with_docs", documents=docs)# similarity searchquery = "What did the president say about Ketanji Brown Jackson"docs = cluster.similarity_search(query)print(docs[0].page_content[:102])    Madam Speaker, Madam Vice President, our First Lady and Second Gentleman. Members of Congress and the Get all text/doc from Cluster‚Äãtexts = ["hello bagel", "this is langchain"]cluster = Bagel.from_texts(cluster_name="testing", texts=texts)cluster_data = cluster.get()# all keyscluster_data.keys()    dict_keys(['ids', 'embeddings', 'metadatas', 'documents'])# all values and keyscluster_data    {'ids': ['578c6d24-3763-11ee-a8ab-b7b7b34f99ba',      '578c6d25-3763-11ee-a8ab-b7b7b34f99ba',      'fb2fc7d8-3762-11ee-a8ab-b7b7b34f99ba',      'fb2fc7d9-3762-11ee-a8ab-b7b7b34f99ba',      '6b40881a-3762-11ee-a8ab-b7b7b34f99ba',      '6b40881b-3762-11ee-a8ab-b7b7b34f99ba',      '581e691e-3762-11ee-a8ab-b7b7b34f99ba',      '581e691f-3762-11ee-a8ab-b7b7b34f99ba'],     'embeddings': None,     'metadatas': [{}, {}, {}, {}, {}, {}, {}, {}],     'documents': ['hello bagel',      'this is langchain',      'hello bagel',      'this is langchain',      'hello bagel',      'this is langchain',      'hello bagel',      'this is langchain']}cluster.delete_cluster()Create cluster with metadata & filter using metadata‚Äãtexts = ["hello bagel", "this is langchain"]metadatas = [{"source": "notion"}, {"source": "google"}]cluster = Bagel.from_texts(cluster_name="testing", texts=texts, metadatas=metadatas)cluster.similarity_search_with_score("hello bagel", where={"source": "notion"})    [(Document(page_content='hello bagel', metadata={'source': 'notion'}), 0.0)]# delete the clustercluster.delete_cluster()PreviousAzure Cognitive SearchNextCassandraInstallation and SetupCreate VectorStore from textsCreate VectorStore from docsGet all text/doc from ClusterCreate cluster with metadata & filter using metadataCommunityDiscordTwitterGitHubPythonJS/TSMoreHomepageBlogCopyright ¬© 2023 LangChain, Inc. Skip to main content ü¶úÔ∏èüîó LangChainDocsUse casesIntegrationsAPILangSmithJS/TS DocsCTRLK ü¶úÔ∏èüîó LangChainDocsUse casesIntegrationsAPI LangSmithJS/TS DocsCTRLK  CTRLK CTRLK  CTRLK   IntegrationsCallbacksChat modelsChat loadersDocument loadersDocument transformersLLMsMemoryRetrieversText embedding modelsAgents & ToolkitsToolsVector storesActiveloop Deep LakeAlibaba Cloud OpenSearchAnalyticDBAnnoyAtlasAwaDBAzure Cognitive SearchBagelDBCassandraChromaClickHouseDashVectorDingoDocArray HnswSearchDocArray InMemorySearchElasticsearchEpsillaFaissHologresLanceDBMarqoGoogle Vertex AI MatchingEngineMeilisearchMilvusMongoDB AtlasMyScaleNeo4j Vector IndexNucliaDBOpenSearchPostgres EmbeddingPGVectorPineconeQdrantRedisRocksetScaNNSingleStoreDBscikit-learnsqlite-vssStarRocksSupabase (Postgres)TairTencent Cloud VectorDBTigrisTypesenseUSearchvearchVectaraWeaviateXataZepZillizGrouped by providerIntegrationsVector storesBagelDBOn this pageBagelDBBagelDB (Open Vector Database for AI), is like GitHub for AI data. internal collaborations for enterprises, and public contributions for data DAOs.Installation and Setup‚Äãpip install betabageldbCreate VectorStore from texts‚Äãfrom langchain.vectorstores import Bageltexts = ["hello bagel", "hello langchain", "I love salad", "my car", "a dog"]# create cluster and add textscluster = Bagel.from_texts(cluster_name="testing", texts=texts)API Reference:Bagel# similarity searchcluster.similarity_search("bagel", k=3)    [Document(page_content='hello bagel', metadata={}),     Document(page_content='my car', metadata={}),     Document(page_content='I love salad', metadata={})]# the score is a distance metric, so lower is bettercluster.similarity_search_with_score("bagel", k=3)    [(Document(page_content='hello bagel', metadata={}), 0.27392977476119995),     (Document(page_content='my car', metadata={}), 1.4783176183700562),     (Document(page_content='I love salad', metadata={}), 1.5342965126037598)]# delete the clustercluster.delete_cluster()Create VectorStore from docs‚Äãfrom langchain.document_loaders import TextLoaderfrom langchain.text_splitter import CharacterTextSplitterloader = TextLoader("../../../state_of_the_union.txt")documents = loader.load()text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)docs = text_splitter.split_documents(documents)[:10]API Reference:TextLoaderCharacterTextSplitter# create cluster with docscluster = Bagel.from_documents(cluster_name="testing_with_docs", documents=docs)# similarity searchquery = "What did the president say about Ketanji Brown Jackson"docs = cluster.similarity_search(query)print(docs[0].page_content[:102])    Madam Speaker, Madam Vice President, our First Lady and Second Gentleman. Members of Congress and the Get all text/doc from Cluster‚Äãtexts = ["hello bagel", "this is langchain"]cluster = Bagel.from_texts(cluster_name="testing", texts=texts)cluster_data = cluster.get()# all keyscluster_data.keys()    dict_keys(['ids', 'embeddings', 'metadatas', 'documents'])# all values and keyscluster_data    {'ids': ['578c6d24-3763-11ee-a8ab-b7b7b34f99ba',      '578c6d25-3763-11ee-a8ab-b7b7b34f99ba',      'fb2fc7d8-3762-11ee-a8ab-b7b7b34f99ba',      'fb2fc7d9-3762-11ee-a8ab-b7b7b34f99ba',      '6b40881a-3762-11ee-a8ab-b7b7b34f99ba',      '6b40881b-3762-11ee-a8ab-b7b7b34f99ba',      '581e691e-3762-11ee-a8ab-b7b7b34f99ba',      '581e691f-3762-11ee-a8ab-b7b7b34f99ba'],     'embeddings': None,     'metadatas': [{}, {}, {}, {}, {}, {}, {}, {}],     'documents': ['hello bagel',      'this is langchain',      'hello bagel',      'this is langchain',      'hello bagel',      'this is langchain',      'hello bagel',      'this is langchain']}cluster.delete_cluster()Create cluster with metadata & filter using metadata‚Äãtexts = ["hello bagel", "this is langchain"]metadatas = [{"source": "notion"}, {"source": "google"}]cluster = Bagel.from_texts(cluster_name="testing", texts=texts, metadatas=metadatas)cluster.similarity_search_with_score("hello bagel", where={"source": "notion"})    [(Document(page_content='hello bagel', metadata={'source': 'notion'}), 0.0)]# delete the clustercluster.delete_cluster()PreviousAzure Cognitive SearchNextCassandraInstallation and SetupCreate VectorStore from textsCreate VectorStore from docsGet all text/doc from ClusterCreate cluster with metadata & filter using metadata IntegrationsCallbacksChat modelsChat loadersDocument loadersDocument transformersLLMsMemoryRetrieversText embedding modelsAgents & ToolkitsToolsVector storesActiveloop Deep LakeAlibaba Cloud OpenSearchAnalyticDBAnnoyAtlasAwaDBAzure Cognitive SearchBagelDBCassandraChromaClickHouseDashVectorDingoDocArray HnswSearchDocArray InMemorySearchElasticsearchEpsillaFaissHologresLanceDBMarqoGoogle Vertex AI MatchingEngineMeilisearchMilvusMongoDB AtlasMyScaleNeo4j Vector IndexNucliaDBOpenSearchPostgres EmbeddingPGVectorPineconeQdrantRedisRocksetScaNNSingleStoreDBscikit-learnsqlite-vssStarRocksSupabase (Postgres)TairTencent Cloud VectorDBTigrisTypesenseUSearchvearchVectaraWeaviateXataZepZillizGrouped by providerIntegrationsVector storesBagelDBOn this pageBagelDBBagelDB (Open Vector Database for AI), is like GitHub for AI data. internal collaborations for enterprises, and public contributions for data DAOs.Installation and Setup‚Äãpip install betabageldbCreate VectorStore from texts‚Äãfrom langchain.vectorstores import Bageltexts = ["hello bagel", "hello langchain", "I love salad", "my car", "a dog"]# create cluster and add textscluster = Bagel.from_texts(cluster_name="testing", texts=texts)API Reference:Bagel# similarity searchcluster.similarity_search("bagel", k=3)    [Document(page_content='hello bagel', metadata={}),     Document(page_content='my car', metadata={}),     Document(page_content='I love salad', metadata={})]# the score is a distance metric, so lower is bettercluster.similarity_search_with_score("bagel", k=3)    [(Document(page_content='hello bagel', metadata={}), 0.27392977476119995),     (Document(page_content='my car', metadata={}), 1.4783176183700562),     (Document(page_content='I love salad', metadata={}), 1.5342965126037598)]# delete the clustercluster.delete_cluster()Create VectorStore from docs‚Äãfrom langchain.document_loaders import TextLoaderfrom langchain.text_splitter import CharacterTextSplitterloader = TextLoader("../../../state_of_the_union.txt")documents = loader.load()text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)docs = text_splitter.split_documents(documents)[:10]API Reference:TextLoaderCharacterTextSplitter# create cluster with docscluster = Bagel.from_documents(cluster_name="testing_with_docs", documents=docs)# similarity searchquery = "What did the president say about Ketanji Brown Jackson"docs = cluster.similarity_search(query)print(docs[0].page_content[:102])    Madam Speaker, Madam Vice President, our First Lady and Second Gentleman. Members of Congress and the Get all text/doc from Cluster‚Äãtexts = ["hello bagel", "this is langchain"]cluster = Bagel.from_texts(cluster_name="testing", texts=texts)cluster_data = cluster.get()# all keyscluster_data.keys()    dict_keys(['ids', 'embeddings', 'metadatas', 'documents'])# all values and keyscluster_data    {'ids': ['578c6d24-3763-11ee-a8ab-b7b7b34f99ba',      '578c6d25-3763-11ee-a8ab-b7b7b34f99ba',      'fb2fc7d8-3762-11ee-a8ab-b7b7b34f99ba',      'fb2fc7d9-3762-11ee-a8ab-b7b7b34f99ba',      '6b40881a-3762-11ee-a8ab-b7b7b34f99ba',      '6b40881b-3762-11ee-a8ab-b7b7b34f99ba',      '581e691e-3762-11ee-a8ab-b7b7b34f99ba',      '581e691f-3762-11ee-a8ab-b7b7b34f99ba'],     'embeddings': None,     'metadatas': [{}, {}, {}, {}, {}, {}, {}, {}],     'documents': ['hello bagel',      'this is langchain',      'hello bagel',      'this is langchain',      'hello bagel',      'this is langchain',      'hello bagel',      'this is langchain']}cluster.delete_cluster()Create cluster with metadata & filter using metadata‚Äãtexts = ["hello bagel", "this is langchain"]metadatas = [{"source": "notion"}, {"source": "google"}]cluster = Bagel.from_texts(cluster_name="testing", texts=texts, metadatas=metadatas)cluster.similarity_search_with_score("hello bagel", where={"source": "notion"})    [(Document(page_content='hello bagel', metadata={'source': 'notion'}), 0.0)]# delete the clustercluster.delete_cluster()PreviousAzure Cognitive SearchNextCassandraInstallation and SetupCreate VectorStore from textsCreate VectorStore from docsGet all text/doc from ClusterCreate cluster with metadata & filter using metadata IntegrationsCallbacksChat modelsChat loadersDocument loadersDocument transformersLLMsMemoryRetrieversText embedding modelsAgents & ToolkitsToolsVector storesActiveloop Deep LakeAlibaba Cloud OpenSearchAnalyticDBAnnoyAtlasAwaDBAzure Cognitive SearchBagelDBCassandraChromaClickHouseDashVectorDingoDocArray HnswSearchDocArray InMemorySearchElasticsearchEpsillaFaissHologresLanceDBMarqoGoogle Vertex AI MatchingEngineMeilisearchMilvusMongoDB AtlasMyScaleNeo4j Vector IndexNucliaDBOpenSearchPostgres EmbeddingPGVectorPineconeQdrantRedisRocksetScaNNSingleStoreDBscikit-learnsqlite-vssStarRocksSupabase (Postgres)TairTencent Cloud VectorDBTigrisTypesenseUSearchvearchVectaraWeaviateXataZepZillizGrouped by provider IntegrationsCallbacksChat modelsChat loadersDocument loadersDocument transformersLLMsMemoryRetrieversText embedding modelsAgents & ToolkitsToolsVector storesActiveloop Deep LakeAlibaba Cloud OpenSearchAnalyticDBAnnoyAtlasAwaDBAzure Cognitive SearchBagelDBCassandraChromaClickHouseDashVectorDingoDocArray HnswSearchDocArray InMemorySearchElasticsearchEpsillaFaissHologresLanceDBMarqoGoogle Vertex AI MatchingEngineMeilisearchMilvusMongoDB AtlasMyScaleNeo4j Vector IndexNucliaDBOpenSearchPostgres EmbeddingPGVectorPineconeQdrantRedisRocksetScaNNSingleStoreDBscikit-learnsqlite-vssStarRocksSupabase (Postgres)TairTencent Cloud VectorDBTigrisTypesenseUSearchvearchVectaraWeaviateXataZepZillizGrouped by provider Integrations Callbacks Chat models Chat loaders Document loaders Document transformers LLMs Memory Retrievers Text embedding models Agents & Toolkits Tools Vector stores Grouped by provider IntegrationsVector storesBagelDBOn this pageBagelDBBagelDB (Open Vector Database for AI), is like GitHub for AI data. internal collaborations for enterprises, and public contributions for data DAOs.Installation and Setup‚Äãpip install betabageldbCreate VectorStore from texts‚Äãfrom langchain.vectorstores import Bageltexts = ["hello bagel", "hello langchain", "I love salad", "my car", "a dog"]# create cluster and add textscluster = Bagel.from_texts(cluster_name="testing", texts=texts)API Reference:Bagel# similarity searchcluster.similarity_search("bagel", k=3)    [Document(page_content='hello bagel', metadata={}),     Document(page_content='my car', metadata={}),     Document(page_content='I love salad', metadata={})]# the score is a distance metric, so lower is bettercluster.similarity_search_with_score("bagel", k=3)    [(Document(page_content='hello bagel', metadata={}), 0.27392977476119995),     (Document(page_content='my car', metadata={}), 1.4783176183700562),     (Document(page_content='I love salad', metadata={}), 1.5342965126037598)]# delete the clustercluster.delete_cluster()Create VectorStore from docs‚Äãfrom langchain.document_loaders import TextLoaderfrom langchain.text_splitter import CharacterTextSplitterloader = TextLoader("../../../state_of_the_union.txt")documents = loader.load()text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)docs = text_splitter.split_documents(documents)[:10]API Reference:TextLoaderCharacterTextSplitter# create cluster with docscluster = Bagel.from_documents(cluster_name="testing_with_docs", documents=docs)# similarity searchquery = "What did the president say about Ketanji Brown Jackson"docs = cluster.similarity_search(query)print(docs[0].page_content[:102])    Madam Speaker, Madam Vice President, our First Lady and Second Gentleman. Members of Congress and the Get all text/doc from Cluster‚Äãtexts = ["hello bagel", "this is langchain"]cluster = Bagel.from_texts(cluster_name="testing", texts=texts)cluster_data = cluster.get()# all keyscluster_data.keys()    dict_keys(['ids', 'embeddings', 'metadatas', 'documents'])# all values and keyscluster_data    {'ids': ['578c6d24-3763-11ee-a8ab-b7b7b34f99ba',      '578c6d25-3763-11ee-a8ab-b7b7b34f99ba',      'fb2fc7d8-3762-11ee-a8ab-b7b7b34f99ba',      'fb2fc7d9-3762-11ee-a8ab-b7b7b34f99ba',      '6b40881a-3762-11ee-a8ab-b7b7b34f99ba',      '6b40881b-3762-11ee-a8ab-b7b7b34f99ba',      '581e691e-3762-11ee-a8ab-b7b7b34f99ba',      '581e691f-3762-11ee-a8ab-b7b7b34f99ba'],     'embeddings': None,     'metadatas': [{}, {}, {}, {}, {}, {}, {}, {}],     'documents': ['hello bagel',      'this is langchain',      'hello bagel',      'this is langchain',      'hello bagel',      'this is langchain',      'hello bagel',      'this is langchain']}cluster.delete_cluster()Create cluster with metadata & filter using metadata‚Äãtexts = ["hello bagel", "this is langchain"]metadatas = [{"source": "notion"}, {"source": "google"}]cluster = Bagel.from_texts(cluster_name="testing", texts=texts, metadatas=metadatas)cluster.similarity_search_with_score("hello bagel", where={"source": "notion"})    [(Document(page_content='hello bagel', metadata={'source': 'notion'}), 0.0)]# delete the clustercluster.delete_cluster()PreviousAzure Cognitive SearchNextCassandraInstallation and SetupCreate VectorStore from textsCreate VectorStore from docsGet all text/doc from ClusterCreate cluster with metadata & filter using metadata IntegrationsVector storesBagelDBOn this pageBagelDBBagelDB (Open Vector Database for AI), is like GitHub for AI data. internal collaborations for enterprises, and public contributions for data DAOs.Installation and Setup‚Äãpip install betabageldbCreate VectorStore from texts‚Äãfrom langchain.vectorstores import Bageltexts = ["hello bagel", "hello langchain", "I love salad", "my car", "a dog"]# create cluster and add textscluster = Bagel.from_texts(cluster_name="testing", texts=texts)API Reference:Bagel# similarity searchcluster.similarity_search("bagel", k=3)    [Document(page_content='hello bagel', metadata={}),     Document(page_content='my car', metadata={}),     Document(page_content='I love salad', metadata={})]# the score is a distance metric, so lower is bettercluster.similarity_search_with_score("bagel", k=3)    [(Document(page_content='hello bagel', metadata={}), 0.27392977476119995),     (Document(page_content='my car', metadata={}), 1.4783176183700562),     (Document(page_content='I love salad', metadata={}), 1.5342965126037598)]# delete the clustercluster.delete_cluster()Create VectorStore from docs‚Äãfrom langchain.document_loaders import TextLoaderfrom langchain.text_splitter import CharacterTextSplitterloader = TextLoader("../../../state_of_the_union.txt")documents = loader.load()text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)docs = text_splitter.split_documents(documents)[:10]API Reference:TextLoaderCharacterTextSplitter# create cluster with docscluster = Bagel.from_documents(cluster_name="testing_with_docs", documents=docs)# similarity searchquery = "What did the president say about Ketanji Brown Jackson"docs = cluster.similarity_search(query)print(docs[0].page_content[:102])    Madam Speaker, Madam Vice President, our First Lady and Second Gentleman. Members of Congress and the Get all text/doc from Cluster‚Äãtexts = ["hello bagel", "this is langchain"]cluster = Bagel.from_texts(cluster_name="testing", texts=texts)cluster_data = cluster.get()# all keyscluster_data.keys()    dict_keys(['ids', 'embeddings', 'metadatas', 'documents'])# all values and keyscluster_data    {'ids': ['578c6d24-3763-11ee-a8ab-b7b7b34f99ba',      '578c6d25-3763-11ee-a8ab-b7b7b34f99ba',      'fb2fc7d8-3762-11ee-a8ab-b7b7b34f99ba',      'fb2fc7d9-3762-11ee-a8ab-b7b7b34f99ba',      '6b40881a-3762-11ee-a8ab-b7b7b34f99ba',      '6b40881b-3762-11ee-a8ab-b7b7b34f99ba',      '581e691e-3762-11ee-a8ab-b7b7b34f99ba',      '581e691f-3762-11ee-a8ab-b7b7b34f99ba'],     'embeddings': None,     'metadatas': [{}, {}, {}, {}, {}, {}, {}, {}],     'documents': ['hello bagel',      'this is langchain',      'hello bagel',      'this is langchain',      'hello bagel',      'this is langchain',      'hello bagel',      'this is langchain']}cluster.delete_cluster()Create cluster with metadata & filter using metadata‚Äãtexts = ["hello bagel", "this is langchain"]metadatas = [{"source": "notion"}, {"source": "google"}]cluster = Bagel.from_texts(cluster_name="testing", texts=texts, metadatas=metadatas)cluster.similarity_search_with_score("hello bagel", where={"source": "notion"})    [(Document(page_content='hello bagel', metadata={'source': 'notion'}), 0.0)]# delete the clustercluster.delete_cluster()PreviousAzure Cognitive SearchNextCassandra IntegrationsVector storesBagelDBOn this pageBagelDBBagelDB (Open Vector Database for AI), is like GitHub for AI data. internal collaborations for enterprises, and public contributions for data DAOs.Installation and Setup‚Äãpip install betabageldbCreate VectorStore from texts‚Äãfrom langchain.vectorstores import Bageltexts = ["hello bagel", "hello langchain", "I love salad", "my car", "a dog"]# create cluster and add textscluster = Bagel.from_texts(cluster_name="testing", texts=texts)API Reference:Bagel# similarity searchcluster.similarity_search("bagel", k=3)    [Document(page_content='hello bagel', metadata={}),     Document(page_content='my car', metadata={}),     Document(page_content='I love salad', metadata={})]# the score is a distance metric, so lower is bettercluster.similarity_search_with_score("bagel", k=3)    [(Document(page_content='hello bagel', metadata={}), 0.27392977476119995),     (Document(page_content='my car', metadata={}), 1.4783176183700562),     (Document(page_content='I love salad', metadata={}), 1.5342965126037598)]# delete the clustercluster.delete_cluster()Create VectorStore from docs‚Äãfrom langchain.document_loaders import TextLoaderfrom langchain.text_splitter import CharacterTextSplitterloader = TextLoader("../../../state_of_the_union.txt")documents = loader.load()text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)docs = text_splitter.split_documents(documents)[:10]API Reference:TextLoaderCharacterTextSplitter# create cluster with docscluster = Bagel.from_documents(cluster_name="testing_with_docs", documents=docs)# similarity searchquery = "What did the president say about Ketanji Brown Jackson"docs = cluster.similarity_search(query)print(docs[0].page_content[:102])    Madam Speaker, Madam Vice President, our First Lady and Second Gentleman. Members of Congress and the Get all text/doc from Cluster‚Äãtexts = ["hello bagel", "this is langchain"]cluster = Bagel.from_texts(cluster_name="testing", texts=texts)cluster_data = cluster.get()# all keyscluster_data.keys()    dict_keys(['ids', 'embeddings', 'metadatas', 'documents'])# all values and keyscluster_data    {'ids': ['578c6d24-3763-11ee-a8ab-b7b7b34f99ba',      '578c6d25-3763-11ee-a8ab-b7b7b34f99ba',      'fb2fc7d8-3762-11ee-a8ab-b7b7b34f99ba',      'fb2fc7d9-3762-11ee-a8ab-b7b7b34f99ba',      '6b40881a-3762-11ee-a8ab-b7b7b34f99ba',      '6b40881b-3762-11ee-a8ab-b7b7b34f99ba',      '581e691e-3762-11ee-a8ab-b7b7b34f99ba',      '581e691f-3762-11ee-a8ab-b7b7b34f99ba'],     'embeddings': None,     'metadatas': [{}, {}, {}, {}, {}, {}, {}, {}],     'documents': ['hello bagel',      'this is langchain',      'hello bagel',      'this is langchain',      'hello bagel',      'this is langchain',      'hello bagel',      'this is langchain']}cluster.delete_cluster()Create cluster with metadata & filter using metadata‚Äãtexts = ["hello bagel", "this is langchain"]metadatas = [{"source": "notion"}, {"source": "google"}]cluster = Bagel.from_texts(cluster_name="testing", texts=texts, metadatas=metadatas)cluster.similarity_search_with_score("hello bagel", where={"source": "notion"})    [(Document(page_content='hello bagel', metadata={'source': 'notion'}), 0.0)]# delete the clustercluster.delete_cluster()PreviousAzure Cognitive SearchNextCassandra On this page BagelDBBagelDB (Open Vector Database for AI), is like GitHub for AI data. internal collaborations for enterprises, and public contributions for data DAOs.Installation and Setup‚Äãpip install betabageldbCreate VectorStore from texts‚Äãfrom langchain.vectorstores import Bageltexts = ["hello bagel", "hello langchain", "I love salad", "my car", "a dog"]# create cluster and add textscluster = Bagel.from_texts(cluster_name="testing", texts=texts)API Reference:Bagel# similarity searchcluster.similarity_search("bagel", k=3)    [Document(page_content='hello bagel', metadata={}),     Document(page_content='my car', metadata={}),     Document(page_content='I love salad', metadata={})]# the score is a distance metric, so lower is bettercluster.similarity_search_with_score("bagel", k=3)    [(Document(page_content='hello bagel', metadata={}), 0.27392977476119995),     (Document(page_content='my car', metadata={}), 1.4783176183700562),     (Document(page_content='I love salad', metadata={}), 1.5342965126037598)]# delete the clustercluster.delete_cluster()Create VectorStore from docs‚Äãfrom langchain.document_loaders import TextLoaderfrom langchain.text_splitter import CharacterTextSplitterloader = TextLoader("../../../state_of_the_union.txt")documents = loader.load()text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)docs = text_splitter.split_documents(documents)[:10]API Reference:TextLoaderCharacterTextSplitter# create cluster with docscluster = Bagel.from_documents(cluster_name="testing_with_docs", documents=docs)# similarity searchquery = "What did the president say about Ketanji Brown Jackson"docs = cluster.similarity_search(query)print(docs[0].page_content[:102])    Madam Speaker, Madam Vice President, our First Lady and Second Gentleman. Members of Congress and the Get all text/doc from Cluster‚Äãtexts = ["hello bagel", "this is langchain"]cluster = Bagel.from_texts(cluster_name="testing", texts=texts)cluster_data = cluster.get()# all keyscluster_data.keys()    dict_keys(['ids', 'embeddings', 'metadatas', 'documents'])# all values and keyscluster_data    {'ids': ['578c6d24-3763-11ee-a8ab-b7b7b34f99ba',      '578c6d25-3763-11ee-a8ab-b7b7b34f99ba',      'fb2fc7d8-3762-11ee-a8ab-b7b7b34f99ba',      'fb2fc7d9-3762-11ee-a8ab-b7b7b34f99ba',      '6b40881a-3762-11ee-a8ab-b7b7b34f99ba',      '6b40881b-3762-11ee-a8ab-b7b7b34f99ba',      '581e691e-3762-11ee-a8ab-b7b7b34f99ba',      '581e691f-3762-11ee-a8ab-b7b7b34f99ba'],     'embeddings': None,     'metadatas': [{}, {}, {}, {}, {}, {}, {}, {}],     'documents': ['hello bagel',      'this is langchain',      'hello bagel',      'this is langchain',      'hello bagel',      'this is langchain',      'hello bagel',      'this is langchain']}cluster.delete_cluster()Create cluster with metadata & filter using metadata‚Äãtexts = ["hello bagel", "this is langchain"]metadatas = [{"source": "notion"}, {"source": "google"}]cluster = Bagel.from_texts(cluster_name="testing", texts=texts, metadatas=metadatas)cluster.similarity_search_with_score("hello bagel", where={"source": "notion"})    [(Document(page_content='hello bagel', metadata={'source': 'notion'}), 0.0)]# delete the clustercluster.delete_cluster() pip install betabageldb pip install betabageldb  from langchain.vectorstores import Bageltexts = ["hello bagel", "hello langchain", "I love salad", "my car", "a dog"]# create cluster and add textscluster = Bagel.from_texts(cluster_name="testing", texts=texts) from langchain.vectorstores import Bageltexts = ["hello bagel", "hello langchain", "I love salad", "my car", "a dog"]# create cluster and add textscluster = Bagel.from_texts(cluster_name="testing", texts=texts)  API Reference:Bagel # similarity searchcluster.similarity_search("bagel", k=3) # similarity searchcluster.similarity_search("bagel", k=3)      [Document(page_content='hello bagel', metadata={}),     Document(page_content='my car', metadata={}),     Document(page_content='I love salad', metadata={})]     [Document(page_content='hello bagel', metadata={}),     Document(page_content='my car', metadata={}),     Document(page_content='I love salad', metadata={})]     [Document(page_content='hello bagel', metadata={}),     Document(page_content='my car', metadata={}),     Document(page_content='I love salad', metadata={})]  # the score is a distance metric, so lower is bettercluster.similarity_search_with_score("bagel", k=3) # the score is a distance metric, so lower is bettercluster.similarity_search_with_score("bagel", k=3)      [(Document(page_content='hello bagel', metadata={}), 0.27392977476119995),     (Document(page_content='my car', metadata={}), 1.4783176183700562),     (Document(page_content='I love salad', metadata={}), 1.5342965126037598)]     [(Document(page_content='hello bagel', metadata={}), 0.27392977476119995),     (Document(page_content='my car', metadata={}), 1.4783176183700562),     (Document(page_content='I love salad', metadata={}), 1.5342965126037598)]     [(Document(page_content='hello bagel', metadata={}), 0.27392977476119995),     (Document(page_content='my car', metadata={}), 1.4783176183700562),     (Document(page_content='I love salad', metadata={}), 1.5342965126037598)]  # delete the clustercluster.delete_cluster() # delete the clustercluster.delete_cluster()  from langchain.document_loaders import TextLoaderfrom langchain.text_splitter import CharacterTextSplitterloader = TextLoader("../../../state_of_the_union.txt")documents = loader.load()text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)docs = text_splitter.split_documents(documents)[:10] from langchain.document_loaders import TextLoaderfrom langchain.text_splitter import CharacterTextSplitterloader = TextLoader("../../../state_of_the_union.txt")documents = loader.load()text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)docs = text_splitter.split_documents(documents)[:10]  API Reference:TextLoaderCharacterTextSplitter # create cluster with docscluster = Bagel.from_documents(cluster_name="testing_with_docs", documents=docs) # create cluster with docscluster = Bagel.from_documents(cluster_name="testing_with_docs", documents=docs)  # similarity searchquery = "What did the president say about Ketanji Brown Jackson"docs = cluster.similarity_search(query)print(docs[0].page_content[:102]) # similarity searchquery = "What did the president say about Ketanji Brown Jackson"docs = cluster.similarity_search(query)print(docs[0].page_content[:102])      Madam Speaker, Madam Vice President, our First Lady and Second Gentleman. Members of Congress and the      Madam Speaker, Madam Vice President, our First Lady and Second Gentleman. Members of Congress and the      Madam Speaker, Madam Vice President, our First Lady and Second Gentleman. Members of Congress and the   texts = ["hello bagel", "this is langchain"]cluster = Bagel.from_texts(cluster_name="testing", texts=texts)cluster_data = cluster.get() texts = ["hello bagel", "this is langchain"]cluster = Bagel.from_texts(cluster_name="testing", texts=texts)cluster_data = cluster.get()  # all keyscluster_data.keys() # all keyscluster_data.keys()      dict_keys(['ids', 'embeddings', 'metadatas', 'documents'])     dict_keys(['ids', 'embeddings', 'metadatas', 'documents'])     dict_keys(['ids', 'embeddings', 'metadatas', 'documents'])  # all values and keyscluster_data # all values and keyscluster_data      {'ids': ['578c6d24-3763-11ee-a8ab-b7b7b34f99ba',      '578c6d25-3763-11ee-a8ab-b7b7b34f99ba',      'fb2fc7d8-3762-11ee-a8ab-b7b7b34f99ba',      'fb2fc7d9-3762-11ee-a8ab-b7b7b34f99ba',      '6b40881a-3762-11ee-a8ab-b7b7b34f99ba',      '6b40881b-3762-11ee-a8ab-b7b7b34f99ba',      '581e691e-3762-11ee-a8ab-b7b7b34f99ba',      '581e691f-3762-11ee-a8ab-b7b7b34f99ba'],     'embeddings': None,     'metadatas': [{}, {}, {}, {}, {}, {}, {}, {}],     'documents': ['hello bagel',      'this is langchain',      'hello bagel',      'this is langchain',      'hello bagel',      'this is langchain',      'hello bagel',      'this is langchain']}     {'ids': ['578c6d24-3763-11ee-a8ab-b7b7b34f99ba',      '578c6d25-3763-11ee-a8ab-b7b7b34f99ba',      'fb2fc7d8-3762-11ee-a8ab-b7b7b34f99ba',      'fb2fc7d9-3762-11ee-a8ab-b7b7b34f99ba',      '6b40881a-3762-11ee-a8ab-b7b7b34f99ba',      '6b40881b-3762-11ee-a8ab-b7b7b34f99ba',      '581e691e-3762-11ee-a8ab-b7b7b34f99ba',      '581e691f-3762-11ee-a8ab-b7b7b34f99ba'],     'embeddings': None,     'metadatas': [{}, {}, {}, {}, {}, {}, {}, {}],     'documents': ['hello bagel',      'this is langchain',      'hello bagel',      'this is langchain',      'hello bagel',      'this is langchain',      'hello bagel',      'this is langchain']}     {'ids': ['578c6d24-3763-11ee-a8ab-b7b7b34f99ba',      '578c6d25-3763-11ee-a8ab-b7b7b34f99ba',      'fb2fc7d8-3762-11ee-a8ab-b7b7b34f99ba',      'fb2fc7d9-3762-11ee-a8ab-b7b7b34f99ba',      '6b40881a-3762-11ee-a8ab-b7b7b34f99ba',      '6b40881b-3762-11ee-a8ab-b7b7b34f99ba',      '581e691e-3762-11ee-a8ab-b7b7b34f99ba',      '581e691f-3762-11ee-a8ab-b7b7b34f99ba'],     'embeddings': None,     'metadatas': [{}, {}, {}, {}, {}, {}, {}, {}],     'documents': ['hello bagel',      'this is langchain',      'hello bagel',      'this is langchain',      'hello bagel',      'this is langchain',      'hello bagel',      'this is langchain']}  cluster.delete_cluster() cluster.delete_cluster()  texts = ["hello bagel", "this is langchain"]metadatas = [{"source": "notion"}, {"source": "google"}]cluster = Bagel.from_texts(cluster_name="testing", texts=texts, metadatas=metadatas)cluster.similarity_search_with_score("hello bagel", where={"source": "notion"}) texts = ["hello bagel", "this is langchain"]metadatas = [{"source": "notion"}, {"source": "google"}]cluster = Bagel.from_texts(cluster_name="testing", texts=texts, metadatas=metadatas)cluster.similarity_search_with_score("hello bagel", where={"source": "notion"})      [(Document(page_content='hello bagel', metadata={'source': 'notion'}), 0.0)]     [(Document(page_content='hello bagel', metadata={'source': 'notion'}), 0.0)]     [(Document(page_content='hello bagel', metadata={'source': 'notion'}), 0.0)]  # delete the clustercluster.delete_cluster() # delete the clustercluster.delete_cluster()  Previous Azure Cognitive Search Next Cassandra Installation and SetupCreate VectorStore from textsCreate VectorStore from docsGet all text/doc from ClusterCreate cluster with metadata & filter using metadata Installation and SetupCreate VectorStore from textsCreate VectorStore from docsGet all text/doc from ClusterCreate cluster with metadata & filter using metadata CommunityDiscordTwitterGitHubPythonJS/TSMoreHomepageBlogCopyright ¬© 2023 LangChain, Inc. CommunityDiscordTwitterGitHubPythonJS/TSMoreHomepageBlog CommunityDiscordTwitter Community GitHubPythonJS/TS GitHub MoreHomepageBlog More Copyright ¬© 2023 LangChain, Inc. Copyright ¬© 2023 LangChain, Inc. Skip to main content (#docusaurus_skipToContent_fallback) ü¶úÔ∏èüîó LangChain (/) Docs (/docs/get_started/introduction) Use cases (/docs/use_cases) Integrations (/docs/integrations) API (https://api.python.langchain.com) LangSmith (https://smith.langchain.com) JS/TS Docs (https://js.langchain.com/docs)  (https://github.com/hwchase17/langchain) Integrations (/docs/integrations) Callbacks (/docs/integrations/callbacks/) Chat models (/docs/integrations/chat/) Chat loaders (/docs/integrations/chat_loaders/) Document loaders (/docs/integrations/document_loaders/) Document transformers (/docs/integrations/document_transformers/) LLMs (/docs/integrations/llms/) Memory (/docs/integrations/memory/) Retrievers (/docs/integrations/retrievers/) Text embedding models (/docs/integrations/text_embedding/) Agents & Toolkits (/docs/integrations/toolkits/) Tools (/docs/integrations/tools/) Vector stores (/docs/integrations/vectorstores/) Activeloop Deep Lake (/docs/integrations/vectorstores/activeloop_deeplake) Alibaba Cloud OpenSearch (/docs/integrations/vectorstores/alibabacloud_opensearch) AnalyticDB (/docs/integrations/vectorstores/analyticdb) Annoy (/docs/integrations/vectorstores/annoy) Atlas (/docs/integrations/vectorstores/atlas) AwaDB (/docs/integrations/vectorstores/awadb) Azure Cognitive Search (/docs/integrations/vectorstores/azuresearch) BagelDB (/docs/integrations/vectorstores/bageldb) Cassandra (/docs/integrations/vectorstores/cassandra) Chroma (/docs/integrations/vectorstores/chroma) ClickHouse (/docs/integrations/vectorstores/clickhouse) DashVector (/docs/integrations/vectorstores/dashvector) Dingo (/docs/integrations/vectorstores/dingo) DocArray HnswSearch (/docs/integrations/vectorstores/docarray_hnsw) DocArray InMemorySearch (/docs/integrations/vectorstores/docarray_in_memory) Elasticsearch (/docs/integrations/vectorstores/elasticsearch) Epsilla (/docs/integrations/vectorstores/epsilla) Faiss (/docs/integrations/vectorstores/faiss) Hologres (/docs/integrations/vectorstores/hologres) LanceDB (/docs/integrations/vectorstores/lancedb) Marqo (/docs/integrations/vectorstores/marqo) Google Vertex AI MatchingEngine (/docs/integrations/vectorstores/matchingengine) Meilisearch (/docs/integrations/vectorstores/meilisearch) Milvus (/docs/integrations/vectorstores/milvus) MongoDB Atlas (/docs/integrations/vectorstores/mongodb_atlas) MyScale (/docs/integrations/vectorstores/myscale) Neo4j Vector Index (/docs/integrations/vectorstores/neo4jvector) NucliaDB (/docs/integrations/vectorstores/nucliadb) OpenSearch (/docs/integrations/vectorstores/opensearch) Postgres Embedding (/docs/integrations/vectorstores/pgembedding) PGVector (/docs/integrations/vectorstores/pgvector) Pinecone (/docs/integrations/vectorstores/pinecone) Qdrant (/docs/integrations/vectorstores/qdrant) Redis (/docs/integrations/vectorstores/redis) Rockset (/docs/integrations/vectorstores/rockset) ScaNN (/docs/integrations/vectorstores/scann) SingleStoreDB (/docs/integrations/vectorstores/singlestoredb) scikit-learn (/docs/integrations/vectorstores/sklearn) sqlite-vss (/docs/integrations/vectorstores/sqlitevss) StarRocks (/docs/integrations/vectorstores/starrocks) Supabase (Postgres) (/docs/integrations/vectorstores/supabase) Tair (/docs/integrations/vectorstores/tair) Tencent Cloud VectorDB (/docs/integrations/vectorstores/tencentvectordb) Tigris (/docs/integrations/vectorstores/tigris) Typesense (/docs/integrations/vectorstores/typesense) USearch (/docs/integrations/vectorstores/usearch) vearch (/docs/integrations/vectorstores/vearch) Vectara (/docs/integrations/vectorstores/vectara) Weaviate (/docs/integrations/vectorstores/weaviate) Xata (/docs/integrations/vectorstores/xata) Zep (/docs/integrations/vectorstores/zep) Zilliz (/docs/integrations/vectorstores/zilliz) Grouped by provider (/docs/integrations/providers/)  (/) Integrations (/docs/integrations) Vector stores (/docs/integrations/vectorstores/) BagelDB (https://www.bageldb.ai/) ‚Äã (#installation-and-setup) ‚Äã (#create-vectorstore-from-texts) Bagel (https://api.python.langchain.com/en/latest/vectorstores/langchain.vectorstores.bageldb.Bagel.html) ‚Äã (#create-vectorstore-from-docs) TextLoader (https://api.python.langchain.com/en/latest/document_loaders/langchain.document_loaders.text.TextLoader.html) CharacterTextSplitter (https://api.python.langchain.com/en/latest/text_splitter/langchain.text_splitter.CharacterTextSplitter.html) ‚Äã (#get-all-textdoc-from-cluster) ‚Äã (#create-cluster-with-metadata--filter-using-metadata) PreviousAzure Cognitive Search (/docs/integrations/vectorstores/azuresearch) NextCassandra (/docs/integrations/vectorstores/cassandra) Installation and Setup (#installation-and-setup) Create VectorStore from texts (#create-vectorstore-from-texts) Create VectorStore from docs (#create-vectorstore-from-docs) Get all text/doc from Cluster (#get-all-textdoc-from-cluster) Create cluster with metadata & filter using metadata (#create-cluster-with-metadata--filter-using-metadata) Discord (https://discord.gg/cU2adEyC7w) Twitter (https://twitter.com/LangChainAI) Python (https://github.com/hwchase17/langchain) JS/TS (https://github.com/hwchase17/langchainjs) Homepage (https://langchain.com) Blog (https://blog.langchain.dev)