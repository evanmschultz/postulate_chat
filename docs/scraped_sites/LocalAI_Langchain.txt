Let's load the LocalAI Embedding class. In order to use the LocalAI Embedding class, you need to have the LocalAI service hosted somewhere and configure the embedding models. See the documentation at https://localai.io/basics/getting_started/index.html and https://localai.io/features/embeddings/index.html. Let's load the LocalAI Embedding class with first generation models (e.g. text-search-ada-doc-001/text-search-ada-query-001). Note: These are not recommended models - see here IntegrationsCallbacksChat modelsChat loadersDocument loadersDocument transformersLLMsMemoryRetrieversText embedding modelsAleph AlphaAwaDBAzureOpenAIBedrockBGE on Hugging FaceClarifaiCohereDashScopeDeepInfraEDEN AIElasticsearchEmbaasERNIE Embedding-V1Fake EmbeddingsGoogle Vertex AI PaLMGPT4AllHugging FaceInstructEmbeddingsJinaLlama-cppLocalAIMiniMaxModelScopeMosaicMLNLP CloudOpenAISageMakerSelf HostedSentence TransformersSpaCyTensorflowHubXorbits inference (Xinference)Agents & ToolkitsToolsVector storesGrouped by provider Callbacks Chat models Chat loaders Document loaders Document transformers LLMs Memory Retrievers Text embedding modelsAleph AlphaAwaDBAzureOpenAIBedrockBGE on Hugging FaceClarifaiCohereDashScopeDeepInfraEDEN AIElasticsearchEmbaasERNIE Embedding-V1Fake EmbeddingsGoogle Vertex AI PaLMGPT4AllHugging FaceInstructEmbeddingsJinaLlama-cppLocalAIMiniMaxModelScopeMosaicMLNLP CloudOpenAISageMakerSelf HostedSentence TransformersSpaCyTensorflowHubXorbits inference (Xinference) Aleph Alpha AwaDB AzureOpenAI Bedrock BGE on Hugging Face Clarifai Cohere DashScope DeepInfra EDEN AI Elasticsearch Embaas ERNIE Embedding-V1 Fake Embeddings Google Vertex AI PaLM GPT4All Hugging Face InstructEmbeddings Jina Llama-cpp LocalAI MiniMax ModelScope MosaicML NLP Cloud OpenAI SageMaker Self Hosted Sentence Transformers SpaCy TensorflowHub Xorbits inference (Xinference) Agents & Toolkits Tools Vector stores Grouped by provider  Integrations Text embedding models LocalAI LocalAIEmbeddings LocalAIEmbeddings Discord Twitter Python JS/TS Homepage Blog Skip to main contentü¶úÔ∏èüîó LangChainDocsUse casesIntegrationsAPILangSmithJS/TS DocsCTRLKIntegrationsCallbacksChat modelsChat loadersDocument loadersDocument transformersLLMsMemoryRetrieversText embedding modelsAleph AlphaAwaDBAzureOpenAIBedrockBGE on Hugging FaceClarifaiCohereDashScopeDeepInfraEDEN AIElasticsearchEmbaasERNIE Embedding-V1Fake EmbeddingsGoogle Vertex AI PaLMGPT4AllHugging FaceInstructEmbeddingsJinaLlama-cppLocalAIMiniMaxModelScopeMosaicMLNLP CloudOpenAISageMakerSelf HostedSentence TransformersSpaCyTensorflowHubXorbits inference (Xinference)Agents & ToolkitsToolsVector storesGrouped by providerIntegrationsText embedding modelsLocalAILocalAILet's load the LocalAI Embedding class. In order to use the LocalAI Embedding class, you need to have the LocalAI service hosted somewhere and configure the embedding models. See the documentation at https://localai.io/basics/getting_started/index.html and https://localai.io/features/embeddings/index.html.from langchain.embeddings import LocalAIEmbeddingsAPI Reference:LocalAIEmbeddingsembeddings = LocalAIEmbeddings(openai_api_base="http://localhost:8080", model="embedding-model-name")text = "This is a test document."query_result = embeddings.embed_query(text)doc_result = embeddings.embed_documents([text])Let's load the LocalAI Embedding class with first generation models (e.g. text-search-ada-doc-001/text-search-ada-query-001). Note: These are not recommended models - see herefrom langchain.embeddings import LocalAIEmbeddingsAPI Reference:LocalAIEmbeddingsembeddings = LocalAIEmbeddings(openai_api_base="http://localhost:8080", model="embedding-model-name")text = "This is a test document."query_result = embeddings.embed_query(text)doc_result = embeddings.embed_documents([text])# if you are behind an explicit proxy, you can use the OPENAI_PROXY environment variable to pass throughos.environ["OPENAI_PROXY"] = "http://proxy.yourcompany.com:8080"PreviousLlama-cppNextMiniMaxCommunityDiscordTwitterGitHubPythonJS/TSMoreHomepageBlogCopyright ¬© 2023 LangChain, Inc. Skip to main content ü¶úÔ∏èüîó LangChainDocsUse casesIntegrationsAPILangSmithJS/TS DocsCTRLK ü¶úÔ∏èüîó LangChainDocsUse casesIntegrationsAPI LangSmithJS/TS DocsCTRLK  CTRLK CTRLK  CTRLK   IntegrationsCallbacksChat modelsChat loadersDocument loadersDocument transformersLLMsMemoryRetrieversText embedding modelsAleph AlphaAwaDBAzureOpenAIBedrockBGE on Hugging FaceClarifaiCohereDashScopeDeepInfraEDEN AIElasticsearchEmbaasERNIE Embedding-V1Fake EmbeddingsGoogle Vertex AI PaLMGPT4AllHugging FaceInstructEmbeddingsJinaLlama-cppLocalAIMiniMaxModelScopeMosaicMLNLP CloudOpenAISageMakerSelf HostedSentence TransformersSpaCyTensorflowHubXorbits inference (Xinference)Agents & ToolkitsToolsVector storesGrouped by providerIntegrationsText embedding modelsLocalAILocalAILet's load the LocalAI Embedding class. In order to use the LocalAI Embedding class, you need to have the LocalAI service hosted somewhere and configure the embedding models. See the documentation at https://localai.io/basics/getting_started/index.html and https://localai.io/features/embeddings/index.html.from langchain.embeddings import LocalAIEmbeddingsAPI Reference:LocalAIEmbeddingsembeddings = LocalAIEmbeddings(openai_api_base="http://localhost:8080", model="embedding-model-name")text = "This is a test document."query_result = embeddings.embed_query(text)doc_result = embeddings.embed_documents([text])Let's load the LocalAI Embedding class with first generation models (e.g. text-search-ada-doc-001/text-search-ada-query-001). Note: These are not recommended models - see herefrom langchain.embeddings import LocalAIEmbeddingsAPI Reference:LocalAIEmbeddingsembeddings = LocalAIEmbeddings(openai_api_base="http://localhost:8080", model="embedding-model-name")text = "This is a test document."query_result = embeddings.embed_query(text)doc_result = embeddings.embed_documents([text])# if you are behind an explicit proxy, you can use the OPENAI_PROXY environment variable to pass throughos.environ["OPENAI_PROXY"] = "http://proxy.yourcompany.com:8080"PreviousLlama-cppNextMiniMax IntegrationsCallbacksChat modelsChat loadersDocument loadersDocument transformersLLMsMemoryRetrieversText embedding modelsAleph AlphaAwaDBAzureOpenAIBedrockBGE on Hugging FaceClarifaiCohereDashScopeDeepInfraEDEN AIElasticsearchEmbaasERNIE Embedding-V1Fake EmbeddingsGoogle Vertex AI PaLMGPT4AllHugging FaceInstructEmbeddingsJinaLlama-cppLocalAIMiniMaxModelScopeMosaicMLNLP CloudOpenAISageMakerSelf HostedSentence TransformersSpaCyTensorflowHubXorbits inference (Xinference)Agents & ToolkitsToolsVector storesGrouped by providerIntegrationsText embedding modelsLocalAILocalAILet's load the LocalAI Embedding class. In order to use the LocalAI Embedding class, you need to have the LocalAI service hosted somewhere and configure the embedding models. See the documentation at https://localai.io/basics/getting_started/index.html and https://localai.io/features/embeddings/index.html.from langchain.embeddings import LocalAIEmbeddingsAPI Reference:LocalAIEmbeddingsembeddings = LocalAIEmbeddings(openai_api_base="http://localhost:8080", model="embedding-model-name")text = "This is a test document."query_result = embeddings.embed_query(text)doc_result = embeddings.embed_documents([text])Let's load the LocalAI Embedding class with first generation models (e.g. text-search-ada-doc-001/text-search-ada-query-001). Note: These are not recommended models - see herefrom langchain.embeddings import LocalAIEmbeddingsAPI Reference:LocalAIEmbeddingsembeddings = LocalAIEmbeddings(openai_api_base="http://localhost:8080", model="embedding-model-name")text = "This is a test document."query_result = embeddings.embed_query(text)doc_result = embeddings.embed_documents([text])# if you are behind an explicit proxy, you can use the OPENAI_PROXY environment variable to pass throughos.environ["OPENAI_PROXY"] = "http://proxy.yourcompany.com:8080"PreviousLlama-cppNextMiniMax IntegrationsCallbacksChat modelsChat loadersDocument loadersDocument transformersLLMsMemoryRetrieversText embedding modelsAleph AlphaAwaDBAzureOpenAIBedrockBGE on Hugging FaceClarifaiCohereDashScopeDeepInfraEDEN AIElasticsearchEmbaasERNIE Embedding-V1Fake EmbeddingsGoogle Vertex AI PaLMGPT4AllHugging FaceInstructEmbeddingsJinaLlama-cppLocalAIMiniMaxModelScopeMosaicMLNLP CloudOpenAISageMakerSelf HostedSentence TransformersSpaCyTensorflowHubXorbits inference (Xinference)Agents & ToolkitsToolsVector storesGrouped by provider IntegrationsCallbacksChat modelsChat loadersDocument loadersDocument transformersLLMsMemoryRetrieversText embedding modelsAleph AlphaAwaDBAzureOpenAIBedrockBGE on Hugging FaceClarifaiCohereDashScopeDeepInfraEDEN AIElasticsearchEmbaasERNIE Embedding-V1Fake EmbeddingsGoogle Vertex AI PaLMGPT4AllHugging FaceInstructEmbeddingsJinaLlama-cppLocalAIMiniMaxModelScopeMosaicMLNLP CloudOpenAISageMakerSelf HostedSentence TransformersSpaCyTensorflowHubXorbits inference (Xinference)Agents & ToolkitsToolsVector storesGrouped by provider Integrations Callbacks Chat models Chat loaders Document loaders Document transformers LLMs Memory Retrievers Text embedding models Agents & Toolkits Tools Vector stores Grouped by provider IntegrationsText embedding modelsLocalAILocalAILet's load the LocalAI Embedding class. In order to use the LocalAI Embedding class, you need to have the LocalAI service hosted somewhere and configure the embedding models. See the documentation at https://localai.io/basics/getting_started/index.html and https://localai.io/features/embeddings/index.html.from langchain.embeddings import LocalAIEmbeddingsAPI Reference:LocalAIEmbeddingsembeddings = LocalAIEmbeddings(openai_api_base="http://localhost:8080", model="embedding-model-name")text = "This is a test document."query_result = embeddings.embed_query(text)doc_result = embeddings.embed_documents([text])Let's load the LocalAI Embedding class with first generation models (e.g. text-search-ada-doc-001/text-search-ada-query-001). Note: These are not recommended models - see herefrom langchain.embeddings import LocalAIEmbeddingsAPI Reference:LocalAIEmbeddingsembeddings = LocalAIEmbeddings(openai_api_base="http://localhost:8080", model="embedding-model-name")text = "This is a test document."query_result = embeddings.embed_query(text)doc_result = embeddings.embed_documents([text])# if you are behind an explicit proxy, you can use the OPENAI_PROXY environment variable to pass throughos.environ["OPENAI_PROXY"] = "http://proxy.yourcompany.com:8080"PreviousLlama-cppNextMiniMax IntegrationsText embedding modelsLocalAILocalAILet's load the LocalAI Embedding class. In order to use the LocalAI Embedding class, you need to have the LocalAI service hosted somewhere and configure the embedding models. See the documentation at https://localai.io/basics/getting_started/index.html and https://localai.io/features/embeddings/index.html.from langchain.embeddings import LocalAIEmbeddingsAPI Reference:LocalAIEmbeddingsembeddings = LocalAIEmbeddings(openai_api_base="http://localhost:8080", model="embedding-model-name")text = "This is a test document."query_result = embeddings.embed_query(text)doc_result = embeddings.embed_documents([text])Let's load the LocalAI Embedding class with first generation models (e.g. text-search-ada-doc-001/text-search-ada-query-001). Note: These are not recommended models - see herefrom langchain.embeddings import LocalAIEmbeddingsAPI Reference:LocalAIEmbeddingsembeddings = LocalAIEmbeddings(openai_api_base="http://localhost:8080", model="embedding-model-name")text = "This is a test document."query_result = embeddings.embed_query(text)doc_result = embeddings.embed_documents([text])# if you are behind an explicit proxy, you can use the OPENAI_PROXY environment variable to pass throughos.environ["OPENAI_PROXY"] = "http://proxy.yourcompany.com:8080"PreviousLlama-cppNextMiniMax IntegrationsText embedding modelsLocalAILocalAILet's load the LocalAI Embedding class. In order to use the LocalAI Embedding class, you need to have the LocalAI service hosted somewhere and configure the embedding models. See the documentation at https://localai.io/basics/getting_started/index.html and https://localai.io/features/embeddings/index.html.from langchain.embeddings import LocalAIEmbeddingsAPI Reference:LocalAIEmbeddingsembeddings = LocalAIEmbeddings(openai_api_base="http://localhost:8080", model="embedding-model-name")text = "This is a test document."query_result = embeddings.embed_query(text)doc_result = embeddings.embed_documents([text])Let's load the LocalAI Embedding class with first generation models (e.g. text-search-ada-doc-001/text-search-ada-query-001). Note: These are not recommended models - see herefrom langchain.embeddings import LocalAIEmbeddingsAPI Reference:LocalAIEmbeddingsembeddings = LocalAIEmbeddings(openai_api_base="http://localhost:8080", model="embedding-model-name")text = "This is a test document."query_result = embeddings.embed_query(text)doc_result = embeddings.embed_documents([text])# if you are behind an explicit proxy, you can use the OPENAI_PROXY environment variable to pass throughos.environ["OPENAI_PROXY"] = "http://proxy.yourcompany.com:8080"PreviousLlama-cppNextMiniMax IntegrationsText embedding modelsLocalAILocalAILet's load the LocalAI Embedding class. In order to use the LocalAI Embedding class, you need to have the LocalAI service hosted somewhere and configure the embedding models. See the documentation at https://localai.io/basics/getting_started/index.html and https://localai.io/features/embeddings/index.html.from langchain.embeddings import LocalAIEmbeddingsAPI Reference:LocalAIEmbeddingsembeddings = LocalAIEmbeddings(openai_api_base="http://localhost:8080", model="embedding-model-name")text = "This is a test document."query_result = embeddings.embed_query(text)doc_result = embeddings.embed_documents([text])Let's load the LocalAI Embedding class with first generation models (e.g. text-search-ada-doc-001/text-search-ada-query-001). Note: These are not recommended models - see herefrom langchain.embeddings import LocalAIEmbeddingsAPI Reference:LocalAIEmbeddingsembeddings = LocalAIEmbeddings(openai_api_base="http://localhost:8080", model="embedding-model-name")text = "This is a test document."query_result = embeddings.embed_query(text)doc_result = embeddings.embed_documents([text])# if you are behind an explicit proxy, you can use the OPENAI_PROXY environment variable to pass throughos.environ["OPENAI_PROXY"] = "http://proxy.yourcompany.com:8080"PreviousLlama-cppNextMiniMax LocalAILet's load the LocalAI Embedding class. In order to use the LocalAI Embedding class, you need to have the LocalAI service hosted somewhere and configure the embedding models. See the documentation at https://localai.io/basics/getting_started/index.html and https://localai.io/features/embeddings/index.html.from langchain.embeddings import LocalAIEmbeddingsAPI Reference:LocalAIEmbeddingsembeddings = LocalAIEmbeddings(openai_api_base="http://localhost:8080", model="embedding-model-name")text = "This is a test document."query_result = embeddings.embed_query(text)doc_result = embeddings.embed_documents([text])Let's load the LocalAI Embedding class with first generation models (e.g. text-search-ada-doc-001/text-search-ada-query-001). Note: These are not recommended models - see herefrom langchain.embeddings import LocalAIEmbeddingsAPI Reference:LocalAIEmbeddingsembeddings = LocalAIEmbeddings(openai_api_base="http://localhost:8080", model="embedding-model-name")text = "This is a test document."query_result = embeddings.embed_query(text)doc_result = embeddings.embed_documents([text])# if you are behind an explicit proxy, you can use the OPENAI_PROXY environment variable to pass throughos.environ["OPENAI_PROXY"] = "http://proxy.yourcompany.com:8080" from langchain.embeddings import LocalAIEmbeddings from langchain.embeddings import LocalAIEmbeddings  API Reference:LocalAIEmbeddings embeddings = LocalAIEmbeddings(openai_api_base="http://localhost:8080", model="embedding-model-name") embeddings = LocalAIEmbeddings(openai_api_base="http://localhost:8080", model="embedding-model-name")  text = "This is a test document." text = "This is a test document."  query_result = embeddings.embed_query(text) query_result = embeddings.embed_query(text)  doc_result = embeddings.embed_documents([text]) doc_result = embeddings.embed_documents([text])  from langchain.embeddings import LocalAIEmbeddings from langchain.embeddings import LocalAIEmbeddings  API Reference:LocalAIEmbeddings embeddings = LocalAIEmbeddings(openai_api_base="http://localhost:8080", model="embedding-model-name") embeddings = LocalAIEmbeddings(openai_api_base="http://localhost:8080", model="embedding-model-name")  text = "This is a test document." text = "This is a test document."  query_result = embeddings.embed_query(text) query_result = embeddings.embed_query(text)  doc_result = embeddings.embed_documents([text]) doc_result = embeddings.embed_documents([text])  # if you are behind an explicit proxy, you can use the OPENAI_PROXY environment variable to pass throughos.environ["OPENAI_PROXY"] = "http://proxy.yourcompany.com:8080" # if you are behind an explicit proxy, you can use the OPENAI_PROXY environment variable to pass throughos.environ["OPENAI_PROXY"] = "http://proxy.yourcompany.com:8080"  Previous Llama-cpp Next MiniMax CommunityDiscordTwitterGitHubPythonJS/TSMoreHomepageBlogCopyright ¬© 2023 LangChain, Inc. CommunityDiscordTwitterGitHubPythonJS/TSMoreHomepageBlog CommunityDiscordTwitter Community GitHubPythonJS/TS GitHub MoreHomepageBlog More Copyright ¬© 2023 LangChain, Inc. Copyright ¬© 2023 LangChain, Inc. Skip to main content (#docusaurus_skipToContent_fallback) ü¶úÔ∏èüîó LangChain (/) Docs (/docs/get_started/introduction) Use cases (/docs/use_cases) Integrations (/docs/integrations) API (https://api.python.langchain.com) LangSmith (https://smith.langchain.com) JS/TS Docs (https://js.langchain.com/docs)  (https://github.com/hwchase17/langchain) Integrations (/docs/integrations) Callbacks (/docs/integrations/callbacks/) Chat models (/docs/integrations/chat/) Chat loaders (/docs/integrations/chat_loaders/) Document loaders (/docs/integrations/document_loaders/) Document transformers (/docs/integrations/document_transformers/) LLMs (/docs/integrations/llms/) Memory (/docs/integrations/memory/) Retrievers (/docs/integrations/retrievers/) Text embedding models (/docs/integrations/text_embedding/) Aleph Alpha (/docs/integrations/text_embedding/aleph_alpha) AwaDB (/docs/integrations/text_embedding/awadb) AzureOpenAI (/docs/integrations/text_embedding/azureopenai) Bedrock (/docs/integrations/text_embedding/bedrock) BGE on Hugging Face (/docs/integrations/text_embedding/bge_huggingface) Clarifai (/docs/integrations/text_embedding/clarifai) Cohere (/docs/integrations/text_embedding/cohere) DashScope (/docs/integrations/text_embedding/dashscope) DeepInfra (/docs/integrations/text_embedding/deepinfra) EDEN AI (/docs/integrations/text_embedding/edenai) Elasticsearch (/docs/integrations/text_embedding/elasticsearch) Embaas (/docs/integrations/text_embedding/embaas) ERNIE Embedding-V1 (/docs/integrations/text_embedding/ernie) Fake Embeddings (/docs/integrations/text_embedding/fake) Google Vertex AI PaLM (/docs/integrations/text_embedding/google_vertex_ai_palm) GPT4All (/docs/integrations/text_embedding/gpt4all) Hugging Face (/docs/integrations/text_embedding/huggingfacehub) InstructEmbeddings (/docs/integrations/text_embedding/instruct_embeddings) Jina (/docs/integrations/text_embedding/jina) Llama-cpp (/docs/integrations/text_embedding/llamacpp) LocalAI (/docs/integrations/text_embedding/localai) MiniMax (/docs/integrations/text_embedding/minimax) ModelScope (/docs/integrations/text_embedding/modelscope_hub) MosaicML (/docs/integrations/text_embedding/mosaicml) NLP Cloud (/docs/integrations/text_embedding/nlp_cloud) OpenAI (/docs/integrations/text_embedding/openai) SageMaker (/docs/integrations/text_embedding/sagemaker-endpoint) Self Hosted (/docs/integrations/text_embedding/self-hosted) Sentence Transformers (/docs/integrations/text_embedding/sentence_transformers) SpaCy (/docs/integrations/text_embedding/spacy_embedding) TensorflowHub (/docs/integrations/text_embedding/tensorflowhub) Xorbits inference (Xinference) (/docs/integrations/text_embedding/xinference) Agents & Toolkits (/docs/integrations/toolkits/) Tools (/docs/integrations/tools/) Vector stores (/docs/integrations/vectorstores/) Grouped by provider (/docs/integrations/providers/)  (/) Integrations (/docs/integrations) Text embedding models (/docs/integrations/text_embedding/) https://localai.io/basics/getting_started/index.html (https://localai.io/basics/getting_started/index.html) https://localai.io/features/embeddings/index.html (https://localai.io/features/embeddings/index.html) LocalAIEmbeddings (https://api.python.langchain.com/en/latest/embeddings/langchain.embeddings.localai.LocalAIEmbeddings.html) here (https://platform.openai.com/docs/guides/embeddings/what-are-embeddings) LocalAIEmbeddings (https://api.python.langchain.com/en/latest/embeddings/langchain.embeddings.localai.LocalAIEmbeddings.html) PreviousLlama-cpp (/docs/integrations/text_embedding/llamacpp) NextMiniMax (/docs/integrations/text_embedding/minimax) Discord (https://discord.gg/cU2adEyC7w) Twitter (https://twitter.com/LangChainAI) Python (https://github.com/hwchase17/langchain) JS/TS (https://github.com/hwchase17/langchainjs) Homepage (https://langchain.com) Blog (https://blog.langchain.dev)