Dingo is a distributed multi-mode vector database, which combines the characteristics of data lakes and vector databases, and can store data of any type and size (Key-Value, PDF, audio, video, etc.). It has real-time low-latency processing capabilities to achieve rapid insight and response, and can efficiently conduct instant analysis and process multi-modal data. This notebook shows how to use functionality related to the DingoDB vector database. To run, you should have a DingoDB instance up and running. We want to use OpenAIEmbeddings so we have to get the OpenAI API Key. More text can embedded and upserted to an existing Dingo index using the add_texts function In addition to using similarity search in the retriever object, you can also use mmr as retriever. Or use max_marginal_relevance_search directly: IntegrationsCallbacksChat modelsChat loadersDocument loadersDocument transformersLLMsMemoryRetrieversText embedding modelsAgents & ToolkitsToolsVector storesActiveloop Deep LakeAlibaba Cloud OpenSearchAnalyticDBAnnoyAtlasAwaDBAzure Cognitive SearchBagelDBCassandraChromaClickHouseDashVectorDingoDocArray HnswSearchDocArray InMemorySearchElasticsearchEpsillaFaissHologresLanceDBMarqoGoogle Vertex AI MatchingEngineMeilisearchMilvusMongoDB AtlasMyScaleNeo4j Vector IndexNucliaDBOpenSearchPostgres EmbeddingPGVectorPineconeQdrantRedisRocksetScaNNSingleStoreDBscikit-learnsqlite-vssStarRocksSupabase (Postgres)TairTencent Cloud VectorDBTigrisTypesenseUSearchvearchVectaraWeaviateXataZepZillizGrouped by provider Callbacks Chat models Chat loaders Document loaders Document transformers LLMs Memory Retrievers Text embedding models Agents & Toolkits Tools Vector storesActiveloop Deep LakeAlibaba Cloud OpenSearchAnalyticDBAnnoyAtlasAwaDBAzure Cognitive SearchBagelDBCassandraChromaClickHouseDashVectorDingoDocArray HnswSearchDocArray InMemorySearchElasticsearchEpsillaFaissHologresLanceDBMarqoGoogle Vertex AI MatchingEngineMeilisearchMilvusMongoDB AtlasMyScaleNeo4j Vector IndexNucliaDBOpenSearchPostgres EmbeddingPGVectorPineconeQdrantRedisRocksetScaNNSingleStoreDBscikit-learnsqlite-vssStarRocksSupabase (Postgres)TairTencent Cloud VectorDBTigrisTypesenseUSearchvearchVectaraWeaviateXataZepZilliz Activeloop Deep Lake Alibaba Cloud OpenSearch AnalyticDB Annoy Atlas AwaDB Azure Cognitive Search BagelDB Cassandra Chroma ClickHouse DashVector Dingo DocArray HnswSearch DocArray InMemorySearch Elasticsearch Epsilla Faiss Hologres LanceDB Marqo Google Vertex AI MatchingEngine Meilisearch Milvus MongoDB Atlas MyScale Neo4j Vector Index NucliaDB OpenSearch Postgres Embedding PGVector Pinecone Qdrant Redis Rockset ScaNN SingleStoreDB scikit-learn sqlite-vss StarRocks Supabase (Postgres) Tair Tencent Cloud VectorDB Tigris Typesense USearch vearch Vectara Weaviate Xata Zep Zilliz Grouped by provider  Integrations Vector stores Dingo OpenAIEmbeddings CharacterTextSplitter Dingo TextLoader TextLoader OpenAIEmbeddings CharacterTextSplitter Dingo TextLoader Adding More Text to an Existing Index Maximal Marginal Relevance Searches Discord Twitter Python JS/TS Homepage Blog Skip to main contentü¶úÔ∏èüîó LangChainDocsUse casesIntegrationsAPILangSmithJS/TS DocsCTRLKIntegrationsCallbacksChat modelsChat loadersDocument loadersDocument transformersLLMsMemoryRetrieversText embedding modelsAgents & ToolkitsToolsVector storesActiveloop Deep LakeAlibaba Cloud OpenSearchAnalyticDBAnnoyAtlasAwaDBAzure Cognitive SearchBagelDBCassandraChromaClickHouseDashVectorDingoDocArray HnswSearchDocArray InMemorySearchElasticsearchEpsillaFaissHologresLanceDBMarqoGoogle Vertex AI MatchingEngineMeilisearchMilvusMongoDB AtlasMyScaleNeo4j Vector IndexNucliaDBOpenSearchPostgres EmbeddingPGVectorPineconeQdrantRedisRocksetScaNNSingleStoreDBscikit-learnsqlite-vssStarRocksSupabase (Postgres)TairTencent Cloud VectorDBTigrisTypesenseUSearchvearchVectaraWeaviateXataZepZillizGrouped by providerIntegrationsVector storesDingoOn this pageDingoDingo is a distributed multi-mode vector database, which combines the characteristics of data lakes and vector databases, and can store data of any type and size (Key-Value, PDF, audio, video, etc.). It has real-time low-latency processing capabilities to achieve rapid insight and response, and can efficiently conduct instant analysis and process multi-modal data.This notebook shows how to use functionality related to the DingoDB vector database.To run, you should have a DingoDB instance up and running.pip install dingodbor install latest:pip install git+https://git@github.com/dingodb/pydingo.gitWe want to use OpenAIEmbeddings so we have to get the OpenAI API Key.import osimport getpassos.environ["OPENAI_API_KEY"] = getpass.getpass("OpenAI API Key:")    OpenAI API Key:¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑from langchain.embeddings.openai import OpenAIEmbeddingsfrom langchain.text_splitter import CharacterTextSplitterfrom langchain.vectorstores import Dingofrom langchain.document_loaders import TextLoaderAPI Reference:OpenAIEmbeddingsCharacterTextSplitterDingoTextLoaderfrom langchain.document_loaders import TextLoaderloader = TextLoader("../../../state_of_the_union.txt")documents = loader.load()text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)docs = text_splitter.split_documents(documents)embeddings = OpenAIEmbeddings()API Reference:TextLoaderfrom dingodb import DingoDBindex_name = "langchain-demo"dingo_client = DingoDB(user="", password="", host=["127.0.0.1:13000"])# First, check if our index already exists. If it doesn't, we create itif index_name not in dingo_client.get_index():    # we create a new index, modify to your own    dingo_client.create_index(      index_name=index_name,      dimension=1536,      metric_type='cosine',      auto_id=False)# The OpenAI embedding model `text-embedding-ada-002 uses 1536 dimensions`docsearch = Dingo.from_documents(docs, embeddings, client=dingo_client, index_name=index_name)from langchain.embeddings.openai import OpenAIEmbeddingsfrom langchain.text_splitter import CharacterTextSplitterfrom langchain.vectorstores import Dingofrom langchain.document_loaders import TextLoaderAPI Reference:OpenAIEmbeddingsCharacterTextSplitterDingoTextLoaderquery = "What did the president say about Ketanji Brown Jackson"docs = docsearch.similarity_search(query)print(docs[0].page_content)Adding More Text to an Existing Index‚ÄãMore text can embedded and upserted to an existing Dingo index using the add_texts functionvectorstore = Dingo(embeddings, "text", client=dingo_client, index_name=index_name)vectorstore.add_texts(["More text!"])Maximal Marginal Relevance Searches‚ÄãIn addition to using similarity search in the retriever object, you can also use mmr as retriever.retriever = docsearch.as_retriever(search_type="mmr")matched_docs = retriever.get_relevant_documents(query)for i, d in enumerate(matched_docs):    print(f"\n## Document {i}\n")    print(d.page_content)Or use max_marginal_relevance_search directly:found_docs = docsearch.max_marginal_relevance_search(query, k=2, fetch_k=10)for i, doc in enumerate(found_docs):    print(f"{i + 1}.", doc.page_content, "\n")PreviousDashVectorNextDocArray HnswSearchAdding More Text to an Existing IndexMaximal Marginal Relevance SearchesCommunityDiscordTwitterGitHubPythonJS/TSMoreHomepageBlogCopyright ¬© 2023 LangChain, Inc. Skip to main content ü¶úÔ∏èüîó LangChainDocsUse casesIntegrationsAPILangSmithJS/TS DocsCTRLK ü¶úÔ∏èüîó LangChainDocsUse casesIntegrationsAPI LangSmithJS/TS DocsCTRLK  CTRLK CTRLK  CTRLK   IntegrationsCallbacksChat modelsChat loadersDocument loadersDocument transformersLLMsMemoryRetrieversText embedding modelsAgents & ToolkitsToolsVector storesActiveloop Deep LakeAlibaba Cloud OpenSearchAnalyticDBAnnoyAtlasAwaDBAzure Cognitive SearchBagelDBCassandraChromaClickHouseDashVectorDingoDocArray HnswSearchDocArray InMemorySearchElasticsearchEpsillaFaissHologresLanceDBMarqoGoogle Vertex AI MatchingEngineMeilisearchMilvusMongoDB AtlasMyScaleNeo4j Vector IndexNucliaDBOpenSearchPostgres EmbeddingPGVectorPineconeQdrantRedisRocksetScaNNSingleStoreDBscikit-learnsqlite-vssStarRocksSupabase (Postgres)TairTencent Cloud VectorDBTigrisTypesenseUSearchvearchVectaraWeaviateXataZepZillizGrouped by providerIntegrationsVector storesDingoOn this pageDingoDingo is a distributed multi-mode vector database, which combines the characteristics of data lakes and vector databases, and can store data of any type and size (Key-Value, PDF, audio, video, etc.). It has real-time low-latency processing capabilities to achieve rapid insight and response, and can efficiently conduct instant analysis and process multi-modal data.This notebook shows how to use functionality related to the DingoDB vector database.To run, you should have a DingoDB instance up and running.pip install dingodbor install latest:pip install git+https://git@github.com/dingodb/pydingo.gitWe want to use OpenAIEmbeddings so we have to get the OpenAI API Key.import osimport getpassos.environ["OPENAI_API_KEY"] = getpass.getpass("OpenAI API Key:")    OpenAI API Key:¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑from langchain.embeddings.openai import OpenAIEmbeddingsfrom langchain.text_splitter import CharacterTextSplitterfrom langchain.vectorstores import Dingofrom langchain.document_loaders import TextLoaderAPI Reference:OpenAIEmbeddingsCharacterTextSplitterDingoTextLoaderfrom langchain.document_loaders import TextLoaderloader = TextLoader("../../../state_of_the_union.txt")documents = loader.load()text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)docs = text_splitter.split_documents(documents)embeddings = OpenAIEmbeddings()API Reference:TextLoaderfrom dingodb import DingoDBindex_name = "langchain-demo"dingo_client = DingoDB(user="", password="", host=["127.0.0.1:13000"])# First, check if our index already exists. If it doesn't, we create itif index_name not in dingo_client.get_index():    # we create a new index, modify to your own    dingo_client.create_index(      index_name=index_name,      dimension=1536,      metric_type='cosine',      auto_id=False)# The OpenAI embedding model `text-embedding-ada-002 uses 1536 dimensions`docsearch = Dingo.from_documents(docs, embeddings, client=dingo_client, index_name=index_name)from langchain.embeddings.openai import OpenAIEmbeddingsfrom langchain.text_splitter import CharacterTextSplitterfrom langchain.vectorstores import Dingofrom langchain.document_loaders import TextLoaderAPI Reference:OpenAIEmbeddingsCharacterTextSplitterDingoTextLoaderquery = "What did the president say about Ketanji Brown Jackson"docs = docsearch.similarity_search(query)print(docs[0].page_content)Adding More Text to an Existing Index‚ÄãMore text can embedded and upserted to an existing Dingo index using the add_texts functionvectorstore = Dingo(embeddings, "text", client=dingo_client, index_name=index_name)vectorstore.add_texts(["More text!"])Maximal Marginal Relevance Searches‚ÄãIn addition to using similarity search in the retriever object, you can also use mmr as retriever.retriever = docsearch.as_retriever(search_type="mmr")matched_docs = retriever.get_relevant_documents(query)for i, d in enumerate(matched_docs):    print(f"\n## Document {i}\n")    print(d.page_content)Or use max_marginal_relevance_search directly:found_docs = docsearch.max_marginal_relevance_search(query, k=2, fetch_k=10)for i, doc in enumerate(found_docs):    print(f"{i + 1}.", doc.page_content, "\n")PreviousDashVectorNextDocArray HnswSearchAdding More Text to an Existing IndexMaximal Marginal Relevance Searches IntegrationsCallbacksChat modelsChat loadersDocument loadersDocument transformersLLMsMemoryRetrieversText embedding modelsAgents & ToolkitsToolsVector storesActiveloop Deep LakeAlibaba Cloud OpenSearchAnalyticDBAnnoyAtlasAwaDBAzure Cognitive SearchBagelDBCassandraChromaClickHouseDashVectorDingoDocArray HnswSearchDocArray InMemorySearchElasticsearchEpsillaFaissHologresLanceDBMarqoGoogle Vertex AI MatchingEngineMeilisearchMilvusMongoDB AtlasMyScaleNeo4j Vector IndexNucliaDBOpenSearchPostgres EmbeddingPGVectorPineconeQdrantRedisRocksetScaNNSingleStoreDBscikit-learnsqlite-vssStarRocksSupabase (Postgres)TairTencent Cloud VectorDBTigrisTypesenseUSearchvearchVectaraWeaviateXataZepZillizGrouped by providerIntegrationsVector storesDingoOn this pageDingoDingo is a distributed multi-mode vector database, which combines the characteristics of data lakes and vector databases, and can store data of any type and size (Key-Value, PDF, audio, video, etc.). It has real-time low-latency processing capabilities to achieve rapid insight and response, and can efficiently conduct instant analysis and process multi-modal data.This notebook shows how to use functionality related to the DingoDB vector database.To run, you should have a DingoDB instance up and running.pip install dingodbor install latest:pip install git+https://git@github.com/dingodb/pydingo.gitWe want to use OpenAIEmbeddings so we have to get the OpenAI API Key.import osimport getpassos.environ["OPENAI_API_KEY"] = getpass.getpass("OpenAI API Key:")    OpenAI API Key:¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑from langchain.embeddings.openai import OpenAIEmbeddingsfrom langchain.text_splitter import CharacterTextSplitterfrom langchain.vectorstores import Dingofrom langchain.document_loaders import TextLoaderAPI Reference:OpenAIEmbeddingsCharacterTextSplitterDingoTextLoaderfrom langchain.document_loaders import TextLoaderloader = TextLoader("../../../state_of_the_union.txt")documents = loader.load()text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)docs = text_splitter.split_documents(documents)embeddings = OpenAIEmbeddings()API Reference:TextLoaderfrom dingodb import DingoDBindex_name = "langchain-demo"dingo_client = DingoDB(user="", password="", host=["127.0.0.1:13000"])# First, check if our index already exists. If it doesn't, we create itif index_name not in dingo_client.get_index():    # we create a new index, modify to your own    dingo_client.create_index(      index_name=index_name,      dimension=1536,      metric_type='cosine',      auto_id=False)# The OpenAI embedding model `text-embedding-ada-002 uses 1536 dimensions`docsearch = Dingo.from_documents(docs, embeddings, client=dingo_client, index_name=index_name)from langchain.embeddings.openai import OpenAIEmbeddingsfrom langchain.text_splitter import CharacterTextSplitterfrom langchain.vectorstores import Dingofrom langchain.document_loaders import TextLoaderAPI Reference:OpenAIEmbeddingsCharacterTextSplitterDingoTextLoaderquery = "What did the president say about Ketanji Brown Jackson"docs = docsearch.similarity_search(query)print(docs[0].page_content)Adding More Text to an Existing Index‚ÄãMore text can embedded and upserted to an existing Dingo index using the add_texts functionvectorstore = Dingo(embeddings, "text", client=dingo_client, index_name=index_name)vectorstore.add_texts(["More text!"])Maximal Marginal Relevance Searches‚ÄãIn addition to using similarity search in the retriever object, you can also use mmr as retriever.retriever = docsearch.as_retriever(search_type="mmr")matched_docs = retriever.get_relevant_documents(query)for i, d in enumerate(matched_docs):    print(f"\n## Document {i}\n")    print(d.page_content)Or use max_marginal_relevance_search directly:found_docs = docsearch.max_marginal_relevance_search(query, k=2, fetch_k=10)for i, doc in enumerate(found_docs):    print(f"{i + 1}.", doc.page_content, "\n")PreviousDashVectorNextDocArray HnswSearchAdding More Text to an Existing IndexMaximal Marginal Relevance Searches IntegrationsCallbacksChat modelsChat loadersDocument loadersDocument transformersLLMsMemoryRetrieversText embedding modelsAgents & ToolkitsToolsVector storesActiveloop Deep LakeAlibaba Cloud OpenSearchAnalyticDBAnnoyAtlasAwaDBAzure Cognitive SearchBagelDBCassandraChromaClickHouseDashVectorDingoDocArray HnswSearchDocArray InMemorySearchElasticsearchEpsillaFaissHologresLanceDBMarqoGoogle Vertex AI MatchingEngineMeilisearchMilvusMongoDB AtlasMyScaleNeo4j Vector IndexNucliaDBOpenSearchPostgres EmbeddingPGVectorPineconeQdrantRedisRocksetScaNNSingleStoreDBscikit-learnsqlite-vssStarRocksSupabase (Postgres)TairTencent Cloud VectorDBTigrisTypesenseUSearchvearchVectaraWeaviateXataZepZillizGrouped by provider IntegrationsCallbacksChat modelsChat loadersDocument loadersDocument transformersLLMsMemoryRetrieversText embedding modelsAgents & ToolkitsToolsVector storesActiveloop Deep LakeAlibaba Cloud OpenSearchAnalyticDBAnnoyAtlasAwaDBAzure Cognitive SearchBagelDBCassandraChromaClickHouseDashVectorDingoDocArray HnswSearchDocArray InMemorySearchElasticsearchEpsillaFaissHologresLanceDBMarqoGoogle Vertex AI MatchingEngineMeilisearchMilvusMongoDB AtlasMyScaleNeo4j Vector IndexNucliaDBOpenSearchPostgres EmbeddingPGVectorPineconeQdrantRedisRocksetScaNNSingleStoreDBscikit-learnsqlite-vssStarRocksSupabase (Postgres)TairTencent Cloud VectorDBTigrisTypesenseUSearchvearchVectaraWeaviateXataZepZillizGrouped by provider Integrations Callbacks Chat models Chat loaders Document loaders Document transformers LLMs Memory Retrievers Text embedding models Agents & Toolkits Tools Vector stores Grouped by provider IntegrationsVector storesDingoOn this pageDingoDingo is a distributed multi-mode vector database, which combines the characteristics of data lakes and vector databases, and can store data of any type and size (Key-Value, PDF, audio, video, etc.). It has real-time low-latency processing capabilities to achieve rapid insight and response, and can efficiently conduct instant analysis and process multi-modal data.This notebook shows how to use functionality related to the DingoDB vector database.To run, you should have a DingoDB instance up and running.pip install dingodbor install latest:pip install git+https://git@github.com/dingodb/pydingo.gitWe want to use OpenAIEmbeddings so we have to get the OpenAI API Key.import osimport getpassos.environ["OPENAI_API_KEY"] = getpass.getpass("OpenAI API Key:")    OpenAI API Key:¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑from langchain.embeddings.openai import OpenAIEmbeddingsfrom langchain.text_splitter import CharacterTextSplitterfrom langchain.vectorstores import Dingofrom langchain.document_loaders import TextLoaderAPI Reference:OpenAIEmbeddingsCharacterTextSplitterDingoTextLoaderfrom langchain.document_loaders import TextLoaderloader = TextLoader("../../../state_of_the_union.txt")documents = loader.load()text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)docs = text_splitter.split_documents(documents)embeddings = OpenAIEmbeddings()API Reference:TextLoaderfrom dingodb import DingoDBindex_name = "langchain-demo"dingo_client = DingoDB(user="", password="", host=["127.0.0.1:13000"])# First, check if our index already exists. If it doesn't, we create itif index_name not in dingo_client.get_index():    # we create a new index, modify to your own    dingo_client.create_index(      index_name=index_name,      dimension=1536,      metric_type='cosine',      auto_id=False)# The OpenAI embedding model `text-embedding-ada-002 uses 1536 dimensions`docsearch = Dingo.from_documents(docs, embeddings, client=dingo_client, index_name=index_name)from langchain.embeddings.openai import OpenAIEmbeddingsfrom langchain.text_splitter import CharacterTextSplitterfrom langchain.vectorstores import Dingofrom langchain.document_loaders import TextLoaderAPI Reference:OpenAIEmbeddingsCharacterTextSplitterDingoTextLoaderquery = "What did the president say about Ketanji Brown Jackson"docs = docsearch.similarity_search(query)print(docs[0].page_content)Adding More Text to an Existing Index‚ÄãMore text can embedded and upserted to an existing Dingo index using the add_texts functionvectorstore = Dingo(embeddings, "text", client=dingo_client, index_name=index_name)vectorstore.add_texts(["More text!"])Maximal Marginal Relevance Searches‚ÄãIn addition to using similarity search in the retriever object, you can also use mmr as retriever.retriever = docsearch.as_retriever(search_type="mmr")matched_docs = retriever.get_relevant_documents(query)for i, d in enumerate(matched_docs):    print(f"\n## Document {i}\n")    print(d.page_content)Or use max_marginal_relevance_search directly:found_docs = docsearch.max_marginal_relevance_search(query, k=2, fetch_k=10)for i, doc in enumerate(found_docs):    print(f"{i + 1}.", doc.page_content, "\n")PreviousDashVectorNextDocArray HnswSearchAdding More Text to an Existing IndexMaximal Marginal Relevance Searches IntegrationsVector storesDingoOn this pageDingoDingo is a distributed multi-mode vector database, which combines the characteristics of data lakes and vector databases, and can store data of any type and size (Key-Value, PDF, audio, video, etc.). It has real-time low-latency processing capabilities to achieve rapid insight and response, and can efficiently conduct instant analysis and process multi-modal data.This notebook shows how to use functionality related to the DingoDB vector database.To run, you should have a DingoDB instance up and running.pip install dingodbor install latest:pip install git+https://git@github.com/dingodb/pydingo.gitWe want to use OpenAIEmbeddings so we have to get the OpenAI API Key.import osimport getpassos.environ["OPENAI_API_KEY"] = getpass.getpass("OpenAI API Key:")    OpenAI API Key:¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑from langchain.embeddings.openai import OpenAIEmbeddingsfrom langchain.text_splitter import CharacterTextSplitterfrom langchain.vectorstores import Dingofrom langchain.document_loaders import TextLoaderAPI Reference:OpenAIEmbeddingsCharacterTextSplitterDingoTextLoaderfrom langchain.document_loaders import TextLoaderloader = TextLoader("../../../state_of_the_union.txt")documents = loader.load()text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)docs = text_splitter.split_documents(documents)embeddings = OpenAIEmbeddings()API Reference:TextLoaderfrom dingodb import DingoDBindex_name = "langchain-demo"dingo_client = DingoDB(user="", password="", host=["127.0.0.1:13000"])# First, check if our index already exists. If it doesn't, we create itif index_name not in dingo_client.get_index():    # we create a new index, modify to your own    dingo_client.create_index(      index_name=index_name,      dimension=1536,      metric_type='cosine',      auto_id=False)# The OpenAI embedding model `text-embedding-ada-002 uses 1536 dimensions`docsearch = Dingo.from_documents(docs, embeddings, client=dingo_client, index_name=index_name)from langchain.embeddings.openai import OpenAIEmbeddingsfrom langchain.text_splitter import CharacterTextSplitterfrom langchain.vectorstores import Dingofrom langchain.document_loaders import TextLoaderAPI Reference:OpenAIEmbeddingsCharacterTextSplitterDingoTextLoaderquery = "What did the president say about Ketanji Brown Jackson"docs = docsearch.similarity_search(query)print(docs[0].page_content)Adding More Text to an Existing Index‚ÄãMore text can embedded and upserted to an existing Dingo index using the add_texts functionvectorstore = Dingo(embeddings, "text", client=dingo_client, index_name=index_name)vectorstore.add_texts(["More text!"])Maximal Marginal Relevance Searches‚ÄãIn addition to using similarity search in the retriever object, you can also use mmr as retriever.retriever = docsearch.as_retriever(search_type="mmr")matched_docs = retriever.get_relevant_documents(query)for i, d in enumerate(matched_docs):    print(f"\n## Document {i}\n")    print(d.page_content)Or use max_marginal_relevance_search directly:found_docs = docsearch.max_marginal_relevance_search(query, k=2, fetch_k=10)for i, doc in enumerate(found_docs):    print(f"{i + 1}.", doc.page_content, "\n")PreviousDashVectorNextDocArray HnswSearchAdding More Text to an Existing IndexMaximal Marginal Relevance Searches IntegrationsVector storesDingoOn this pageDingoDingo is a distributed multi-mode vector database, which combines the characteristics of data lakes and vector databases, and can store data of any type and size (Key-Value, PDF, audio, video, etc.). It has real-time low-latency processing capabilities to achieve rapid insight and response, and can efficiently conduct instant analysis and process multi-modal data.This notebook shows how to use functionality related to the DingoDB vector database.To run, you should have a DingoDB instance up and running.pip install dingodbor install latest:pip install git+https://git@github.com/dingodb/pydingo.gitWe want to use OpenAIEmbeddings so we have to get the OpenAI API Key.import osimport getpassos.environ["OPENAI_API_KEY"] = getpass.getpass("OpenAI API Key:")    OpenAI API Key:¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑from langchain.embeddings.openai import OpenAIEmbeddingsfrom langchain.text_splitter import CharacterTextSplitterfrom langchain.vectorstores import Dingofrom langchain.document_loaders import TextLoaderAPI Reference:OpenAIEmbeddingsCharacterTextSplitterDingoTextLoaderfrom langchain.document_loaders import TextLoaderloader = TextLoader("../../../state_of_the_union.txt")documents = loader.load()text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)docs = text_splitter.split_documents(documents)embeddings = OpenAIEmbeddings()API Reference:TextLoaderfrom dingodb import DingoDBindex_name = "langchain-demo"dingo_client = DingoDB(user="", password="", host=["127.0.0.1:13000"])# First, check if our index already exists. If it doesn't, we create itif index_name not in dingo_client.get_index():    # we create a new index, modify to your own    dingo_client.create_index(      index_name=index_name,      dimension=1536,      metric_type='cosine',      auto_id=False)# The OpenAI embedding model `text-embedding-ada-002 uses 1536 dimensions`docsearch = Dingo.from_documents(docs, embeddings, client=dingo_client, index_name=index_name)from langchain.embeddings.openai import OpenAIEmbeddingsfrom langchain.text_splitter import CharacterTextSplitterfrom langchain.vectorstores import Dingofrom langchain.document_loaders import TextLoaderAPI Reference:OpenAIEmbeddingsCharacterTextSplitterDingoTextLoaderquery = "What did the president say about Ketanji Brown Jackson"docs = docsearch.similarity_search(query)print(docs[0].page_content)Adding More Text to an Existing Index‚ÄãMore text can embedded and upserted to an existing Dingo index using the add_texts functionvectorstore = Dingo(embeddings, "text", client=dingo_client, index_name=index_name)vectorstore.add_texts(["More text!"])Maximal Marginal Relevance Searches‚ÄãIn addition to using similarity search in the retriever object, you can also use mmr as retriever.retriever = docsearch.as_retriever(search_type="mmr")matched_docs = retriever.get_relevant_documents(query)for i, d in enumerate(matched_docs):    print(f"\n## Document {i}\n")    print(d.page_content)Or use max_marginal_relevance_search directly:found_docs = docsearch.max_marginal_relevance_search(query, k=2, fetch_k=10)for i, doc in enumerate(found_docs):    print(f"{i + 1}.", doc.page_content, "\n")PreviousDashVectorNextDocArray HnswSearch IntegrationsVector storesDingoOn this pageDingoDingo is a distributed multi-mode vector database, which combines the characteristics of data lakes and vector databases, and can store data of any type and size (Key-Value, PDF, audio, video, etc.). It has real-time low-latency processing capabilities to achieve rapid insight and response, and can efficiently conduct instant analysis and process multi-modal data.This notebook shows how to use functionality related to the DingoDB vector database.To run, you should have a DingoDB instance up and running.pip install dingodbor install latest:pip install git+https://git@github.com/dingodb/pydingo.gitWe want to use OpenAIEmbeddings so we have to get the OpenAI API Key.import osimport getpassos.environ["OPENAI_API_KEY"] = getpass.getpass("OpenAI API Key:")    OpenAI API Key:¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑from langchain.embeddings.openai import OpenAIEmbeddingsfrom langchain.text_splitter import CharacterTextSplitterfrom langchain.vectorstores import Dingofrom langchain.document_loaders import TextLoaderAPI Reference:OpenAIEmbeddingsCharacterTextSplitterDingoTextLoaderfrom langchain.document_loaders import TextLoaderloader = TextLoader("../../../state_of_the_union.txt")documents = loader.load()text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)docs = text_splitter.split_documents(documents)embeddings = OpenAIEmbeddings()API Reference:TextLoaderfrom dingodb import DingoDBindex_name = "langchain-demo"dingo_client = DingoDB(user="", password="", host=["127.0.0.1:13000"])# First, check if our index already exists. If it doesn't, we create itif index_name not in dingo_client.get_index():    # we create a new index, modify to your own    dingo_client.create_index(      index_name=index_name,      dimension=1536,      metric_type='cosine',      auto_id=False)# The OpenAI embedding model `text-embedding-ada-002 uses 1536 dimensions`docsearch = Dingo.from_documents(docs, embeddings, client=dingo_client, index_name=index_name)from langchain.embeddings.openai import OpenAIEmbeddingsfrom langchain.text_splitter import CharacterTextSplitterfrom langchain.vectorstores import Dingofrom langchain.document_loaders import TextLoaderAPI Reference:OpenAIEmbeddingsCharacterTextSplitterDingoTextLoaderquery = "What did the president say about Ketanji Brown Jackson"docs = docsearch.similarity_search(query)print(docs[0].page_content)Adding More Text to an Existing Index‚ÄãMore text can embedded and upserted to an existing Dingo index using the add_texts functionvectorstore = Dingo(embeddings, "text", client=dingo_client, index_name=index_name)vectorstore.add_texts(["More text!"])Maximal Marginal Relevance Searches‚ÄãIn addition to using similarity search in the retriever object, you can also use mmr as retriever.retriever = docsearch.as_retriever(search_type="mmr")matched_docs = retriever.get_relevant_documents(query)for i, d in enumerate(matched_docs):    print(f"\n## Document {i}\n")    print(d.page_content)Or use max_marginal_relevance_search directly:found_docs = docsearch.max_marginal_relevance_search(query, k=2, fetch_k=10)for i, doc in enumerate(found_docs):    print(f"{i + 1}.", doc.page_content, "\n")PreviousDashVectorNextDocArray HnswSearch On this page DingoDingo is a distributed multi-mode vector database, which combines the characteristics of data lakes and vector databases, and can store data of any type and size (Key-Value, PDF, audio, video, etc.). It has real-time low-latency processing capabilities to achieve rapid insight and response, and can efficiently conduct instant analysis and process multi-modal data.This notebook shows how to use functionality related to the DingoDB vector database.To run, you should have a DingoDB instance up and running.pip install dingodbor install latest:pip install git+https://git@github.com/dingodb/pydingo.gitWe want to use OpenAIEmbeddings so we have to get the OpenAI API Key.import osimport getpassos.environ["OPENAI_API_KEY"] = getpass.getpass("OpenAI API Key:")    OpenAI API Key:¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑from langchain.embeddings.openai import OpenAIEmbeddingsfrom langchain.text_splitter import CharacterTextSplitterfrom langchain.vectorstores import Dingofrom langchain.document_loaders import TextLoaderAPI Reference:OpenAIEmbeddingsCharacterTextSplitterDingoTextLoaderfrom langchain.document_loaders import TextLoaderloader = TextLoader("../../../state_of_the_union.txt")documents = loader.load()text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)docs = text_splitter.split_documents(documents)embeddings = OpenAIEmbeddings()API Reference:TextLoaderfrom dingodb import DingoDBindex_name = "langchain-demo"dingo_client = DingoDB(user="", password="", host=["127.0.0.1:13000"])# First, check if our index already exists. If it doesn't, we create itif index_name not in dingo_client.get_index():    # we create a new index, modify to your own    dingo_client.create_index(      index_name=index_name,      dimension=1536,      metric_type='cosine',      auto_id=False)# The OpenAI embedding model `text-embedding-ada-002 uses 1536 dimensions`docsearch = Dingo.from_documents(docs, embeddings, client=dingo_client, index_name=index_name)from langchain.embeddings.openai import OpenAIEmbeddingsfrom langchain.text_splitter import CharacterTextSplitterfrom langchain.vectorstores import Dingofrom langchain.document_loaders import TextLoaderAPI Reference:OpenAIEmbeddingsCharacterTextSplitterDingoTextLoaderquery = "What did the president say about Ketanji Brown Jackson"docs = docsearch.similarity_search(query)print(docs[0].page_content)Adding More Text to an Existing Index‚ÄãMore text can embedded and upserted to an existing Dingo index using the add_texts functionvectorstore = Dingo(embeddings, "text", client=dingo_client, index_name=index_name)vectorstore.add_texts(["More text!"])Maximal Marginal Relevance Searches‚ÄãIn addition to using similarity search in the retriever object, you can also use mmr as retriever.retriever = docsearch.as_retriever(search_type="mmr")matched_docs = retriever.get_relevant_documents(query)for i, d in enumerate(matched_docs):    print(f"\n## Document {i}\n")    print(d.page_content)Or use max_marginal_relevance_search directly:found_docs = docsearch.max_marginal_relevance_search(query, k=2, fetch_k=10)for i, doc in enumerate(found_docs):    print(f"{i + 1}.", doc.page_content, "\n") pip install dingodbor install latest:pip install git+https://git@github.com/dingodb/pydingo.git pip install dingodbor install latest:pip install git+https://git@github.com/dingodb/pydingo.git  import osimport getpassos.environ["OPENAI_API_KEY"] = getpass.getpass("OpenAI API Key:") import osimport getpassos.environ["OPENAI_API_KEY"] = getpass.getpass("OpenAI API Key:")      OpenAI API Key:¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑     OpenAI API Key:¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑     OpenAI API Key:¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑  from langchain.embeddings.openai import OpenAIEmbeddingsfrom langchain.text_splitter import CharacterTextSplitterfrom langchain.vectorstores import Dingofrom langchain.document_loaders import TextLoader from langchain.embeddings.openai import OpenAIEmbeddingsfrom langchain.text_splitter import CharacterTextSplitterfrom langchain.vectorstores import Dingofrom langchain.document_loaders import TextLoader  API Reference:OpenAIEmbeddingsCharacterTextSplitterDingoTextLoader from langchain.document_loaders import TextLoaderloader = TextLoader("../../../state_of_the_union.txt")documents = loader.load()text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)docs = text_splitter.split_documents(documents)embeddings = OpenAIEmbeddings() from langchain.document_loaders import TextLoaderloader = TextLoader("../../../state_of_the_union.txt")documents = loader.load()text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)docs = text_splitter.split_documents(documents)embeddings = OpenAIEmbeddings()  API Reference:TextLoader from dingodb import DingoDBindex_name = "langchain-demo"dingo_client = DingoDB(user="", password="", host=["127.0.0.1:13000"])# First, check if our index already exists. If it doesn't, we create itif index_name not in dingo_client.get_index():    # we create a new index, modify to your own    dingo_client.create_index(      index_name=index_name,      dimension=1536,      metric_type='cosine',      auto_id=False)# The OpenAI embedding model `text-embedding-ada-002 uses 1536 dimensions`docsearch = Dingo.from_documents(docs, embeddings, client=dingo_client, index_name=index_name) from dingodb import DingoDBindex_name = "langchain-demo"dingo_client = DingoDB(user="", password="", host=["127.0.0.1:13000"])# First, check if our index already exists. If it doesn't, we create itif index_name not in dingo_client.get_index():    # we create a new index, modify to your own    dingo_client.create_index(      index_name=index_name,      dimension=1536,      metric_type='cosine',      auto_id=False)# The OpenAI embedding model `text-embedding-ada-002 uses 1536 dimensions`docsearch = Dingo.from_documents(docs, embeddings, client=dingo_client, index_name=index_name)  from langchain.embeddings.openai import OpenAIEmbeddingsfrom langchain.text_splitter import CharacterTextSplitterfrom langchain.vectorstores import Dingofrom langchain.document_loaders import TextLoader from langchain.embeddings.openai import OpenAIEmbeddingsfrom langchain.text_splitter import CharacterTextSplitterfrom langchain.vectorstores import Dingofrom langchain.document_loaders import TextLoader  API Reference:OpenAIEmbeddingsCharacterTextSplitterDingoTextLoader query = "What did the president say about Ketanji Brown Jackson"docs = docsearch.similarity_search(query) query = "What did the president say about Ketanji Brown Jackson"docs = docsearch.similarity_search(query)  print(docs[0].page_content) print(docs[0].page_content)  vectorstore = Dingo(embeddings, "text", client=dingo_client, index_name=index_name)vectorstore.add_texts(["More text!"]) vectorstore = Dingo(embeddings, "text", client=dingo_client, index_name=index_name)vectorstore.add_texts(["More text!"])  retriever = docsearch.as_retriever(search_type="mmr")matched_docs = retriever.get_relevant_documents(query)for i, d in enumerate(matched_docs):    print(f"\n## Document {i}\n")    print(d.page_content) retriever = docsearch.as_retriever(search_type="mmr")matched_docs = retriever.get_relevant_documents(query)for i, d in enumerate(matched_docs):    print(f"\n## Document {i}\n")    print(d.page_content)  found_docs = docsearch.max_marginal_relevance_search(query, k=2, fetch_k=10)for i, doc in enumerate(found_docs):    print(f"{i + 1}.", doc.page_content, "\n") found_docs = docsearch.max_marginal_relevance_search(query, k=2, fetch_k=10)for i, doc in enumerate(found_docs):    print(f"{i + 1}.", doc.page_content, "\n")  Previous DashVector Next DocArray HnswSearch Adding More Text to an Existing IndexMaximal Marginal Relevance Searches Adding More Text to an Existing IndexMaximal Marginal Relevance Searches CommunityDiscordTwitterGitHubPythonJS/TSMoreHomepageBlogCopyright ¬© 2023 LangChain, Inc. CommunityDiscordTwitterGitHubPythonJS/TSMoreHomepageBlog CommunityDiscordTwitter Community GitHubPythonJS/TS GitHub MoreHomepageBlog More Copyright ¬© 2023 LangChain, Inc. Copyright ¬© 2023 LangChain, Inc. Skip to main content (#docusaurus_skipToContent_fallback) ü¶úÔ∏èüîó LangChain (/) Docs (/docs/get_started/introduction) Use cases (/docs/use_cases) Integrations (/docs/integrations) API (https://api.python.langchain.com) LangSmith (https://smith.langchain.com) JS/TS Docs (https://js.langchain.com/docs)  (https://github.com/hwchase17/langchain) Integrations (/docs/integrations) Callbacks (/docs/integrations/callbacks/) Chat models (/docs/integrations/chat/) Chat loaders (/docs/integrations/chat_loaders/) Document loaders (/docs/integrations/document_loaders/) Document transformers (/docs/integrations/document_transformers/) LLMs (/docs/integrations/llms/) Memory (/docs/integrations/memory/) Retrievers (/docs/integrations/retrievers/) Text embedding models (/docs/integrations/text_embedding/) Agents & Toolkits (/docs/integrations/toolkits/) Tools (/docs/integrations/tools/) Vector stores (/docs/integrations/vectorstores/) Activeloop Deep Lake (/docs/integrations/vectorstores/activeloop_deeplake) Alibaba Cloud OpenSearch (/docs/integrations/vectorstores/alibabacloud_opensearch) AnalyticDB (/docs/integrations/vectorstores/analyticdb) Annoy (/docs/integrations/vectorstores/annoy) Atlas (/docs/integrations/vectorstores/atlas) AwaDB (/docs/integrations/vectorstores/awadb) Azure Cognitive Search (/docs/integrations/vectorstores/azuresearch) BagelDB (/docs/integrations/vectorstores/bageldb) Cassandra (/docs/integrations/vectorstores/cassandra) Chroma (/docs/integrations/vectorstores/chroma) ClickHouse (/docs/integrations/vectorstores/clickhouse) DashVector (/docs/integrations/vectorstores/dashvector) Dingo (/docs/integrations/vectorstores/dingo) DocArray HnswSearch (/docs/integrations/vectorstores/docarray_hnsw) DocArray InMemorySearch (/docs/integrations/vectorstores/docarray_in_memory) Elasticsearch (/docs/integrations/vectorstores/elasticsearch) Epsilla (/docs/integrations/vectorstores/epsilla) Faiss (/docs/integrations/vectorstores/faiss) Hologres (/docs/integrations/vectorstores/hologres) LanceDB (/docs/integrations/vectorstores/lancedb) Marqo (/docs/integrations/vectorstores/marqo) Google Vertex AI MatchingEngine (/docs/integrations/vectorstores/matchingengine) Meilisearch (/docs/integrations/vectorstores/meilisearch) Milvus (/docs/integrations/vectorstores/milvus) MongoDB Atlas (/docs/integrations/vectorstores/mongodb_atlas) MyScale (/docs/integrations/vectorstores/myscale) Neo4j Vector Index (/docs/integrations/vectorstores/neo4jvector) NucliaDB (/docs/integrations/vectorstores/nucliadb) OpenSearch (/docs/integrations/vectorstores/opensearch) Postgres Embedding (/docs/integrations/vectorstores/pgembedding) PGVector (/docs/integrations/vectorstores/pgvector) Pinecone (/docs/integrations/vectorstores/pinecone) Qdrant (/docs/integrations/vectorstores/qdrant) Redis (/docs/integrations/vectorstores/redis) Rockset (/docs/integrations/vectorstores/rockset) ScaNN (/docs/integrations/vectorstores/scann) SingleStoreDB (/docs/integrations/vectorstores/singlestoredb) scikit-learn (/docs/integrations/vectorstores/sklearn) sqlite-vss (/docs/integrations/vectorstores/sqlitevss) StarRocks (/docs/integrations/vectorstores/starrocks) Supabase (Postgres) (/docs/integrations/vectorstores/supabase) Tair (/docs/integrations/vectorstores/tair) Tencent Cloud VectorDB (/docs/integrations/vectorstores/tencentvectordb) Tigris (/docs/integrations/vectorstores/tigris) Typesense (/docs/integrations/vectorstores/typesense) USearch (/docs/integrations/vectorstores/usearch) vearch (/docs/integrations/vectorstores/vearch) Vectara (/docs/integrations/vectorstores/vectara) Weaviate (/docs/integrations/vectorstores/weaviate) Xata (/docs/integrations/vectorstores/xata) Zep (/docs/integrations/vectorstores/zep) Zilliz (/docs/integrations/vectorstores/zilliz) Grouped by provider (/docs/integrations/providers/)  (/) Integrations (/docs/integrations) Vector stores (/docs/integrations/vectorstores/) Dingo (https://dingodb.readthedocs.io/en/latest/) DingoDB instance up and running (https://github.com/dingodb/dingo-deploy/blob/main/README.md) OpenAIEmbeddings (https://api.python.langchain.com/en/latest/embeddings/langchain.embeddings.openai.OpenAIEmbeddings.html) CharacterTextSplitter (https://api.python.langchain.com/en/latest/text_splitter/langchain.text_splitter.CharacterTextSplitter.html) Dingo (https://api.python.langchain.com/en/latest/vectorstores/langchain.vectorstores.dingo.Dingo.html) TextLoader (https://api.python.langchain.com/en/latest/document_loaders/langchain.document_loaders.text.TextLoader.html) TextLoader (https://api.python.langchain.com/en/latest/document_loaders/langchain.document_loaders.text.TextLoader.html) OpenAIEmbeddings (https://api.python.langchain.com/en/latest/embeddings/langchain.embeddings.openai.OpenAIEmbeddings.html) CharacterTextSplitter (https://api.python.langchain.com/en/latest/text_splitter/langchain.text_splitter.CharacterTextSplitter.html) Dingo (https://api.python.langchain.com/en/latest/vectorstores/langchain.vectorstores.dingo.Dingo.html) TextLoader (https://api.python.langchain.com/en/latest/document_loaders/langchain.document_loaders.text.TextLoader.html) ‚Äã (#adding-more-text-to-an-existing-index) ‚Äã (#maximal-marginal-relevance-searches) PreviousDashVector (/docs/integrations/vectorstores/dashvector) NextDocArray HnswSearch (/docs/integrations/vectorstores/docarray_hnsw) Adding More Text to an Existing Index (#adding-more-text-to-an-existing-index) Maximal Marginal Relevance Searches (#maximal-marginal-relevance-searches) Discord (https://discord.gg/cU2adEyC7w) Twitter (https://twitter.com/LangChainAI) Python (https://github.com/hwchase17/langchain) JS/TS (https://github.com/hwchase17/langchainjs) Homepage (https://langchain.com) Blog (https://blog.langchain.dev)