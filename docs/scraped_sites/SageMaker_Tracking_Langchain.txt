This notebook shows how LangChain Callback can be used to log and track prompts and other LLM hyperparameters into SageMaker Experiments. Here, we use different scenarios to showcase the capability: Amazon SageMaker is a fully managed service that is used to quickly and easily build, train and deploy machine learning (ML) models.  Amazon SageMaker Experiments is a capability of Amazon SageMaker that lets you organize, track, compare and evaluate ML experiments and model versions. In this notebook, we will create a single experiment to log the prompts from each scenario. First, setup the required API keys Once the prompts are logged, we can easily load and convert them to Pandas DataFrame as follows. As can be seen above, there are three runs (rows) in the experiment corresponding to each scenario. Each run logs the prompts and related LLM settings/hyperparameters as json and are saved in s3 bucket. Feel free to load and explore the log data from each json path. IntegrationsCallbacksChat modelsChat loadersDocument loadersDocument transformersLLMsMemoryRetrieversText embedding modelsAgents & ToolkitsToolsVector storesGrouped by providerActiveloop Deep LakeAI21 LabsAimAINetworkAirbyteAirtableAleph AlphaAlibaba Cloud OpensearchAmazon API GatewayAnalyticDBAnnoyAnyscaleApifyArangoDBArgillaArthurArxivAtlasAwaDBAWS S3 DirectoryAZLyricsAzure Blob StorageAzure Cognitive SearchAzure OpenAIBagelDBBananaBasetenBeamBedrockBiliBiliNIBittensorBlackboardBrave SearchCassandraCerebriumAIChaindeskChromaClarifaiClearMLClickHouseCnosDBCohereCollege ConfidentialCometConfident AIConfluenceC TransformersDashVectorDatabricksDatadog TracingDatadog LogsDataForSEODeepInfraDeepSparseDiffbotDingoDiscordDocArrayDocugamiDuckDBElasticsearchEpsillaEverNoteFacebook ChatFacebook FaissFigmaFireworksFlyteForefrontAIGitGitBookGoldenGoogle BigQueryGoogle Cloud StorageGoogle DriveGoogle SearchGoogle SerperGoogle Vertex AI MatchingEngineGooseAIGPT4AllGraphsignalGrobidGutenbergHacker NewsHazy ResearchHeliconeHologresHugging FaceiFixitIMSDbInfinoJinaKonkoLanceDBLangChain Decorators ‚ú®Llama.cppLog10MarqoMediaWikiDumpMeilisearchMetalMicrosoft OneDriveMicrosoft PowerPointMicrosoft WordMilvusMinimaxMLflow AI GatewayMLflowModalModelScopeModern TreasuryMomentoMongoDB AtlasMotherduckMyScaleNeo4jNLPCloudNotion DBObsidianOpenAIOpenLLMOpenSearchOpenWeatherMapPetalsPostgres EmbeddingPGVectorPineconePipelineAIPortkeyPredibasePrediction GuardPromptLayerPsychicPubMedQdrantRay ServeRebuffRedditRedisReplicateRoamRocksetRunhouseRWKV-4SageMaker EndpointSageMaker TrackingScaNNSearxNG Search APISerpAPIShale ProtocolSingleStoreDBscikit-learnSlackspaCySpreedlyStarRocksStochasticAIStripeSupabase (Postgres)NebulaTairTelegramTencentVectorDBTensorFlow DatasetsTigris2MarkdownTrelloTruLensTwitterTypesenseUnstructuredUSearchVearchVectaraVespaWandB TracingWeights & BiasesWeatherWeaviateWhatsAppWhyLabsWikipediaWolfram AlphaWriterXataXorbits Inference (Xinference)Yeager.aiYouTubeZepZilliz Callbacks Chat models Chat loaders Document loaders Document transformers LLMs Memory Retrievers Text embedding models Agents & Toolkits Tools Vector stores Grouped by providerActiveloop Deep LakeAI21 LabsAimAINetworkAirbyteAirtableAleph AlphaAlibaba Cloud OpensearchAmazon API GatewayAnalyticDBAnnoyAnyscaleApifyArangoDBArgillaArthurArxivAtlasAwaDBAWS S3 DirectoryAZLyricsAzure Blob StorageAzure Cognitive SearchAzure OpenAIBagelDBBananaBasetenBeamBedrockBiliBiliNIBittensorBlackboardBrave SearchCassandraCerebriumAIChaindeskChromaClarifaiClearMLClickHouseCnosDBCohereCollege ConfidentialCometConfident AIConfluenceC TransformersDashVectorDatabricksDatadog TracingDatadog LogsDataForSEODeepInfraDeepSparseDiffbotDingoDiscordDocArrayDocugamiDuckDBElasticsearchEpsillaEverNoteFacebook ChatFacebook FaissFigmaFireworksFlyteForefrontAIGitGitBookGoldenGoogle BigQueryGoogle Cloud StorageGoogle DriveGoogle SearchGoogle SerperGoogle Vertex AI MatchingEngineGooseAIGPT4AllGraphsignalGrobidGutenbergHacker NewsHazy ResearchHeliconeHologresHugging FaceiFixitIMSDbInfinoJinaKonkoLanceDBLangChain Decorators ‚ú®Llama.cppLog10MarqoMediaWikiDumpMeilisearchMetalMicrosoft OneDriveMicrosoft PowerPointMicrosoft WordMilvusMinimaxMLflow AI GatewayMLflowModalModelScopeModern TreasuryMomentoMongoDB AtlasMotherduckMyScaleNeo4jNLPCloudNotion DBObsidianOpenAIOpenLLMOpenSearchOpenWeatherMapPetalsPostgres EmbeddingPGVectorPineconePipelineAIPortkeyPredibasePrediction GuardPromptLayerPsychicPubMedQdrantRay ServeRebuffRedditRedisReplicateRoamRocksetRunhouseRWKV-4SageMaker EndpointSageMaker TrackingScaNNSearxNG Search APISerpAPIShale ProtocolSingleStoreDBscikit-learnSlackspaCySpreedlyStarRocksStochasticAIStripeSupabase (Postgres)NebulaTairTelegramTencentVectorDBTensorFlow DatasetsTigris2MarkdownTrelloTruLensTwitterTypesenseUnstructuredUSearchVearchVectaraVespaWandB TracingWeights & BiasesWeatherWeaviateWhatsAppWhyLabsWikipediaWolfram AlphaWriterXataXorbits Inference (Xinference)Yeager.aiYouTubeZepZilliz Activeloop Deep Lake AI21 Labs Aim AINetwork Airbyte Airtable Aleph Alpha Alibaba Cloud Opensearch Amazon API Gateway AnalyticDB Annoy Anyscale Apify ArangoDB Argilla Arthur Arxiv Atlas AwaDB AWS S3 Directory AZLyrics Azure Blob Storage Azure Cognitive Search Azure OpenAI BagelDB Banana Baseten Beam Bedrock BiliBili NIBittensor Blackboard Brave Search Cassandra CerebriumAI Chaindesk Chroma Clarifai ClearML ClickHouse CnosDB Cohere College Confidential Comet Confident AI Confluence C Transformers DashVector Databricks Datadog Tracing Datadog Logs DataForSEO DeepInfra DeepSparse Diffbot Dingo Discord DocArray Docugami DuckDB Elasticsearch Epsilla EverNote Facebook Chat Facebook Faiss Figma Fireworks Flyte ForefrontAI Git GitBook Golden Google BigQuery Google Cloud Storage Google Drive Google Search Google Serper Google Vertex AI MatchingEngine GooseAI GPT4All Graphsignal Grobid Gutenberg Hacker News Hazy Research Helicone Hologres Hugging Face iFixit IMSDb Infino Jina Konko LanceDB LangChain Decorators ‚ú® Llama.cpp Log10 Marqo MediaWikiDump Meilisearch Metal Microsoft OneDrive Microsoft PowerPoint Microsoft Word Milvus Minimax MLflow AI Gateway MLflow Modal ModelScope Modern Treasury Momento MongoDB Atlas Motherduck MyScale Neo4j NLPCloud Notion DB Obsidian OpenAI OpenLLM OpenSearch OpenWeatherMap Petals Postgres Embedding PGVector Pinecone PipelineAI Portkey Predibase Prediction Guard PromptLayer Psychic PubMed Qdrant Ray Serve Rebuff Reddit Redis Replicate Roam Rockset Runhouse RWKV-4 SageMaker Endpoint SageMaker Tracking ScaNN SearxNG Search API SerpAPI Shale Protocol SingleStoreDB scikit-learn Slack spaCy Spreedly StarRocks StochasticAI Stripe Supabase (Postgres) Nebula Tair Telegram TencentVectorDB TensorFlow Datasets Tigris 2Markdown Trello TruLens Twitter Typesense Unstructured USearch Vearch Vectara Vespa WandB Tracing Weights & Biases Weather Weaviate WhatsApp WhyLabs Wikipedia Wolfram Alpha Writer Xata Xorbits Inference (Xinference) Yeager.ai YouTube Zep Zilliz  Integrations Grouped by provider SageMaker Tracking Scenario 1: Single LLM - A case where a single LLM model is used to generate output based on a given prompt. Scenario 2: Sequential Chain - A case where a sequential chain of two LLM models is used. Scenario 3: Agent with Tools (Chain of Thought) - A case where multiple tools (search and math) are used in addition to an LLM. OpenAI: https://platform.openai.com/account/api-keys (For OpenAI LLM model) Google SERP API: https://serpapi.com/manage-api-key (For Google Search Tool) OpenAI PromptTemplate LLMChain SimpleSequentialChain initialize_agent load_tools Tool SageMakerCallbackHandler Installation and Setup LLM Prompt TrackingScenario 1 - LLMScenario 2 - Sequential ChainScenario 3 - Agent with Tools Scenario 1 - LLM Scenario 2 - Sequential Chain Scenario 3 - Agent with Tools Load Log Data Discord Twitter Python JS/TS Homepage Blog Skip to main contentü¶úÔ∏èüîó LangChainDocsUse casesIntegrationsAPILangSmithJS/TS DocsCTRLKIntegrationsCallbacksChat modelsChat loadersDocument loadersDocument transformersLLMsMemoryRetrieversText embedding modelsAgents & ToolkitsToolsVector storesGrouped by providerActiveloop Deep LakeAI21 LabsAimAINetworkAirbyteAirtableAleph AlphaAlibaba Cloud OpensearchAmazon API GatewayAnalyticDBAnnoyAnyscaleApifyArangoDBArgillaArthurArxivAtlasAwaDBAWS S3 DirectoryAZLyricsAzure Blob StorageAzure Cognitive SearchAzure OpenAIBagelDBBananaBasetenBeamBedrockBiliBiliNIBittensorBlackboardBrave SearchCassandraCerebriumAIChaindeskChromaClarifaiClearMLClickHouseCnosDBCohereCollege ConfidentialCometConfident AIConfluenceC TransformersDashVectorDatabricksDatadog TracingDatadog LogsDataForSEODeepInfraDeepSparseDiffbotDingoDiscordDocArrayDocugamiDuckDBElasticsearchEpsillaEverNoteFacebook ChatFacebook FaissFigmaFireworksFlyteForefrontAIGitGitBookGoldenGoogle BigQueryGoogle Cloud StorageGoogle DriveGoogle SearchGoogle SerperGoogle Vertex AI MatchingEngineGooseAIGPT4AllGraphsignalGrobidGutenbergHacker NewsHazy ResearchHeliconeHologresHugging FaceiFixitIMSDbInfinoJinaKonkoLanceDBLangChain Decorators ‚ú®Llama.cppLog10MarqoMediaWikiDumpMeilisearchMetalMicrosoft OneDriveMicrosoft PowerPointMicrosoft WordMilvusMinimaxMLflow AI GatewayMLflowModalModelScopeModern TreasuryMomentoMongoDB AtlasMotherduckMyScaleNeo4jNLPCloudNotion DBObsidianOpenAIOpenLLMOpenSearchOpenWeatherMapPetalsPostgres EmbeddingPGVectorPineconePipelineAIPortkeyPredibasePrediction GuardPromptLayerPsychicPubMedQdrantRay ServeRebuffRedditRedisReplicateRoamRocksetRunhouseRWKV-4SageMaker EndpointSageMaker TrackingScaNNSearxNG Search APISerpAPIShale ProtocolSingleStoreDBscikit-learnSlackspaCySpreedlyStarRocksStochasticAIStripeSupabase (Postgres)NebulaTairTelegramTencentVectorDBTensorFlow DatasetsTigris2MarkdownTrelloTruLensTwitterTypesenseUnstructuredUSearchVearchVectaraVespaWandB TracingWeights & BiasesWeatherWeaviateWhatsAppWhyLabsWikipediaWolfram AlphaWriterXataXorbits Inference (Xinference)Yeager.aiYouTubeZepZillizIntegrationsGrouped by providerSageMaker TrackingOn this pageSageMaker TrackingThis notebook shows how LangChain Callback can be used to log and track prompts and other LLM hyperparameters into SageMaker Experiments. Here, we use different scenarios to showcase the capability:Scenario 1: Single LLM - A case where a single LLM model is used to generate output based on a given prompt.Scenario 2: Sequential Chain - A case where a sequential chain of two LLM models is used.Scenario 3: Agent with Tools (Chain of Thought) - A case where multiple tools (search and math) are used in addition to an LLM.Amazon SageMaker is a fully managed service that is used to quickly and easily build, train and deploy machine learning (ML) models. Amazon SageMaker Experiments is a capability of Amazon SageMaker that lets you organize, track, compare and evaluate ML experiments and model versions.In this notebook, we will create a single experiment to log the prompts from each scenario.Installation and Setup‚Äãpip install sagemakerpip install openaipip install google-search-resultsFirst, setup the required API keysOpenAI: https://platform.openai.com/account/api-keys (For OpenAI LLM model)Google SERP API: https://serpapi.com/manage-api-key (For Google Search Tool)import os## Add your API keys belowos.environ["OPENAI_API_KEY"] = "<ADD-KEY-HERE>"os.environ["SERPAPI_API_KEY"] = "<ADD-KEY-HERE>"from langchain.llms import OpenAIfrom langchain.prompts import PromptTemplatefrom langchain.chains import LLMChain, SimpleSequentialChainfrom langchain.agents import initialize_agent, load_toolsfrom langchain.agents import Toolfrom langchain.callbacks import SageMakerCallbackHandlerfrom sagemaker.analytics import ExperimentAnalyticsfrom sagemaker.session import Sessionfrom sagemaker.experiments.run import RunAPI Reference:OpenAIPromptTemplateLLMChainSimpleSequentialChaininitialize_agentload_toolsToolSageMakerCallbackHandlerLLM Prompt Tracking‚Äã#LLM HyperparametersHPARAMS = {    "temperature": 0.1,    "model_name": "text-davinci-003",}#Bucket used to save prompt logs (Use `None` is used to save the default bucket or otherwise change it)BUCKET_NAME = None#Experiment nameEXPERIMENT_NAME = "langchain-sagemaker-tracker"#Create SageMaker Session with the given bucketsession = Session(default_bucket=BUCKET_NAME)Scenario 1 - LLM‚ÄãRUN_NAME = "run-scenario-1"PROMPT_TEMPLATE = "tell me a joke about {topic}"INPUT_VARIABLES = {"topic": "fish"}with Run(experiment_name=EXPERIMENT_NAME, run_name=RUN_NAME, sagemaker_session=session) as run:    # Create SageMaker Callback    sagemaker_callback = SageMakerCallbackHandler(run)    # Define LLM model with callback    llm = OpenAI(callbacks=[sagemaker_callback], **HPARAMS)    # Create prompt template    prompt = PromptTemplate.from_template(template=PROMPT_TEMPLATE)    # Create LLM Chain    chain = LLMChain(llm=llm, prompt=prompt, callbacks=[sagemaker_callback])    # Run chain    chain.run(**INPUT_VARIABLES)    # Reset the callback    sagemaker_callback.flush_tracker()Scenario 2 - Sequential Chain‚ÄãRUN_NAME = "run-scenario-2"PROMPT_TEMPLATE_1 = """You are a playwright. Given the title of play, it is your job to write a synopsis for that title.Title: {title}Playwright: This is a synopsis for the above play:"""PROMPT_TEMPLATE_2 = """You are a play critic from the New York Times. Given the synopsis of play, it is your job to write a review for that play.Play Synopsis: {synopsis}Review from a New York Times play critic of the above play:"""INPUT_VARIABLES = {    "input": "documentary about good video games that push the boundary of game design"}with Run(experiment_name=EXPERIMENT_NAME, run_name=RUN_NAME, sagemaker_session=session) as run:    # Create SageMaker Callback    sagemaker_callback = SageMakerCallbackHandler(run)    # Create prompt templates for the chain    prompt_template1 = PromptTemplate.from_template(template=PROMPT_TEMPLATE_1)    prompt_template2 = PromptTemplate.from_template(template=PROMPT_TEMPLATE_2)    # Define LLM model with callback    llm = OpenAI(callbacks=[sagemaker_callback], **HPARAMS)    # Create chain1    chain1 = LLMChain(llm=llm, prompt=prompt_template1, callbacks=[sagemaker_callback])    # Create chain2    chain2 = LLMChain(llm=llm, prompt=prompt_template2, callbacks=[sagemaker_callback])    # Create Sequential chain    overall_chain = SimpleSequentialChain(chains=[chain1, chain2], callbacks=[sagemaker_callback])    # Run overall sequential chain    overall_chain.run(**INPUT_VARIABLES)    # Reset the callback    sagemaker_callback.flush_tracker()Scenario 3 - Agent with Tools‚ÄãRUN_NAME = "run-scenario-3"PROMPT_TEMPLATE = "Who is the oldest person alive? And what is their current age raised to the power of 1.51?"with Run(experiment_name=EXPERIMENT_NAME, run_name=RUN_NAME, sagemaker_session=session) as run:    # Create SageMaker Callback    sagemaker_callback = SageMakerCallbackHandler(run)    # Define LLM model with callback    llm = OpenAI(callbacks=[sagemaker_callback], **HPARAMS)    # Define tools    tools = load_tools(["serpapi", "llm-math"], llm=llm, callbacks=[sagemaker_callback])    # Initialize agent with all the tools    agent = initialize_agent(tools, llm, agent="zero-shot-react-description", callbacks=[sagemaker_callback])    # Run agent    agent.run(input=PROMPT_TEMPLATE)    # Reset the callback    sagemaker_callback.flush_tracker()Load Log Data‚ÄãOnce the prompts are logged, we can easily load and convert them to Pandas DataFrame as follows.#Loadlogs = ExperimentAnalytics(experiment_name=EXPERIMENT_NAME)#Convert as pandas dataframedf = logs.dataframe(force_refresh=True)print(df.shape)df.head()As can be seen above, there are three runs (rows) in the experiment corresponding to each scenario. Each run logs the prompts and related LLM settings/hyperparameters as json and are saved in s3 bucket. Feel free to load and explore the log data from each json path.PreviousSageMaker EndpointNextScaNNInstallation and SetupLLM Prompt TrackingScenario 1 - LLMScenario 2 - Sequential ChainScenario 3 - Agent with ToolsLoad Log DataCommunityDiscordTwitterGitHubPythonJS/TSMoreHomepageBlogCopyright ¬© 2023 LangChain, Inc. Skip to main content ü¶úÔ∏èüîó LangChainDocsUse casesIntegrationsAPILangSmithJS/TS DocsCTRLK ü¶úÔ∏èüîó LangChainDocsUse casesIntegrationsAPI LangSmithJS/TS DocsCTRLK  CTRLK CTRLK  CTRLK   IntegrationsCallbacksChat modelsChat loadersDocument loadersDocument transformersLLMsMemoryRetrieversText embedding modelsAgents & ToolkitsToolsVector storesGrouped by providerActiveloop Deep LakeAI21 LabsAimAINetworkAirbyteAirtableAleph AlphaAlibaba Cloud OpensearchAmazon API GatewayAnalyticDBAnnoyAnyscaleApifyArangoDBArgillaArthurArxivAtlasAwaDBAWS S3 DirectoryAZLyricsAzure Blob StorageAzure Cognitive SearchAzure OpenAIBagelDBBananaBasetenBeamBedrockBiliBiliNIBittensorBlackboardBrave SearchCassandraCerebriumAIChaindeskChromaClarifaiClearMLClickHouseCnosDBCohereCollege ConfidentialCometConfident AIConfluenceC TransformersDashVectorDatabricksDatadog TracingDatadog LogsDataForSEODeepInfraDeepSparseDiffbotDingoDiscordDocArrayDocugamiDuckDBElasticsearchEpsillaEverNoteFacebook ChatFacebook FaissFigmaFireworksFlyteForefrontAIGitGitBookGoldenGoogle BigQueryGoogle Cloud StorageGoogle DriveGoogle SearchGoogle SerperGoogle Vertex AI MatchingEngineGooseAIGPT4AllGraphsignalGrobidGutenbergHacker NewsHazy ResearchHeliconeHologresHugging FaceiFixitIMSDbInfinoJinaKonkoLanceDBLangChain Decorators ‚ú®Llama.cppLog10MarqoMediaWikiDumpMeilisearchMetalMicrosoft OneDriveMicrosoft PowerPointMicrosoft WordMilvusMinimaxMLflow AI GatewayMLflowModalModelScopeModern TreasuryMomentoMongoDB AtlasMotherduckMyScaleNeo4jNLPCloudNotion DBObsidianOpenAIOpenLLMOpenSearchOpenWeatherMapPetalsPostgres EmbeddingPGVectorPineconePipelineAIPortkeyPredibasePrediction GuardPromptLayerPsychicPubMedQdrantRay ServeRebuffRedditRedisReplicateRoamRocksetRunhouseRWKV-4SageMaker EndpointSageMaker TrackingScaNNSearxNG Search APISerpAPIShale ProtocolSingleStoreDBscikit-learnSlackspaCySpreedlyStarRocksStochasticAIStripeSupabase (Postgres)NebulaTairTelegramTencentVectorDBTensorFlow DatasetsTigris2MarkdownTrelloTruLensTwitterTypesenseUnstructuredUSearchVearchVectaraVespaWandB TracingWeights & BiasesWeatherWeaviateWhatsAppWhyLabsWikipediaWolfram AlphaWriterXataXorbits Inference (Xinference)Yeager.aiYouTubeZepZillizIntegrationsGrouped by providerSageMaker TrackingOn this pageSageMaker TrackingThis notebook shows how LangChain Callback can be used to log and track prompts and other LLM hyperparameters into SageMaker Experiments. Here, we use different scenarios to showcase the capability:Scenario 1: Single LLM - A case where a single LLM model is used to generate output based on a given prompt.Scenario 2: Sequential Chain - A case where a sequential chain of two LLM models is used.Scenario 3: Agent with Tools (Chain of Thought) - A case where multiple tools (search and math) are used in addition to an LLM.Amazon SageMaker is a fully managed service that is used to quickly and easily build, train and deploy machine learning (ML) models. Amazon SageMaker Experiments is a capability of Amazon SageMaker that lets you organize, track, compare and evaluate ML experiments and model versions.In this notebook, we will create a single experiment to log the prompts from each scenario.Installation and Setup‚Äãpip install sagemakerpip install openaipip install google-search-resultsFirst, setup the required API keysOpenAI: https://platform.openai.com/account/api-keys (For OpenAI LLM model)Google SERP API: https://serpapi.com/manage-api-key (For Google Search Tool)import os## Add your API keys belowos.environ["OPENAI_API_KEY"] = "<ADD-KEY-HERE>"os.environ["SERPAPI_API_KEY"] = "<ADD-KEY-HERE>"from langchain.llms import OpenAIfrom langchain.prompts import PromptTemplatefrom langchain.chains import LLMChain, SimpleSequentialChainfrom langchain.agents import initialize_agent, load_toolsfrom langchain.agents import Toolfrom langchain.callbacks import SageMakerCallbackHandlerfrom sagemaker.analytics import ExperimentAnalyticsfrom sagemaker.session import Sessionfrom sagemaker.experiments.run import RunAPI Reference:OpenAIPromptTemplateLLMChainSimpleSequentialChaininitialize_agentload_toolsToolSageMakerCallbackHandlerLLM Prompt Tracking‚Äã#LLM HyperparametersHPARAMS = {    "temperature": 0.1,    "model_name": "text-davinci-003",}#Bucket used to save prompt logs (Use `None` is used to save the default bucket or otherwise change it)BUCKET_NAME = None#Experiment nameEXPERIMENT_NAME = "langchain-sagemaker-tracker"#Create SageMaker Session with the given bucketsession = Session(default_bucket=BUCKET_NAME)Scenario 1 - LLM‚ÄãRUN_NAME = "run-scenario-1"PROMPT_TEMPLATE = "tell me a joke about {topic}"INPUT_VARIABLES = {"topic": "fish"}with Run(experiment_name=EXPERIMENT_NAME, run_name=RUN_NAME, sagemaker_session=session) as run:    # Create SageMaker Callback    sagemaker_callback = SageMakerCallbackHandler(run)    # Define LLM model with callback    llm = OpenAI(callbacks=[sagemaker_callback], **HPARAMS)    # Create prompt template    prompt = PromptTemplate.from_template(template=PROMPT_TEMPLATE)    # Create LLM Chain    chain = LLMChain(llm=llm, prompt=prompt, callbacks=[sagemaker_callback])    # Run chain    chain.run(**INPUT_VARIABLES)    # Reset the callback    sagemaker_callback.flush_tracker()Scenario 2 - Sequential Chain‚ÄãRUN_NAME = "run-scenario-2"PROMPT_TEMPLATE_1 = """You are a playwright. Given the title of play, it is your job to write a synopsis for that title.Title: {title}Playwright: This is a synopsis for the above play:"""PROMPT_TEMPLATE_2 = """You are a play critic from the New York Times. Given the synopsis of play, it is your job to write a review for that play.Play Synopsis: {synopsis}Review from a New York Times play critic of the above play:"""INPUT_VARIABLES = {    "input": "documentary about good video games that push the boundary of game design"}with Run(experiment_name=EXPERIMENT_NAME, run_name=RUN_NAME, sagemaker_session=session) as run:    # Create SageMaker Callback    sagemaker_callback = SageMakerCallbackHandler(run)    # Create prompt templates for the chain    prompt_template1 = PromptTemplate.from_template(template=PROMPT_TEMPLATE_1)    prompt_template2 = PromptTemplate.from_template(template=PROMPT_TEMPLATE_2)    # Define LLM model with callback    llm = OpenAI(callbacks=[sagemaker_callback], **HPARAMS)    # Create chain1    chain1 = LLMChain(llm=llm, prompt=prompt_template1, callbacks=[sagemaker_callback])    # Create chain2    chain2 = LLMChain(llm=llm, prompt=prompt_template2, callbacks=[sagemaker_callback])    # Create Sequential chain    overall_chain = SimpleSequentialChain(chains=[chain1, chain2], callbacks=[sagemaker_callback])    # Run overall sequential chain    overall_chain.run(**INPUT_VARIABLES)    # Reset the callback    sagemaker_callback.flush_tracker()Scenario 3 - Agent with Tools‚ÄãRUN_NAME = "run-scenario-3"PROMPT_TEMPLATE = "Who is the oldest person alive? And what is their current age raised to the power of 1.51?"with Run(experiment_name=EXPERIMENT_NAME, run_name=RUN_NAME, sagemaker_session=session) as run:    # Create SageMaker Callback    sagemaker_callback = SageMakerCallbackHandler(run)    # Define LLM model with callback    llm = OpenAI(callbacks=[sagemaker_callback], **HPARAMS)    # Define tools    tools = load_tools(["serpapi", "llm-math"], llm=llm, callbacks=[sagemaker_callback])    # Initialize agent with all the tools    agent = initialize_agent(tools, llm, agent="zero-shot-react-description", callbacks=[sagemaker_callback])    # Run agent    agent.run(input=PROMPT_TEMPLATE)    # Reset the callback    sagemaker_callback.flush_tracker()Load Log Data‚ÄãOnce the prompts are logged, we can easily load and convert them to Pandas DataFrame as follows.#Loadlogs = ExperimentAnalytics(experiment_name=EXPERIMENT_NAME)#Convert as pandas dataframedf = logs.dataframe(force_refresh=True)print(df.shape)df.head()As can be seen above, there are three runs (rows) in the experiment corresponding to each scenario. Each run logs the prompts and related LLM settings/hyperparameters as json and are saved in s3 bucket. Feel free to load and explore the log data from each json path.PreviousSageMaker EndpointNextScaNNInstallation and SetupLLM Prompt TrackingScenario 1 - LLMScenario 2 - Sequential ChainScenario 3 - Agent with ToolsLoad Log Data IntegrationsCallbacksChat modelsChat loadersDocument loadersDocument transformersLLMsMemoryRetrieversText embedding modelsAgents & ToolkitsToolsVector storesGrouped by providerActiveloop Deep LakeAI21 LabsAimAINetworkAirbyteAirtableAleph AlphaAlibaba Cloud OpensearchAmazon API GatewayAnalyticDBAnnoyAnyscaleApifyArangoDBArgillaArthurArxivAtlasAwaDBAWS S3 DirectoryAZLyricsAzure Blob StorageAzure Cognitive SearchAzure OpenAIBagelDBBananaBasetenBeamBedrockBiliBiliNIBittensorBlackboardBrave SearchCassandraCerebriumAIChaindeskChromaClarifaiClearMLClickHouseCnosDBCohereCollege ConfidentialCometConfident AIConfluenceC TransformersDashVectorDatabricksDatadog TracingDatadog LogsDataForSEODeepInfraDeepSparseDiffbotDingoDiscordDocArrayDocugamiDuckDBElasticsearchEpsillaEverNoteFacebook ChatFacebook FaissFigmaFireworksFlyteForefrontAIGitGitBookGoldenGoogle BigQueryGoogle Cloud StorageGoogle DriveGoogle SearchGoogle SerperGoogle Vertex AI MatchingEngineGooseAIGPT4AllGraphsignalGrobidGutenbergHacker NewsHazy ResearchHeliconeHologresHugging FaceiFixitIMSDbInfinoJinaKonkoLanceDBLangChain Decorators ‚ú®Llama.cppLog10MarqoMediaWikiDumpMeilisearchMetalMicrosoft OneDriveMicrosoft PowerPointMicrosoft WordMilvusMinimaxMLflow AI GatewayMLflowModalModelScopeModern TreasuryMomentoMongoDB AtlasMotherduckMyScaleNeo4jNLPCloudNotion DBObsidianOpenAIOpenLLMOpenSearchOpenWeatherMapPetalsPostgres EmbeddingPGVectorPineconePipelineAIPortkeyPredibasePrediction GuardPromptLayerPsychicPubMedQdrantRay ServeRebuffRedditRedisReplicateRoamRocksetRunhouseRWKV-4SageMaker EndpointSageMaker TrackingScaNNSearxNG Search APISerpAPIShale ProtocolSingleStoreDBscikit-learnSlackspaCySpreedlyStarRocksStochasticAIStripeSupabase (Postgres)NebulaTairTelegramTencentVectorDBTensorFlow DatasetsTigris2MarkdownTrelloTruLensTwitterTypesenseUnstructuredUSearchVearchVectaraVespaWandB TracingWeights & BiasesWeatherWeaviateWhatsAppWhyLabsWikipediaWolfram AlphaWriterXataXorbits Inference (Xinference)Yeager.aiYouTubeZepZillizIntegrationsGrouped by providerSageMaker TrackingOn this pageSageMaker TrackingThis notebook shows how LangChain Callback can be used to log and track prompts and other LLM hyperparameters into SageMaker Experiments. Here, we use different scenarios to showcase the capability:Scenario 1: Single LLM - A case where a single LLM model is used to generate output based on a given prompt.Scenario 2: Sequential Chain - A case where a sequential chain of two LLM models is used.Scenario 3: Agent with Tools (Chain of Thought) - A case where multiple tools (search and math) are used in addition to an LLM.Amazon SageMaker is a fully managed service that is used to quickly and easily build, train and deploy machine learning (ML) models. Amazon SageMaker Experiments is a capability of Amazon SageMaker that lets you organize, track, compare and evaluate ML experiments and model versions.In this notebook, we will create a single experiment to log the prompts from each scenario.Installation and Setup‚Äãpip install sagemakerpip install openaipip install google-search-resultsFirst, setup the required API keysOpenAI: https://platform.openai.com/account/api-keys (For OpenAI LLM model)Google SERP API: https://serpapi.com/manage-api-key (For Google Search Tool)import os## Add your API keys belowos.environ["OPENAI_API_KEY"] = "<ADD-KEY-HERE>"os.environ["SERPAPI_API_KEY"] = "<ADD-KEY-HERE>"from langchain.llms import OpenAIfrom langchain.prompts import PromptTemplatefrom langchain.chains import LLMChain, SimpleSequentialChainfrom langchain.agents import initialize_agent, load_toolsfrom langchain.agents import Toolfrom langchain.callbacks import SageMakerCallbackHandlerfrom sagemaker.analytics import ExperimentAnalyticsfrom sagemaker.session import Sessionfrom sagemaker.experiments.run import RunAPI Reference:OpenAIPromptTemplateLLMChainSimpleSequentialChaininitialize_agentload_toolsToolSageMakerCallbackHandlerLLM Prompt Tracking‚Äã#LLM HyperparametersHPARAMS = {    "temperature": 0.1,    "model_name": "text-davinci-003",}#Bucket used to save prompt logs (Use `None` is used to save the default bucket or otherwise change it)BUCKET_NAME = None#Experiment nameEXPERIMENT_NAME = "langchain-sagemaker-tracker"#Create SageMaker Session with the given bucketsession = Session(default_bucket=BUCKET_NAME)Scenario 1 - LLM‚ÄãRUN_NAME = "run-scenario-1"PROMPT_TEMPLATE = "tell me a joke about {topic}"INPUT_VARIABLES = {"topic": "fish"}with Run(experiment_name=EXPERIMENT_NAME, run_name=RUN_NAME, sagemaker_session=session) as run:    # Create SageMaker Callback    sagemaker_callback = SageMakerCallbackHandler(run)    # Define LLM model with callback    llm = OpenAI(callbacks=[sagemaker_callback], **HPARAMS)    # Create prompt template    prompt = PromptTemplate.from_template(template=PROMPT_TEMPLATE)    # Create LLM Chain    chain = LLMChain(llm=llm, prompt=prompt, callbacks=[sagemaker_callback])    # Run chain    chain.run(**INPUT_VARIABLES)    # Reset the callback    sagemaker_callback.flush_tracker()Scenario 2 - Sequential Chain‚ÄãRUN_NAME = "run-scenario-2"PROMPT_TEMPLATE_1 = """You are a playwright. Given the title of play, it is your job to write a synopsis for that title.Title: {title}Playwright: This is a synopsis for the above play:"""PROMPT_TEMPLATE_2 = """You are a play critic from the New York Times. Given the synopsis of play, it is your job to write a review for that play.Play Synopsis: {synopsis}Review from a New York Times play critic of the above play:"""INPUT_VARIABLES = {    "input": "documentary about good video games that push the boundary of game design"}with Run(experiment_name=EXPERIMENT_NAME, run_name=RUN_NAME, sagemaker_session=session) as run:    # Create SageMaker Callback    sagemaker_callback = SageMakerCallbackHandler(run)    # Create prompt templates for the chain    prompt_template1 = PromptTemplate.from_template(template=PROMPT_TEMPLATE_1)    prompt_template2 = PromptTemplate.from_template(template=PROMPT_TEMPLATE_2)    # Define LLM model with callback    llm = OpenAI(callbacks=[sagemaker_callback], **HPARAMS)    # Create chain1    chain1 = LLMChain(llm=llm, prompt=prompt_template1, callbacks=[sagemaker_callback])    # Create chain2    chain2 = LLMChain(llm=llm, prompt=prompt_template2, callbacks=[sagemaker_callback])    # Create Sequential chain    overall_chain = SimpleSequentialChain(chains=[chain1, chain2], callbacks=[sagemaker_callback])    # Run overall sequential chain    overall_chain.run(**INPUT_VARIABLES)    # Reset the callback    sagemaker_callback.flush_tracker()Scenario 3 - Agent with Tools‚ÄãRUN_NAME = "run-scenario-3"PROMPT_TEMPLATE = "Who is the oldest person alive? And what is their current age raised to the power of 1.51?"with Run(experiment_name=EXPERIMENT_NAME, run_name=RUN_NAME, sagemaker_session=session) as run:    # Create SageMaker Callback    sagemaker_callback = SageMakerCallbackHandler(run)    # Define LLM model with callback    llm = OpenAI(callbacks=[sagemaker_callback], **HPARAMS)    # Define tools    tools = load_tools(["serpapi", "llm-math"], llm=llm, callbacks=[sagemaker_callback])    # Initialize agent with all the tools    agent = initialize_agent(tools, llm, agent="zero-shot-react-description", callbacks=[sagemaker_callback])    # Run agent    agent.run(input=PROMPT_TEMPLATE)    # Reset the callback    sagemaker_callback.flush_tracker()Load Log Data‚ÄãOnce the prompts are logged, we can easily load and convert them to Pandas DataFrame as follows.#Loadlogs = ExperimentAnalytics(experiment_name=EXPERIMENT_NAME)#Convert as pandas dataframedf = logs.dataframe(force_refresh=True)print(df.shape)df.head()As can be seen above, there are three runs (rows) in the experiment corresponding to each scenario. Each run logs the prompts and related LLM settings/hyperparameters as json and are saved in s3 bucket. Feel free to load and explore the log data from each json path.PreviousSageMaker EndpointNextScaNNInstallation and SetupLLM Prompt TrackingScenario 1 - LLMScenario 2 - Sequential ChainScenario 3 - Agent with ToolsLoad Log Data IntegrationsCallbacksChat modelsChat loadersDocument loadersDocument transformersLLMsMemoryRetrieversText embedding modelsAgents & ToolkitsToolsVector storesGrouped by providerActiveloop Deep LakeAI21 LabsAimAINetworkAirbyteAirtableAleph AlphaAlibaba Cloud OpensearchAmazon API GatewayAnalyticDBAnnoyAnyscaleApifyArangoDBArgillaArthurArxivAtlasAwaDBAWS S3 DirectoryAZLyricsAzure Blob StorageAzure Cognitive SearchAzure OpenAIBagelDBBananaBasetenBeamBedrockBiliBiliNIBittensorBlackboardBrave SearchCassandraCerebriumAIChaindeskChromaClarifaiClearMLClickHouseCnosDBCohereCollege ConfidentialCometConfident AIConfluenceC TransformersDashVectorDatabricksDatadog TracingDatadog LogsDataForSEODeepInfraDeepSparseDiffbotDingoDiscordDocArrayDocugamiDuckDBElasticsearchEpsillaEverNoteFacebook ChatFacebook FaissFigmaFireworksFlyteForefrontAIGitGitBookGoldenGoogle BigQueryGoogle Cloud StorageGoogle DriveGoogle SearchGoogle SerperGoogle Vertex AI MatchingEngineGooseAIGPT4AllGraphsignalGrobidGutenbergHacker NewsHazy ResearchHeliconeHologresHugging FaceiFixitIMSDbInfinoJinaKonkoLanceDBLangChain Decorators ‚ú®Llama.cppLog10MarqoMediaWikiDumpMeilisearchMetalMicrosoft OneDriveMicrosoft PowerPointMicrosoft WordMilvusMinimaxMLflow AI GatewayMLflowModalModelScopeModern TreasuryMomentoMongoDB AtlasMotherduckMyScaleNeo4jNLPCloudNotion DBObsidianOpenAIOpenLLMOpenSearchOpenWeatherMapPetalsPostgres EmbeddingPGVectorPineconePipelineAIPortkeyPredibasePrediction GuardPromptLayerPsychicPubMedQdrantRay ServeRebuffRedditRedisReplicateRoamRocksetRunhouseRWKV-4SageMaker EndpointSageMaker TrackingScaNNSearxNG Search APISerpAPIShale ProtocolSingleStoreDBscikit-learnSlackspaCySpreedlyStarRocksStochasticAIStripeSupabase (Postgres)NebulaTairTelegramTencentVectorDBTensorFlow DatasetsTigris2MarkdownTrelloTruLensTwitterTypesenseUnstructuredUSearchVearchVectaraVespaWandB TracingWeights & BiasesWeatherWeaviateWhatsAppWhyLabsWikipediaWolfram AlphaWriterXataXorbits Inference (Xinference)Yeager.aiYouTubeZepZilliz IntegrationsCallbacksChat modelsChat loadersDocument loadersDocument transformersLLMsMemoryRetrieversText embedding modelsAgents & ToolkitsToolsVector storesGrouped by providerActiveloop Deep LakeAI21 LabsAimAINetworkAirbyteAirtableAleph AlphaAlibaba Cloud OpensearchAmazon API GatewayAnalyticDBAnnoyAnyscaleApifyArangoDBArgillaArthurArxivAtlasAwaDBAWS S3 DirectoryAZLyricsAzure Blob StorageAzure Cognitive SearchAzure OpenAIBagelDBBananaBasetenBeamBedrockBiliBiliNIBittensorBlackboardBrave SearchCassandraCerebriumAIChaindeskChromaClarifaiClearMLClickHouseCnosDBCohereCollege ConfidentialCometConfident AIConfluenceC TransformersDashVectorDatabricksDatadog TracingDatadog LogsDataForSEODeepInfraDeepSparseDiffbotDingoDiscordDocArrayDocugamiDuckDBElasticsearchEpsillaEverNoteFacebook ChatFacebook FaissFigmaFireworksFlyteForefrontAIGitGitBookGoldenGoogle BigQueryGoogle Cloud StorageGoogle DriveGoogle SearchGoogle SerperGoogle Vertex AI MatchingEngineGooseAIGPT4AllGraphsignalGrobidGutenbergHacker NewsHazy ResearchHeliconeHologresHugging FaceiFixitIMSDbInfinoJinaKonkoLanceDBLangChain Decorators ‚ú®Llama.cppLog10MarqoMediaWikiDumpMeilisearchMetalMicrosoft OneDriveMicrosoft PowerPointMicrosoft WordMilvusMinimaxMLflow AI GatewayMLflowModalModelScopeModern TreasuryMomentoMongoDB AtlasMotherduckMyScaleNeo4jNLPCloudNotion DBObsidianOpenAIOpenLLMOpenSearchOpenWeatherMapPetalsPostgres EmbeddingPGVectorPineconePipelineAIPortkeyPredibasePrediction GuardPromptLayerPsychicPubMedQdrantRay ServeRebuffRedditRedisReplicateRoamRocksetRunhouseRWKV-4SageMaker EndpointSageMaker TrackingScaNNSearxNG Search APISerpAPIShale ProtocolSingleStoreDBscikit-learnSlackspaCySpreedlyStarRocksStochasticAIStripeSupabase (Postgres)NebulaTairTelegramTencentVectorDBTensorFlow DatasetsTigris2MarkdownTrelloTruLensTwitterTypesenseUnstructuredUSearchVearchVectaraVespaWandB TracingWeights & BiasesWeatherWeaviateWhatsAppWhyLabsWikipediaWolfram AlphaWriterXataXorbits Inference (Xinference)Yeager.aiYouTubeZepZilliz Integrations Callbacks Chat models Chat loaders Document loaders Document transformers LLMs Memory Retrievers Text embedding models Agents & Toolkits Tools Vector stores Grouped by provider Portkey Vectara IntegrationsGrouped by providerSageMaker TrackingOn this pageSageMaker TrackingThis notebook shows how LangChain Callback can be used to log and track prompts and other LLM hyperparameters into SageMaker Experiments. Here, we use different scenarios to showcase the capability:Scenario 1: Single LLM - A case where a single LLM model is used to generate output based on a given prompt.Scenario 2: Sequential Chain - A case where a sequential chain of two LLM models is used.Scenario 3: Agent with Tools (Chain of Thought) - A case where multiple tools (search and math) are used in addition to an LLM.Amazon SageMaker is a fully managed service that is used to quickly and easily build, train and deploy machine learning (ML) models. Amazon SageMaker Experiments is a capability of Amazon SageMaker that lets you organize, track, compare and evaluate ML experiments and model versions.In this notebook, we will create a single experiment to log the prompts from each scenario.Installation and Setup‚Äãpip install sagemakerpip install openaipip install google-search-resultsFirst, setup the required API keysOpenAI: https://platform.openai.com/account/api-keys (For OpenAI LLM model)Google SERP API: https://serpapi.com/manage-api-key (For Google Search Tool)import os## Add your API keys belowos.environ["OPENAI_API_KEY"] = "<ADD-KEY-HERE>"os.environ["SERPAPI_API_KEY"] = "<ADD-KEY-HERE>"from langchain.llms import OpenAIfrom langchain.prompts import PromptTemplatefrom langchain.chains import LLMChain, SimpleSequentialChainfrom langchain.agents import initialize_agent, load_toolsfrom langchain.agents import Toolfrom langchain.callbacks import SageMakerCallbackHandlerfrom sagemaker.analytics import ExperimentAnalyticsfrom sagemaker.session import Sessionfrom sagemaker.experiments.run import RunAPI Reference:OpenAIPromptTemplateLLMChainSimpleSequentialChaininitialize_agentload_toolsToolSageMakerCallbackHandlerLLM Prompt Tracking‚Äã#LLM HyperparametersHPARAMS = {    "temperature": 0.1,    "model_name": "text-davinci-003",}#Bucket used to save prompt logs (Use `None` is used to save the default bucket or otherwise change it)BUCKET_NAME = None#Experiment nameEXPERIMENT_NAME = "langchain-sagemaker-tracker"#Create SageMaker Session with the given bucketsession = Session(default_bucket=BUCKET_NAME)Scenario 1 - LLM‚ÄãRUN_NAME = "run-scenario-1"PROMPT_TEMPLATE = "tell me a joke about {topic}"INPUT_VARIABLES = {"topic": "fish"}with Run(experiment_name=EXPERIMENT_NAME, run_name=RUN_NAME, sagemaker_session=session) as run:    # Create SageMaker Callback    sagemaker_callback = SageMakerCallbackHandler(run)    # Define LLM model with callback    llm = OpenAI(callbacks=[sagemaker_callback], **HPARAMS)    # Create prompt template    prompt = PromptTemplate.from_template(template=PROMPT_TEMPLATE)    # Create LLM Chain    chain = LLMChain(llm=llm, prompt=prompt, callbacks=[sagemaker_callback])    # Run chain    chain.run(**INPUT_VARIABLES)    # Reset the callback    sagemaker_callback.flush_tracker()Scenario 2 - Sequential Chain‚ÄãRUN_NAME = "run-scenario-2"PROMPT_TEMPLATE_1 = """You are a playwright. Given the title of play, it is your job to write a synopsis for that title.Title: {title}Playwright: This is a synopsis for the above play:"""PROMPT_TEMPLATE_2 = """You are a play critic from the New York Times. Given the synopsis of play, it is your job to write a review for that play.Play Synopsis: {synopsis}Review from a New York Times play critic of the above play:"""INPUT_VARIABLES = {    "input": "documentary about good video games that push the boundary of game design"}with Run(experiment_name=EXPERIMENT_NAME, run_name=RUN_NAME, sagemaker_session=session) as run:    # Create SageMaker Callback    sagemaker_callback = SageMakerCallbackHandler(run)    # Create prompt templates for the chain    prompt_template1 = PromptTemplate.from_template(template=PROMPT_TEMPLATE_1)    prompt_template2 = PromptTemplate.from_template(template=PROMPT_TEMPLATE_2)    # Define LLM model with callback    llm = OpenAI(callbacks=[sagemaker_callback], **HPARAMS)    # Create chain1    chain1 = LLMChain(llm=llm, prompt=prompt_template1, callbacks=[sagemaker_callback])    # Create chain2    chain2 = LLMChain(llm=llm, prompt=prompt_template2, callbacks=[sagemaker_callback])    # Create Sequential chain    overall_chain = SimpleSequentialChain(chains=[chain1, chain2], callbacks=[sagemaker_callback])    # Run overall sequential chain    overall_chain.run(**INPUT_VARIABLES)    # Reset the callback    sagemaker_callback.flush_tracker()Scenario 3 - Agent with Tools‚ÄãRUN_NAME = "run-scenario-3"PROMPT_TEMPLATE = "Who is the oldest person alive? And what is their current age raised to the power of 1.51?"with Run(experiment_name=EXPERIMENT_NAME, run_name=RUN_NAME, sagemaker_session=session) as run:    # Create SageMaker Callback    sagemaker_callback = SageMakerCallbackHandler(run)    # Define LLM model with callback    llm = OpenAI(callbacks=[sagemaker_callback], **HPARAMS)    # Define tools    tools = load_tools(["serpapi", "llm-math"], llm=llm, callbacks=[sagemaker_callback])    # Initialize agent with all the tools    agent = initialize_agent(tools, llm, agent="zero-shot-react-description", callbacks=[sagemaker_callback])    # Run agent    agent.run(input=PROMPT_TEMPLATE)    # Reset the callback    sagemaker_callback.flush_tracker()Load Log Data‚ÄãOnce the prompts are logged, we can easily load and convert them to Pandas DataFrame as follows.#Loadlogs = ExperimentAnalytics(experiment_name=EXPERIMENT_NAME)#Convert as pandas dataframedf = logs.dataframe(force_refresh=True)print(df.shape)df.head()As can be seen above, there are three runs (rows) in the experiment corresponding to each scenario. Each run logs the prompts and related LLM settings/hyperparameters as json and are saved in s3 bucket. Feel free to load and explore the log data from each json path.PreviousSageMaker EndpointNextScaNNInstallation and SetupLLM Prompt TrackingScenario 1 - LLMScenario 2 - Sequential ChainScenario 3 - Agent with ToolsLoad Log Data IntegrationsGrouped by providerSageMaker TrackingOn this pageSageMaker TrackingThis notebook shows how LangChain Callback can be used to log and track prompts and other LLM hyperparameters into SageMaker Experiments. Here, we use different scenarios to showcase the capability:Scenario 1: Single LLM - A case where a single LLM model is used to generate output based on a given prompt.Scenario 2: Sequential Chain - A case where a sequential chain of two LLM models is used.Scenario 3: Agent with Tools (Chain of Thought) - A case where multiple tools (search and math) are used in addition to an LLM.Amazon SageMaker is a fully managed service that is used to quickly and easily build, train and deploy machine learning (ML) models. Amazon SageMaker Experiments is a capability of Amazon SageMaker that lets you organize, track, compare and evaluate ML experiments and model versions.In this notebook, we will create a single experiment to log the prompts from each scenario.Installation and Setup‚Äãpip install sagemakerpip install openaipip install google-search-resultsFirst, setup the required API keysOpenAI: https://platform.openai.com/account/api-keys (For OpenAI LLM model)Google SERP API: https://serpapi.com/manage-api-key (For Google Search Tool)import os## Add your API keys belowos.environ["OPENAI_API_KEY"] = "<ADD-KEY-HERE>"os.environ["SERPAPI_API_KEY"] = "<ADD-KEY-HERE>"from langchain.llms import OpenAIfrom langchain.prompts import PromptTemplatefrom langchain.chains import LLMChain, SimpleSequentialChainfrom langchain.agents import initialize_agent, load_toolsfrom langchain.agents import Toolfrom langchain.callbacks import SageMakerCallbackHandlerfrom sagemaker.analytics import ExperimentAnalyticsfrom sagemaker.session import Sessionfrom sagemaker.experiments.run import RunAPI Reference:OpenAIPromptTemplateLLMChainSimpleSequentialChaininitialize_agentload_toolsToolSageMakerCallbackHandlerLLM Prompt Tracking‚Äã#LLM HyperparametersHPARAMS = {    "temperature": 0.1,    "model_name": "text-davinci-003",}#Bucket used to save prompt logs (Use `None` is used to save the default bucket or otherwise change it)BUCKET_NAME = None#Experiment nameEXPERIMENT_NAME = "langchain-sagemaker-tracker"#Create SageMaker Session with the given bucketsession = Session(default_bucket=BUCKET_NAME)Scenario 1 - LLM‚ÄãRUN_NAME = "run-scenario-1"PROMPT_TEMPLATE = "tell me a joke about {topic}"INPUT_VARIABLES = {"topic": "fish"}with Run(experiment_name=EXPERIMENT_NAME, run_name=RUN_NAME, sagemaker_session=session) as run:    # Create SageMaker Callback    sagemaker_callback = SageMakerCallbackHandler(run)    # Define LLM model with callback    llm = OpenAI(callbacks=[sagemaker_callback], **HPARAMS)    # Create prompt template    prompt = PromptTemplate.from_template(template=PROMPT_TEMPLATE)    # Create LLM Chain    chain = LLMChain(llm=llm, prompt=prompt, callbacks=[sagemaker_callback])    # Run chain    chain.run(**INPUT_VARIABLES)    # Reset the callback    sagemaker_callback.flush_tracker()Scenario 2 - Sequential Chain‚ÄãRUN_NAME = "run-scenario-2"PROMPT_TEMPLATE_1 = """You are a playwright. Given the title of play, it is your job to write a synopsis for that title.Title: {title}Playwright: This is a synopsis for the above play:"""PROMPT_TEMPLATE_2 = """You are a play critic from the New York Times. Given the synopsis of play, it is your job to write a review for that play.Play Synopsis: {synopsis}Review from a New York Times play critic of the above play:"""INPUT_VARIABLES = {    "input": "documentary about good video games that push the boundary of game design"}with Run(experiment_name=EXPERIMENT_NAME, run_name=RUN_NAME, sagemaker_session=session) as run:    # Create SageMaker Callback    sagemaker_callback = SageMakerCallbackHandler(run)    # Create prompt templates for the chain    prompt_template1 = PromptTemplate.from_template(template=PROMPT_TEMPLATE_1)    prompt_template2 = PromptTemplate.from_template(template=PROMPT_TEMPLATE_2)    # Define LLM model with callback    llm = OpenAI(callbacks=[sagemaker_callback], **HPARAMS)    # Create chain1    chain1 = LLMChain(llm=llm, prompt=prompt_template1, callbacks=[sagemaker_callback])    # Create chain2    chain2 = LLMChain(llm=llm, prompt=prompt_template2, callbacks=[sagemaker_callback])    # Create Sequential chain    overall_chain = SimpleSequentialChain(chains=[chain1, chain2], callbacks=[sagemaker_callback])    # Run overall sequential chain    overall_chain.run(**INPUT_VARIABLES)    # Reset the callback    sagemaker_callback.flush_tracker()Scenario 3 - Agent with Tools‚ÄãRUN_NAME = "run-scenario-3"PROMPT_TEMPLATE = "Who is the oldest person alive? And what is their current age raised to the power of 1.51?"with Run(experiment_name=EXPERIMENT_NAME, run_name=RUN_NAME, sagemaker_session=session) as run:    # Create SageMaker Callback    sagemaker_callback = SageMakerCallbackHandler(run)    # Define LLM model with callback    llm = OpenAI(callbacks=[sagemaker_callback], **HPARAMS)    # Define tools    tools = load_tools(["serpapi", "llm-math"], llm=llm, callbacks=[sagemaker_callback])    # Initialize agent with all the tools    agent = initialize_agent(tools, llm, agent="zero-shot-react-description", callbacks=[sagemaker_callback])    # Run agent    agent.run(input=PROMPT_TEMPLATE)    # Reset the callback    sagemaker_callback.flush_tracker()Load Log Data‚ÄãOnce the prompts are logged, we can easily load and convert them to Pandas DataFrame as follows.#Loadlogs = ExperimentAnalytics(experiment_name=EXPERIMENT_NAME)#Convert as pandas dataframedf = logs.dataframe(force_refresh=True)print(df.shape)df.head()As can be seen above, there are three runs (rows) in the experiment corresponding to each scenario. Each run logs the prompts and related LLM settings/hyperparameters as json and are saved in s3 bucket. Feel free to load and explore the log data from each json path.PreviousSageMaker EndpointNextScaNNInstallation and SetupLLM Prompt TrackingScenario 1 - LLMScenario 2 - Sequential ChainScenario 3 - Agent with ToolsLoad Log Data IntegrationsGrouped by providerSageMaker TrackingOn this pageSageMaker TrackingThis notebook shows how LangChain Callback can be used to log and track prompts and other LLM hyperparameters into SageMaker Experiments. Here, we use different scenarios to showcase the capability:Scenario 1: Single LLM - A case where a single LLM model is used to generate output based on a given prompt.Scenario 2: Sequential Chain - A case where a sequential chain of two LLM models is used.Scenario 3: Agent with Tools (Chain of Thought) - A case where multiple tools (search and math) are used in addition to an LLM.Amazon SageMaker is a fully managed service that is used to quickly and easily build, train and deploy machine learning (ML) models. Amazon SageMaker Experiments is a capability of Amazon SageMaker that lets you organize, track, compare and evaluate ML experiments and model versions.In this notebook, we will create a single experiment to log the prompts from each scenario.Installation and Setup‚Äãpip install sagemakerpip install openaipip install google-search-resultsFirst, setup the required API keysOpenAI: https://platform.openai.com/account/api-keys (For OpenAI LLM model)Google SERP API: https://serpapi.com/manage-api-key (For Google Search Tool)import os## Add your API keys belowos.environ["OPENAI_API_KEY"] = "<ADD-KEY-HERE>"os.environ["SERPAPI_API_KEY"] = "<ADD-KEY-HERE>"from langchain.llms import OpenAIfrom langchain.prompts import PromptTemplatefrom langchain.chains import LLMChain, SimpleSequentialChainfrom langchain.agents import initialize_agent, load_toolsfrom langchain.agents import Toolfrom langchain.callbacks import SageMakerCallbackHandlerfrom sagemaker.analytics import ExperimentAnalyticsfrom sagemaker.session import Sessionfrom sagemaker.experiments.run import RunAPI Reference:OpenAIPromptTemplateLLMChainSimpleSequentialChaininitialize_agentload_toolsToolSageMakerCallbackHandlerLLM Prompt Tracking‚Äã#LLM HyperparametersHPARAMS = {    "temperature": 0.1,    "model_name": "text-davinci-003",}#Bucket used to save prompt logs (Use `None` is used to save the default bucket or otherwise change it)BUCKET_NAME = None#Experiment nameEXPERIMENT_NAME = "langchain-sagemaker-tracker"#Create SageMaker Session with the given bucketsession = Session(default_bucket=BUCKET_NAME)Scenario 1 - LLM‚ÄãRUN_NAME = "run-scenario-1"PROMPT_TEMPLATE = "tell me a joke about {topic}"INPUT_VARIABLES = {"topic": "fish"}with Run(experiment_name=EXPERIMENT_NAME, run_name=RUN_NAME, sagemaker_session=session) as run:    # Create SageMaker Callback    sagemaker_callback = SageMakerCallbackHandler(run)    # Define LLM model with callback    llm = OpenAI(callbacks=[sagemaker_callback], **HPARAMS)    # Create prompt template    prompt = PromptTemplate.from_template(template=PROMPT_TEMPLATE)    # Create LLM Chain    chain = LLMChain(llm=llm, prompt=prompt, callbacks=[sagemaker_callback])    # Run chain    chain.run(**INPUT_VARIABLES)    # Reset the callback    sagemaker_callback.flush_tracker()Scenario 2 - Sequential Chain‚ÄãRUN_NAME = "run-scenario-2"PROMPT_TEMPLATE_1 = """You are a playwright. Given the title of play, it is your job to write a synopsis for that title.Title: {title}Playwright: This is a synopsis for the above play:"""PROMPT_TEMPLATE_2 = """You are a play critic from the New York Times. Given the synopsis of play, it is your job to write a review for that play.Play Synopsis: {synopsis}Review from a New York Times play critic of the above play:"""INPUT_VARIABLES = {    "input": "documentary about good video games that push the boundary of game design"}with Run(experiment_name=EXPERIMENT_NAME, run_name=RUN_NAME, sagemaker_session=session) as run:    # Create SageMaker Callback    sagemaker_callback = SageMakerCallbackHandler(run)    # Create prompt templates for the chain    prompt_template1 = PromptTemplate.from_template(template=PROMPT_TEMPLATE_1)    prompt_template2 = PromptTemplate.from_template(template=PROMPT_TEMPLATE_2)    # Define LLM model with callback    llm = OpenAI(callbacks=[sagemaker_callback], **HPARAMS)    # Create chain1    chain1 = LLMChain(llm=llm, prompt=prompt_template1, callbacks=[sagemaker_callback])    # Create chain2    chain2 = LLMChain(llm=llm, prompt=prompt_template2, callbacks=[sagemaker_callback])    # Create Sequential chain    overall_chain = SimpleSequentialChain(chains=[chain1, chain2], callbacks=[sagemaker_callback])    # Run overall sequential chain    overall_chain.run(**INPUT_VARIABLES)    # Reset the callback    sagemaker_callback.flush_tracker()Scenario 3 - Agent with Tools‚ÄãRUN_NAME = "run-scenario-3"PROMPT_TEMPLATE = "Who is the oldest person alive? And what is their current age raised to the power of 1.51?"with Run(experiment_name=EXPERIMENT_NAME, run_name=RUN_NAME, sagemaker_session=session) as run:    # Create SageMaker Callback    sagemaker_callback = SageMakerCallbackHandler(run)    # Define LLM model with callback    llm = OpenAI(callbacks=[sagemaker_callback], **HPARAMS)    # Define tools    tools = load_tools(["serpapi", "llm-math"], llm=llm, callbacks=[sagemaker_callback])    # Initialize agent with all the tools    agent = initialize_agent(tools, llm, agent="zero-shot-react-description", callbacks=[sagemaker_callback])    # Run agent    agent.run(input=PROMPT_TEMPLATE)    # Reset the callback    sagemaker_callback.flush_tracker()Load Log Data‚ÄãOnce the prompts are logged, we can easily load and convert them to Pandas DataFrame as follows.#Loadlogs = ExperimentAnalytics(experiment_name=EXPERIMENT_NAME)#Convert as pandas dataframedf = logs.dataframe(force_refresh=True)print(df.shape)df.head()As can be seen above, there are three runs (rows) in the experiment corresponding to each scenario. Each run logs the prompts and related LLM settings/hyperparameters as json and are saved in s3 bucket. Feel free to load and explore the log data from each json path.PreviousSageMaker EndpointNextScaNN IntegrationsGrouped by providerSageMaker TrackingOn this pageSageMaker TrackingThis notebook shows how LangChain Callback can be used to log and track prompts and other LLM hyperparameters into SageMaker Experiments. Here, we use different scenarios to showcase the capability:Scenario 1: Single LLM - A case where a single LLM model is used to generate output based on a given prompt.Scenario 2: Sequential Chain - A case where a sequential chain of two LLM models is used.Scenario 3: Agent with Tools (Chain of Thought) - A case where multiple tools (search and math) are used in addition to an LLM.Amazon SageMaker is a fully managed service that is used to quickly and easily build, train and deploy machine learning (ML) models. Amazon SageMaker Experiments is a capability of Amazon SageMaker that lets you organize, track, compare and evaluate ML experiments and model versions.In this notebook, we will create a single experiment to log the prompts from each scenario.Installation and Setup‚Äãpip install sagemakerpip install openaipip install google-search-resultsFirst, setup the required API keysOpenAI: https://platform.openai.com/account/api-keys (For OpenAI LLM model)Google SERP API: https://serpapi.com/manage-api-key (For Google Search Tool)import os## Add your API keys belowos.environ["OPENAI_API_KEY"] = "<ADD-KEY-HERE>"os.environ["SERPAPI_API_KEY"] = "<ADD-KEY-HERE>"from langchain.llms import OpenAIfrom langchain.prompts import PromptTemplatefrom langchain.chains import LLMChain, SimpleSequentialChainfrom langchain.agents import initialize_agent, load_toolsfrom langchain.agents import Toolfrom langchain.callbacks import SageMakerCallbackHandlerfrom sagemaker.analytics import ExperimentAnalyticsfrom sagemaker.session import Sessionfrom sagemaker.experiments.run import RunAPI Reference:OpenAIPromptTemplateLLMChainSimpleSequentialChaininitialize_agentload_toolsToolSageMakerCallbackHandlerLLM Prompt Tracking‚Äã#LLM HyperparametersHPARAMS = {    "temperature": 0.1,    "model_name": "text-davinci-003",}#Bucket used to save prompt logs (Use `None` is used to save the default bucket or otherwise change it)BUCKET_NAME = None#Experiment nameEXPERIMENT_NAME = "langchain-sagemaker-tracker"#Create SageMaker Session with the given bucketsession = Session(default_bucket=BUCKET_NAME)Scenario 1 - LLM‚ÄãRUN_NAME = "run-scenario-1"PROMPT_TEMPLATE = "tell me a joke about {topic}"INPUT_VARIABLES = {"topic": "fish"}with Run(experiment_name=EXPERIMENT_NAME, run_name=RUN_NAME, sagemaker_session=session) as run:    # Create SageMaker Callback    sagemaker_callback = SageMakerCallbackHandler(run)    # Define LLM model with callback    llm = OpenAI(callbacks=[sagemaker_callback], **HPARAMS)    # Create prompt template    prompt = PromptTemplate.from_template(template=PROMPT_TEMPLATE)    # Create LLM Chain    chain = LLMChain(llm=llm, prompt=prompt, callbacks=[sagemaker_callback])    # Run chain    chain.run(**INPUT_VARIABLES)    # Reset the callback    sagemaker_callback.flush_tracker()Scenario 2 - Sequential Chain‚ÄãRUN_NAME = "run-scenario-2"PROMPT_TEMPLATE_1 = """You are a playwright. Given the title of play, it is your job to write a synopsis for that title.Title: {title}Playwright: This is a synopsis for the above play:"""PROMPT_TEMPLATE_2 = """You are a play critic from the New York Times. Given the synopsis of play, it is your job to write a review for that play.Play Synopsis: {synopsis}Review from a New York Times play critic of the above play:"""INPUT_VARIABLES = {    "input": "documentary about good video games that push the boundary of game design"}with Run(experiment_name=EXPERIMENT_NAME, run_name=RUN_NAME, sagemaker_session=session) as run:    # Create SageMaker Callback    sagemaker_callback = SageMakerCallbackHandler(run)    # Create prompt templates for the chain    prompt_template1 = PromptTemplate.from_template(template=PROMPT_TEMPLATE_1)    prompt_template2 = PromptTemplate.from_template(template=PROMPT_TEMPLATE_2)    # Define LLM model with callback    llm = OpenAI(callbacks=[sagemaker_callback], **HPARAMS)    # Create chain1    chain1 = LLMChain(llm=llm, prompt=prompt_template1, callbacks=[sagemaker_callback])    # Create chain2    chain2 = LLMChain(llm=llm, prompt=prompt_template2, callbacks=[sagemaker_callback])    # Create Sequential chain    overall_chain = SimpleSequentialChain(chains=[chain1, chain2], callbacks=[sagemaker_callback])    # Run overall sequential chain    overall_chain.run(**INPUT_VARIABLES)    # Reset the callback    sagemaker_callback.flush_tracker()Scenario 3 - Agent with Tools‚ÄãRUN_NAME = "run-scenario-3"PROMPT_TEMPLATE = "Who is the oldest person alive? And what is their current age raised to the power of 1.51?"with Run(experiment_name=EXPERIMENT_NAME, run_name=RUN_NAME, sagemaker_session=session) as run:    # Create SageMaker Callback    sagemaker_callback = SageMakerCallbackHandler(run)    # Define LLM model with callback    llm = OpenAI(callbacks=[sagemaker_callback], **HPARAMS)    # Define tools    tools = load_tools(["serpapi", "llm-math"], llm=llm, callbacks=[sagemaker_callback])    # Initialize agent with all the tools    agent = initialize_agent(tools, llm, agent="zero-shot-react-description", callbacks=[sagemaker_callback])    # Run agent    agent.run(input=PROMPT_TEMPLATE)    # Reset the callback    sagemaker_callback.flush_tracker()Load Log Data‚ÄãOnce the prompts are logged, we can easily load and convert them to Pandas DataFrame as follows.#Loadlogs = ExperimentAnalytics(experiment_name=EXPERIMENT_NAME)#Convert as pandas dataframedf = logs.dataframe(force_refresh=True)print(df.shape)df.head()As can be seen above, there are three runs (rows) in the experiment corresponding to each scenario. Each run logs the prompts and related LLM settings/hyperparameters as json and are saved in s3 bucket. Feel free to load and explore the log data from each json path.PreviousSageMaker EndpointNextScaNN On this page SageMaker TrackingThis notebook shows how LangChain Callback can be used to log and track prompts and other LLM hyperparameters into SageMaker Experiments. Here, we use different scenarios to showcase the capability:Scenario 1: Single LLM - A case where a single LLM model is used to generate output based on a given prompt.Scenario 2: Sequential Chain - A case where a sequential chain of two LLM models is used.Scenario 3: Agent with Tools (Chain of Thought) - A case where multiple tools (search and math) are used in addition to an LLM.Amazon SageMaker is a fully managed service that is used to quickly and easily build, train and deploy machine learning (ML) models. Amazon SageMaker Experiments is a capability of Amazon SageMaker that lets you organize, track, compare and evaluate ML experiments and model versions.In this notebook, we will create a single experiment to log the prompts from each scenario.Installation and Setup‚Äãpip install sagemakerpip install openaipip install google-search-resultsFirst, setup the required API keysOpenAI: https://platform.openai.com/account/api-keys (For OpenAI LLM model)Google SERP API: https://serpapi.com/manage-api-key (For Google Search Tool)import os## Add your API keys belowos.environ["OPENAI_API_KEY"] = "<ADD-KEY-HERE>"os.environ["SERPAPI_API_KEY"] = "<ADD-KEY-HERE>"from langchain.llms import OpenAIfrom langchain.prompts import PromptTemplatefrom langchain.chains import LLMChain, SimpleSequentialChainfrom langchain.agents import initialize_agent, load_toolsfrom langchain.agents import Toolfrom langchain.callbacks import SageMakerCallbackHandlerfrom sagemaker.analytics import ExperimentAnalyticsfrom sagemaker.session import Sessionfrom sagemaker.experiments.run import RunAPI Reference:OpenAIPromptTemplateLLMChainSimpleSequentialChaininitialize_agentload_toolsToolSageMakerCallbackHandlerLLM Prompt Tracking‚Äã#LLM HyperparametersHPARAMS = {    "temperature": 0.1,    "model_name": "text-davinci-003",}#Bucket used to save prompt logs (Use `None` is used to save the default bucket or otherwise change it)BUCKET_NAME = None#Experiment nameEXPERIMENT_NAME = "langchain-sagemaker-tracker"#Create SageMaker Session with the given bucketsession = Session(default_bucket=BUCKET_NAME)Scenario 1 - LLM‚ÄãRUN_NAME = "run-scenario-1"PROMPT_TEMPLATE = "tell me a joke about {topic}"INPUT_VARIABLES = {"topic": "fish"}with Run(experiment_name=EXPERIMENT_NAME, run_name=RUN_NAME, sagemaker_session=session) as run:    # Create SageMaker Callback    sagemaker_callback = SageMakerCallbackHandler(run)    # Define LLM model with callback    llm = OpenAI(callbacks=[sagemaker_callback], **HPARAMS)    # Create prompt template    prompt = PromptTemplate.from_template(template=PROMPT_TEMPLATE)    # Create LLM Chain    chain = LLMChain(llm=llm, prompt=prompt, callbacks=[sagemaker_callback])    # Run chain    chain.run(**INPUT_VARIABLES)    # Reset the callback    sagemaker_callback.flush_tracker()Scenario 2 - Sequential Chain‚ÄãRUN_NAME = "run-scenario-2"PROMPT_TEMPLATE_1 = """You are a playwright. Given the title of play, it is your job to write a synopsis for that title.Title: {title}Playwright: This is a synopsis for the above play:"""PROMPT_TEMPLATE_2 = """You are a play critic from the New York Times. Given the synopsis of play, it is your job to write a review for that play.Play Synopsis: {synopsis}Review from a New York Times play critic of the above play:"""INPUT_VARIABLES = {    "input": "documentary about good video games that push the boundary of game design"}with Run(experiment_name=EXPERIMENT_NAME, run_name=RUN_NAME, sagemaker_session=session) as run:    # Create SageMaker Callback    sagemaker_callback = SageMakerCallbackHandler(run)    # Create prompt templates for the chain    prompt_template1 = PromptTemplate.from_template(template=PROMPT_TEMPLATE_1)    prompt_template2 = PromptTemplate.from_template(template=PROMPT_TEMPLATE_2)    # Define LLM model with callback    llm = OpenAI(callbacks=[sagemaker_callback], **HPARAMS)    # Create chain1    chain1 = LLMChain(llm=llm, prompt=prompt_template1, callbacks=[sagemaker_callback])    # Create chain2    chain2 = LLMChain(llm=llm, prompt=prompt_template2, callbacks=[sagemaker_callback])    # Create Sequential chain    overall_chain = SimpleSequentialChain(chains=[chain1, chain2], callbacks=[sagemaker_callback])    # Run overall sequential chain    overall_chain.run(**INPUT_VARIABLES)    # Reset the callback    sagemaker_callback.flush_tracker()Scenario 3 - Agent with Tools‚ÄãRUN_NAME = "run-scenario-3"PROMPT_TEMPLATE = "Who is the oldest person alive? And what is their current age raised to the power of 1.51?"with Run(experiment_name=EXPERIMENT_NAME, run_name=RUN_NAME, sagemaker_session=session) as run:    # Create SageMaker Callback    sagemaker_callback = SageMakerCallbackHandler(run)    # Define LLM model with callback    llm = OpenAI(callbacks=[sagemaker_callback], **HPARAMS)    # Define tools    tools = load_tools(["serpapi", "llm-math"], llm=llm, callbacks=[sagemaker_callback])    # Initialize agent with all the tools    agent = initialize_agent(tools, llm, agent="zero-shot-react-description", callbacks=[sagemaker_callback])    # Run agent    agent.run(input=PROMPT_TEMPLATE)    # Reset the callback    sagemaker_callback.flush_tracker()Load Log Data‚ÄãOnce the prompts are logged, we can easily load and convert them to Pandas DataFrame as follows.#Loadlogs = ExperimentAnalytics(experiment_name=EXPERIMENT_NAME)#Convert as pandas dataframedf = logs.dataframe(force_refresh=True)print(df.shape)df.head()As can be seen above, there are three runs (rows) in the experiment corresponding to each scenario. Each run logs the prompts and related LLM settings/hyperparameters as json and are saved in s3 bucket. Feel free to load and explore the log data from each json path. pip install sagemakerpip install openaipip install google-search-results pip install sagemakerpip install openaipip install google-search-results  import os## Add your API keys belowos.environ["OPENAI_API_KEY"] = "<ADD-KEY-HERE>"os.environ["SERPAPI_API_KEY"] = "<ADD-KEY-HERE>" import os## Add your API keys belowos.environ["OPENAI_API_KEY"] = "<ADD-KEY-HERE>"os.environ["SERPAPI_API_KEY"] = "<ADD-KEY-HERE>"  from langchain.llms import OpenAIfrom langchain.prompts import PromptTemplatefrom langchain.chains import LLMChain, SimpleSequentialChainfrom langchain.agents import initialize_agent, load_toolsfrom langchain.agents import Toolfrom langchain.callbacks import SageMakerCallbackHandlerfrom sagemaker.analytics import ExperimentAnalyticsfrom sagemaker.session import Sessionfrom sagemaker.experiments.run import Run from langchain.llms import OpenAIfrom langchain.prompts import PromptTemplatefrom langchain.chains import LLMChain, SimpleSequentialChainfrom langchain.agents import initialize_agent, load_toolsfrom langchain.agents import Toolfrom langchain.callbacks import SageMakerCallbackHandlerfrom sagemaker.analytics import ExperimentAnalyticsfrom sagemaker.session import Sessionfrom sagemaker.experiments.run import Run  API Reference:OpenAIPromptTemplateLLMChainSimpleSequentialChaininitialize_agentload_toolsToolSageMakerCallbackHandler #LLM HyperparametersHPARAMS = {    "temperature": 0.1,    "model_name": "text-davinci-003",}#Bucket used to save prompt logs (Use `None` is used to save the default bucket or otherwise change it)BUCKET_NAME = None#Experiment nameEXPERIMENT_NAME = "langchain-sagemaker-tracker"#Create SageMaker Session with the given bucketsession = Session(default_bucket=BUCKET_NAME) #LLM HyperparametersHPARAMS = {    "temperature": 0.1,    "model_name": "text-davinci-003",}#Bucket used to save prompt logs (Use `None` is used to save the default bucket or otherwise change it)BUCKET_NAME = None#Experiment nameEXPERIMENT_NAME = "langchain-sagemaker-tracker"#Create SageMaker Session with the given bucketsession = Session(default_bucket=BUCKET_NAME)  RUN_NAME = "run-scenario-1"PROMPT_TEMPLATE = "tell me a joke about {topic}"INPUT_VARIABLES = {"topic": "fish"} RUN_NAME = "run-scenario-1"PROMPT_TEMPLATE = "tell me a joke about {topic}"INPUT_VARIABLES = {"topic": "fish"}  with Run(experiment_name=EXPERIMENT_NAME, run_name=RUN_NAME, sagemaker_session=session) as run:    # Create SageMaker Callback    sagemaker_callback = SageMakerCallbackHandler(run)    # Define LLM model with callback    llm = OpenAI(callbacks=[sagemaker_callback], **HPARAMS)    # Create prompt template    prompt = PromptTemplate.from_template(template=PROMPT_TEMPLATE)    # Create LLM Chain    chain = LLMChain(llm=llm, prompt=prompt, callbacks=[sagemaker_callback])    # Run chain    chain.run(**INPUT_VARIABLES)    # Reset the callback    sagemaker_callback.flush_tracker() with Run(experiment_name=EXPERIMENT_NAME, run_name=RUN_NAME, sagemaker_session=session) as run:    # Create SageMaker Callback    sagemaker_callback = SageMakerCallbackHandler(run)    # Define LLM model with callback    llm = OpenAI(callbacks=[sagemaker_callback], **HPARAMS)    # Create prompt template    prompt = PromptTemplate.from_template(template=PROMPT_TEMPLATE)    # Create LLM Chain    chain = LLMChain(llm=llm, prompt=prompt, callbacks=[sagemaker_callback])    # Run chain    chain.run(**INPUT_VARIABLES)    # Reset the callback    sagemaker_callback.flush_tracker()  RUN_NAME = "run-scenario-2"PROMPT_TEMPLATE_1 = """You are a playwright. Given the title of play, it is your job to write a synopsis for that title.Title: {title}Playwright: This is a synopsis for the above play:"""PROMPT_TEMPLATE_2 = """You are a play critic from the New York Times. Given the synopsis of play, it is your job to write a review for that play.Play Synopsis: {synopsis}Review from a New York Times play critic of the above play:"""INPUT_VARIABLES = {    "input": "documentary about good video games that push the boundary of game design"} RUN_NAME = "run-scenario-2"PROMPT_TEMPLATE_1 = """You are a playwright. Given the title of play, it is your job to write a synopsis for that title.Title: {title}Playwright: This is a synopsis for the above play:"""PROMPT_TEMPLATE_2 = """You are a play critic from the New York Times. Given the synopsis of play, it is your job to write a review for that play.Play Synopsis: {synopsis}Review from a New York Times play critic of the above play:"""INPUT_VARIABLES = {    "input": "documentary about good video games that push the boundary of game design"}  with Run(experiment_name=EXPERIMENT_NAME, run_name=RUN_NAME, sagemaker_session=session) as run:    # Create SageMaker Callback    sagemaker_callback = SageMakerCallbackHandler(run)    # Create prompt templates for the chain    prompt_template1 = PromptTemplate.from_template(template=PROMPT_TEMPLATE_1)    prompt_template2 = PromptTemplate.from_template(template=PROMPT_TEMPLATE_2)    # Define LLM model with callback    llm = OpenAI(callbacks=[sagemaker_callback], **HPARAMS)    # Create chain1    chain1 = LLMChain(llm=llm, prompt=prompt_template1, callbacks=[sagemaker_callback])    # Create chain2    chain2 = LLMChain(llm=llm, prompt=prompt_template2, callbacks=[sagemaker_callback])    # Create Sequential chain    overall_chain = SimpleSequentialChain(chains=[chain1, chain2], callbacks=[sagemaker_callback])    # Run overall sequential chain    overall_chain.run(**INPUT_VARIABLES)    # Reset the callback    sagemaker_callback.flush_tracker() with Run(experiment_name=EXPERIMENT_NAME, run_name=RUN_NAME, sagemaker_session=session) as run:    # Create SageMaker Callback    sagemaker_callback = SageMakerCallbackHandler(run)    # Create prompt templates for the chain    prompt_template1 = PromptTemplate.from_template(template=PROMPT_TEMPLATE_1)    prompt_template2 = PromptTemplate.from_template(template=PROMPT_TEMPLATE_2)    # Define LLM model with callback    llm = OpenAI(callbacks=[sagemaker_callback], **HPARAMS)    # Create chain1    chain1 = LLMChain(llm=llm, prompt=prompt_template1, callbacks=[sagemaker_callback])    # Create chain2    chain2 = LLMChain(llm=llm, prompt=prompt_template2, callbacks=[sagemaker_callback])    # Create Sequential chain    overall_chain = SimpleSequentialChain(chains=[chain1, chain2], callbacks=[sagemaker_callback])    # Run overall sequential chain    overall_chain.run(**INPUT_VARIABLES)    # Reset the callback    sagemaker_callback.flush_tracker()  RUN_NAME = "run-scenario-3"PROMPT_TEMPLATE = "Who is the oldest person alive? And what is their current age raised to the power of 1.51?" RUN_NAME = "run-scenario-3"PROMPT_TEMPLATE = "Who is the oldest person alive? And what is their current age raised to the power of 1.51?"  with Run(experiment_name=EXPERIMENT_NAME, run_name=RUN_NAME, sagemaker_session=session) as run:    # Create SageMaker Callback    sagemaker_callback = SageMakerCallbackHandler(run)    # Define LLM model with callback    llm = OpenAI(callbacks=[sagemaker_callback], **HPARAMS)    # Define tools    tools = load_tools(["serpapi", "llm-math"], llm=llm, callbacks=[sagemaker_callback])    # Initialize agent with all the tools    agent = initialize_agent(tools, llm, agent="zero-shot-react-description", callbacks=[sagemaker_callback])    # Run agent    agent.run(input=PROMPT_TEMPLATE)    # Reset the callback    sagemaker_callback.flush_tracker() with Run(experiment_name=EXPERIMENT_NAME, run_name=RUN_NAME, sagemaker_session=session) as run:    # Create SageMaker Callback    sagemaker_callback = SageMakerCallbackHandler(run)    # Define LLM model with callback    llm = OpenAI(callbacks=[sagemaker_callback], **HPARAMS)    # Define tools    tools = load_tools(["serpapi", "llm-math"], llm=llm, callbacks=[sagemaker_callback])    # Initialize agent with all the tools    agent = initialize_agent(tools, llm, agent="zero-shot-react-description", callbacks=[sagemaker_callback])    # Run agent    agent.run(input=PROMPT_TEMPLATE)    # Reset the callback    sagemaker_callback.flush_tracker()  #Loadlogs = ExperimentAnalytics(experiment_name=EXPERIMENT_NAME)#Convert as pandas dataframedf = logs.dataframe(force_refresh=True)print(df.shape)df.head() #Loadlogs = ExperimentAnalytics(experiment_name=EXPERIMENT_NAME)#Convert as pandas dataframedf = logs.dataframe(force_refresh=True)print(df.shape)df.head()  Previous SageMaker Endpoint Next ScaNN Installation and SetupLLM Prompt TrackingScenario 1 - LLMScenario 2 - Sequential ChainScenario 3 - Agent with ToolsLoad Log Data Installation and SetupLLM Prompt TrackingScenario 1 - LLMScenario 2 - Sequential ChainScenario 3 - Agent with ToolsLoad Log Data CommunityDiscordTwitterGitHubPythonJS/TSMoreHomepageBlogCopyright ¬© 2023 LangChain, Inc. CommunityDiscordTwitterGitHubPythonJS/TSMoreHomepageBlog CommunityDiscordTwitter Community GitHubPythonJS/TS GitHub MoreHomepageBlog More Copyright ¬© 2023 LangChain, Inc. Copyright ¬© 2023 LangChain, Inc. Skip to main content (#docusaurus_skipToContent_fallback) ü¶úÔ∏èüîó LangChain (/) Docs (/docs/get_started/introduction) Use cases (/docs/use_cases) Integrations (/docs/integrations) API (https://api.python.langchain.com) LangSmith (https://smith.langchain.com) JS/TS Docs (https://js.langchain.com/docs)  (https://github.com/hwchase17/langchain) Integrations (/docs/integrations) Callbacks (/docs/integrations/callbacks/) Chat models (/docs/integrations/chat/) Chat loaders (/docs/integrations/chat_loaders/) Document loaders (/docs/integrations/document_loaders/) Document transformers (/docs/integrations/document_transformers/) LLMs (/docs/integrations/llms/) Memory (/docs/integrations/memory/) Retrievers (/docs/integrations/retrievers/) Text embedding models (/docs/integrations/text_embedding/) Agents & Toolkits (/docs/integrations/toolkits/) Tools (/docs/integrations/tools/) Vector stores (/docs/integrations/vectorstores/) Grouped by provider (/docs/integrations/providers/) Activeloop Deep Lake (/docs/integrations/providers/activeloop_deeplake) AI21 Labs (/docs/integrations/providers/ai21) Aim (/docs/integrations/providers/aim_tracking) AINetwork (/docs/integrations/providers/ainetwork) Airbyte (/docs/integrations/providers/airbyte) Airtable (/docs/integrations/providers/airtable) Aleph Alpha (/docs/integrations/providers/aleph_alpha) Alibaba Cloud Opensearch (/docs/integrations/providers/alibabacloud_opensearch) Amazon API Gateway (/docs/integrations/providers/amazon_api_gateway) AnalyticDB (/docs/integrations/providers/analyticdb) Annoy (/docs/integrations/providers/annoy) Anyscale (/docs/integrations/providers/anyscale) Apify (/docs/integrations/providers/apify) ArangoDB (/docs/integrations/providers/arangodb) Argilla (/docs/integrations/providers/argilla) Arthur (/docs/integrations/providers/arthur_tracking) Arxiv (/docs/integrations/providers/arxiv) Atlas (/docs/integrations/providers/atlas) AwaDB (/docs/integrations/providers/awadb) AWS S3 Directory (/docs/integrations/providers/aws_s3) AZLyrics (/docs/integrations/providers/azlyrics) Azure Blob Storage (/docs/integrations/providers/azure_blob_storage) Azure Cognitive Search (/docs/integrations/providers/azure_cognitive_search_) Azure OpenAI (/docs/integrations/providers/azure_openai) BagelDB (/docs/integrations/providers/bageldb) Banana (/docs/integrations/providers/bananadev) Baseten (/docs/integrations/providers/baseten) Beam (/docs/integrations/providers/beam) Bedrock (/docs/integrations/providers/bedrock) BiliBili (/docs/integrations/providers/bilibili) NIBittensor (/docs/integrations/providers/bittensor) Blackboard (/docs/integrations/providers/blackboard) Brave Search (/docs/integrations/providers/brave_search) Cassandra (/docs/integrations/providers/cassandra) CerebriumAI (/docs/integrations/providers/cerebriumai) Chaindesk (/docs/integrations/providers/chaindesk) Chroma (/docs/integrations/providers/chroma) Clarifai (/docs/integrations/providers/clarifai) ClearML (/docs/integrations/providers/clearml_tracking) ClickHouse (/docs/integrations/providers/clickhouse) CnosDB (/docs/integrations/providers/cnosdb) Cohere (/docs/integrations/providers/cohere) College Confidential (/docs/integrations/providers/college_confidential) Comet (/docs/integrations/providers/comet_tracking) Confident AI (/docs/integrations/providers/confident) Confluence (/docs/integrations/providers/confluence) C Transformers (/docs/integrations/providers/ctransformers) DashVector (/docs/integrations/providers/dashvector) Databricks (/docs/integrations/providers/databricks) Datadog Tracing (/docs/integrations/providers/datadog) Datadog Logs (/docs/integrations/providers/datadog_logs) DataForSEO (/docs/integrations/providers/dataforseo) DeepInfra (/docs/integrations/providers/deepinfra) DeepSparse (/docs/integrations/providers/deepsparse) Diffbot (/docs/integrations/providers/diffbot) Dingo (/docs/integrations/providers/dingo) Discord (/docs/integrations/providers/discord) DocArray (/docs/integrations/providers/docarray) Docugami (/docs/integrations/providers/docugami) DuckDB (/docs/integrations/providers/duckdb) Elasticsearch (/docs/integrations/providers/elasticsearch) Epsilla (/docs/integrations/providers/epsilla) EverNote (/docs/integrations/providers/evernote) Facebook Chat (/docs/integrations/providers/facebook_chat) Facebook Faiss (/docs/integrations/providers/facebook_faiss) Figma (/docs/integrations/providers/figma) Fireworks (/docs/integrations/providers/fireworks) Flyte (/docs/integrations/providers/flyte) ForefrontAI (/docs/integrations/providers/forefrontai) Git (/docs/integrations/providers/git) GitBook (/docs/integrations/providers/gitbook) Golden (/docs/integrations/providers/golden) Google BigQuery (/docs/integrations/providers/google_bigquery) Google Cloud Storage (/docs/integrations/providers/google_cloud_storage) Google Drive (/docs/integrations/providers/google_drive) Google Search (/docs/integrations/providers/google_search) Google Serper (/docs/integrations/providers/google_serper) Google Vertex AI MatchingEngine (/docs/integrations/providers/google_vertex_ai_matchingengine) GooseAI (/docs/integrations/providers/gooseai) GPT4All (/docs/integrations/providers/gpt4all) Graphsignal (/docs/integrations/providers/graphsignal) Grobid (/docs/integrations/providers/grobid) Gutenberg (/docs/integrations/providers/gutenberg) Hacker News (/docs/integrations/providers/hacker_news) Hazy Research (/docs/integrations/providers/hazy_research) Helicone (/docs/integrations/providers/helicone) Hologres (/docs/integrations/providers/hologres) Hugging Face (/docs/integrations/providers/huggingface) iFixit (/docs/integrations/providers/ifixit) IMSDb (/docs/integrations/providers/imsdb) Infino (/docs/integrations/providers/infino) Jina (/docs/integrations/providers/jina) Konko (/docs/integrations/providers/konko) LanceDB (/docs/integrations/providers/lancedb) LangChain Decorators ‚ú® (/docs/integrations/providers/langchain_decorators) Llama.cpp (/docs/integrations/providers/llamacpp) Log10 (/docs/integrations/providers/log10) Marqo (/docs/integrations/providers/marqo) MediaWikiDump (/docs/integrations/providers/mediawikidump) Meilisearch (/docs/integrations/providers/meilisearch) Metal (/docs/integrations/providers/metal) Microsoft OneDrive (/docs/integrations/providers/microsoft_onedrive) Microsoft PowerPoint (/docs/integrations/providers/microsoft_powerpoint) Microsoft Word (/docs/integrations/providers/microsoft_word) Milvus (/docs/integrations/providers/milvus) Minimax (/docs/integrations/providers/minimax) MLflow AI Gateway (/docs/integrations/providers/mlflow_ai_gateway) MLflow (/docs/integrations/providers/mlflow_tracking) Modal (/docs/integrations/providers/modal) ModelScope (/docs/integrations/providers/modelscope) Modern Treasury (/docs/integrations/providers/modern_treasury) Momento (/docs/integrations/providers/momento) MongoDB Atlas (/docs/integrations/providers/mongodb_atlas) Motherduck (/docs/integrations/providers/motherduck) MyScale (/docs/integrations/providers/myscale) Neo4j (/docs/integrations/providers/neo4j) NLPCloud (/docs/integrations/providers/nlpcloud) Notion DB (/docs/integrations/providers/notion) Obsidian (/docs/integrations/providers/obsidian) OpenAI (/docs/integrations/providers/openai) OpenLLM (/docs/integrations/providers/openllm) OpenSearch (/docs/integrations/providers/opensearch) OpenWeatherMap (/docs/integrations/providers/openweathermap) Petals (/docs/integrations/providers/petals) Postgres Embedding (/docs/integrations/providers/pg_embedding) PGVector (/docs/integrations/providers/pgvector) Pinecone (/docs/integrations/providers/pinecone) PipelineAI (/docs/integrations/providers/pipelineai) Portkey (/docs/integrations/providers/portkey/) Predibase (/docs/integrations/providers/predibase) Prediction Guard (/docs/integrations/providers/predictionguard) PromptLayer (/docs/integrations/providers/promptlayer) Psychic (/docs/integrations/providers/psychic) PubMed (/docs/integrations/providers/pubmed) Qdrant (/docs/integrations/providers/qdrant) Ray Serve (/docs/integrations/providers/ray_serve) Rebuff (/docs/integrations/providers/rebuff) Reddit (/docs/integrations/providers/reddit) Redis (/docs/integrations/providers/redis) Replicate (/docs/integrations/providers/replicate) Roam (/docs/integrations/providers/roam) Rockset (/docs/integrations/providers/rockset) Runhouse (/docs/integrations/providers/runhouse) RWKV-4 (/docs/integrations/providers/rwkv) SageMaker Endpoint (/docs/integrations/providers/sagemaker_endpoint) SageMaker Tracking (/docs/integrations/providers/sagemaker_tracking) ScaNN (/docs/integrations/providers/scann) SearxNG Search API (/docs/integrations/providers/searx) SerpAPI (/docs/integrations/providers/serpapi) Shale Protocol (/docs/integrations/providers/shaleprotocol) SingleStoreDB (/docs/integrations/providers/singlestoredb) scikit-learn (/docs/integrations/providers/sklearn) Slack (/docs/integrations/providers/slack) spaCy (/docs/integrations/providers/spacy) Spreedly (/docs/integrations/providers/spreedly) StarRocks (/docs/integrations/providers/starrocks) StochasticAI (/docs/integrations/providers/stochasticai) Stripe (/docs/integrations/providers/stripe) Supabase (Postgres) (/docs/integrations/providers/supabase) Nebula (/docs/integrations/providers/symblai_nebula) Tair (/docs/integrations/providers/tair) Telegram (/docs/integrations/providers/telegram) TencentVectorDB (/docs/integrations/providers/tencentvectordb) TensorFlow Datasets (/docs/integrations/providers/tensorflow_datasets) Tigris (/docs/integrations/providers/tigris) 2Markdown (/docs/integrations/providers/tomarkdown) Trello (/docs/integrations/providers/trello) TruLens (/docs/integrations/providers/trulens) Twitter (/docs/integrations/providers/twitter) Typesense (/docs/integrations/providers/typesense) Unstructured (/docs/integrations/providers/unstructured) USearch (/docs/integrations/providers/usearch) Vearch (/docs/integrations/providers/vearch) Vectara (/docs/integrations/providers/vectara/) Vespa (/docs/integrations/providers/vespa) WandB Tracing (/docs/integrations/providers/wandb_tracing) Weights & Biases (/docs/integrations/providers/wandb_tracking) Weather (/docs/integrations/providers/weather) Weaviate (/docs/integrations/providers/weaviate) WhatsApp (/docs/integrations/providers/whatsapp) WhyLabs (/docs/integrations/providers/whylabs_profiling) Wikipedia (/docs/integrations/providers/wikipedia) Wolfram Alpha (/docs/integrations/providers/wolfram_alpha) Writer (/docs/integrations/providers/writer) Xata (/docs/integrations/providers/xata) Xorbits Inference (Xinference) (/docs/integrations/providers/xinference) Yeager.ai (/docs/integrations/providers/yeagerai) YouTube (/docs/integrations/providers/youtube) Zep (/docs/integrations/providers/zep) Zilliz (/docs/integrations/providers/zilliz)  (/) Integrations (/docs/integrations) Grouped by provider (/docs/integrations/providers/) Amazon SageMaker (https://aws.amazon.com/sagemaker/) Amazon SageMaker Experiments (https://docs.aws.amazon.com/sagemaker/latest/dg/experiments.html) ‚Äã (#installation-and-setup) https://platform.openai.com/account/api-keys (https://platform.openai.com/account/api-keys) https://serpapi.com/manage-api-key (https://serpapi.com/manage-api-key) OpenAI (https://api.python.langchain.com/en/latest/llms/langchain.llms.openai.OpenAI.html) PromptTemplate (https://api.python.langchain.com/en/latest/prompts/langchain.prompts.prompt.PromptTemplate.html) LLMChain (https://api.python.langchain.com/en/latest/chains/langchain.chains.llm.LLMChain.html) SimpleSequentialChain (https://api.python.langchain.com/en/latest/chains/langchain.chains.sequential.SimpleSequentialChain.html) initialize_agent (https://api.python.langchain.com/en/latest/agents/langchain.agents.initialize.initialize_agent.html) load_tools (https://api.python.langchain.com/en/latest/agents/langchain.agents.load_tools.load_tools.html) Tool (https://api.python.langchain.com/en/latest/tools/langchain.tools.base.Tool.html) SageMakerCallbackHandler (https://api.python.langchain.com/en/latest/callbacks/langchain.callbacks.sagemaker_callback.SageMakerCallbackHandler.html) ‚Äã (#llm-prompt-tracking) ‚Äã (#scenario-1---llm) ‚Äã (#scenario-2---sequential-chain) ‚Äã (#scenario-3---agent-with-tools) ‚Äã (#load-log-data) PreviousSageMaker Endpoint (/docs/integrations/providers/sagemaker_endpoint) NextScaNN (/docs/integrations/providers/scann) Installation and Setup (#installation-and-setup) LLM Prompt Tracking (#llm-prompt-tracking) Scenario 1 - LLM (#scenario-1---llm) Scenario 2 - Sequential Chain (#scenario-2---sequential-chain) Scenario 3 - Agent with Tools (#scenario-3---agent-with-tools) Load Log Data (#load-log-data) Discord (https://discord.gg/cU2adEyC7w) Twitter (https://twitter.com/LangChainAI) Python (https://github.com/hwchase17/langchain) JS/TS (https://github.com/hwchase17/langchainjs) Homepage (https://langchain.com) Blog (https://blog.langchain.dev)