Let's load the SageMaker Endpoints Embeddings class. The class can be used if you host, e.g. your own Hugging Face model on SageMaker. For instructions on how to do this, please see here.  Note: In order to handle batched requests, you will need to adjust the return line in the predict_fn() function within the custom inference.py script: Change from return {"vectors": sentence_embeddings[0].tolist()} to: return {"vectors": sentence_embeddings.tolist()}. IntegrationsCallbacksChat modelsChat loadersDocument loadersDocument transformersLLMsMemoryRetrieversText embedding modelsAleph AlphaAwaDBAzureOpenAIBedrockBGE on Hugging FaceClarifaiCohereDashScopeDeepInfraEDEN AIElasticsearchEmbaasERNIE Embedding-V1Fake EmbeddingsGoogle Vertex AI PaLMGPT4AllHugging FaceInstructEmbeddingsJinaLlama-cppLocalAIMiniMaxModelScopeMosaicMLNLP CloudOpenAISageMakerSelf HostedSentence TransformersSpaCyTensorflowHubXorbits inference (Xinference)Agents & ToolkitsToolsVector storesGrouped by provider Callbacks Chat models Chat loaders Document loaders Document transformers LLMs Memory Retrievers Text embedding modelsAleph AlphaAwaDBAzureOpenAIBedrockBGE on Hugging FaceClarifaiCohereDashScopeDeepInfraEDEN AIElasticsearchEmbaasERNIE Embedding-V1Fake EmbeddingsGoogle Vertex AI PaLMGPT4AllHugging FaceInstructEmbeddingsJinaLlama-cppLocalAIMiniMaxModelScopeMosaicMLNLP CloudOpenAISageMakerSelf HostedSentence TransformersSpaCyTensorflowHubXorbits inference (Xinference) Aleph Alpha AwaDB AzureOpenAI Bedrock BGE on Hugging Face Clarifai Cohere DashScope DeepInfra EDEN AI Elasticsearch Embaas ERNIE Embedding-V1 Fake Embeddings Google Vertex AI PaLM GPT4All Hugging Face InstructEmbeddings Jina Llama-cpp LocalAI MiniMax ModelScope MosaicML NLP Cloud OpenAI SageMaker Self Hosted Sentence Transformers SpaCy TensorflowHub Xorbits inference (Xinference) Agents & Toolkits Tools Vector stores Grouped by provider  Integrations Text embedding models SageMaker SagemakerEndpointEmbeddings EmbeddingsContentHandler Discord Twitter Python JS/TS Homepage Blog Skip to main content🦜️🔗 LangChainDocsUse casesIntegrationsAPILangSmithJS/TS DocsCTRLKIntegrationsCallbacksChat modelsChat loadersDocument loadersDocument transformersLLMsMemoryRetrieversText embedding modelsAleph AlphaAwaDBAzureOpenAIBedrockBGE on Hugging FaceClarifaiCohereDashScopeDeepInfraEDEN AIElasticsearchEmbaasERNIE Embedding-V1Fake EmbeddingsGoogle Vertex AI PaLMGPT4AllHugging FaceInstructEmbeddingsJinaLlama-cppLocalAIMiniMaxModelScopeMosaicMLNLP CloudOpenAISageMakerSelf HostedSentence TransformersSpaCyTensorflowHubXorbits inference (Xinference)Agents & ToolkitsToolsVector storesGrouped by providerIntegrationsText embedding modelsSageMakerSageMakerLet's load the SageMaker Endpoints Embeddings class. The class can be used if you host, e.g. your own Hugging Face model on SageMaker.For instructions on how to do this, please see here. Note: In order to handle batched requests, you will need to adjust the return line in the predict_fn() function within the custom inference.py script:Change fromreturn {"vectors": sentence_embeddings[0].tolist()}to:return {"vectors": sentence_embeddings.tolist()}.pip3 install langchain boto3from typing import Dict, Listfrom langchain.embeddings import SagemakerEndpointEmbeddingsfrom langchain.embeddings.sagemaker_endpoint import EmbeddingsContentHandlerimport jsonclass ContentHandler(EmbeddingsContentHandler):    content_type = "application/json"    accepts = "application/json"    def transform_input(self, inputs: list[str], model_kwargs: Dict) -> bytes:        """        Transforms the input into bytes that can be consumed by SageMaker endpoint.        Args:            inputs: List of input strings.            model_kwargs: Additional keyword arguments to be passed to the endpoint.        Returns:            The transformed bytes input.        """        # Example: inference.py expects a JSON string with a "inputs" key:        input_str = json.dumps({"inputs": inputs, **model_kwargs})          return input_str.encode("utf-8")    def transform_output(self, output: bytes) -> List[List[float]]:        """        Transforms the bytes output from the endpoint into a list of embeddings.        Args:            output: The bytes output from SageMaker endpoint.        Returns:            The transformed output - list of embeddings        Note:            The length of the outer list is the number of input strings.            The length of the inner lists is the embedding dimension.        """        # Example: inference.py returns a JSON string with the list of        # embeddings in a "vectors" key:        response_json = json.loads(output.read().decode("utf-8"))        return response_json["vectors"]content_handler = ContentHandler()embeddings = SagemakerEndpointEmbeddings(    # credentials_profile_name="credentials-profile-name",    endpoint_name="huggingface-pytorch-inference-2023-03-21-16-14-03-834",    region_name="us-east-1",    content_handler=content_handler,)API Reference:SagemakerEndpointEmbeddingsEmbeddingsContentHandlerquery_result = embeddings.embed_query("foo")doc_results = embeddings.embed_documents(["foo"])doc_resultsPreviousOpenAINextSelf HostedCommunityDiscordTwitterGitHubPythonJS/TSMoreHomepageBlogCopyright © 2023 LangChain, Inc. Skip to main content 🦜️🔗 LangChainDocsUse casesIntegrationsAPILangSmithJS/TS DocsCTRLK 🦜️🔗 LangChainDocsUse casesIntegrationsAPI LangSmithJS/TS DocsCTRLK  CTRLK CTRLK  CTRLK   IntegrationsCallbacksChat modelsChat loadersDocument loadersDocument transformersLLMsMemoryRetrieversText embedding modelsAleph AlphaAwaDBAzureOpenAIBedrockBGE on Hugging FaceClarifaiCohereDashScopeDeepInfraEDEN AIElasticsearchEmbaasERNIE Embedding-V1Fake EmbeddingsGoogle Vertex AI PaLMGPT4AllHugging FaceInstructEmbeddingsJinaLlama-cppLocalAIMiniMaxModelScopeMosaicMLNLP CloudOpenAISageMakerSelf HostedSentence TransformersSpaCyTensorflowHubXorbits inference (Xinference)Agents & ToolkitsToolsVector storesGrouped by providerIntegrationsText embedding modelsSageMakerSageMakerLet's load the SageMaker Endpoints Embeddings class. The class can be used if you host, e.g. your own Hugging Face model on SageMaker.For instructions on how to do this, please see here. Note: In order to handle batched requests, you will need to adjust the return line in the predict_fn() function within the custom inference.py script:Change fromreturn {"vectors": sentence_embeddings[0].tolist()}to:return {"vectors": sentence_embeddings.tolist()}.pip3 install langchain boto3from typing import Dict, Listfrom langchain.embeddings import SagemakerEndpointEmbeddingsfrom langchain.embeddings.sagemaker_endpoint import EmbeddingsContentHandlerimport jsonclass ContentHandler(EmbeddingsContentHandler):    content_type = "application/json"    accepts = "application/json"    def transform_input(self, inputs: list[str], model_kwargs: Dict) -> bytes:        """        Transforms the input into bytes that can be consumed by SageMaker endpoint.        Args:            inputs: List of input strings.            model_kwargs: Additional keyword arguments to be passed to the endpoint.        Returns:            The transformed bytes input.        """        # Example: inference.py expects a JSON string with a "inputs" key:        input_str = json.dumps({"inputs": inputs, **model_kwargs})          return input_str.encode("utf-8")    def transform_output(self, output: bytes) -> List[List[float]]:        """        Transforms the bytes output from the endpoint into a list of embeddings.        Args:            output: The bytes output from SageMaker endpoint.        Returns:            The transformed output - list of embeddings        Note:            The length of the outer list is the number of input strings.            The length of the inner lists is the embedding dimension.        """        # Example: inference.py returns a JSON string with the list of        # embeddings in a "vectors" key:        response_json = json.loads(output.read().decode("utf-8"))        return response_json["vectors"]content_handler = ContentHandler()embeddings = SagemakerEndpointEmbeddings(    # credentials_profile_name="credentials-profile-name",    endpoint_name="huggingface-pytorch-inference-2023-03-21-16-14-03-834",    region_name="us-east-1",    content_handler=content_handler,)API Reference:SagemakerEndpointEmbeddingsEmbeddingsContentHandlerquery_result = embeddings.embed_query("foo")doc_results = embeddings.embed_documents(["foo"])doc_resultsPreviousOpenAINextSelf Hosted IntegrationsCallbacksChat modelsChat loadersDocument loadersDocument transformersLLMsMemoryRetrieversText embedding modelsAleph AlphaAwaDBAzureOpenAIBedrockBGE on Hugging FaceClarifaiCohereDashScopeDeepInfraEDEN AIElasticsearchEmbaasERNIE Embedding-V1Fake EmbeddingsGoogle Vertex AI PaLMGPT4AllHugging FaceInstructEmbeddingsJinaLlama-cppLocalAIMiniMaxModelScopeMosaicMLNLP CloudOpenAISageMakerSelf HostedSentence TransformersSpaCyTensorflowHubXorbits inference (Xinference)Agents & ToolkitsToolsVector storesGrouped by providerIntegrationsText embedding modelsSageMakerSageMakerLet's load the SageMaker Endpoints Embeddings class. The class can be used if you host, e.g. your own Hugging Face model on SageMaker.For instructions on how to do this, please see here. Note: In order to handle batched requests, you will need to adjust the return line in the predict_fn() function within the custom inference.py script:Change fromreturn {"vectors": sentence_embeddings[0].tolist()}to:return {"vectors": sentence_embeddings.tolist()}.pip3 install langchain boto3from typing import Dict, Listfrom langchain.embeddings import SagemakerEndpointEmbeddingsfrom langchain.embeddings.sagemaker_endpoint import EmbeddingsContentHandlerimport jsonclass ContentHandler(EmbeddingsContentHandler):    content_type = "application/json"    accepts = "application/json"    def transform_input(self, inputs: list[str], model_kwargs: Dict) -> bytes:        """        Transforms the input into bytes that can be consumed by SageMaker endpoint.        Args:            inputs: List of input strings.            model_kwargs: Additional keyword arguments to be passed to the endpoint.        Returns:            The transformed bytes input.        """        # Example: inference.py expects a JSON string with a "inputs" key:        input_str = json.dumps({"inputs": inputs, **model_kwargs})          return input_str.encode("utf-8")    def transform_output(self, output: bytes) -> List[List[float]]:        """        Transforms the bytes output from the endpoint into a list of embeddings.        Args:            output: The bytes output from SageMaker endpoint.        Returns:            The transformed output - list of embeddings        Note:            The length of the outer list is the number of input strings.            The length of the inner lists is the embedding dimension.        """        # Example: inference.py returns a JSON string with the list of        # embeddings in a "vectors" key:        response_json = json.loads(output.read().decode("utf-8"))        return response_json["vectors"]content_handler = ContentHandler()embeddings = SagemakerEndpointEmbeddings(    # credentials_profile_name="credentials-profile-name",    endpoint_name="huggingface-pytorch-inference-2023-03-21-16-14-03-834",    region_name="us-east-1",    content_handler=content_handler,)API Reference:SagemakerEndpointEmbeddingsEmbeddingsContentHandlerquery_result = embeddings.embed_query("foo")doc_results = embeddings.embed_documents(["foo"])doc_resultsPreviousOpenAINextSelf Hosted IntegrationsCallbacksChat modelsChat loadersDocument loadersDocument transformersLLMsMemoryRetrieversText embedding modelsAleph AlphaAwaDBAzureOpenAIBedrockBGE on Hugging FaceClarifaiCohereDashScopeDeepInfraEDEN AIElasticsearchEmbaasERNIE Embedding-V1Fake EmbeddingsGoogle Vertex AI PaLMGPT4AllHugging FaceInstructEmbeddingsJinaLlama-cppLocalAIMiniMaxModelScopeMosaicMLNLP CloudOpenAISageMakerSelf HostedSentence TransformersSpaCyTensorflowHubXorbits inference (Xinference)Agents & ToolkitsToolsVector storesGrouped by provider IntegrationsCallbacksChat modelsChat loadersDocument loadersDocument transformersLLMsMemoryRetrieversText embedding modelsAleph AlphaAwaDBAzureOpenAIBedrockBGE on Hugging FaceClarifaiCohereDashScopeDeepInfraEDEN AIElasticsearchEmbaasERNIE Embedding-V1Fake EmbeddingsGoogle Vertex AI PaLMGPT4AllHugging FaceInstructEmbeddingsJinaLlama-cppLocalAIMiniMaxModelScopeMosaicMLNLP CloudOpenAISageMakerSelf HostedSentence TransformersSpaCyTensorflowHubXorbits inference (Xinference)Agents & ToolkitsToolsVector storesGrouped by provider Integrations Callbacks Chat models Chat loaders Document loaders Document transformers LLMs Memory Retrievers Text embedding models Agents & Toolkits Tools Vector stores Grouped by provider IntegrationsText embedding modelsSageMakerSageMakerLet's load the SageMaker Endpoints Embeddings class. The class can be used if you host, e.g. your own Hugging Face model on SageMaker.For instructions on how to do this, please see here. Note: In order to handle batched requests, you will need to adjust the return line in the predict_fn() function within the custom inference.py script:Change fromreturn {"vectors": sentence_embeddings[0].tolist()}to:return {"vectors": sentence_embeddings.tolist()}.pip3 install langchain boto3from typing import Dict, Listfrom langchain.embeddings import SagemakerEndpointEmbeddingsfrom langchain.embeddings.sagemaker_endpoint import EmbeddingsContentHandlerimport jsonclass ContentHandler(EmbeddingsContentHandler):    content_type = "application/json"    accepts = "application/json"    def transform_input(self, inputs: list[str], model_kwargs: Dict) -> bytes:        """        Transforms the input into bytes that can be consumed by SageMaker endpoint.        Args:            inputs: List of input strings.            model_kwargs: Additional keyword arguments to be passed to the endpoint.        Returns:            The transformed bytes input.        """        # Example: inference.py expects a JSON string with a "inputs" key:        input_str = json.dumps({"inputs": inputs, **model_kwargs})          return input_str.encode("utf-8")    def transform_output(self, output: bytes) -> List[List[float]]:        """        Transforms the bytes output from the endpoint into a list of embeddings.        Args:            output: The bytes output from SageMaker endpoint.        Returns:            The transformed output - list of embeddings        Note:            The length of the outer list is the number of input strings.            The length of the inner lists is the embedding dimension.        """        # Example: inference.py returns a JSON string with the list of        # embeddings in a "vectors" key:        response_json = json.loads(output.read().decode("utf-8"))        return response_json["vectors"]content_handler = ContentHandler()embeddings = SagemakerEndpointEmbeddings(    # credentials_profile_name="credentials-profile-name",    endpoint_name="huggingface-pytorch-inference-2023-03-21-16-14-03-834",    region_name="us-east-1",    content_handler=content_handler,)API Reference:SagemakerEndpointEmbeddingsEmbeddingsContentHandlerquery_result = embeddings.embed_query("foo")doc_results = embeddings.embed_documents(["foo"])doc_resultsPreviousOpenAINextSelf Hosted IntegrationsText embedding modelsSageMakerSageMakerLet's load the SageMaker Endpoints Embeddings class. The class can be used if you host, e.g. your own Hugging Face model on SageMaker.For instructions on how to do this, please see here. Note: In order to handle batched requests, you will need to adjust the return line in the predict_fn() function within the custom inference.py script:Change fromreturn {"vectors": sentence_embeddings[0].tolist()}to:return {"vectors": sentence_embeddings.tolist()}.pip3 install langchain boto3from typing import Dict, Listfrom langchain.embeddings import SagemakerEndpointEmbeddingsfrom langchain.embeddings.sagemaker_endpoint import EmbeddingsContentHandlerimport jsonclass ContentHandler(EmbeddingsContentHandler):    content_type = "application/json"    accepts = "application/json"    def transform_input(self, inputs: list[str], model_kwargs: Dict) -> bytes:        """        Transforms the input into bytes that can be consumed by SageMaker endpoint.        Args:            inputs: List of input strings.            model_kwargs: Additional keyword arguments to be passed to the endpoint.        Returns:            The transformed bytes input.        """        # Example: inference.py expects a JSON string with a "inputs" key:        input_str = json.dumps({"inputs": inputs, **model_kwargs})          return input_str.encode("utf-8")    def transform_output(self, output: bytes) -> List[List[float]]:        """        Transforms the bytes output from the endpoint into a list of embeddings.        Args:            output: The bytes output from SageMaker endpoint.        Returns:            The transformed output - list of embeddings        Note:            The length of the outer list is the number of input strings.            The length of the inner lists is the embedding dimension.        """        # Example: inference.py returns a JSON string with the list of        # embeddings in a "vectors" key:        response_json = json.loads(output.read().decode("utf-8"))        return response_json["vectors"]content_handler = ContentHandler()embeddings = SagemakerEndpointEmbeddings(    # credentials_profile_name="credentials-profile-name",    endpoint_name="huggingface-pytorch-inference-2023-03-21-16-14-03-834",    region_name="us-east-1",    content_handler=content_handler,)API Reference:SagemakerEndpointEmbeddingsEmbeddingsContentHandlerquery_result = embeddings.embed_query("foo")doc_results = embeddings.embed_documents(["foo"])doc_resultsPreviousOpenAINextSelf Hosted IntegrationsText embedding modelsSageMakerSageMakerLet's load the SageMaker Endpoints Embeddings class. The class can be used if you host, e.g. your own Hugging Face model on SageMaker.For instructions on how to do this, please see here. Note: In order to handle batched requests, you will need to adjust the return line in the predict_fn() function within the custom inference.py script:Change fromreturn {"vectors": sentence_embeddings[0].tolist()}to:return {"vectors": sentence_embeddings.tolist()}.pip3 install langchain boto3from typing import Dict, Listfrom langchain.embeddings import SagemakerEndpointEmbeddingsfrom langchain.embeddings.sagemaker_endpoint import EmbeddingsContentHandlerimport jsonclass ContentHandler(EmbeddingsContentHandler):    content_type = "application/json"    accepts = "application/json"    def transform_input(self, inputs: list[str], model_kwargs: Dict) -> bytes:        """        Transforms the input into bytes that can be consumed by SageMaker endpoint.        Args:            inputs: List of input strings.            model_kwargs: Additional keyword arguments to be passed to the endpoint.        Returns:            The transformed bytes input.        """        # Example: inference.py expects a JSON string with a "inputs" key:        input_str = json.dumps({"inputs": inputs, **model_kwargs})          return input_str.encode("utf-8")    def transform_output(self, output: bytes) -> List[List[float]]:        """        Transforms the bytes output from the endpoint into a list of embeddings.        Args:            output: The bytes output from SageMaker endpoint.        Returns:            The transformed output - list of embeddings        Note:            The length of the outer list is the number of input strings.            The length of the inner lists is the embedding dimension.        """        # Example: inference.py returns a JSON string with the list of        # embeddings in a "vectors" key:        response_json = json.loads(output.read().decode("utf-8"))        return response_json["vectors"]content_handler = ContentHandler()embeddings = SagemakerEndpointEmbeddings(    # credentials_profile_name="credentials-profile-name",    endpoint_name="huggingface-pytorch-inference-2023-03-21-16-14-03-834",    region_name="us-east-1",    content_handler=content_handler,)API Reference:SagemakerEndpointEmbeddingsEmbeddingsContentHandlerquery_result = embeddings.embed_query("foo")doc_results = embeddings.embed_documents(["foo"])doc_resultsPreviousOpenAINextSelf Hosted IntegrationsText embedding modelsSageMakerSageMakerLet's load the SageMaker Endpoints Embeddings class. The class can be used if you host, e.g. your own Hugging Face model on SageMaker.For instructions on how to do this, please see here. Note: In order to handle batched requests, you will need to adjust the return line in the predict_fn() function within the custom inference.py script:Change fromreturn {"vectors": sentence_embeddings[0].tolist()}to:return {"vectors": sentence_embeddings.tolist()}.pip3 install langchain boto3from typing import Dict, Listfrom langchain.embeddings import SagemakerEndpointEmbeddingsfrom langchain.embeddings.sagemaker_endpoint import EmbeddingsContentHandlerimport jsonclass ContentHandler(EmbeddingsContentHandler):    content_type = "application/json"    accepts = "application/json"    def transform_input(self, inputs: list[str], model_kwargs: Dict) -> bytes:        """        Transforms the input into bytes that can be consumed by SageMaker endpoint.        Args:            inputs: List of input strings.            model_kwargs: Additional keyword arguments to be passed to the endpoint.        Returns:            The transformed bytes input.        """        # Example: inference.py expects a JSON string with a "inputs" key:        input_str = json.dumps({"inputs": inputs, **model_kwargs})          return input_str.encode("utf-8")    def transform_output(self, output: bytes) -> List[List[float]]:        """        Transforms the bytes output from the endpoint into a list of embeddings.        Args:            output: The bytes output from SageMaker endpoint.        Returns:            The transformed output - list of embeddings        Note:            The length of the outer list is the number of input strings.            The length of the inner lists is the embedding dimension.        """        # Example: inference.py returns a JSON string with the list of        # embeddings in a "vectors" key:        response_json = json.loads(output.read().decode("utf-8"))        return response_json["vectors"]content_handler = ContentHandler()embeddings = SagemakerEndpointEmbeddings(    # credentials_profile_name="credentials-profile-name",    endpoint_name="huggingface-pytorch-inference-2023-03-21-16-14-03-834",    region_name="us-east-1",    content_handler=content_handler,)API Reference:SagemakerEndpointEmbeddingsEmbeddingsContentHandlerquery_result = embeddings.embed_query("foo")doc_results = embeddings.embed_documents(["foo"])doc_resultsPreviousOpenAINextSelf Hosted SageMakerLet's load the SageMaker Endpoints Embeddings class. The class can be used if you host, e.g. your own Hugging Face model on SageMaker.For instructions on how to do this, please see here. Note: In order to handle batched requests, you will need to adjust the return line in the predict_fn() function within the custom inference.py script:Change fromreturn {"vectors": sentence_embeddings[0].tolist()}to:return {"vectors": sentence_embeddings.tolist()}.pip3 install langchain boto3from typing import Dict, Listfrom langchain.embeddings import SagemakerEndpointEmbeddingsfrom langchain.embeddings.sagemaker_endpoint import EmbeddingsContentHandlerimport jsonclass ContentHandler(EmbeddingsContentHandler):    content_type = "application/json"    accepts = "application/json"    def transform_input(self, inputs: list[str], model_kwargs: Dict) -> bytes:        """        Transforms the input into bytes that can be consumed by SageMaker endpoint.        Args:            inputs: List of input strings.            model_kwargs: Additional keyword arguments to be passed to the endpoint.        Returns:            The transformed bytes input.        """        # Example: inference.py expects a JSON string with a "inputs" key:        input_str = json.dumps({"inputs": inputs, **model_kwargs})          return input_str.encode("utf-8")    def transform_output(self, output: bytes) -> List[List[float]]:        """        Transforms the bytes output from the endpoint into a list of embeddings.        Args:            output: The bytes output from SageMaker endpoint.        Returns:            The transformed output - list of embeddings        Note:            The length of the outer list is the number of input strings.            The length of the inner lists is the embedding dimension.        """        # Example: inference.py returns a JSON string with the list of        # embeddings in a "vectors" key:        response_json = json.loads(output.read().decode("utf-8"))        return response_json["vectors"]content_handler = ContentHandler()embeddings = SagemakerEndpointEmbeddings(    # credentials_profile_name="credentials-profile-name",    endpoint_name="huggingface-pytorch-inference-2023-03-21-16-14-03-834",    region_name="us-east-1",    content_handler=content_handler,)API Reference:SagemakerEndpointEmbeddingsEmbeddingsContentHandlerquery_result = embeddings.embed_query("foo")doc_results = embeddings.embed_documents(["foo"])doc_results pip3 install langchain boto3 pip3 install langchain boto3  from typing import Dict, Listfrom langchain.embeddings import SagemakerEndpointEmbeddingsfrom langchain.embeddings.sagemaker_endpoint import EmbeddingsContentHandlerimport jsonclass ContentHandler(EmbeddingsContentHandler):    content_type = "application/json"    accepts = "application/json"    def transform_input(self, inputs: list[str], model_kwargs: Dict) -> bytes:        """        Transforms the input into bytes that can be consumed by SageMaker endpoint.        Args:            inputs: List of input strings.            model_kwargs: Additional keyword arguments to be passed to the endpoint.        Returns:            The transformed bytes input.        """        # Example: inference.py expects a JSON string with a "inputs" key:        input_str = json.dumps({"inputs": inputs, **model_kwargs})          return input_str.encode("utf-8")    def transform_output(self, output: bytes) -> List[List[float]]:        """        Transforms the bytes output from the endpoint into a list of embeddings.        Args:            output: The bytes output from SageMaker endpoint.        Returns:            The transformed output - list of embeddings        Note:            The length of the outer list is the number of input strings.            The length of the inner lists is the embedding dimension.        """        # Example: inference.py returns a JSON string with the list of        # embeddings in a "vectors" key:        response_json = json.loads(output.read().decode("utf-8"))        return response_json["vectors"]content_handler = ContentHandler()embeddings = SagemakerEndpointEmbeddings(    # credentials_profile_name="credentials-profile-name",    endpoint_name="huggingface-pytorch-inference-2023-03-21-16-14-03-834",    region_name="us-east-1",    content_handler=content_handler,) from typing import Dict, Listfrom langchain.embeddings import SagemakerEndpointEmbeddingsfrom langchain.embeddings.sagemaker_endpoint import EmbeddingsContentHandlerimport jsonclass ContentHandler(EmbeddingsContentHandler):    content_type = "application/json"    accepts = "application/json"    def transform_input(self, inputs: list[str], model_kwargs: Dict) -> bytes:        """        Transforms the input into bytes that can be consumed by SageMaker endpoint.        Args:            inputs: List of input strings.            model_kwargs: Additional keyword arguments to be passed to the endpoint.        Returns:            The transformed bytes input.        """        # Example: inference.py expects a JSON string with a "inputs" key:        input_str = json.dumps({"inputs": inputs, **model_kwargs})          return input_str.encode("utf-8")    def transform_output(self, output: bytes) -> List[List[float]]:        """        Transforms the bytes output from the endpoint into a list of embeddings.        Args:            output: The bytes output from SageMaker endpoint.        Returns:            The transformed output - list of embeddings        Note:            The length of the outer list is the number of input strings.            The length of the inner lists is the embedding dimension.        """        # Example: inference.py returns a JSON string with the list of        # embeddings in a "vectors" key:        response_json = json.loads(output.read().decode("utf-8"))        return response_json["vectors"]content_handler = ContentHandler()embeddings = SagemakerEndpointEmbeddings(    # credentials_profile_name="credentials-profile-name",    endpoint_name="huggingface-pytorch-inference-2023-03-21-16-14-03-834",    region_name="us-east-1",    content_handler=content_handler,)  API Reference:SagemakerEndpointEmbeddingsEmbeddingsContentHandler query_result = embeddings.embed_query("foo") query_result = embeddings.embed_query("foo")  doc_results = embeddings.embed_documents(["foo"]) doc_results = embeddings.embed_documents(["foo"])  doc_results doc_results  Previous OpenAI Next Self Hosted CommunityDiscordTwitterGitHubPythonJS/TSMoreHomepageBlogCopyright © 2023 LangChain, Inc. CommunityDiscordTwitterGitHubPythonJS/TSMoreHomepageBlog CommunityDiscordTwitter Community GitHubPythonJS/TS GitHub MoreHomepageBlog More Copyright © 2023 LangChain, Inc. Copyright © 2023 LangChain, Inc. Skip to main content (#docusaurus_skipToContent_fallback) 🦜️🔗 LangChain (/) Docs (/docs/get_started/introduction) Use cases (/docs/use_cases) Integrations (/docs/integrations) API (https://api.python.langchain.com) LangSmith (https://smith.langchain.com) JS/TS Docs (https://js.langchain.com/docs)  (https://github.com/hwchase17/langchain) Integrations (/docs/integrations) Callbacks (/docs/integrations/callbacks/) Chat models (/docs/integrations/chat/) Chat loaders (/docs/integrations/chat_loaders/) Document loaders (/docs/integrations/document_loaders/) Document transformers (/docs/integrations/document_transformers/) LLMs (/docs/integrations/llms/) Memory (/docs/integrations/memory/) Retrievers (/docs/integrations/retrievers/) Text embedding models (/docs/integrations/text_embedding/) Aleph Alpha (/docs/integrations/text_embedding/aleph_alpha) AwaDB (/docs/integrations/text_embedding/awadb) AzureOpenAI (/docs/integrations/text_embedding/azureopenai) Bedrock (/docs/integrations/text_embedding/bedrock) BGE on Hugging Face (/docs/integrations/text_embedding/bge_huggingface) Clarifai (/docs/integrations/text_embedding/clarifai) Cohere (/docs/integrations/text_embedding/cohere) DashScope (/docs/integrations/text_embedding/dashscope) DeepInfra (/docs/integrations/text_embedding/deepinfra) EDEN AI (/docs/integrations/text_embedding/edenai) Elasticsearch (/docs/integrations/text_embedding/elasticsearch) Embaas (/docs/integrations/text_embedding/embaas) ERNIE Embedding-V1 (/docs/integrations/text_embedding/ernie) Fake Embeddings (/docs/integrations/text_embedding/fake) Google Vertex AI PaLM (/docs/integrations/text_embedding/google_vertex_ai_palm) GPT4All (/docs/integrations/text_embedding/gpt4all) Hugging Face (/docs/integrations/text_embedding/huggingfacehub) InstructEmbeddings (/docs/integrations/text_embedding/instruct_embeddings) Jina (/docs/integrations/text_embedding/jina) Llama-cpp (/docs/integrations/text_embedding/llamacpp) LocalAI (/docs/integrations/text_embedding/localai) MiniMax (/docs/integrations/text_embedding/minimax) ModelScope (/docs/integrations/text_embedding/modelscope_hub) MosaicML (/docs/integrations/text_embedding/mosaicml) NLP Cloud (/docs/integrations/text_embedding/nlp_cloud) OpenAI (/docs/integrations/text_embedding/openai) SageMaker (/docs/integrations/text_embedding/sagemaker-endpoint) Self Hosted (/docs/integrations/text_embedding/self-hosted) Sentence Transformers (/docs/integrations/text_embedding/sentence_transformers) SpaCy (/docs/integrations/text_embedding/spacy_embedding) TensorflowHub (/docs/integrations/text_embedding/tensorflowhub) Xorbits inference (Xinference) (/docs/integrations/text_embedding/xinference) Agents & Toolkits (/docs/integrations/toolkits/) Tools (/docs/integrations/tools/) Vector stores (/docs/integrations/vectorstores/) Grouped by provider (/docs/integrations/providers/)  (/) Integrations (/docs/integrations) Text embedding models (/docs/integrations/text_embedding/) here (https://www.philschmid.de/custom-inference-huggingface-sagemaker) SagemakerEndpointEmbeddings (https://api.python.langchain.com/en/latest/embeddings/langchain.embeddings.sagemaker_endpoint.SagemakerEndpointEmbeddings.html) EmbeddingsContentHandler (https://api.python.langchain.com/en/latest/embeddings/langchain.embeddings.sagemaker_endpoint.EmbeddingsContentHandler.html) PreviousOpenAI (/docs/integrations/text_embedding/openai) NextSelf Hosted (/docs/integrations/text_embedding/self-hosted) Discord (https://discord.gg/cU2adEyC7w) Twitter (https://twitter.com/LangChainAI) Python (https://github.com/hwchase17/langchain) JS/TS (https://github.com/hwchase17/langchainjs) Homepage (https://langchain.com) Blog (https://blog.langchain.dev)