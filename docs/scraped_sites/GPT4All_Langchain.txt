GitHub:nomic-ai/gpt4all an ecosystem of open-source chatbots trained on a massive collections of clean assistant data including code, stories and dialogue. This example goes over how to use LangChain to interact with GPT4All models. To run locally, download a compatible ggml-formatted model.  The gpt4all page has a useful Model Explorer section: For more info, visit https://github.com/nomic-ai/gpt4all. Justin Bieber was born on March 1, 1994. In 1994, The Cowboys won Super Bowl XXVIII. IntegrationsCallbacksChat modelsChat loadersDocument loadersDocument transformersLLMsAI21Aleph AlphaAmazon API GatewayAnyscaleAzure MLAzure OpenAIBananaBasetenBeamBedrockBittensorCerebriumAIChatGLMClarifaiCohereC TransformersCTranslate2DatabricksDeepInfraDeepSparseEden AIFireworksForefrontAIGoogle Vertex AI PaLMGooseAIGPT4AllHugging Face HubHugging Face Local PipelinesHuggingface TextGen InferenceJSONFormerKoboldAI APILlama.cppLLM Caching integrationsManifestMinimaxModalMosaicMLNLP CloudOctoAIOllamaOpaquePromptsOpenAIOpenLLMOpenLMPetalsPipelineAIPredibasePrediction GuardPromptLayer OpenAIRELLMReplicateRunhouseSageMakerEndpointStochasticAINebula (Symbl.ai)TextGenTitan TakeoffTongyi QwenvLLMWriterXorbits Inference (Xinference)MemoryRetrieversText embedding modelsAgents & ToolkitsToolsVector storesGrouped by provider Callbacks Chat models Chat loaders Document loaders Document transformers LLMsAI21Aleph AlphaAmazon API GatewayAnyscaleAzure MLAzure OpenAIBananaBasetenBeamBedrockBittensorCerebriumAIChatGLMClarifaiCohereC TransformersCTranslate2DatabricksDeepInfraDeepSparseEden AIFireworksForefrontAIGoogle Vertex AI PaLMGooseAIGPT4AllHugging Face HubHugging Face Local PipelinesHuggingface TextGen InferenceJSONFormerKoboldAI APILlama.cppLLM Caching integrationsManifestMinimaxModalMosaicMLNLP CloudOctoAIOllamaOpaquePromptsOpenAIOpenLLMOpenLMPetalsPipelineAIPredibasePrediction GuardPromptLayer OpenAIRELLMReplicateRunhouseSageMakerEndpointStochasticAINebula (Symbl.ai)TextGenTitan TakeoffTongyi QwenvLLMWriterXorbits Inference (Xinference) AI21 Aleph Alpha Amazon API Gateway Anyscale Azure ML Azure OpenAI Banana Baseten Beam Bedrock Bittensor CerebriumAI ChatGLM Clarifai Cohere C Transformers CTranslate2 Databricks DeepInfra DeepSparse Eden AI Fireworks ForefrontAI Google Vertex AI PaLM GooseAI GPT4All Hugging Face Hub Hugging Face Local Pipelines Huggingface TextGen Inference JSONFormer KoboldAI API Llama.cpp LLM Caching integrations Manifest Minimax Modal MosaicML NLP Cloud OctoAI Ollama OpaquePrompts OpenAI OpenLLM OpenLM Petals PipelineAI Predibase Prediction Guard PromptLayer OpenAI RELLM Replicate Runhouse SageMakerEndpoint StochasticAI Nebula (Symbl.ai) TextGen Titan Takeoff Tongyi Qwen vLLM Writer Xorbits Inference (Xinference) Memory Retrievers Text embedding models Agents & Toolkits Tools Vector stores Grouped by provider  Integrations LLMs GPT4All GPT4All StreamingStdOutCallbackHandler Select a model of interest Download using the UI and move the .bin to the local_path (noted below) Import GPT4All Set Up Question to pass to LLM Specify Model Discord Twitter Python JS/TS Homepage Blog Skip to main content🦜️🔗 LangChainDocsUse casesIntegrationsAPILangSmithJS/TS DocsCTRLKIntegrationsCallbacksChat modelsChat loadersDocument loadersDocument transformersLLMsAI21Aleph AlphaAmazon API GatewayAnyscaleAzure MLAzure OpenAIBananaBasetenBeamBedrockBittensorCerebriumAIChatGLMClarifaiCohereC TransformersCTranslate2DatabricksDeepInfraDeepSparseEden AIFireworksForefrontAIGoogle Vertex AI PaLMGooseAIGPT4AllHugging Face HubHugging Face Local PipelinesHuggingface TextGen InferenceJSONFormerKoboldAI APILlama.cppLLM Caching integrationsManifestMinimaxModalMosaicMLNLP CloudOctoAIOllamaOpaquePromptsOpenAIOpenLLMOpenLMPetalsPipelineAIPredibasePrediction GuardPromptLayer OpenAIRELLMReplicateRunhouseSageMakerEndpointStochasticAINebula (Symbl.ai)TextGenTitan TakeoffTongyi QwenvLLMWriterXorbits Inference (Xinference)MemoryRetrieversText embedding modelsAgents & ToolkitsToolsVector storesGrouped by providerIntegrationsLLMsGPT4AllOn this pageGPT4AllGitHub:nomic-ai/gpt4all an ecosystem of open-source chatbots trained on a massive collections of clean assistant data including code, stories and dialogue.This example goes over how to use LangChain to interact with GPT4All models.%pip install gpt4all > /dev/null    Note: you may need to restart the kernel to use updated packages.Import GPT4All​from langchain import PromptTemplate, LLMChainfrom langchain.llms import GPT4Allfrom langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandlerAPI Reference:GPT4AllStreamingStdOutCallbackHandlerSet Up Question to pass to LLM​template = """Question: {question}Answer: Let's think step by step."""prompt = PromptTemplate(template=template, input_variables=["question"])Specify Model​To run locally, download a compatible ggml-formatted model. The gpt4all page has a useful Model Explorer section:Select a model of interestDownload using the UI and move the .bin to the local_path (noted below)For more info, visit https://github.com/nomic-ai/gpt4all.local_path = (    "./models/ggml-gpt4all-l13b-snoozy.bin"  # replace with your desired local file path)# Callbacks support token-wise streamingcallbacks = [StreamingStdOutCallbackHandler()]# Verbose is required to pass to the callback managerllm = GPT4All(model=local_path, callbacks=callbacks, verbose=True)# If you want to use a custom model add the backend parameter# Check https://docs.gpt4all.io/gpt4all_python.html for supported backendsllm = GPT4All(model=local_path, backend="gptj", callbacks=callbacks, verbose=True)llm_chain = LLMChain(prompt=prompt, llm=llm)question = "What NFL team won the Super Bowl in the year Justin Bieber was born?"llm_chain.run(question)Justin Bieber was born on March 1, 1994. In 1994, The Cowboys won Super Bowl XXVIII.PreviousGooseAINextHugging Face HubImport GPT4AllSet Up Question to pass to LLMSpecify ModelCommunityDiscordTwitterGitHubPythonJS/TSMoreHomepageBlogCopyright © 2023 LangChain, Inc. Skip to main content 🦜️🔗 LangChainDocsUse casesIntegrationsAPILangSmithJS/TS DocsCTRLK 🦜️🔗 LangChainDocsUse casesIntegrationsAPI LangSmithJS/TS DocsCTRLK  CTRLK CTRLK  CTRLK   IntegrationsCallbacksChat modelsChat loadersDocument loadersDocument transformersLLMsAI21Aleph AlphaAmazon API GatewayAnyscaleAzure MLAzure OpenAIBananaBasetenBeamBedrockBittensorCerebriumAIChatGLMClarifaiCohereC TransformersCTranslate2DatabricksDeepInfraDeepSparseEden AIFireworksForefrontAIGoogle Vertex AI PaLMGooseAIGPT4AllHugging Face HubHugging Face Local PipelinesHuggingface TextGen InferenceJSONFormerKoboldAI APILlama.cppLLM Caching integrationsManifestMinimaxModalMosaicMLNLP CloudOctoAIOllamaOpaquePromptsOpenAIOpenLLMOpenLMPetalsPipelineAIPredibasePrediction GuardPromptLayer OpenAIRELLMReplicateRunhouseSageMakerEndpointStochasticAINebula (Symbl.ai)TextGenTitan TakeoffTongyi QwenvLLMWriterXorbits Inference (Xinference)MemoryRetrieversText embedding modelsAgents & ToolkitsToolsVector storesGrouped by providerIntegrationsLLMsGPT4AllOn this pageGPT4AllGitHub:nomic-ai/gpt4all an ecosystem of open-source chatbots trained on a massive collections of clean assistant data including code, stories and dialogue.This example goes over how to use LangChain to interact with GPT4All models.%pip install gpt4all > /dev/null    Note: you may need to restart the kernel to use updated packages.Import GPT4All​from langchain import PromptTemplate, LLMChainfrom langchain.llms import GPT4Allfrom langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandlerAPI Reference:GPT4AllStreamingStdOutCallbackHandlerSet Up Question to pass to LLM​template = """Question: {question}Answer: Let's think step by step."""prompt = PromptTemplate(template=template, input_variables=["question"])Specify Model​To run locally, download a compatible ggml-formatted model. The gpt4all page has a useful Model Explorer section:Select a model of interestDownload using the UI and move the .bin to the local_path (noted below)For more info, visit https://github.com/nomic-ai/gpt4all.local_path = (    "./models/ggml-gpt4all-l13b-snoozy.bin"  # replace with your desired local file path)# Callbacks support token-wise streamingcallbacks = [StreamingStdOutCallbackHandler()]# Verbose is required to pass to the callback managerllm = GPT4All(model=local_path, callbacks=callbacks, verbose=True)# If you want to use a custom model add the backend parameter# Check https://docs.gpt4all.io/gpt4all_python.html for supported backendsllm = GPT4All(model=local_path, backend="gptj", callbacks=callbacks, verbose=True)llm_chain = LLMChain(prompt=prompt, llm=llm)question = "What NFL team won the Super Bowl in the year Justin Bieber was born?"llm_chain.run(question)Justin Bieber was born on March 1, 1994. In 1994, The Cowboys won Super Bowl XXVIII.PreviousGooseAINextHugging Face HubImport GPT4AllSet Up Question to pass to LLMSpecify Model IntegrationsCallbacksChat modelsChat loadersDocument loadersDocument transformersLLMsAI21Aleph AlphaAmazon API GatewayAnyscaleAzure MLAzure OpenAIBananaBasetenBeamBedrockBittensorCerebriumAIChatGLMClarifaiCohereC TransformersCTranslate2DatabricksDeepInfraDeepSparseEden AIFireworksForefrontAIGoogle Vertex AI PaLMGooseAIGPT4AllHugging Face HubHugging Face Local PipelinesHuggingface TextGen InferenceJSONFormerKoboldAI APILlama.cppLLM Caching integrationsManifestMinimaxModalMosaicMLNLP CloudOctoAIOllamaOpaquePromptsOpenAIOpenLLMOpenLMPetalsPipelineAIPredibasePrediction GuardPromptLayer OpenAIRELLMReplicateRunhouseSageMakerEndpointStochasticAINebula (Symbl.ai)TextGenTitan TakeoffTongyi QwenvLLMWriterXorbits Inference (Xinference)MemoryRetrieversText embedding modelsAgents & ToolkitsToolsVector storesGrouped by providerIntegrationsLLMsGPT4AllOn this pageGPT4AllGitHub:nomic-ai/gpt4all an ecosystem of open-source chatbots trained on a massive collections of clean assistant data including code, stories and dialogue.This example goes over how to use LangChain to interact with GPT4All models.%pip install gpt4all > /dev/null    Note: you may need to restart the kernel to use updated packages.Import GPT4All​from langchain import PromptTemplate, LLMChainfrom langchain.llms import GPT4Allfrom langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandlerAPI Reference:GPT4AllStreamingStdOutCallbackHandlerSet Up Question to pass to LLM​template = """Question: {question}Answer: Let's think step by step."""prompt = PromptTemplate(template=template, input_variables=["question"])Specify Model​To run locally, download a compatible ggml-formatted model. The gpt4all page has a useful Model Explorer section:Select a model of interestDownload using the UI and move the .bin to the local_path (noted below)For more info, visit https://github.com/nomic-ai/gpt4all.local_path = (    "./models/ggml-gpt4all-l13b-snoozy.bin"  # replace with your desired local file path)# Callbacks support token-wise streamingcallbacks = [StreamingStdOutCallbackHandler()]# Verbose is required to pass to the callback managerllm = GPT4All(model=local_path, callbacks=callbacks, verbose=True)# If you want to use a custom model add the backend parameter# Check https://docs.gpt4all.io/gpt4all_python.html for supported backendsllm = GPT4All(model=local_path, backend="gptj", callbacks=callbacks, verbose=True)llm_chain = LLMChain(prompt=prompt, llm=llm)question = "What NFL team won the Super Bowl in the year Justin Bieber was born?"llm_chain.run(question)Justin Bieber was born on March 1, 1994. In 1994, The Cowboys won Super Bowl XXVIII.PreviousGooseAINextHugging Face HubImport GPT4AllSet Up Question to pass to LLMSpecify Model IntegrationsCallbacksChat modelsChat loadersDocument loadersDocument transformersLLMsAI21Aleph AlphaAmazon API GatewayAnyscaleAzure MLAzure OpenAIBananaBasetenBeamBedrockBittensorCerebriumAIChatGLMClarifaiCohereC TransformersCTranslate2DatabricksDeepInfraDeepSparseEden AIFireworksForefrontAIGoogle Vertex AI PaLMGooseAIGPT4AllHugging Face HubHugging Face Local PipelinesHuggingface TextGen InferenceJSONFormerKoboldAI APILlama.cppLLM Caching integrationsManifestMinimaxModalMosaicMLNLP CloudOctoAIOllamaOpaquePromptsOpenAIOpenLLMOpenLMPetalsPipelineAIPredibasePrediction GuardPromptLayer OpenAIRELLMReplicateRunhouseSageMakerEndpointStochasticAINebula (Symbl.ai)TextGenTitan TakeoffTongyi QwenvLLMWriterXorbits Inference (Xinference)MemoryRetrieversText embedding modelsAgents & ToolkitsToolsVector storesGrouped by provider IntegrationsCallbacksChat modelsChat loadersDocument loadersDocument transformersLLMsAI21Aleph AlphaAmazon API GatewayAnyscaleAzure MLAzure OpenAIBananaBasetenBeamBedrockBittensorCerebriumAIChatGLMClarifaiCohereC TransformersCTranslate2DatabricksDeepInfraDeepSparseEden AIFireworksForefrontAIGoogle Vertex AI PaLMGooseAIGPT4AllHugging Face HubHugging Face Local PipelinesHuggingface TextGen InferenceJSONFormerKoboldAI APILlama.cppLLM Caching integrationsManifestMinimaxModalMosaicMLNLP CloudOctoAIOllamaOpaquePromptsOpenAIOpenLLMOpenLMPetalsPipelineAIPredibasePrediction GuardPromptLayer OpenAIRELLMReplicateRunhouseSageMakerEndpointStochasticAINebula (Symbl.ai)TextGenTitan TakeoffTongyi QwenvLLMWriterXorbits Inference (Xinference)MemoryRetrieversText embedding modelsAgents & ToolkitsToolsVector storesGrouped by provider Integrations Callbacks Chat models Chat loaders Document loaders Document transformers LLMs Memory Retrievers Text embedding models Agents & Toolkits Tools Vector stores Grouped by provider IntegrationsLLMsGPT4AllOn this pageGPT4AllGitHub:nomic-ai/gpt4all an ecosystem of open-source chatbots trained on a massive collections of clean assistant data including code, stories and dialogue.This example goes over how to use LangChain to interact with GPT4All models.%pip install gpt4all > /dev/null    Note: you may need to restart the kernel to use updated packages.Import GPT4All​from langchain import PromptTemplate, LLMChainfrom langchain.llms import GPT4Allfrom langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandlerAPI Reference:GPT4AllStreamingStdOutCallbackHandlerSet Up Question to pass to LLM​template = """Question: {question}Answer: Let's think step by step."""prompt = PromptTemplate(template=template, input_variables=["question"])Specify Model​To run locally, download a compatible ggml-formatted model. The gpt4all page has a useful Model Explorer section:Select a model of interestDownload using the UI and move the .bin to the local_path (noted below)For more info, visit https://github.com/nomic-ai/gpt4all.local_path = (    "./models/ggml-gpt4all-l13b-snoozy.bin"  # replace with your desired local file path)# Callbacks support token-wise streamingcallbacks = [StreamingStdOutCallbackHandler()]# Verbose is required to pass to the callback managerllm = GPT4All(model=local_path, callbacks=callbacks, verbose=True)# If you want to use a custom model add the backend parameter# Check https://docs.gpt4all.io/gpt4all_python.html for supported backendsllm = GPT4All(model=local_path, backend="gptj", callbacks=callbacks, verbose=True)llm_chain = LLMChain(prompt=prompt, llm=llm)question = "What NFL team won the Super Bowl in the year Justin Bieber was born?"llm_chain.run(question)Justin Bieber was born on March 1, 1994. In 1994, The Cowboys won Super Bowl XXVIII.PreviousGooseAINextHugging Face HubImport GPT4AllSet Up Question to pass to LLMSpecify Model IntegrationsLLMsGPT4AllOn this pageGPT4AllGitHub:nomic-ai/gpt4all an ecosystem of open-source chatbots trained on a massive collections of clean assistant data including code, stories and dialogue.This example goes over how to use LangChain to interact with GPT4All models.%pip install gpt4all > /dev/null    Note: you may need to restart the kernel to use updated packages.Import GPT4All​from langchain import PromptTemplate, LLMChainfrom langchain.llms import GPT4Allfrom langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandlerAPI Reference:GPT4AllStreamingStdOutCallbackHandlerSet Up Question to pass to LLM​template = """Question: {question}Answer: Let's think step by step."""prompt = PromptTemplate(template=template, input_variables=["question"])Specify Model​To run locally, download a compatible ggml-formatted model. The gpt4all page has a useful Model Explorer section:Select a model of interestDownload using the UI and move the .bin to the local_path (noted below)For more info, visit https://github.com/nomic-ai/gpt4all.local_path = (    "./models/ggml-gpt4all-l13b-snoozy.bin"  # replace with your desired local file path)# Callbacks support token-wise streamingcallbacks = [StreamingStdOutCallbackHandler()]# Verbose is required to pass to the callback managerllm = GPT4All(model=local_path, callbacks=callbacks, verbose=True)# If you want to use a custom model add the backend parameter# Check https://docs.gpt4all.io/gpt4all_python.html for supported backendsllm = GPT4All(model=local_path, backend="gptj", callbacks=callbacks, verbose=True)llm_chain = LLMChain(prompt=prompt, llm=llm)question = "What NFL team won the Super Bowl in the year Justin Bieber was born?"llm_chain.run(question)Justin Bieber was born on March 1, 1994. In 1994, The Cowboys won Super Bowl XXVIII.PreviousGooseAINextHugging Face HubImport GPT4AllSet Up Question to pass to LLMSpecify Model IntegrationsLLMsGPT4AllOn this pageGPT4AllGitHub:nomic-ai/gpt4all an ecosystem of open-source chatbots trained on a massive collections of clean assistant data including code, stories and dialogue.This example goes over how to use LangChain to interact with GPT4All models.%pip install gpt4all > /dev/null    Note: you may need to restart the kernel to use updated packages.Import GPT4All​from langchain import PromptTemplate, LLMChainfrom langchain.llms import GPT4Allfrom langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandlerAPI Reference:GPT4AllStreamingStdOutCallbackHandlerSet Up Question to pass to LLM​template = """Question: {question}Answer: Let's think step by step."""prompt = PromptTemplate(template=template, input_variables=["question"])Specify Model​To run locally, download a compatible ggml-formatted model. The gpt4all page has a useful Model Explorer section:Select a model of interestDownload using the UI and move the .bin to the local_path (noted below)For more info, visit https://github.com/nomic-ai/gpt4all.local_path = (    "./models/ggml-gpt4all-l13b-snoozy.bin"  # replace with your desired local file path)# Callbacks support token-wise streamingcallbacks = [StreamingStdOutCallbackHandler()]# Verbose is required to pass to the callback managerllm = GPT4All(model=local_path, callbacks=callbacks, verbose=True)# If you want to use a custom model add the backend parameter# Check https://docs.gpt4all.io/gpt4all_python.html for supported backendsllm = GPT4All(model=local_path, backend="gptj", callbacks=callbacks, verbose=True)llm_chain = LLMChain(prompt=prompt, llm=llm)question = "What NFL team won the Super Bowl in the year Justin Bieber was born?"llm_chain.run(question)Justin Bieber was born on March 1, 1994. In 1994, The Cowboys won Super Bowl XXVIII.PreviousGooseAINextHugging Face Hub IntegrationsLLMsGPT4AllOn this pageGPT4AllGitHub:nomic-ai/gpt4all an ecosystem of open-source chatbots trained on a massive collections of clean assistant data including code, stories and dialogue.This example goes over how to use LangChain to interact with GPT4All models.%pip install gpt4all > /dev/null    Note: you may need to restart the kernel to use updated packages.Import GPT4All​from langchain import PromptTemplate, LLMChainfrom langchain.llms import GPT4Allfrom langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandlerAPI Reference:GPT4AllStreamingStdOutCallbackHandlerSet Up Question to pass to LLM​template = """Question: {question}Answer: Let's think step by step."""prompt = PromptTemplate(template=template, input_variables=["question"])Specify Model​To run locally, download a compatible ggml-formatted model. The gpt4all page has a useful Model Explorer section:Select a model of interestDownload using the UI and move the .bin to the local_path (noted below)For more info, visit https://github.com/nomic-ai/gpt4all.local_path = (    "./models/ggml-gpt4all-l13b-snoozy.bin"  # replace with your desired local file path)# Callbacks support token-wise streamingcallbacks = [StreamingStdOutCallbackHandler()]# Verbose is required to pass to the callback managerllm = GPT4All(model=local_path, callbacks=callbacks, verbose=True)# If you want to use a custom model add the backend parameter# Check https://docs.gpt4all.io/gpt4all_python.html for supported backendsllm = GPT4All(model=local_path, backend="gptj", callbacks=callbacks, verbose=True)llm_chain = LLMChain(prompt=prompt, llm=llm)question = "What NFL team won the Super Bowl in the year Justin Bieber was born?"llm_chain.run(question)Justin Bieber was born on March 1, 1994. In 1994, The Cowboys won Super Bowl XXVIII.PreviousGooseAINextHugging Face Hub On this page GPT4AllGitHub:nomic-ai/gpt4all an ecosystem of open-source chatbots trained on a massive collections of clean assistant data including code, stories and dialogue.This example goes over how to use LangChain to interact with GPT4All models.%pip install gpt4all > /dev/null    Note: you may need to restart the kernel to use updated packages.Import GPT4All​from langchain import PromptTemplate, LLMChainfrom langchain.llms import GPT4Allfrom langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandlerAPI Reference:GPT4AllStreamingStdOutCallbackHandlerSet Up Question to pass to LLM​template = """Question: {question}Answer: Let's think step by step."""prompt = PromptTemplate(template=template, input_variables=["question"])Specify Model​To run locally, download a compatible ggml-formatted model. The gpt4all page has a useful Model Explorer section:Select a model of interestDownload using the UI and move the .bin to the local_path (noted below)For more info, visit https://github.com/nomic-ai/gpt4all.local_path = (    "./models/ggml-gpt4all-l13b-snoozy.bin"  # replace with your desired local file path)# Callbacks support token-wise streamingcallbacks = [StreamingStdOutCallbackHandler()]# Verbose is required to pass to the callback managerllm = GPT4All(model=local_path, callbacks=callbacks, verbose=True)# If you want to use a custom model add the backend parameter# Check https://docs.gpt4all.io/gpt4all_python.html for supported backendsllm = GPT4All(model=local_path, backend="gptj", callbacks=callbacks, verbose=True)llm_chain = LLMChain(prompt=prompt, llm=llm)question = "What NFL team won the Super Bowl in the year Justin Bieber was born?"llm_chain.run(question)Justin Bieber was born on March 1, 1994. In 1994, The Cowboys won Super Bowl XXVIII. %pip install gpt4all > /dev/null %pip install gpt4all > /dev/null      Note: you may need to restart the kernel to use updated packages.     Note: you may need to restart the kernel to use updated packages.     Note: you may need to restart the kernel to use updated packages.  from langchain import PromptTemplate, LLMChainfrom langchain.llms import GPT4Allfrom langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler from langchain import PromptTemplate, LLMChainfrom langchain.llms import GPT4Allfrom langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler  API Reference:GPT4AllStreamingStdOutCallbackHandler template = """Question: {question}Answer: Let's think step by step."""prompt = PromptTemplate(template=template, input_variables=["question"]) template = """Question: {question}Answer: Let's think step by step."""prompt = PromptTemplate(template=template, input_variables=["question"])  local_path = (    "./models/ggml-gpt4all-l13b-snoozy.bin"  # replace with your desired local file path) local_path = (    "./models/ggml-gpt4all-l13b-snoozy.bin"  # replace with your desired local file path)  # Callbacks support token-wise streamingcallbacks = [StreamingStdOutCallbackHandler()]# Verbose is required to pass to the callback managerllm = GPT4All(model=local_path, callbacks=callbacks, verbose=True)# If you want to use a custom model add the backend parameter# Check https://docs.gpt4all.io/gpt4all_python.html for supported backendsllm = GPT4All(model=local_path, backend="gptj", callbacks=callbacks, verbose=True) # Callbacks support token-wise streamingcallbacks = [StreamingStdOutCallbackHandler()]# Verbose is required to pass to the callback managerllm = GPT4All(model=local_path, callbacks=callbacks, verbose=True)# If you want to use a custom model add the backend parameter# Check https://docs.gpt4all.io/gpt4all_python.html for supported backendsllm = GPT4All(model=local_path, backend="gptj", callbacks=callbacks, verbose=True)  llm_chain = LLMChain(prompt=prompt, llm=llm) llm_chain = LLMChain(prompt=prompt, llm=llm)  question = "What NFL team won the Super Bowl in the year Justin Bieber was born?"llm_chain.run(question) question = "What NFL team won the Super Bowl in the year Justin Bieber was born?"llm_chain.run(question)  Previous GooseAI Next Hugging Face Hub Import GPT4AllSet Up Question to pass to LLMSpecify Model Import GPT4AllSet Up Question to pass to LLMSpecify Model CommunityDiscordTwitterGitHubPythonJS/TSMoreHomepageBlogCopyright © 2023 LangChain, Inc. CommunityDiscordTwitterGitHubPythonJS/TSMoreHomepageBlog CommunityDiscordTwitter Community GitHubPythonJS/TS GitHub MoreHomepageBlog More Copyright © 2023 LangChain, Inc. Copyright © 2023 LangChain, Inc. Skip to main content (#docusaurus_skipToContent_fallback) 🦜️🔗 LangChain (/) Docs (/docs/get_started/introduction) Use cases (/docs/use_cases) Integrations (/docs/integrations) API (https://api.python.langchain.com) LangSmith (https://smith.langchain.com) JS/TS Docs (https://js.langchain.com/docs)  (https://github.com/hwchase17/langchain) Integrations (/docs/integrations) Callbacks (/docs/integrations/callbacks/) Chat models (/docs/integrations/chat/) Chat loaders (/docs/integrations/chat_loaders/) Document loaders (/docs/integrations/document_loaders/) Document transformers (/docs/integrations/document_transformers/) LLMs (/docs/integrations/llms/) AI21 (/docs/integrations/llms/ai21) Aleph Alpha (/docs/integrations/llms/aleph_alpha) Amazon API Gateway (/docs/integrations/llms/amazon_api_gateway) Anyscale (/docs/integrations/llms/anyscale) Azure ML (/docs/integrations/llms/azure_ml) Azure OpenAI (/docs/integrations/llms/azure_openai) Banana (/docs/integrations/llms/banana) Baseten (/docs/integrations/llms/baseten) Beam (/docs/integrations/llms/beam) Bedrock (/docs/integrations/llms/bedrock) Bittensor (/docs/integrations/llms/bittensor) CerebriumAI (/docs/integrations/llms/cerebriumai) ChatGLM (/docs/integrations/llms/chatglm) Clarifai (/docs/integrations/llms/clarifai) Cohere (/docs/integrations/llms/cohere) C Transformers (/docs/integrations/llms/ctransformers) CTranslate2 (/docs/integrations/llms/ctranslate2) Databricks (/docs/integrations/llms/databricks) DeepInfra (/docs/integrations/llms/deepinfra) DeepSparse (/docs/integrations/llms/deepsparse) Eden AI (/docs/integrations/llms/edenai) Fireworks (/docs/integrations/llms/fireworks) ForefrontAI (/docs/integrations/llms/forefrontai) Google Vertex AI PaLM (/docs/integrations/llms/google_vertex_ai_palm) GooseAI (/docs/integrations/llms/gooseai) GPT4All (/docs/integrations/llms/gpt4all) Hugging Face Hub (/docs/integrations/llms/huggingface_hub) Hugging Face Local Pipelines (/docs/integrations/llms/huggingface_pipelines) Huggingface TextGen Inference (/docs/integrations/llms/huggingface_textgen_inference) JSONFormer (/docs/integrations/llms/jsonformer_experimental) KoboldAI API (/docs/integrations/llms/koboldai) Llama.cpp (/docs/integrations/llms/llamacpp) LLM Caching integrations (/docs/integrations/llms/llm_caching) Manifest (/docs/integrations/llms/manifest) Minimax (/docs/integrations/llms/minimax) Modal (/docs/integrations/llms/modal) MosaicML (/docs/integrations/llms/mosaicml) NLP Cloud (/docs/integrations/llms/nlpcloud) OctoAI (/docs/integrations/llms/octoai) Ollama (/docs/integrations/llms/ollama) OpaquePrompts (/docs/integrations/llms/opaqueprompts) OpenAI (/docs/integrations/llms/openai) OpenLLM (/docs/integrations/llms/openllm) OpenLM (/docs/integrations/llms/openlm) Petals (/docs/integrations/llms/petals) PipelineAI (/docs/integrations/llms/pipelineai) Predibase (/docs/integrations/llms/predibase) Prediction Guard (/docs/integrations/llms/predictionguard) PromptLayer OpenAI (/docs/integrations/llms/promptlayer_openai) RELLM (/docs/integrations/llms/rellm_experimental) Replicate (/docs/integrations/llms/replicate) Runhouse (/docs/integrations/llms/runhouse) SageMakerEndpoint (/docs/integrations/llms/sagemaker) StochasticAI (/docs/integrations/llms/stochasticai) Nebula (Symbl.ai) (/docs/integrations/llms/symblai_nebula) TextGen (/docs/integrations/llms/textgen) Titan Takeoff (/docs/integrations/llms/titan_takeoff) Tongyi Qwen (/docs/integrations/llms/tongyi) vLLM (/docs/integrations/llms/vllm) Writer (/docs/integrations/llms/writer) Xorbits Inference (Xinference) (/docs/integrations/llms/xinference) Memory (/docs/integrations/memory/) Retrievers (/docs/integrations/retrievers/) Text embedding models (/docs/integrations/text_embedding/) Agents & Toolkits (/docs/integrations/toolkits/) Tools (/docs/integrations/tools/) Vector stores (/docs/integrations/vectorstores/) Grouped by provider (/docs/integrations/providers/)  (/) Integrations (/docs/integrations) LLMs (/docs/integrations/llms/) GitHub:nomic-ai/gpt4all (https://github.com/nomic-ai/gpt4all) ​ (#import-gpt4all) GPT4All (https://api.python.langchain.com/en/latest/llms/langchain.llms.gpt4all.GPT4All.html) StreamingStdOutCallbackHandler (https://api.python.langchain.com/en/latest/callbacks/langchain.callbacks.streaming_stdout.StreamingStdOutCallbackHandler.html) ​ (#set-up-question-to-pass-to-llm) ​ (#specify-model) gpt4all page (https://gpt4all.io/index.html) https://github.com/nomic-ai/gpt4all (https://github.com/nomic-ai/gpt4all) PreviousGooseAI (/docs/integrations/llms/gooseai) NextHugging Face Hub (/docs/integrations/llms/huggingface_hub) Import GPT4All (#import-gpt4all) Set Up Question to pass to LLM (#set-up-question-to-pass-to-llm) Specify Model (#specify-model) Discord (https://discord.gg/cU2adEyC7w) Twitter (https://twitter.com/LangChainAI) Python (https://github.com/hwchase17/langchain) JS/TS (https://github.com/hwchase17/langchainjs) Homepage (https://langchain.com) Blog (https://blog.langchain.dev)