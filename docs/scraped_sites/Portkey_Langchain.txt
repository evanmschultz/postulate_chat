Portkey is a platform designed to streamline the deployment and management of Generative AI applications. It provides comprehensive features for monitoring, managing models, and improving the performance of your AI applications. Portkey brings production readiness to Langchain. With Portkey, you can  Using Portkey is as simple as just choosing which Portkey features you want, enabling them via headers=Portkey.Config and passing it in your LLM calls. To start, get your Portkey API key by signing up here. (Click the profile icon on the top left, then click on "Copy API Key") For OpenAI, a simple integration with logging feature would look like this: Your logs will be captured on your Portkey dashboard. A common Portkey X Langchain use case is to trace a chain or an agent and view all the LLM calls originating from that request.  You can see the requests' logs along with the trace id on Portkey dashboard: For detailed information on each feature and how to use it, please refer to the Portkey docs. If you have any questions or need further assistance, reach out to us on Twitter.. IntegrationsCallbacksChat modelsChat loadersDocument loadersDocument transformersLLMsMemoryRetrieversText embedding modelsAgents & ToolkitsToolsVector storesGrouped by providerActiveloop Deep LakeAI21 LabsAimAINetworkAirbyteAirtableAleph AlphaAlibaba Cloud OpensearchAmazon API GatewayAnalyticDBAnnoyAnyscaleApifyArangoDBArgillaArthurArxivAtlasAwaDBAWS S3 DirectoryAZLyricsAzure Blob StorageAzure Cognitive SearchAzure OpenAIBagelDBBananaBasetenBeamBedrockBiliBiliNIBittensorBlackboardBrave SearchCassandraCerebriumAIChaindeskChromaClarifaiClearMLClickHouseCnosDBCohereCollege ConfidentialCometConfident AIConfluenceC TransformersDashVectorDatabricksDatadog TracingDatadog LogsDataForSEODeepInfraDeepSparseDiffbotDingoDiscordDocArrayDocugamiDuckDBElasticsearchEpsillaEverNoteFacebook ChatFacebook FaissFigmaFireworksFlyteForefrontAIGitGitBookGoldenGoogle BigQueryGoogle Cloud StorageGoogle DriveGoogle SearchGoogle SerperGoogle Vertex AI MatchingEngineGooseAIGPT4AllGraphsignalGrobidGutenbergHacker NewsHazy ResearchHeliconeHologresHugging FaceiFixitIMSDbInfinoJinaKonkoLanceDBLangChain Decorators ‚ú®Llama.cppLog10MarqoMediaWikiDumpMeilisearchMetalMicrosoft OneDriveMicrosoft PowerPointMicrosoft WordMilvusMinimaxMLflow AI GatewayMLflowModalModelScopeModern TreasuryMomentoMongoDB AtlasMotherduckMyScaleNeo4jNLPCloudNotion DBObsidianOpenAIOpenLLMOpenSearchOpenWeatherMapPetalsPostgres EmbeddingPGVectorPineconePipelineAIPortkeyLog, Trace, and MonitorPredibasePrediction GuardPromptLayerPsychicPubMedQdrantRay ServeRebuffRedditRedisReplicateRoamRocksetRunhouseRWKV-4SageMaker EndpointSageMaker TrackingScaNNSearxNG Search APISerpAPIShale ProtocolSingleStoreDBscikit-learnSlackspaCySpreedlyStarRocksStochasticAIStripeSupabase (Postgres)NebulaTairTelegramTencentVectorDBTensorFlow DatasetsTigris2MarkdownTrelloTruLensTwitterTypesenseUnstructuredUSearchVearchVectaraVespaWandB TracingWeights & BiasesWeatherWeaviateWhatsAppWhyLabsWikipediaWolfram AlphaWriterXataXorbits Inference (Xinference)Yeager.aiYouTubeZepZilliz Callbacks Chat models Chat loaders Document loaders Document transformers LLMs Memory Retrievers Text embedding models Agents & Toolkits Tools Vector stores Grouped by providerActiveloop Deep LakeAI21 LabsAimAINetworkAirbyteAirtableAleph AlphaAlibaba Cloud OpensearchAmazon API GatewayAnalyticDBAnnoyAnyscaleApifyArangoDBArgillaArthurArxivAtlasAwaDBAWS S3 DirectoryAZLyricsAzure Blob StorageAzure Cognitive SearchAzure OpenAIBagelDBBananaBasetenBeamBedrockBiliBiliNIBittensorBlackboardBrave SearchCassandraCerebriumAIChaindeskChromaClarifaiClearMLClickHouseCnosDBCohereCollege ConfidentialCometConfident AIConfluenceC TransformersDashVectorDatabricksDatadog TracingDatadog LogsDataForSEODeepInfraDeepSparseDiffbotDingoDiscordDocArrayDocugamiDuckDBElasticsearchEpsillaEverNoteFacebook ChatFacebook FaissFigmaFireworksFlyteForefrontAIGitGitBookGoldenGoogle BigQueryGoogle Cloud StorageGoogle DriveGoogle SearchGoogle SerperGoogle Vertex AI MatchingEngineGooseAIGPT4AllGraphsignalGrobidGutenbergHacker NewsHazy ResearchHeliconeHologresHugging FaceiFixitIMSDbInfinoJinaKonkoLanceDBLangChain Decorators ‚ú®Llama.cppLog10MarqoMediaWikiDumpMeilisearchMetalMicrosoft OneDriveMicrosoft PowerPointMicrosoft WordMilvusMinimaxMLflow AI GatewayMLflowModalModelScopeModern TreasuryMomentoMongoDB AtlasMotherduckMyScaleNeo4jNLPCloudNotion DBObsidianOpenAIOpenLLMOpenSearchOpenWeatherMapPetalsPostgres EmbeddingPGVectorPineconePipelineAIPortkeyLog, Trace, and MonitorPredibasePrediction GuardPromptLayerPsychicPubMedQdrantRay ServeRebuffRedditRedisReplicateRoamRocksetRunhouseRWKV-4SageMaker EndpointSageMaker TrackingScaNNSearxNG Search APISerpAPIShale ProtocolSingleStoreDBscikit-learnSlackspaCySpreedlyStarRocksStochasticAIStripeSupabase (Postgres)NebulaTairTelegramTencentVectorDBTensorFlow DatasetsTigris2MarkdownTrelloTruLensTwitterTypesenseUnstructuredUSearchVearchVectaraVespaWandB TracingWeights & BiasesWeatherWeaviateWhatsAppWhyLabsWikipediaWolfram AlphaWriterXataXorbits Inference (Xinference)Yeager.aiYouTubeZepZilliz Activeloop Deep Lake AI21 Labs Aim AINetwork Airbyte Airtable Aleph Alpha Alibaba Cloud Opensearch Amazon API Gateway AnalyticDB Annoy Anyscale Apify ArangoDB Argilla Arthur Arxiv Atlas AwaDB AWS S3 Directory AZLyrics Azure Blob Storage Azure Cognitive Search Azure OpenAI BagelDB Banana Baseten Beam Bedrock BiliBili NIBittensor Blackboard Brave Search Cassandra CerebriumAI Chaindesk Chroma Clarifai ClearML ClickHouse CnosDB Cohere College Confidential Comet Confident AI Confluence C Transformers DashVector Databricks Datadog Tracing Datadog Logs DataForSEO DeepInfra DeepSparse Diffbot Dingo Discord DocArray Docugami DuckDB Elasticsearch Epsilla EverNote Facebook Chat Facebook Faiss Figma Fireworks Flyte ForefrontAI Git GitBook Golden Google BigQuery Google Cloud Storage Google Drive Google Search Google Serper Google Vertex AI MatchingEngine GooseAI GPT4All Graphsignal Grobid Gutenberg Hacker News Hazy Research Helicone Hologres Hugging Face iFixit IMSDb Infino Jina Konko LanceDB LangChain Decorators ‚ú® Llama.cpp Log10 Marqo MediaWikiDump Meilisearch Metal Microsoft OneDrive Microsoft PowerPoint Microsoft Word Milvus Minimax MLflow AI Gateway MLflow Modal ModelScope Modern Treasury Momento MongoDB Atlas Motherduck MyScale Neo4j NLPCloud Notion DB Obsidian OpenAI OpenLLM OpenSearch OpenWeatherMap Petals Postgres Embedding PGVector Pinecone PipelineAI PortkeyLog, Trace, and Monitor Log, Trace, and Monitor Predibase Prediction Guard PromptLayer Psychic PubMed Qdrant Ray Serve Rebuff Reddit Redis Replicate Roam Rockset Runhouse RWKV-4 SageMaker Endpoint SageMaker Tracking ScaNN SearxNG Search API SerpAPI Shale Protocol SingleStoreDB scikit-learn Slack spaCy Spreedly StarRocks StochasticAI Stripe Supabase (Postgres) Nebula Tair Telegram TencentVectorDB TensorFlow Datasets Tigris 2Markdown Trello TruLens Twitter Typesense Unstructured USearch Vearch Vectara Vespa WandB Tracing Weights & Biases Weather Weaviate WhatsApp WhyLabs Wikipedia Wolfram Alpha Writer Xata Xorbits Inference (Xinference) Yeager.ai YouTube Zep Zilliz  Integrations Grouped by provider Portkey  view detailed metrics & logs for all requests,   enable semantic cache to reduce latency & costs,   implement automatic retries & fallbacks for failed requests,   add custom tags to requests for better tracking and analysis and more. OpenAI Portkey AgentType initialize_agent load_tools OpenAI Portkey Logging: Log all your LLM requests automatically by sending them through Portkey. Each request log contains timestamp, model name, total cost, request time, request json, response json, and additional Portkey features. Tracing: Trace id can be passed along with each request and is visibe on the logs on Portkey dashboard. You can also set a distinct trace id for each request. You can append user feedback to a trace id as well. Caching: Respond to previously served customers queries from cache instead of sending them again to OpenAI. Match exact strings OR semantically similar strings. Cache can save costs and reduce latencies by 20x. Retries: Automatically reprocess any unsuccessful API requests upto 5 times. Uses an exponential backoff strategy, which spaces out retry attempts to prevent network overload. Tagging: Track and audit each user interaction in high detail with predefined tags. LLMOps for LangchainUsing Portkey with LangchainTracing Chains & Agents Using Portkey with Langchain Tracing Chains & Agents Advanced Features Enabling all Portkey Features: Discord Twitter Python JS/TS Homepage Blog Skip to main contentü¶úÔ∏èüîó LangChainDocsUse casesIntegrationsAPILangSmithJS/TS DocsCTRLKIntegrationsCallbacksChat modelsChat loadersDocument loadersDocument transformersLLMsMemoryRetrieversText embedding modelsAgents & ToolkitsToolsVector storesGrouped by providerActiveloop Deep LakeAI21 LabsAimAINetworkAirbyteAirtableAleph AlphaAlibaba Cloud OpensearchAmazon API GatewayAnalyticDBAnnoyAnyscaleApifyArangoDBArgillaArthurArxivAtlasAwaDBAWS S3 DirectoryAZLyricsAzure Blob StorageAzure Cognitive SearchAzure OpenAIBagelDBBananaBasetenBeamBedrockBiliBiliNIBittensorBlackboardBrave SearchCassandraCerebriumAIChaindeskChromaClarifaiClearMLClickHouseCnosDBCohereCollege ConfidentialCometConfident AIConfluenceC TransformersDashVectorDatabricksDatadog TracingDatadog LogsDataForSEODeepInfraDeepSparseDiffbotDingoDiscordDocArrayDocugamiDuckDBElasticsearchEpsillaEverNoteFacebook ChatFacebook FaissFigmaFireworksFlyteForefrontAIGitGitBookGoldenGoogle BigQueryGoogle Cloud StorageGoogle DriveGoogle SearchGoogle SerperGoogle Vertex AI MatchingEngineGooseAIGPT4AllGraphsignalGrobidGutenbergHacker NewsHazy ResearchHeliconeHologresHugging FaceiFixitIMSDbInfinoJinaKonkoLanceDBLangChain Decorators ‚ú®Llama.cppLog10MarqoMediaWikiDumpMeilisearchMetalMicrosoft OneDriveMicrosoft PowerPointMicrosoft WordMilvusMinimaxMLflow AI GatewayMLflowModalModelScopeModern TreasuryMomentoMongoDB AtlasMotherduckMyScaleNeo4jNLPCloudNotion DBObsidianOpenAIOpenLLMOpenSearchOpenWeatherMapPetalsPostgres EmbeddingPGVectorPineconePipelineAIPortkeyLog, Trace, and MonitorPredibasePrediction GuardPromptLayerPsychicPubMedQdrantRay ServeRebuffRedditRedisReplicateRoamRocksetRunhouseRWKV-4SageMaker EndpointSageMaker TrackingScaNNSearxNG Search APISerpAPIShale ProtocolSingleStoreDBscikit-learnSlackspaCySpreedlyStarRocksStochasticAIStripeSupabase (Postgres)NebulaTairTelegramTencentVectorDBTensorFlow DatasetsTigris2MarkdownTrelloTruLensTwitterTypesenseUnstructuredUSearchVearchVectaraVespaWandB TracingWeights & BiasesWeatherWeaviateWhatsAppWhyLabsWikipediaWolfram AlphaWriterXataXorbits Inference (Xinference)Yeager.aiYouTubeZepZillizIntegrationsGrouped by providerPortkeyOn this pagePortkeyPortkey is a platform designed to streamline the deployment and improving the performance of your AI applications.LLMOps for Langchain‚ÄãPortkey brings production readiness to Langchain. With Portkey, you can  view detailed metrics & logs for all requests,  enable semantic cache to reduce latency & costs,  implement automatic retries & fallbacks for failed requests,  add custom tags to requests for better tracking and analysis and more.Using Portkey with Langchain‚ÄãUsing Portkey is as simple as just choosing which Portkey features you want, enabling them via headers=Portkey.Config and passing it in your LLM calls.To start, get your Portkey API key by signing up here. (Click the profile icon on the top left, then click on "Copy API Key")For OpenAI, a simple integration with logging feature would look like this:from langchain.llms import OpenAIfrom langchain.utilities import Portkey# Add the Portkey API Key from your accountheaders = Portkey.Config(    api_key = "<PORTKEY_API_KEY>")llm = OpenAI(temperature=0.9, headers=headers)llm.predict("What would be a good company name for a company that makes colorful socks?")API Reference:OpenAIPortkeyYour logs will be captured on your Portkey dashboard.A common Portkey X Langchain use case is to trace a chain or an agent and view all the LLM calls originating from that request. Tracing Chains & Agents‚Äãfrom langchain.agents import AgentType, initialize_agent, load_tools  from langchain.llms import OpenAIfrom langchain.utilities import Portkey# Add the Portkey API Key from your accountheaders = Portkey.Config(    api_key = "<PORTKEY_API_KEY>",    trace_id = "fef659")llm = OpenAI(temperature=0, headers=headers)  tools = load_tools(["serpapi", "llm-math"], llm=llm)  agent = initialize_agent(tools, llm, agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION, verbose=True)    # Let's test it out!  agent.run("What was the high temperature in SF yesterday in Fahrenheit? What is that number raised to the .023 power?")API Reference:AgentTypeinitialize_agentload_toolsOpenAIPortkeyYou can see the requests' logs along with the trace id on Portkey dashboard:Advanced Features‚ÄãLogging: Log all your LLM requests automatically by sending them through Portkey. Each request log contains timestamp, model name, total cost, request time, request json, response json, and additional Portkey features.Tracing: Trace id can be passed along with each request and is visibe on the logs on Portkey dashboard. You can also set a distinct trace id for each request. You can append user feedback to a trace id as well.Caching: Respond to previously served customers queries from cache instead of sending them again to OpenAI. Match exact strings OR semantically similar strings. Cache can save costs and reduce latencies by 20x.Retries: Automatically reprocess any unsuccessful API requests upto 5 times. Uses an exponential backoff strategy, which spaces out retry attempts to prevent network overload.Tagging: Track and audit each user interaction in high detail with predefined tags.FeatureConfig KeyValue (Type)Required/OptionalAPI Keyapi_keyAPI Key (string)‚úÖ RequiredTracing Requeststrace_idCustom string‚ùî OptionalAutomatic Retriesretry_countinteger [1,2,3,4,5]‚ùî OptionalEnabling Cachecachesimple OR semantic‚ùî OptionalCache Force Refreshcache_force_refreshTrue‚ùî OptionalSet Cache Expirycache_ageinteger (in seconds)‚ùî OptionalAdd Useruserstring‚ùî OptionalAdd Organisationorganisationstring‚ùî OptionalAdd Environmentenvironmentstring‚ùî OptionalAdd Prompt (version/id/string)promptstring‚ùî OptionalEnabling all Portkey Features:‚Äãheaders = Portkey.Config(        # Mandatory    api_key="<PORTKEY_API_KEY>",          # Cache Options    cache="semantic",                     cache_force_refresh="True",                 cache_age=1729,      # Advanced    retry_count=5,                                               trace_id="langchain_agent",                              # Metadata    environment="production",            user="john",                          organisation="acme",                 prompt="Frost"    )For detailed information on each feature and how to use it, please refer to the Portkey docs. If you have any questions or need further assistance, reach out to us on Twitter..PreviousPipelineAINextLog, Trace, and MonitorLLMOps for LangchainUsing Portkey with LangchainTracing Chains & AgentsAdvanced FeaturesEnabling all Portkey Features:CommunityDiscordTwitterGitHubPythonJS/TSMoreHomepageBlogCopyright ¬© 2023 LangChain, Inc. Skip to main content ü¶úÔ∏èüîó LangChainDocsUse casesIntegrationsAPILangSmithJS/TS DocsCTRLK ü¶úÔ∏èüîó LangChainDocsUse casesIntegrationsAPI LangSmithJS/TS DocsCTRLK  CTRLK CTRLK  CTRLK   IntegrationsCallbacksChat modelsChat loadersDocument loadersDocument transformersLLMsMemoryRetrieversText embedding modelsAgents & ToolkitsToolsVector storesGrouped by providerActiveloop Deep LakeAI21 LabsAimAINetworkAirbyteAirtableAleph AlphaAlibaba Cloud OpensearchAmazon API GatewayAnalyticDBAnnoyAnyscaleApifyArangoDBArgillaArthurArxivAtlasAwaDBAWS S3 DirectoryAZLyricsAzure Blob StorageAzure Cognitive SearchAzure OpenAIBagelDBBananaBasetenBeamBedrockBiliBiliNIBittensorBlackboardBrave SearchCassandraCerebriumAIChaindeskChromaClarifaiClearMLClickHouseCnosDBCohereCollege ConfidentialCometConfident AIConfluenceC TransformersDashVectorDatabricksDatadog TracingDatadog LogsDataForSEODeepInfraDeepSparseDiffbotDingoDiscordDocArrayDocugamiDuckDBElasticsearchEpsillaEverNoteFacebook ChatFacebook FaissFigmaFireworksFlyteForefrontAIGitGitBookGoldenGoogle BigQueryGoogle Cloud StorageGoogle DriveGoogle SearchGoogle SerperGoogle Vertex AI MatchingEngineGooseAIGPT4AllGraphsignalGrobidGutenbergHacker NewsHazy ResearchHeliconeHologresHugging FaceiFixitIMSDbInfinoJinaKonkoLanceDBLangChain Decorators ‚ú®Llama.cppLog10MarqoMediaWikiDumpMeilisearchMetalMicrosoft OneDriveMicrosoft PowerPointMicrosoft WordMilvusMinimaxMLflow AI GatewayMLflowModalModelScopeModern TreasuryMomentoMongoDB AtlasMotherduckMyScaleNeo4jNLPCloudNotion DBObsidianOpenAIOpenLLMOpenSearchOpenWeatherMapPetalsPostgres EmbeddingPGVectorPineconePipelineAIPortkeyLog, Trace, and MonitorPredibasePrediction GuardPromptLayerPsychicPubMedQdrantRay ServeRebuffRedditRedisReplicateRoamRocksetRunhouseRWKV-4SageMaker EndpointSageMaker TrackingScaNNSearxNG Search APISerpAPIShale ProtocolSingleStoreDBscikit-learnSlackspaCySpreedlyStarRocksStochasticAIStripeSupabase (Postgres)NebulaTairTelegramTencentVectorDBTensorFlow DatasetsTigris2MarkdownTrelloTruLensTwitterTypesenseUnstructuredUSearchVearchVectaraVespaWandB TracingWeights & BiasesWeatherWeaviateWhatsAppWhyLabsWikipediaWolfram AlphaWriterXataXorbits Inference (Xinference)Yeager.aiYouTubeZepZillizIntegrationsGrouped by providerPortkeyOn this pagePortkeyPortkey is a platform designed to streamline the deployment and improving the performance of your AI applications.LLMOps for Langchain‚ÄãPortkey brings production readiness to Langchain. With Portkey, you can  view detailed metrics & logs for all requests,  enable semantic cache to reduce latency & costs,  implement automatic retries & fallbacks for failed requests,  add custom tags to requests for better tracking and analysis and more.Using Portkey with Langchain‚ÄãUsing Portkey is as simple as just choosing which Portkey features you want, enabling them via headers=Portkey.Config and passing it in your LLM calls.To start, get your Portkey API key by signing up here. (Click the profile icon on the top left, then click on "Copy API Key")For OpenAI, a simple integration with logging feature would look like this:from langchain.llms import OpenAIfrom langchain.utilities import Portkey# Add the Portkey API Key from your accountheaders = Portkey.Config(    api_key = "<PORTKEY_API_KEY>")llm = OpenAI(temperature=0.9, headers=headers)llm.predict("What would be a good company name for a company that makes colorful socks?")API Reference:OpenAIPortkeyYour logs will be captured on your Portkey dashboard.A common Portkey X Langchain use case is to trace a chain or an agent and view all the LLM calls originating from that request. Tracing Chains & Agents‚Äãfrom langchain.agents import AgentType, initialize_agent, load_tools  from langchain.llms import OpenAIfrom langchain.utilities import Portkey# Add the Portkey API Key from your accountheaders = Portkey.Config(    api_key = "<PORTKEY_API_KEY>",    trace_id = "fef659")llm = OpenAI(temperature=0, headers=headers)  tools = load_tools(["serpapi", "llm-math"], llm=llm)  agent = initialize_agent(tools, llm, agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION, verbose=True)    # Let's test it out!  agent.run("What was the high temperature in SF yesterday in Fahrenheit? What is that number raised to the .023 power?")API Reference:AgentTypeinitialize_agentload_toolsOpenAIPortkeyYou can see the requests' logs along with the trace id on Portkey dashboard:Advanced Features‚ÄãLogging: Log all your LLM requests automatically by sending them through Portkey. Each request log contains timestamp, model name, total cost, request time, request json, response json, and additional Portkey features.Tracing: Trace id can be passed along with each request and is visibe on the logs on Portkey dashboard. You can also set a distinct trace id for each request. You can append user feedback to a trace id as well.Caching: Respond to previously served customers queries from cache instead of sending them again to OpenAI. Match exact strings OR semantically similar strings. Cache can save costs and reduce latencies by 20x.Retries: Automatically reprocess any unsuccessful API requests upto 5 times. Uses an exponential backoff strategy, which spaces out retry attempts to prevent network overload.Tagging: Track and audit each user interaction in high detail with predefined tags.FeatureConfig KeyValue (Type)Required/OptionalAPI Keyapi_keyAPI Key (string)‚úÖ RequiredTracing Requeststrace_idCustom string‚ùî OptionalAutomatic Retriesretry_countinteger [1,2,3,4,5]‚ùî OptionalEnabling Cachecachesimple OR semantic‚ùî OptionalCache Force Refreshcache_force_refreshTrue‚ùî OptionalSet Cache Expirycache_ageinteger (in seconds)‚ùî OptionalAdd Useruserstring‚ùî OptionalAdd Organisationorganisationstring‚ùî OptionalAdd Environmentenvironmentstring‚ùî OptionalAdd Prompt (version/id/string)promptstring‚ùî OptionalEnabling all Portkey Features:‚Äãheaders = Portkey.Config(        # Mandatory    api_key="<PORTKEY_API_KEY>",          # Cache Options    cache="semantic",                     cache_force_refresh="True",                 cache_age=1729,      # Advanced    retry_count=5,                                               trace_id="langchain_agent",                              # Metadata    environment="production",            user="john",                          organisation="acme",                 prompt="Frost"    )For detailed information on each feature and how to use it, please refer to the Portkey docs. If you have any questions or need further assistance, reach out to us on Twitter..PreviousPipelineAINextLog, Trace, and MonitorLLMOps for LangchainUsing Portkey with LangchainTracing Chains & AgentsAdvanced FeaturesEnabling all Portkey Features: IntegrationsCallbacksChat modelsChat loadersDocument loadersDocument transformersLLMsMemoryRetrieversText embedding modelsAgents & ToolkitsToolsVector storesGrouped by providerActiveloop Deep LakeAI21 LabsAimAINetworkAirbyteAirtableAleph AlphaAlibaba Cloud OpensearchAmazon API GatewayAnalyticDBAnnoyAnyscaleApifyArangoDBArgillaArthurArxivAtlasAwaDBAWS S3 DirectoryAZLyricsAzure Blob StorageAzure Cognitive SearchAzure OpenAIBagelDBBananaBasetenBeamBedrockBiliBiliNIBittensorBlackboardBrave SearchCassandraCerebriumAIChaindeskChromaClarifaiClearMLClickHouseCnosDBCohereCollege ConfidentialCometConfident AIConfluenceC TransformersDashVectorDatabricksDatadog TracingDatadog LogsDataForSEODeepInfraDeepSparseDiffbotDingoDiscordDocArrayDocugamiDuckDBElasticsearchEpsillaEverNoteFacebook ChatFacebook FaissFigmaFireworksFlyteForefrontAIGitGitBookGoldenGoogle BigQueryGoogle Cloud StorageGoogle DriveGoogle SearchGoogle SerperGoogle Vertex AI MatchingEngineGooseAIGPT4AllGraphsignalGrobidGutenbergHacker NewsHazy ResearchHeliconeHologresHugging FaceiFixitIMSDbInfinoJinaKonkoLanceDBLangChain Decorators ‚ú®Llama.cppLog10MarqoMediaWikiDumpMeilisearchMetalMicrosoft OneDriveMicrosoft PowerPointMicrosoft WordMilvusMinimaxMLflow AI GatewayMLflowModalModelScopeModern TreasuryMomentoMongoDB AtlasMotherduckMyScaleNeo4jNLPCloudNotion DBObsidianOpenAIOpenLLMOpenSearchOpenWeatherMapPetalsPostgres EmbeddingPGVectorPineconePipelineAIPortkeyLog, Trace, and MonitorPredibasePrediction GuardPromptLayerPsychicPubMedQdrantRay ServeRebuffRedditRedisReplicateRoamRocksetRunhouseRWKV-4SageMaker EndpointSageMaker TrackingScaNNSearxNG Search APISerpAPIShale ProtocolSingleStoreDBscikit-learnSlackspaCySpreedlyStarRocksStochasticAIStripeSupabase (Postgres)NebulaTairTelegramTencentVectorDBTensorFlow DatasetsTigris2MarkdownTrelloTruLensTwitterTypesenseUnstructuredUSearchVearchVectaraVespaWandB TracingWeights & BiasesWeatherWeaviateWhatsAppWhyLabsWikipediaWolfram AlphaWriterXataXorbits Inference (Xinference)Yeager.aiYouTubeZepZillizIntegrationsGrouped by providerPortkeyOn this pagePortkeyPortkey is a platform designed to streamline the deployment and improving the performance of your AI applications.LLMOps for Langchain‚ÄãPortkey brings production readiness to Langchain. With Portkey, you can  view detailed metrics & logs for all requests,  enable semantic cache to reduce latency & costs,  implement automatic retries & fallbacks for failed requests,  add custom tags to requests for better tracking and analysis and more.Using Portkey with Langchain‚ÄãUsing Portkey is as simple as just choosing which Portkey features you want, enabling them via headers=Portkey.Config and passing it in your LLM calls.To start, get your Portkey API key by signing up here. (Click the profile icon on the top left, then click on "Copy API Key")For OpenAI, a simple integration with logging feature would look like this:from langchain.llms import OpenAIfrom langchain.utilities import Portkey# Add the Portkey API Key from your accountheaders = Portkey.Config(    api_key = "<PORTKEY_API_KEY>")llm = OpenAI(temperature=0.9, headers=headers)llm.predict("What would be a good company name for a company that makes colorful socks?")API Reference:OpenAIPortkeyYour logs will be captured on your Portkey dashboard.A common Portkey X Langchain use case is to trace a chain or an agent and view all the LLM calls originating from that request. Tracing Chains & Agents‚Äãfrom langchain.agents import AgentType, initialize_agent, load_tools  from langchain.llms import OpenAIfrom langchain.utilities import Portkey# Add the Portkey API Key from your accountheaders = Portkey.Config(    api_key = "<PORTKEY_API_KEY>",    trace_id = "fef659")llm = OpenAI(temperature=0, headers=headers)  tools = load_tools(["serpapi", "llm-math"], llm=llm)  agent = initialize_agent(tools, llm, agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION, verbose=True)    # Let's test it out!  agent.run("What was the high temperature in SF yesterday in Fahrenheit? What is that number raised to the .023 power?")API Reference:AgentTypeinitialize_agentload_toolsOpenAIPortkeyYou can see the requests' logs along with the trace id on Portkey dashboard:Advanced Features‚ÄãLogging: Log all your LLM requests automatically by sending them through Portkey. Each request log contains timestamp, model name, total cost, request time, request json, response json, and additional Portkey features.Tracing: Trace id can be passed along with each request and is visibe on the logs on Portkey dashboard. You can also set a distinct trace id for each request. You can append user feedback to a trace id as well.Caching: Respond to previously served customers queries from cache instead of sending them again to OpenAI. Match exact strings OR semantically similar strings. Cache can save costs and reduce latencies by 20x.Retries: Automatically reprocess any unsuccessful API requests upto 5 times. Uses an exponential backoff strategy, which spaces out retry attempts to prevent network overload.Tagging: Track and audit each user interaction in high detail with predefined tags.FeatureConfig KeyValue (Type)Required/OptionalAPI Keyapi_keyAPI Key (string)‚úÖ RequiredTracing Requeststrace_idCustom string‚ùî OptionalAutomatic Retriesretry_countinteger [1,2,3,4,5]‚ùî OptionalEnabling Cachecachesimple OR semantic‚ùî OptionalCache Force Refreshcache_force_refreshTrue‚ùî OptionalSet Cache Expirycache_ageinteger (in seconds)‚ùî OptionalAdd Useruserstring‚ùî OptionalAdd Organisationorganisationstring‚ùî OptionalAdd Environmentenvironmentstring‚ùî OptionalAdd Prompt (version/id/string)promptstring‚ùî OptionalEnabling all Portkey Features:‚Äãheaders = Portkey.Config(        # Mandatory    api_key="<PORTKEY_API_KEY>",          # Cache Options    cache="semantic",                     cache_force_refresh="True",                 cache_age=1729,      # Advanced    retry_count=5,                                               trace_id="langchain_agent",                              # Metadata    environment="production",            user="john",                          organisation="acme",                 prompt="Frost"    )For detailed information on each feature and how to use it, please refer to the Portkey docs. If you have any questions or need further assistance, reach out to us on Twitter..PreviousPipelineAINextLog, Trace, and MonitorLLMOps for LangchainUsing Portkey with LangchainTracing Chains & AgentsAdvanced FeaturesEnabling all Portkey Features: IntegrationsCallbacksChat modelsChat loadersDocument loadersDocument transformersLLMsMemoryRetrieversText embedding modelsAgents & ToolkitsToolsVector storesGrouped by providerActiveloop Deep LakeAI21 LabsAimAINetworkAirbyteAirtableAleph AlphaAlibaba Cloud OpensearchAmazon API GatewayAnalyticDBAnnoyAnyscaleApifyArangoDBArgillaArthurArxivAtlasAwaDBAWS S3 DirectoryAZLyricsAzure Blob StorageAzure Cognitive SearchAzure OpenAIBagelDBBananaBasetenBeamBedrockBiliBiliNIBittensorBlackboardBrave SearchCassandraCerebriumAIChaindeskChromaClarifaiClearMLClickHouseCnosDBCohereCollege ConfidentialCometConfident AIConfluenceC TransformersDashVectorDatabricksDatadog TracingDatadog LogsDataForSEODeepInfraDeepSparseDiffbotDingoDiscordDocArrayDocugamiDuckDBElasticsearchEpsillaEverNoteFacebook ChatFacebook FaissFigmaFireworksFlyteForefrontAIGitGitBookGoldenGoogle BigQueryGoogle Cloud StorageGoogle DriveGoogle SearchGoogle SerperGoogle Vertex AI MatchingEngineGooseAIGPT4AllGraphsignalGrobidGutenbergHacker NewsHazy ResearchHeliconeHologresHugging FaceiFixitIMSDbInfinoJinaKonkoLanceDBLangChain Decorators ‚ú®Llama.cppLog10MarqoMediaWikiDumpMeilisearchMetalMicrosoft OneDriveMicrosoft PowerPointMicrosoft WordMilvusMinimaxMLflow AI GatewayMLflowModalModelScopeModern TreasuryMomentoMongoDB AtlasMotherduckMyScaleNeo4jNLPCloudNotion DBObsidianOpenAIOpenLLMOpenSearchOpenWeatherMapPetalsPostgres EmbeddingPGVectorPineconePipelineAIPortkeyLog, Trace, and MonitorPredibasePrediction GuardPromptLayerPsychicPubMedQdrantRay ServeRebuffRedditRedisReplicateRoamRocksetRunhouseRWKV-4SageMaker EndpointSageMaker TrackingScaNNSearxNG Search APISerpAPIShale ProtocolSingleStoreDBscikit-learnSlackspaCySpreedlyStarRocksStochasticAIStripeSupabase (Postgres)NebulaTairTelegramTencentVectorDBTensorFlow DatasetsTigris2MarkdownTrelloTruLensTwitterTypesenseUnstructuredUSearchVearchVectaraVespaWandB TracingWeights & BiasesWeatherWeaviateWhatsAppWhyLabsWikipediaWolfram AlphaWriterXataXorbits Inference (Xinference)Yeager.aiYouTubeZepZilliz IntegrationsCallbacksChat modelsChat loadersDocument loadersDocument transformersLLMsMemoryRetrieversText embedding modelsAgents & ToolkitsToolsVector storesGrouped by providerActiveloop Deep LakeAI21 LabsAimAINetworkAirbyteAirtableAleph AlphaAlibaba Cloud OpensearchAmazon API GatewayAnalyticDBAnnoyAnyscaleApifyArangoDBArgillaArthurArxivAtlasAwaDBAWS S3 DirectoryAZLyricsAzure Blob StorageAzure Cognitive SearchAzure OpenAIBagelDBBananaBasetenBeamBedrockBiliBiliNIBittensorBlackboardBrave SearchCassandraCerebriumAIChaindeskChromaClarifaiClearMLClickHouseCnosDBCohereCollege ConfidentialCometConfident AIConfluenceC TransformersDashVectorDatabricksDatadog TracingDatadog LogsDataForSEODeepInfraDeepSparseDiffbotDingoDiscordDocArrayDocugamiDuckDBElasticsearchEpsillaEverNoteFacebook ChatFacebook FaissFigmaFireworksFlyteForefrontAIGitGitBookGoldenGoogle BigQueryGoogle Cloud StorageGoogle DriveGoogle SearchGoogle SerperGoogle Vertex AI MatchingEngineGooseAIGPT4AllGraphsignalGrobidGutenbergHacker NewsHazy ResearchHeliconeHologresHugging FaceiFixitIMSDbInfinoJinaKonkoLanceDBLangChain Decorators ‚ú®Llama.cppLog10MarqoMediaWikiDumpMeilisearchMetalMicrosoft OneDriveMicrosoft PowerPointMicrosoft WordMilvusMinimaxMLflow AI GatewayMLflowModalModelScopeModern TreasuryMomentoMongoDB AtlasMotherduckMyScaleNeo4jNLPCloudNotion DBObsidianOpenAIOpenLLMOpenSearchOpenWeatherMapPetalsPostgres EmbeddingPGVectorPineconePipelineAIPortkeyLog, Trace, and MonitorPredibasePrediction GuardPromptLayerPsychicPubMedQdrantRay ServeRebuffRedditRedisReplicateRoamRocksetRunhouseRWKV-4SageMaker EndpointSageMaker TrackingScaNNSearxNG Search APISerpAPIShale ProtocolSingleStoreDBscikit-learnSlackspaCySpreedlyStarRocksStochasticAIStripeSupabase (Postgres)NebulaTairTelegramTencentVectorDBTensorFlow DatasetsTigris2MarkdownTrelloTruLensTwitterTypesenseUnstructuredUSearchVearchVectaraVespaWandB TracingWeights & BiasesWeatherWeaviateWhatsAppWhyLabsWikipediaWolfram AlphaWriterXataXorbits Inference (Xinference)Yeager.aiYouTubeZepZilliz Integrations Callbacks Chat models Chat loaders Document loaders Document transformers LLMs Memory Retrievers Text embedding models Agents & Toolkits Tools Vector stores Grouped by provider Portkey Vectara IntegrationsGrouped by providerPortkeyOn this pagePortkeyPortkey is a platform designed to streamline the deployment and improving the performance of your AI applications.LLMOps for Langchain‚ÄãPortkey brings production readiness to Langchain. With Portkey, you can  view detailed metrics & logs for all requests,  enable semantic cache to reduce latency & costs,  implement automatic retries & fallbacks for failed requests,  add custom tags to requests for better tracking and analysis and more.Using Portkey with Langchain‚ÄãUsing Portkey is as simple as just choosing which Portkey features you want, enabling them via headers=Portkey.Config and passing it in your LLM calls.To start, get your Portkey API key by signing up here. (Click the profile icon on the top left, then click on "Copy API Key")For OpenAI, a simple integration with logging feature would look like this:from langchain.llms import OpenAIfrom langchain.utilities import Portkey# Add the Portkey API Key from your accountheaders = Portkey.Config(    api_key = "<PORTKEY_API_KEY>")llm = OpenAI(temperature=0.9, headers=headers)llm.predict("What would be a good company name for a company that makes colorful socks?")API Reference:OpenAIPortkeyYour logs will be captured on your Portkey dashboard.A common Portkey X Langchain use case is to trace a chain or an agent and view all the LLM calls originating from that request. Tracing Chains & Agents‚Äãfrom langchain.agents import AgentType, initialize_agent, load_tools  from langchain.llms import OpenAIfrom langchain.utilities import Portkey# Add the Portkey API Key from your accountheaders = Portkey.Config(    api_key = "<PORTKEY_API_KEY>",    trace_id = "fef659")llm = OpenAI(temperature=0, headers=headers)  tools = load_tools(["serpapi", "llm-math"], llm=llm)  agent = initialize_agent(tools, llm, agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION, verbose=True)    # Let's test it out!  agent.run("What was the high temperature in SF yesterday in Fahrenheit? What is that number raised to the .023 power?")API Reference:AgentTypeinitialize_agentload_toolsOpenAIPortkeyYou can see the requests' logs along with the trace id on Portkey dashboard:Advanced Features‚ÄãLogging: Log all your LLM requests automatically by sending them through Portkey. Each request log contains timestamp, model name, total cost, request time, request json, response json, and additional Portkey features.Tracing: Trace id can be passed along with each request and is visibe on the logs on Portkey dashboard. You can also set a distinct trace id for each request. You can append user feedback to a trace id as well.Caching: Respond to previously served customers queries from cache instead of sending them again to OpenAI. Match exact strings OR semantically similar strings. Cache can save costs and reduce latencies by 20x.Retries: Automatically reprocess any unsuccessful API requests upto 5 times. Uses an exponential backoff strategy, which spaces out retry attempts to prevent network overload.Tagging: Track and audit each user interaction in high detail with predefined tags.FeatureConfig KeyValue (Type)Required/OptionalAPI Keyapi_keyAPI Key (string)‚úÖ RequiredTracing Requeststrace_idCustom string‚ùî OptionalAutomatic Retriesretry_countinteger [1,2,3,4,5]‚ùî OptionalEnabling Cachecachesimple OR semantic‚ùî OptionalCache Force Refreshcache_force_refreshTrue‚ùî OptionalSet Cache Expirycache_ageinteger (in seconds)‚ùî OptionalAdd Useruserstring‚ùî OptionalAdd Organisationorganisationstring‚ùî OptionalAdd Environmentenvironmentstring‚ùî OptionalAdd Prompt (version/id/string)promptstring‚ùî OptionalEnabling all Portkey Features:‚Äãheaders = Portkey.Config(        # Mandatory    api_key="<PORTKEY_API_KEY>",          # Cache Options    cache="semantic",                     cache_force_refresh="True",                 cache_age=1729,      # Advanced    retry_count=5,                                               trace_id="langchain_agent",                              # Metadata    environment="production",            user="john",                          organisation="acme",                 prompt="Frost"    )For detailed information on each feature and how to use it, please refer to the Portkey docs. If you have any questions or need further assistance, reach out to us on Twitter..PreviousPipelineAINextLog, Trace, and MonitorLLMOps for LangchainUsing Portkey with LangchainTracing Chains & AgentsAdvanced FeaturesEnabling all Portkey Features: IntegrationsGrouped by providerPortkeyOn this pagePortkeyPortkey is a platform designed to streamline the deployment and improving the performance of your AI applications.LLMOps for Langchain‚ÄãPortkey brings production readiness to Langchain. With Portkey, you can  view detailed metrics & logs for all requests,  enable semantic cache to reduce latency & costs,  implement automatic retries & fallbacks for failed requests,  add custom tags to requests for better tracking and analysis and more.Using Portkey with Langchain‚ÄãUsing Portkey is as simple as just choosing which Portkey features you want, enabling them via headers=Portkey.Config and passing it in your LLM calls.To start, get your Portkey API key by signing up here. (Click the profile icon on the top left, then click on "Copy API Key")For OpenAI, a simple integration with logging feature would look like this:from langchain.llms import OpenAIfrom langchain.utilities import Portkey# Add the Portkey API Key from your accountheaders = Portkey.Config(    api_key = "<PORTKEY_API_KEY>")llm = OpenAI(temperature=0.9, headers=headers)llm.predict("What would be a good company name for a company that makes colorful socks?")API Reference:OpenAIPortkeyYour logs will be captured on your Portkey dashboard.A common Portkey X Langchain use case is to trace a chain or an agent and view all the LLM calls originating from that request. Tracing Chains & Agents‚Äãfrom langchain.agents import AgentType, initialize_agent, load_tools  from langchain.llms import OpenAIfrom langchain.utilities import Portkey# Add the Portkey API Key from your accountheaders = Portkey.Config(    api_key = "<PORTKEY_API_KEY>",    trace_id = "fef659")llm = OpenAI(temperature=0, headers=headers)  tools = load_tools(["serpapi", "llm-math"], llm=llm)  agent = initialize_agent(tools, llm, agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION, verbose=True)    # Let's test it out!  agent.run("What was the high temperature in SF yesterday in Fahrenheit? What is that number raised to the .023 power?")API Reference:AgentTypeinitialize_agentload_toolsOpenAIPortkeyYou can see the requests' logs along with the trace id on Portkey dashboard:Advanced Features‚ÄãLogging: Log all your LLM requests automatically by sending them through Portkey. Each request log contains timestamp, model name, total cost, request time, request json, response json, and additional Portkey features.Tracing: Trace id can be passed along with each request and is visibe on the logs on Portkey dashboard. You can also set a distinct trace id for each request. You can append user feedback to a trace id as well.Caching: Respond to previously served customers queries from cache instead of sending them again to OpenAI. Match exact strings OR semantically similar strings. Cache can save costs and reduce latencies by 20x.Retries: Automatically reprocess any unsuccessful API requests upto 5 times. Uses an exponential backoff strategy, which spaces out retry attempts to prevent network overload.Tagging: Track and audit each user interaction in high detail with predefined tags.FeatureConfig KeyValue (Type)Required/OptionalAPI Keyapi_keyAPI Key (string)‚úÖ RequiredTracing Requeststrace_idCustom string‚ùî OptionalAutomatic Retriesretry_countinteger [1,2,3,4,5]‚ùî OptionalEnabling Cachecachesimple OR semantic‚ùî OptionalCache Force Refreshcache_force_refreshTrue‚ùî OptionalSet Cache Expirycache_ageinteger (in seconds)‚ùî OptionalAdd Useruserstring‚ùî OptionalAdd Organisationorganisationstring‚ùî OptionalAdd Environmentenvironmentstring‚ùî OptionalAdd Prompt (version/id/string)promptstring‚ùî OptionalEnabling all Portkey Features:‚Äãheaders = Portkey.Config(        # Mandatory    api_key="<PORTKEY_API_KEY>",          # Cache Options    cache="semantic",                     cache_force_refresh="True",                 cache_age=1729,      # Advanced    retry_count=5,                                               trace_id="langchain_agent",                              # Metadata    environment="production",            user="john",                          organisation="acme",                 prompt="Frost"    )For detailed information on each feature and how to use it, please refer to the Portkey docs. If you have any questions or need further assistance, reach out to us on Twitter..PreviousPipelineAINextLog, Trace, and Monitor IntegrationsGrouped by providerPortkeyOn this pagePortkeyPortkey is a platform designed to streamline the deployment and improving the performance of your AI applications.LLMOps for Langchain‚ÄãPortkey brings production readiness to Langchain. With Portkey, you can  view detailed metrics & logs for all requests,  enable semantic cache to reduce latency & costs,  implement automatic retries & fallbacks for failed requests,  add custom tags to requests for better tracking and analysis and more.Using Portkey with Langchain‚ÄãUsing Portkey is as simple as just choosing which Portkey features you want, enabling them via headers=Portkey.Config and passing it in your LLM calls.To start, get your Portkey API key by signing up here. (Click the profile icon on the top left, then click on "Copy API Key")For OpenAI, a simple integration with logging feature would look like this:from langchain.llms import OpenAIfrom langchain.utilities import Portkey# Add the Portkey API Key from your accountheaders = Portkey.Config(    api_key = "<PORTKEY_API_KEY>")llm = OpenAI(temperature=0.9, headers=headers)llm.predict("What would be a good company name for a company that makes colorful socks?")API Reference:OpenAIPortkeyYour logs will be captured on your Portkey dashboard.A common Portkey X Langchain use case is to trace a chain or an agent and view all the LLM calls originating from that request. Tracing Chains & Agents‚Äãfrom langchain.agents import AgentType, initialize_agent, load_tools  from langchain.llms import OpenAIfrom langchain.utilities import Portkey# Add the Portkey API Key from your accountheaders = Portkey.Config(    api_key = "<PORTKEY_API_KEY>",    trace_id = "fef659")llm = OpenAI(temperature=0, headers=headers)  tools = load_tools(["serpapi", "llm-math"], llm=llm)  agent = initialize_agent(tools, llm, agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION, verbose=True)    # Let's test it out!  agent.run("What was the high temperature in SF yesterday in Fahrenheit? What is that number raised to the .023 power?")API Reference:AgentTypeinitialize_agentload_toolsOpenAIPortkeyYou can see the requests' logs along with the trace id on Portkey dashboard:Advanced Features‚ÄãLogging: Log all your LLM requests automatically by sending them through Portkey. Each request log contains timestamp, model name, total cost, request time, request json, response json, and additional Portkey features.Tracing: Trace id can be passed along with each request and is visibe on the logs on Portkey dashboard. You can also set a distinct trace id for each request. You can append user feedback to a trace id as well.Caching: Respond to previously served customers queries from cache instead of sending them again to OpenAI. Match exact strings OR semantically similar strings. Cache can save costs and reduce latencies by 20x.Retries: Automatically reprocess any unsuccessful API requests upto 5 times. Uses an exponential backoff strategy, which spaces out retry attempts to prevent network overload.Tagging: Track and audit each user interaction in high detail with predefined tags.FeatureConfig KeyValue (Type)Required/OptionalAPI Keyapi_keyAPI Key (string)‚úÖ RequiredTracing Requeststrace_idCustom string‚ùî OptionalAutomatic Retriesretry_countinteger [1,2,3,4,5]‚ùî OptionalEnabling Cachecachesimple OR semantic‚ùî OptionalCache Force Refreshcache_force_refreshTrue‚ùî OptionalSet Cache Expirycache_ageinteger (in seconds)‚ùî OptionalAdd Useruserstring‚ùî OptionalAdd Organisationorganisationstring‚ùî OptionalAdd Environmentenvironmentstring‚ùî OptionalAdd Prompt (version/id/string)promptstring‚ùî OptionalEnabling all Portkey Features:‚Äãheaders = Portkey.Config(        # Mandatory    api_key="<PORTKEY_API_KEY>",          # Cache Options    cache="semantic",                     cache_force_refresh="True",                 cache_age=1729,      # Advanced    retry_count=5,                                               trace_id="langchain_agent",                              # Metadata    environment="production",            user="john",                          organisation="acme",                 prompt="Frost"    )For detailed information on each feature and how to use it, please refer to the Portkey docs. If you have any questions or need further assistance, reach out to us on Twitter..PreviousPipelineAINextLog, Trace, and Monitor On this page PortkeyPortkey is a platform designed to streamline the deployment and improving the performance of your AI applications.LLMOps for Langchain‚ÄãPortkey brings production readiness to Langchain. With Portkey, you can  view detailed metrics & logs for all requests,  enable semantic cache to reduce latency & costs,  implement automatic retries & fallbacks for failed requests,  add custom tags to requests for better tracking and analysis and more.Using Portkey with Langchain‚ÄãUsing Portkey is as simple as just choosing which Portkey features you want, enabling them via headers=Portkey.Config and passing it in your LLM calls.To start, get your Portkey API key by signing up here. (Click the profile icon on the top left, then click on "Copy API Key")For OpenAI, a simple integration with logging feature would look like this:from langchain.llms import OpenAIfrom langchain.utilities import Portkey# Add the Portkey API Key from your accountheaders = Portkey.Config(    api_key = "<PORTKEY_API_KEY>")llm = OpenAI(temperature=0.9, headers=headers)llm.predict("What would be a good company name for a company that makes colorful socks?")API Reference:OpenAIPortkeyYour logs will be captured on your Portkey dashboard.A common Portkey X Langchain use case is to trace a chain or an agent and view all the LLM calls originating from that request. Tracing Chains & Agents‚Äãfrom langchain.agents import AgentType, initialize_agent, load_tools  from langchain.llms import OpenAIfrom langchain.utilities import Portkey# Add the Portkey API Key from your accountheaders = Portkey.Config(    api_key = "<PORTKEY_API_KEY>",    trace_id = "fef659")llm = OpenAI(temperature=0, headers=headers)  tools = load_tools(["serpapi", "llm-math"], llm=llm)  agent = initialize_agent(tools, llm, agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION, verbose=True)    # Let's test it out!  agent.run("What was the high temperature in SF yesterday in Fahrenheit? What is that number raised to the .023 power?")API Reference:AgentTypeinitialize_agentload_toolsOpenAIPortkeyYou can see the requests' logs along with the trace id on Portkey dashboard:Advanced Features‚ÄãLogging: Log all your LLM requests automatically by sending them through Portkey. Each request log contains timestamp, model name, total cost, request time, request json, response json, and additional Portkey features.Tracing: Trace id can be passed along with each request and is visibe on the logs on Portkey dashboard. You can also set a distinct trace id for each request. You can append user feedback to a trace id as well.Caching: Respond to previously served customers queries from cache instead of sending them again to OpenAI. Match exact strings OR semantically similar strings. Cache can save costs and reduce latencies by 20x.Retries: Automatically reprocess any unsuccessful API requests upto 5 times. Uses an exponential backoff strategy, which spaces out retry attempts to prevent network overload.Tagging: Track and audit each user interaction in high detail with predefined tags.FeatureConfig KeyValue (Type)Required/OptionalAPI Keyapi_keyAPI Key (string)‚úÖ RequiredTracing Requeststrace_idCustom string‚ùî OptionalAutomatic Retriesretry_countinteger [1,2,3,4,5]‚ùî OptionalEnabling Cachecachesimple OR semantic‚ùî OptionalCache Force Refreshcache_force_refreshTrue‚ùî OptionalSet Cache Expirycache_ageinteger (in seconds)‚ùî OptionalAdd Useruserstring‚ùî OptionalAdd Organisationorganisationstring‚ùî OptionalAdd Environmentenvironmentstring‚ùî OptionalAdd Prompt (version/id/string)promptstring‚ùî OptionalEnabling all Portkey Features:‚Äãheaders = Portkey.Config(        # Mandatory    api_key="<PORTKEY_API_KEY>",          # Cache Options    cache="semantic",                     cache_force_refresh="True",                 cache_age=1729,      # Advanced    retry_count=5,                                               trace_id="langchain_agent",                              # Metadata    environment="production",            user="john",                          organisation="acme",                 prompt="Frost"    )For detailed information on each feature and how to use it, please refer to the Portkey docs. If you have any questions or need further assistance, reach out to us on Twitter.. from langchain.llms import OpenAIfrom langchain.utilities import Portkey# Add the Portkey API Key from your accountheaders = Portkey.Config(    api_key = "<PORTKEY_API_KEY>")llm = OpenAI(temperature=0.9, headers=headers)llm.predict("What would be a good company name for a company that makes colorful socks?") from langchain.llms import OpenAIfrom langchain.utilities import Portkey# Add the Portkey API Key from your accountheaders = Portkey.Config(    api_key = "<PORTKEY_API_KEY>")llm = OpenAI(temperature=0.9, headers=headers)llm.predict("What would be a good company name for a company that makes colorful socks?")  API Reference:OpenAIPortkey from langchain.agents import AgentType, initialize_agent, load_tools  from langchain.llms import OpenAIfrom langchain.utilities import Portkey# Add the Portkey API Key from your accountheaders = Portkey.Config(    api_key = "<PORTKEY_API_KEY>",    trace_id = "fef659")llm = OpenAI(temperature=0, headers=headers)  tools = load_tools(["serpapi", "llm-math"], llm=llm)  agent = initialize_agent(tools, llm, agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION, verbose=True)    # Let's test it out!  agent.run("What was the high temperature in SF yesterday in Fahrenheit? What is that number raised to the .023 power?") from langchain.agents import AgentType, initialize_agent, load_tools  from langchain.llms import OpenAIfrom langchain.utilities import Portkey# Add the Portkey API Key from your accountheaders = Portkey.Config(    api_key = "<PORTKEY_API_KEY>",    trace_id = "fef659")llm = OpenAI(temperature=0, headers=headers)  tools = load_tools(["serpapi", "llm-math"], llm=llm)  agent = initialize_agent(tools, llm, agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION, verbose=True)    # Let's test it out!  agent.run("What was the high temperature in SF yesterday in Fahrenheit? What is that number raised to the .023 power?")  API Reference:AgentTypeinitialize_agentload_toolsOpenAIPortkey headers = Portkey.Config(        # Mandatory    api_key="<PORTKEY_API_KEY>",          # Cache Options    cache="semantic",                     cache_force_refresh="True",                 cache_age=1729,      # Advanced    retry_count=5,                                               trace_id="langchain_agent",                              # Metadata    environment="production",            user="john",                          organisation="acme",                 prompt="Frost"    ) headers = Portkey.Config(        # Mandatory    api_key="<PORTKEY_API_KEY>",          # Cache Options    cache="semantic",                     cache_force_refresh="True",                 cache_age=1729,      # Advanced    retry_count=5,                                               trace_id="langchain_agent",                              # Metadata    environment="production",            user="john",                          organisation="acme",                 prompt="Frost"    )  Previous PipelineAI Next Log, Trace, and Monitor LLMOps for LangchainUsing Portkey with LangchainTracing Chains & AgentsAdvanced FeaturesEnabling all Portkey Features: LLMOps for LangchainUsing Portkey with LangchainTracing Chains & AgentsAdvanced FeaturesEnabling all Portkey Features: CommunityDiscordTwitterGitHubPythonJS/TSMoreHomepageBlogCopyright ¬© 2023 LangChain, Inc. CommunityDiscordTwitterGitHubPythonJS/TSMoreHomepageBlog CommunityDiscordTwitter Community GitHubPythonJS/TS GitHub MoreHomepageBlog More Copyright ¬© 2023 LangChain, Inc. Copyright ¬© 2023 LangChain, Inc. Skip to main content (#docusaurus_skipToContent_fallback) ü¶úÔ∏èüîó LangChain (/) Docs (/docs/get_started/introduction) Use cases (/docs/use_cases) Integrations (/docs/integrations) API (https://api.python.langchain.com) LangSmith (https://smith.langchain.com) JS/TS Docs (https://js.langchain.com/docs)  (https://github.com/hwchase17/langchain) Integrations (/docs/integrations) Callbacks (/docs/integrations/callbacks/) Chat models (/docs/integrations/chat/) Chat loaders (/docs/integrations/chat_loaders/) Document loaders (/docs/integrations/document_loaders/) Document transformers (/docs/integrations/document_transformers/) LLMs (/docs/integrations/llms/) Memory (/docs/integrations/memory/) Retrievers (/docs/integrations/retrievers/) Text embedding models (/docs/integrations/text_embedding/) Agents & Toolkits (/docs/integrations/toolkits/) Tools (/docs/integrations/tools/) Vector stores (/docs/integrations/vectorstores/) Grouped by provider (/docs/integrations/providers/) Activeloop Deep Lake (/docs/integrations/providers/activeloop_deeplake) AI21 Labs (/docs/integrations/providers/ai21) Aim (/docs/integrations/providers/aim_tracking) AINetwork (/docs/integrations/providers/ainetwork) Airbyte (/docs/integrations/providers/airbyte) Airtable (/docs/integrations/providers/airtable) Aleph Alpha (/docs/integrations/providers/aleph_alpha) Alibaba Cloud Opensearch (/docs/integrations/providers/alibabacloud_opensearch) Amazon API Gateway (/docs/integrations/providers/amazon_api_gateway) AnalyticDB (/docs/integrations/providers/analyticdb) Annoy (/docs/integrations/providers/annoy) Anyscale (/docs/integrations/providers/anyscale) Apify (/docs/integrations/providers/apify) ArangoDB (/docs/integrations/providers/arangodb) Argilla (/docs/integrations/providers/argilla) Arthur (/docs/integrations/providers/arthur_tracking) Arxiv (/docs/integrations/providers/arxiv) Atlas (/docs/integrations/providers/atlas) AwaDB (/docs/integrations/providers/awadb) AWS S3 Directory (/docs/integrations/providers/aws_s3) AZLyrics (/docs/integrations/providers/azlyrics) Azure Blob Storage (/docs/integrations/providers/azure_blob_storage) Azure Cognitive Search (/docs/integrations/providers/azure_cognitive_search_) Azure OpenAI (/docs/integrations/providers/azure_openai) BagelDB (/docs/integrations/providers/bageldb) Banana (/docs/integrations/providers/bananadev) Baseten (/docs/integrations/providers/baseten) Beam (/docs/integrations/providers/beam) Bedrock (/docs/integrations/providers/bedrock) BiliBili (/docs/integrations/providers/bilibili) NIBittensor (/docs/integrations/providers/bittensor) Blackboard (/docs/integrations/providers/blackboard) Brave Search (/docs/integrations/providers/brave_search) Cassandra (/docs/integrations/providers/cassandra) CerebriumAI (/docs/integrations/providers/cerebriumai) Chaindesk (/docs/integrations/providers/chaindesk) Chroma (/docs/integrations/providers/chroma) Clarifai (/docs/integrations/providers/clarifai) ClearML (/docs/integrations/providers/clearml_tracking) ClickHouse (/docs/integrations/providers/clickhouse) CnosDB (/docs/integrations/providers/cnosdb) Cohere (/docs/integrations/providers/cohere) College Confidential (/docs/integrations/providers/college_confidential) Comet (/docs/integrations/providers/comet_tracking) Confident AI (/docs/integrations/providers/confident) Confluence (/docs/integrations/providers/confluence) C Transformers (/docs/integrations/providers/ctransformers) DashVector (/docs/integrations/providers/dashvector) Databricks (/docs/integrations/providers/databricks) Datadog Tracing (/docs/integrations/providers/datadog) Datadog Logs (/docs/integrations/providers/datadog_logs) DataForSEO (/docs/integrations/providers/dataforseo) DeepInfra (/docs/integrations/providers/deepinfra) DeepSparse (/docs/integrations/providers/deepsparse) Diffbot (/docs/integrations/providers/diffbot) Dingo (/docs/integrations/providers/dingo) Discord (/docs/integrations/providers/discord) DocArray (/docs/integrations/providers/docarray) Docugami (/docs/integrations/providers/docugami) DuckDB (/docs/integrations/providers/duckdb) Elasticsearch (/docs/integrations/providers/elasticsearch) Epsilla (/docs/integrations/providers/epsilla) EverNote (/docs/integrations/providers/evernote) Facebook Chat (/docs/integrations/providers/facebook_chat) Facebook Faiss (/docs/integrations/providers/facebook_faiss) Figma (/docs/integrations/providers/figma) Fireworks (/docs/integrations/providers/fireworks) Flyte (/docs/integrations/providers/flyte) ForefrontAI (/docs/integrations/providers/forefrontai) Git (/docs/integrations/providers/git) GitBook (/docs/integrations/providers/gitbook) Golden (/docs/integrations/providers/golden) Google BigQuery (/docs/integrations/providers/google_bigquery) Google Cloud Storage (/docs/integrations/providers/google_cloud_storage) Google Drive (/docs/integrations/providers/google_drive) Google Search (/docs/integrations/providers/google_search) Google Serper (/docs/integrations/providers/google_serper) Google Vertex AI MatchingEngine (/docs/integrations/providers/google_vertex_ai_matchingengine) GooseAI (/docs/integrations/providers/gooseai) GPT4All (/docs/integrations/providers/gpt4all) Graphsignal (/docs/integrations/providers/graphsignal) Grobid (/docs/integrations/providers/grobid) Gutenberg (/docs/integrations/providers/gutenberg) Hacker News (/docs/integrations/providers/hacker_news) Hazy Research (/docs/integrations/providers/hazy_research) Helicone (/docs/integrations/providers/helicone) Hologres (/docs/integrations/providers/hologres) Hugging Face (/docs/integrations/providers/huggingface) iFixit (/docs/integrations/providers/ifixit) IMSDb (/docs/integrations/providers/imsdb) Infino (/docs/integrations/providers/infino) Jina (/docs/integrations/providers/jina) Konko (/docs/integrations/providers/konko) LanceDB (/docs/integrations/providers/lancedb) LangChain Decorators ‚ú® (/docs/integrations/providers/langchain_decorators) Llama.cpp (/docs/integrations/providers/llamacpp) Log10 (/docs/integrations/providers/log10) Marqo (/docs/integrations/providers/marqo) MediaWikiDump (/docs/integrations/providers/mediawikidump) Meilisearch (/docs/integrations/providers/meilisearch) Metal (/docs/integrations/providers/metal) Microsoft OneDrive (/docs/integrations/providers/microsoft_onedrive) Microsoft PowerPoint (/docs/integrations/providers/microsoft_powerpoint) Microsoft Word (/docs/integrations/providers/microsoft_word) Milvus (/docs/integrations/providers/milvus) Minimax (/docs/integrations/providers/minimax) MLflow AI Gateway (/docs/integrations/providers/mlflow_ai_gateway) MLflow (/docs/integrations/providers/mlflow_tracking) Modal (/docs/integrations/providers/modal) ModelScope (/docs/integrations/providers/modelscope) Modern Treasury (/docs/integrations/providers/modern_treasury) Momento (/docs/integrations/providers/momento) MongoDB Atlas (/docs/integrations/providers/mongodb_atlas) Motherduck (/docs/integrations/providers/motherduck) MyScale (/docs/integrations/providers/myscale) Neo4j (/docs/integrations/providers/neo4j) NLPCloud (/docs/integrations/providers/nlpcloud) Notion DB (/docs/integrations/providers/notion) Obsidian (/docs/integrations/providers/obsidian) OpenAI (/docs/integrations/providers/openai) OpenLLM (/docs/integrations/providers/openllm) OpenSearch (/docs/integrations/providers/opensearch) OpenWeatherMap (/docs/integrations/providers/openweathermap) Petals (/docs/integrations/providers/petals) Postgres Embedding (/docs/integrations/providers/pg_embedding) PGVector (/docs/integrations/providers/pgvector) Pinecone (/docs/integrations/providers/pinecone) PipelineAI (/docs/integrations/providers/pipelineai) Portkey (/docs/integrations/providers/portkey/) Log, Trace, and Monitor (/docs/integrations/providers/portkey/logging_tracing_portkey) Predibase (/docs/integrations/providers/predibase) Prediction Guard (/docs/integrations/providers/predictionguard) PromptLayer (/docs/integrations/providers/promptlayer) Psychic (/docs/integrations/providers/psychic) PubMed (/docs/integrations/providers/pubmed) Qdrant (/docs/integrations/providers/qdrant) Ray Serve (/docs/integrations/providers/ray_serve) Rebuff (/docs/integrations/providers/rebuff) Reddit (/docs/integrations/providers/reddit) Redis (/docs/integrations/providers/redis) Replicate (/docs/integrations/providers/replicate) Roam (/docs/integrations/providers/roam) Rockset (/docs/integrations/providers/rockset) Runhouse (/docs/integrations/providers/runhouse) RWKV-4 (/docs/integrations/providers/rwkv) SageMaker Endpoint (/docs/integrations/providers/sagemaker_endpoint) SageMaker Tracking (/docs/integrations/providers/sagemaker_tracking) ScaNN (/docs/integrations/providers/scann) SearxNG Search API (/docs/integrations/providers/searx) SerpAPI (/docs/integrations/providers/serpapi) Shale Protocol (/docs/integrations/providers/shaleprotocol) SingleStoreDB (/docs/integrations/providers/singlestoredb) scikit-learn (/docs/integrations/providers/sklearn) Slack (/docs/integrations/providers/slack) spaCy (/docs/integrations/providers/spacy) Spreedly (/docs/integrations/providers/spreedly) StarRocks (/docs/integrations/providers/starrocks) StochasticAI (/docs/integrations/providers/stochasticai) Stripe (/docs/integrations/providers/stripe) Supabase (Postgres) (/docs/integrations/providers/supabase) Nebula (/docs/integrations/providers/symblai_nebula) Tair (/docs/integrations/providers/tair) Telegram (/docs/integrations/providers/telegram) TencentVectorDB (/docs/integrations/providers/tencentvectordb) TensorFlow Datasets (/docs/integrations/providers/tensorflow_datasets) Tigris (/docs/integrations/providers/tigris) 2Markdown (/docs/integrations/providers/tomarkdown) Trello (/docs/integrations/providers/trello) TruLens (/docs/integrations/providers/trulens) Twitter (/docs/integrations/providers/twitter) Typesense (/docs/integrations/providers/typesense) Unstructured (/docs/integrations/providers/unstructured) USearch (/docs/integrations/providers/usearch) Vearch (/docs/integrations/providers/vearch) Vectara (/docs/integrations/providers/vectara/) Vespa (/docs/integrations/providers/vespa) WandB Tracing (/docs/integrations/providers/wandb_tracing) Weights & Biases (/docs/integrations/providers/wandb_tracking) Weather (/docs/integrations/providers/weather) Weaviate (/docs/integrations/providers/weaviate) WhatsApp (/docs/integrations/providers/whatsapp) WhyLabs (/docs/integrations/providers/whylabs_profiling) Wikipedia (/docs/integrations/providers/wikipedia) Wolfram Alpha (/docs/integrations/providers/wolfram_alpha) Writer (/docs/integrations/providers/writer) Xata (/docs/integrations/providers/xata) Xorbits Inference (Xinference) (/docs/integrations/providers/xinference) Yeager.ai (/docs/integrations/providers/yeagerai) YouTube (/docs/integrations/providers/youtube) Zep (/docs/integrations/providers/zep) Zilliz (/docs/integrations/providers/zilliz)  (/) Integrations (/docs/integrations) Grouped by provider (/docs/integrations/providers/) Portkey (https://docs.portkey.ai/overview/introduction) ‚Äã (#llmops-for-langchain) more (https://docs.portkey.ai) ‚Äã (#using-portkey-with-langchain) signing up here (https://app.portkey.ai/login) OpenAI (https://api.python.langchain.com/en/latest/llms/langchain.llms.openai.OpenAI.html) Portkey (https://api.python.langchain.com/en/latest/utilities/langchain.utilities.portkey.Portkey.html) Portkey dashboard (https://app.portkey.ai) ‚Äã (#tracing-chains--agents) AgentType (https://api.python.langchain.com/en/latest/agents/langchain.agents.agent_types.AgentType.html) initialize_agent (https://api.python.langchain.com/en/latest/agents/langchain.agents.initialize.initialize_agent.html) load_tools (https://api.python.langchain.com/en/latest/agents/langchain.agents.load_tools.load_tools.html) OpenAI (https://api.python.langchain.com/en/latest/llms/langchain.llms.openai.OpenAI.html) Portkey (https://api.python.langchain.com/en/latest/utilities/langchain.utilities.portkey.Portkey.html) ‚Äã (#advanced-features) append user feedback (https://docs.portkey.ai/key-features/feedback-api) Tracing Requests (https://docs.portkey.ai/key-features/request-tracing) Automatic Retries (https://docs.portkey.ai/key-features/automatic-retries) Enabling Cache (https://docs.portkey.ai/key-features/request-caching) Add User (https://docs.portkey.ai/key-features/custom-metadata) Add Organisation (https://docs.portkey.ai/key-features/custom-metadata) Add Environment (https://docs.portkey.ai/key-features/custom-metadata) Add Prompt (version/id/string) (https://docs.portkey.ai/key-features/custom-metadata) ‚Äã (#enabling-all-portkey-features) please refer to the Portkey docs (https://docs.portkey.ai) reach out to us on Twitter. (https://twitter.com/portkeyai) PreviousPipelineAI (/docs/integrations/providers/pipelineai) NextLog, Trace, and Monitor (/docs/integrations/providers/portkey/logging_tracing_portkey) LLMOps for Langchain (#llmops-for-langchain) Using Portkey with Langchain (#using-portkey-with-langchain) Tracing Chains & Agents (#tracing-chains--agents) Advanced Features (#advanced-features) Enabling all Portkey Features: (#enabling-all-portkey-features) Discord (https://discord.gg/cU2adEyC7w) Twitter (https://twitter.com/LangChainAI) Python (https://github.com/hwchase17/langchain) JS/TS (https://github.com/hwchase17/langchainjs) Homepage (https://langchain.com) Blog (https://blog.langchain.dev)