Eden AI is revolutionizing the AI landscape by uniting the best AI providers, empowering users to unlock limitless possibilities and tap into the true potential of artificial intelligence. With an all-in-one comprehensive and hassle-free platform, it allows users to deploy AI features to production lightning fast, enabling effortless access to the full breadth of AI capabilities via a single API. (website: https://edenai.co/) This example goes over how to use LangChain to interact with Eden AI models Accessing the EDENAI's API requires an API key,  which you can get by creating an account https://app.edenai.run/user/register  and heading here https://app.edenai.run/admin/account/settings Once we have a key we'll want to set it as an environment variable by running: If you'd prefer not to set an environment variable you can pass the key in directly via the edenai_api_key named parameter  when initiating the EdenAI LLM class: The EdenAI API brings together various providers, each offering multiple models. To access a specific model, you can simply add 'model' during instantiation. For instance, let's explore the models provided by OpenAI, such as GPT3.5  IntegrationsCallbacksChat modelsChat loadersDocument loadersDocument transformersLLMsAI21Aleph AlphaAmazon API GatewayAnyscaleAzure MLAzure OpenAIBananaBasetenBeamBedrockBittensorCerebriumAIChatGLMClarifaiCohereC TransformersCTranslate2DatabricksDeepInfraDeepSparseEden AIFireworksForefrontAIGoogle Vertex AI PaLMGooseAIGPT4AllHugging Face HubHugging Face Local PipelinesHuggingface TextGen InferenceJSONFormerKoboldAI APILlama.cppLLM Caching integrationsManifestMinimaxModalMosaicMLNLP CloudOctoAIOllamaOpaquePromptsOpenAIOpenLLMOpenLMPetalsPipelineAIPredibasePrediction GuardPromptLayer OpenAIRELLMReplicateRunhouseSageMakerEndpointStochasticAINebula (Symbl.ai)TextGenTitan TakeoffTongyi QwenvLLMWriterXorbits Inference (Xinference)MemoryRetrieversText embedding modelsAgents & ToolkitsToolsVector storesGrouped by provider Callbacks Chat models Chat loaders Document loaders Document transformers LLMsAI21Aleph AlphaAmazon API GatewayAnyscaleAzure MLAzure OpenAIBananaBasetenBeamBedrockBittensorCerebriumAIChatGLMClarifaiCohereC TransformersCTranslate2DatabricksDeepInfraDeepSparseEden AIFireworksForefrontAIGoogle Vertex AI PaLMGooseAIGPT4AllHugging Face HubHugging Face Local PipelinesHuggingface TextGen InferenceJSONFormerKoboldAI APILlama.cppLLM Caching integrationsManifestMinimaxModalMosaicMLNLP CloudOctoAIOllamaOpaquePromptsOpenAIOpenLLMOpenLMPetalsPipelineAIPredibasePrediction GuardPromptLayer OpenAIRELLMReplicateRunhouseSageMakerEndpointStochasticAINebula (Symbl.ai)TextGenTitan TakeoffTongyi QwenvLLMWriterXorbits Inference (Xinference) AI21 Aleph Alpha Amazon API Gateway Anyscale Azure ML Azure OpenAI Banana Baseten Beam Bedrock Bittensor CerebriumAI ChatGLM Clarifai Cohere C Transformers CTranslate2 Databricks DeepInfra DeepSparse Eden AI Fireworks ForefrontAI Google Vertex AI PaLM GooseAI GPT4All Hugging Face Hub Hugging Face Local Pipelines Huggingface TextGen Inference JSONFormer KoboldAI API Llama.cpp LLM Caching integrations Manifest Minimax Modal MosaicML NLP Cloud OctoAI Ollama OpaquePrompts OpenAI OpenLLM OpenLM Petals PipelineAI Predibase Prediction Guard PromptLayer OpenAI RELLM Replicate Runhouse SageMakerEndpoint StochasticAI Nebula (Symbl.ai) TextGen Titan Takeoff Tongyi Qwen vLLM Writer Xorbits Inference (Xinference) Memory Retrievers Text embedding models Agents & Toolkits Tools Vector stores Grouped by provider  Integrations LLMs Eden AI EdenAI EdenAI StreamingStdOutCallbackHandler SimpleSequentialChain PromptTemplate LLMChain Calling a modeltext generationimage generationtext generation with callback text generation image generation text generation with callback Chaining Calls Discord Twitter Python JS/TS Homepage Blog Skip to main contentü¶úÔ∏èüîó LangChainDocsUse casesIntegrationsAPILangSmithJS/TS DocsCTRLKIntegrationsCallbacksChat modelsChat loadersDocument loadersDocument transformersLLMsAI21Aleph AlphaAmazon API GatewayAnyscaleAzure MLAzure OpenAIBananaBasetenBeamBedrockBittensorCerebriumAIChatGLMClarifaiCohereC TransformersCTranslate2DatabricksDeepInfraDeepSparseEden AIFireworksForefrontAIGoogle Vertex AI PaLMGooseAIGPT4AllHugging Face HubHugging Face Local PipelinesHuggingface TextGen InferenceJSONFormerKoboldAI APILlama.cppLLM Caching integrationsManifestMinimaxModalMosaicMLNLP CloudOctoAIOllamaOpaquePromptsOpenAIOpenLLMOpenLMPetalsPipelineAIPredibasePrediction GuardPromptLayer OpenAIRELLMReplicateRunhouseSageMakerEndpointStochasticAINebula (Symbl.ai)TextGenTitan TakeoffTongyi QwenvLLMWriterXorbits Inference (Xinference)MemoryRetrieversText embedding modelsAgents & ToolkitsToolsVector storesGrouped by providerIntegrationsLLMsEden AIOn this pageEden AIEden AI is revolutionizing the AI landscape by uniting the best AI providers, empowering users to unlock limitless possibilities and tap into the true potential of artificial intelligence. With an all-in-one comprehensive and hassle-free platform, it allows users to deploy AI features to production lightning fast, enabling effortless access to the full breadth of AI capabilities via a single API. (website: https://edenai.co/)This example goes over how to use LangChain to interact with Eden AI modelsAccessing the EDENAI's API requires an API key, which you can get by creating an account https://app.edenai.run/user/register  and heading here https://app.edenai.run/admin/account/settingsOnce we have a key we'll want to set it as an environment variable by running:export EDENAI_API_KEY="..."If you'd prefer not to set an environment variable you can pass the key in directly via the edenai_api_key named parameter when initiating the EdenAI LLM class:from langchain.llms import EdenAIAPI Reference:EdenAIllm = EdenAI(edenai_api_key="...",provider="openai", temperature=0.2, max_tokens=250)Calling a model‚ÄãThe EdenAI API brings together various providers, each offering multiple models.To access a specific model, you can simply add 'model' during instantiation.For instance, let's explore the models provided by OpenAI, such as GPT3.5 text generation‚Äãfrom langchain import PromptTemplate, LLMChainllm=EdenAI(feature="text",provider="openai",model="text-davinci-003",temperature=0.2, max_tokens=250)prompt = """User: Answer the following yes/no question by reasoning step by step. Can a dog drive a car?Assistant:"""llm(prompt)image generation‚Äãimport base64from io import BytesIOfrom PIL import Imageimport jsondef print_base64_image(base64_string):    # Decode the base64 string into binary data    decoded_data = base64.b64decode(base64_string)    # Create an in-memory stream to read the binary data    image_stream = BytesIO(decoded_data)    # Open the image using PIL    image = Image.open(image_stream)    # Display the image    image.show()text2image = EdenAI(    feature="image" ,    provider= "openai",    resolution="512x512")image_output = text2image("A cat riding a motorcycle by Picasso")print_base64_image(image_output)text generation with callback‚Äãfrom langchain.llms import EdenAIfrom langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandlerllm = EdenAI(    callbacks=[StreamingStdOutCallbackHandler()],    feature="text",provider="openai", temperature=0.2,max_tokens=250)prompt = """User: Answer the following yes/no question by reasoning step by step. Can a dog drive a car?Assistant:"""print(llm(prompt))API Reference:EdenAIStreamingStdOutCallbackHandlerChaining Calls‚Äãfrom langchain.chains import SimpleSequentialChainfrom langchain.prompts import PromptTemplatefrom langchain.chains import LLMChainAPI Reference:SimpleSequentialChainPromptTemplateLLMChainllm = EdenAI(feature="text", provider="openai", temperature=0.2, max_tokens=250)text2image = EdenAI(feature="image", provider="openai", resolution="512x512")prompt = PromptTemplate(    input_variables=["product"],    template="What is a good name for a company that makes {product}?",)chain = LLMChain(llm=llm, prompt=prompt)second_prompt = PromptTemplate(    input_variables=["company_name"],    template="Write a description of a logo for this company: {company_name}, the logo should not contain text at all ",)chain_two = LLMChain(llm=llm, prompt=second_prompt)third_prompt = PromptTemplate(    input_variables=["company_logo_description"],    template="{company_logo_description}",)chain_three = LLMChain(llm=text2image, prompt=third_prompt)# Run the chain specifying only the input variable for the first chain.overall_chain = SimpleSequentialChain(    chains=[chain, chain_two, chain_three],verbose=True)output = overall_chain.run("hats")#print the imageprint_base64_image(output)PreviousDeepSparseNextFireworksCalling a modeltext generationimage generationtext generation with callbackChaining CallsCommunityDiscordTwitterGitHubPythonJS/TSMoreHomepageBlogCopyright ¬© 2023 LangChain, Inc. Skip to main content ü¶úÔ∏èüîó LangChainDocsUse casesIntegrationsAPILangSmithJS/TS DocsCTRLK ü¶úÔ∏èüîó LangChainDocsUse casesIntegrationsAPI LangSmithJS/TS DocsCTRLK  CTRLK CTRLK  CTRLK   IntegrationsCallbacksChat modelsChat loadersDocument loadersDocument transformersLLMsAI21Aleph AlphaAmazon API GatewayAnyscaleAzure MLAzure OpenAIBananaBasetenBeamBedrockBittensorCerebriumAIChatGLMClarifaiCohereC TransformersCTranslate2DatabricksDeepInfraDeepSparseEden AIFireworksForefrontAIGoogle Vertex AI PaLMGooseAIGPT4AllHugging Face HubHugging Face Local PipelinesHuggingface TextGen InferenceJSONFormerKoboldAI APILlama.cppLLM Caching integrationsManifestMinimaxModalMosaicMLNLP CloudOctoAIOllamaOpaquePromptsOpenAIOpenLLMOpenLMPetalsPipelineAIPredibasePrediction GuardPromptLayer OpenAIRELLMReplicateRunhouseSageMakerEndpointStochasticAINebula (Symbl.ai)TextGenTitan TakeoffTongyi QwenvLLMWriterXorbits Inference (Xinference)MemoryRetrieversText embedding modelsAgents & ToolkitsToolsVector storesGrouped by providerIntegrationsLLMsEden AIOn this pageEden AIEden AI is revolutionizing the AI landscape by uniting the best AI providers, empowering users to unlock limitless possibilities and tap into the true potential of artificial intelligence. With an all-in-one comprehensive and hassle-free platform, it allows users to deploy AI features to production lightning fast, enabling effortless access to the full breadth of AI capabilities via a single API. (website: https://edenai.co/)This example goes over how to use LangChain to interact with Eden AI modelsAccessing the EDENAI's API requires an API key, which you can get by creating an account https://app.edenai.run/user/register  and heading here https://app.edenai.run/admin/account/settingsOnce we have a key we'll want to set it as an environment variable by running:export EDENAI_API_KEY="..."If you'd prefer not to set an environment variable you can pass the key in directly via the edenai_api_key named parameter when initiating the EdenAI LLM class:from langchain.llms import EdenAIAPI Reference:EdenAIllm = EdenAI(edenai_api_key="...",provider="openai", temperature=0.2, max_tokens=250)Calling a model‚ÄãThe EdenAI API brings together various providers, each offering multiple models.To access a specific model, you can simply add 'model' during instantiation.For instance, let's explore the models provided by OpenAI, such as GPT3.5 text generation‚Äãfrom langchain import PromptTemplate, LLMChainllm=EdenAI(feature="text",provider="openai",model="text-davinci-003",temperature=0.2, max_tokens=250)prompt = """User: Answer the following yes/no question by reasoning step by step. Can a dog drive a car?Assistant:"""llm(prompt)image generation‚Äãimport base64from io import BytesIOfrom PIL import Imageimport jsondef print_base64_image(base64_string):    # Decode the base64 string into binary data    decoded_data = base64.b64decode(base64_string)    # Create an in-memory stream to read the binary data    image_stream = BytesIO(decoded_data)    # Open the image using PIL    image = Image.open(image_stream)    # Display the image    image.show()text2image = EdenAI(    feature="image" ,    provider= "openai",    resolution="512x512")image_output = text2image("A cat riding a motorcycle by Picasso")print_base64_image(image_output)text generation with callback‚Äãfrom langchain.llms import EdenAIfrom langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandlerllm = EdenAI(    callbacks=[StreamingStdOutCallbackHandler()],    feature="text",provider="openai", temperature=0.2,max_tokens=250)prompt = """User: Answer the following yes/no question by reasoning step by step. Can a dog drive a car?Assistant:"""print(llm(prompt))API Reference:EdenAIStreamingStdOutCallbackHandlerChaining Calls‚Äãfrom langchain.chains import SimpleSequentialChainfrom langchain.prompts import PromptTemplatefrom langchain.chains import LLMChainAPI Reference:SimpleSequentialChainPromptTemplateLLMChainllm = EdenAI(feature="text", provider="openai", temperature=0.2, max_tokens=250)text2image = EdenAI(feature="image", provider="openai", resolution="512x512")prompt = PromptTemplate(    input_variables=["product"],    template="What is a good name for a company that makes {product}?",)chain = LLMChain(llm=llm, prompt=prompt)second_prompt = PromptTemplate(    input_variables=["company_name"],    template="Write a description of a logo for this company: {company_name}, the logo should not contain text at all ",)chain_two = LLMChain(llm=llm, prompt=second_prompt)third_prompt = PromptTemplate(    input_variables=["company_logo_description"],    template="{company_logo_description}",)chain_three = LLMChain(llm=text2image, prompt=third_prompt)# Run the chain specifying only the input variable for the first chain.overall_chain = SimpleSequentialChain(    chains=[chain, chain_two, chain_three],verbose=True)output = overall_chain.run("hats")#print the imageprint_base64_image(output)PreviousDeepSparseNextFireworksCalling a modeltext generationimage generationtext generation with callbackChaining Calls IntegrationsCallbacksChat modelsChat loadersDocument loadersDocument transformersLLMsAI21Aleph AlphaAmazon API GatewayAnyscaleAzure MLAzure OpenAIBananaBasetenBeamBedrockBittensorCerebriumAIChatGLMClarifaiCohereC TransformersCTranslate2DatabricksDeepInfraDeepSparseEden AIFireworksForefrontAIGoogle Vertex AI PaLMGooseAIGPT4AllHugging Face HubHugging Face Local PipelinesHuggingface TextGen InferenceJSONFormerKoboldAI APILlama.cppLLM Caching integrationsManifestMinimaxModalMosaicMLNLP CloudOctoAIOllamaOpaquePromptsOpenAIOpenLLMOpenLMPetalsPipelineAIPredibasePrediction GuardPromptLayer OpenAIRELLMReplicateRunhouseSageMakerEndpointStochasticAINebula (Symbl.ai)TextGenTitan TakeoffTongyi QwenvLLMWriterXorbits Inference (Xinference)MemoryRetrieversText embedding modelsAgents & ToolkitsToolsVector storesGrouped by providerIntegrationsLLMsEden AIOn this pageEden AIEden AI is revolutionizing the AI landscape by uniting the best AI providers, empowering users to unlock limitless possibilities and tap into the true potential of artificial intelligence. With an all-in-one comprehensive and hassle-free platform, it allows users to deploy AI features to production lightning fast, enabling effortless access to the full breadth of AI capabilities via a single API. (website: https://edenai.co/)This example goes over how to use LangChain to interact with Eden AI modelsAccessing the EDENAI's API requires an API key, which you can get by creating an account https://app.edenai.run/user/register  and heading here https://app.edenai.run/admin/account/settingsOnce we have a key we'll want to set it as an environment variable by running:export EDENAI_API_KEY="..."If you'd prefer not to set an environment variable you can pass the key in directly via the edenai_api_key named parameter when initiating the EdenAI LLM class:from langchain.llms import EdenAIAPI Reference:EdenAIllm = EdenAI(edenai_api_key="...",provider="openai", temperature=0.2, max_tokens=250)Calling a model‚ÄãThe EdenAI API brings together various providers, each offering multiple models.To access a specific model, you can simply add 'model' during instantiation.For instance, let's explore the models provided by OpenAI, such as GPT3.5 text generation‚Äãfrom langchain import PromptTemplate, LLMChainllm=EdenAI(feature="text",provider="openai",model="text-davinci-003",temperature=0.2, max_tokens=250)prompt = """User: Answer the following yes/no question by reasoning step by step. Can a dog drive a car?Assistant:"""llm(prompt)image generation‚Äãimport base64from io import BytesIOfrom PIL import Imageimport jsondef print_base64_image(base64_string):    # Decode the base64 string into binary data    decoded_data = base64.b64decode(base64_string)    # Create an in-memory stream to read the binary data    image_stream = BytesIO(decoded_data)    # Open the image using PIL    image = Image.open(image_stream)    # Display the image    image.show()text2image = EdenAI(    feature="image" ,    provider= "openai",    resolution="512x512")image_output = text2image("A cat riding a motorcycle by Picasso")print_base64_image(image_output)text generation with callback‚Äãfrom langchain.llms import EdenAIfrom langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandlerllm = EdenAI(    callbacks=[StreamingStdOutCallbackHandler()],    feature="text",provider="openai", temperature=0.2,max_tokens=250)prompt = """User: Answer the following yes/no question by reasoning step by step. Can a dog drive a car?Assistant:"""print(llm(prompt))API Reference:EdenAIStreamingStdOutCallbackHandlerChaining Calls‚Äãfrom langchain.chains import SimpleSequentialChainfrom langchain.prompts import PromptTemplatefrom langchain.chains import LLMChainAPI Reference:SimpleSequentialChainPromptTemplateLLMChainllm = EdenAI(feature="text", provider="openai", temperature=0.2, max_tokens=250)text2image = EdenAI(feature="image", provider="openai", resolution="512x512")prompt = PromptTemplate(    input_variables=["product"],    template="What is a good name for a company that makes {product}?",)chain = LLMChain(llm=llm, prompt=prompt)second_prompt = PromptTemplate(    input_variables=["company_name"],    template="Write a description of a logo for this company: {company_name}, the logo should not contain text at all ",)chain_two = LLMChain(llm=llm, prompt=second_prompt)third_prompt = PromptTemplate(    input_variables=["company_logo_description"],    template="{company_logo_description}",)chain_three = LLMChain(llm=text2image, prompt=third_prompt)# Run the chain specifying only the input variable for the first chain.overall_chain = SimpleSequentialChain(    chains=[chain, chain_two, chain_three],verbose=True)output = overall_chain.run("hats")#print the imageprint_base64_image(output)PreviousDeepSparseNextFireworksCalling a modeltext generationimage generationtext generation with callbackChaining Calls IntegrationsCallbacksChat modelsChat loadersDocument loadersDocument transformersLLMsAI21Aleph AlphaAmazon API GatewayAnyscaleAzure MLAzure OpenAIBananaBasetenBeamBedrockBittensorCerebriumAIChatGLMClarifaiCohereC TransformersCTranslate2DatabricksDeepInfraDeepSparseEden AIFireworksForefrontAIGoogle Vertex AI PaLMGooseAIGPT4AllHugging Face HubHugging Face Local PipelinesHuggingface TextGen InferenceJSONFormerKoboldAI APILlama.cppLLM Caching integrationsManifestMinimaxModalMosaicMLNLP CloudOctoAIOllamaOpaquePromptsOpenAIOpenLLMOpenLMPetalsPipelineAIPredibasePrediction GuardPromptLayer OpenAIRELLMReplicateRunhouseSageMakerEndpointStochasticAINebula (Symbl.ai)TextGenTitan TakeoffTongyi QwenvLLMWriterXorbits Inference (Xinference)MemoryRetrieversText embedding modelsAgents & ToolkitsToolsVector storesGrouped by provider IntegrationsCallbacksChat modelsChat loadersDocument loadersDocument transformersLLMsAI21Aleph AlphaAmazon API GatewayAnyscaleAzure MLAzure OpenAIBananaBasetenBeamBedrockBittensorCerebriumAIChatGLMClarifaiCohereC TransformersCTranslate2DatabricksDeepInfraDeepSparseEden AIFireworksForefrontAIGoogle Vertex AI PaLMGooseAIGPT4AllHugging Face HubHugging Face Local PipelinesHuggingface TextGen InferenceJSONFormerKoboldAI APILlama.cppLLM Caching integrationsManifestMinimaxModalMosaicMLNLP CloudOctoAIOllamaOpaquePromptsOpenAIOpenLLMOpenLMPetalsPipelineAIPredibasePrediction GuardPromptLayer OpenAIRELLMReplicateRunhouseSageMakerEndpointStochasticAINebula (Symbl.ai)TextGenTitan TakeoffTongyi QwenvLLMWriterXorbits Inference (Xinference)MemoryRetrieversText embedding modelsAgents & ToolkitsToolsVector storesGrouped by provider Integrations Callbacks Chat models Chat loaders Document loaders Document transformers LLMs Memory Retrievers Text embedding models Agents & Toolkits Tools Vector stores Grouped by provider IntegrationsLLMsEden AIOn this pageEden AIEden AI is revolutionizing the AI landscape by uniting the best AI providers, empowering users to unlock limitless possibilities and tap into the true potential of artificial intelligence. With an all-in-one comprehensive and hassle-free platform, it allows users to deploy AI features to production lightning fast, enabling effortless access to the full breadth of AI capabilities via a single API. (website: https://edenai.co/)This example goes over how to use LangChain to interact with Eden AI modelsAccessing the EDENAI's API requires an API key, which you can get by creating an account https://app.edenai.run/user/register  and heading here https://app.edenai.run/admin/account/settingsOnce we have a key we'll want to set it as an environment variable by running:export EDENAI_API_KEY="..."If you'd prefer not to set an environment variable you can pass the key in directly via the edenai_api_key named parameter when initiating the EdenAI LLM class:from langchain.llms import EdenAIAPI Reference:EdenAIllm = EdenAI(edenai_api_key="...",provider="openai", temperature=0.2, max_tokens=250)Calling a model‚ÄãThe EdenAI API brings together various providers, each offering multiple models.To access a specific model, you can simply add 'model' during instantiation.For instance, let's explore the models provided by OpenAI, such as GPT3.5 text generation‚Äãfrom langchain import PromptTemplate, LLMChainllm=EdenAI(feature="text",provider="openai",model="text-davinci-003",temperature=0.2, max_tokens=250)prompt = """User: Answer the following yes/no question by reasoning step by step. Can a dog drive a car?Assistant:"""llm(prompt)image generation‚Äãimport base64from io import BytesIOfrom PIL import Imageimport jsondef print_base64_image(base64_string):    # Decode the base64 string into binary data    decoded_data = base64.b64decode(base64_string)    # Create an in-memory stream to read the binary data    image_stream = BytesIO(decoded_data)    # Open the image using PIL    image = Image.open(image_stream)    # Display the image    image.show()text2image = EdenAI(    feature="image" ,    provider= "openai",    resolution="512x512")image_output = text2image("A cat riding a motorcycle by Picasso")print_base64_image(image_output)text generation with callback‚Äãfrom langchain.llms import EdenAIfrom langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandlerllm = EdenAI(    callbacks=[StreamingStdOutCallbackHandler()],    feature="text",provider="openai", temperature=0.2,max_tokens=250)prompt = """User: Answer the following yes/no question by reasoning step by step. Can a dog drive a car?Assistant:"""print(llm(prompt))API Reference:EdenAIStreamingStdOutCallbackHandlerChaining Calls‚Äãfrom langchain.chains import SimpleSequentialChainfrom langchain.prompts import PromptTemplatefrom langchain.chains import LLMChainAPI Reference:SimpleSequentialChainPromptTemplateLLMChainllm = EdenAI(feature="text", provider="openai", temperature=0.2, max_tokens=250)text2image = EdenAI(feature="image", provider="openai", resolution="512x512")prompt = PromptTemplate(    input_variables=["product"],    template="What is a good name for a company that makes {product}?",)chain = LLMChain(llm=llm, prompt=prompt)second_prompt = PromptTemplate(    input_variables=["company_name"],    template="Write a description of a logo for this company: {company_name}, the logo should not contain text at all ",)chain_two = LLMChain(llm=llm, prompt=second_prompt)third_prompt = PromptTemplate(    input_variables=["company_logo_description"],    template="{company_logo_description}",)chain_three = LLMChain(llm=text2image, prompt=third_prompt)# Run the chain specifying only the input variable for the first chain.overall_chain = SimpleSequentialChain(    chains=[chain, chain_two, chain_three],verbose=True)output = overall_chain.run("hats")#print the imageprint_base64_image(output)PreviousDeepSparseNextFireworksCalling a modeltext generationimage generationtext generation with callbackChaining Calls IntegrationsLLMsEden AIOn this pageEden AIEden AI is revolutionizing the AI landscape by uniting the best AI providers, empowering users to unlock limitless possibilities and tap into the true potential of artificial intelligence. With an all-in-one comprehensive and hassle-free platform, it allows users to deploy AI features to production lightning fast, enabling effortless access to the full breadth of AI capabilities via a single API. (website: https://edenai.co/)This example goes over how to use LangChain to interact with Eden AI modelsAccessing the EDENAI's API requires an API key, which you can get by creating an account https://app.edenai.run/user/register  and heading here https://app.edenai.run/admin/account/settingsOnce we have a key we'll want to set it as an environment variable by running:export EDENAI_API_KEY="..."If you'd prefer not to set an environment variable you can pass the key in directly via the edenai_api_key named parameter when initiating the EdenAI LLM class:from langchain.llms import EdenAIAPI Reference:EdenAIllm = EdenAI(edenai_api_key="...",provider="openai", temperature=0.2, max_tokens=250)Calling a model‚ÄãThe EdenAI API brings together various providers, each offering multiple models.To access a specific model, you can simply add 'model' during instantiation.For instance, let's explore the models provided by OpenAI, such as GPT3.5 text generation‚Äãfrom langchain import PromptTemplate, LLMChainllm=EdenAI(feature="text",provider="openai",model="text-davinci-003",temperature=0.2, max_tokens=250)prompt = """User: Answer the following yes/no question by reasoning step by step. Can a dog drive a car?Assistant:"""llm(prompt)image generation‚Äãimport base64from io import BytesIOfrom PIL import Imageimport jsondef print_base64_image(base64_string):    # Decode the base64 string into binary data    decoded_data = base64.b64decode(base64_string)    # Create an in-memory stream to read the binary data    image_stream = BytesIO(decoded_data)    # Open the image using PIL    image = Image.open(image_stream)    # Display the image    image.show()text2image = EdenAI(    feature="image" ,    provider= "openai",    resolution="512x512")image_output = text2image("A cat riding a motorcycle by Picasso")print_base64_image(image_output)text generation with callback‚Äãfrom langchain.llms import EdenAIfrom langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandlerllm = EdenAI(    callbacks=[StreamingStdOutCallbackHandler()],    feature="text",provider="openai", temperature=0.2,max_tokens=250)prompt = """User: Answer the following yes/no question by reasoning step by step. Can a dog drive a car?Assistant:"""print(llm(prompt))API Reference:EdenAIStreamingStdOutCallbackHandlerChaining Calls‚Äãfrom langchain.chains import SimpleSequentialChainfrom langchain.prompts import PromptTemplatefrom langchain.chains import LLMChainAPI Reference:SimpleSequentialChainPromptTemplateLLMChainllm = EdenAI(feature="text", provider="openai", temperature=0.2, max_tokens=250)text2image = EdenAI(feature="image", provider="openai", resolution="512x512")prompt = PromptTemplate(    input_variables=["product"],    template="What is a good name for a company that makes {product}?",)chain = LLMChain(llm=llm, prompt=prompt)second_prompt = PromptTemplate(    input_variables=["company_name"],    template="Write a description of a logo for this company: {company_name}, the logo should not contain text at all ",)chain_two = LLMChain(llm=llm, prompt=second_prompt)third_prompt = PromptTemplate(    input_variables=["company_logo_description"],    template="{company_logo_description}",)chain_three = LLMChain(llm=text2image, prompt=third_prompt)# Run the chain specifying only the input variable for the first chain.overall_chain = SimpleSequentialChain(    chains=[chain, chain_two, chain_three],verbose=True)output = overall_chain.run("hats")#print the imageprint_base64_image(output)PreviousDeepSparseNextFireworksCalling a modeltext generationimage generationtext generation with callbackChaining Calls IntegrationsLLMsEden AIOn this pageEden AIEden AI is revolutionizing the AI landscape by uniting the best AI providers, empowering users to unlock limitless possibilities and tap into the true potential of artificial intelligence. With an all-in-one comprehensive and hassle-free platform, it allows users to deploy AI features to production lightning fast, enabling effortless access to the full breadth of AI capabilities via a single API. (website: https://edenai.co/)This example goes over how to use LangChain to interact with Eden AI modelsAccessing the EDENAI's API requires an API key, which you can get by creating an account https://app.edenai.run/user/register  and heading here https://app.edenai.run/admin/account/settingsOnce we have a key we'll want to set it as an environment variable by running:export EDENAI_API_KEY="..."If you'd prefer not to set an environment variable you can pass the key in directly via the edenai_api_key named parameter when initiating the EdenAI LLM class:from langchain.llms import EdenAIAPI Reference:EdenAIllm = EdenAI(edenai_api_key="...",provider="openai", temperature=0.2, max_tokens=250)Calling a model‚ÄãThe EdenAI API brings together various providers, each offering multiple models.To access a specific model, you can simply add 'model' during instantiation.For instance, let's explore the models provided by OpenAI, such as GPT3.5 text generation‚Äãfrom langchain import PromptTemplate, LLMChainllm=EdenAI(feature="text",provider="openai",model="text-davinci-003",temperature=0.2, max_tokens=250)prompt = """User: Answer the following yes/no question by reasoning step by step. Can a dog drive a car?Assistant:"""llm(prompt)image generation‚Äãimport base64from io import BytesIOfrom PIL import Imageimport jsondef print_base64_image(base64_string):    # Decode the base64 string into binary data    decoded_data = base64.b64decode(base64_string)    # Create an in-memory stream to read the binary data    image_stream = BytesIO(decoded_data)    # Open the image using PIL    image = Image.open(image_stream)    # Display the image    image.show()text2image = EdenAI(    feature="image" ,    provider= "openai",    resolution="512x512")image_output = text2image("A cat riding a motorcycle by Picasso")print_base64_image(image_output)text generation with callback‚Äãfrom langchain.llms import EdenAIfrom langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandlerllm = EdenAI(    callbacks=[StreamingStdOutCallbackHandler()],    feature="text",provider="openai", temperature=0.2,max_tokens=250)prompt = """User: Answer the following yes/no question by reasoning step by step. Can a dog drive a car?Assistant:"""print(llm(prompt))API Reference:EdenAIStreamingStdOutCallbackHandlerChaining Calls‚Äãfrom langchain.chains import SimpleSequentialChainfrom langchain.prompts import PromptTemplatefrom langchain.chains import LLMChainAPI Reference:SimpleSequentialChainPromptTemplateLLMChainllm = EdenAI(feature="text", provider="openai", temperature=0.2, max_tokens=250)text2image = EdenAI(feature="image", provider="openai", resolution="512x512")prompt = PromptTemplate(    input_variables=["product"],    template="What is a good name for a company that makes {product}?",)chain = LLMChain(llm=llm, prompt=prompt)second_prompt = PromptTemplate(    input_variables=["company_name"],    template="Write a description of a logo for this company: {company_name}, the logo should not contain text at all ",)chain_two = LLMChain(llm=llm, prompt=second_prompt)third_prompt = PromptTemplate(    input_variables=["company_logo_description"],    template="{company_logo_description}",)chain_three = LLMChain(llm=text2image, prompt=third_prompt)# Run the chain specifying only the input variable for the first chain.overall_chain = SimpleSequentialChain(    chains=[chain, chain_two, chain_three],verbose=True)output = overall_chain.run("hats")#print the imageprint_base64_image(output)PreviousDeepSparseNextFireworks IntegrationsLLMsEden AIOn this pageEden AIEden AI is revolutionizing the AI landscape by uniting the best AI providers, empowering users to unlock limitless possibilities and tap into the true potential of artificial intelligence. With an all-in-one comprehensive and hassle-free platform, it allows users to deploy AI features to production lightning fast, enabling effortless access to the full breadth of AI capabilities via a single API. (website: https://edenai.co/)This example goes over how to use LangChain to interact with Eden AI modelsAccessing the EDENAI's API requires an API key, which you can get by creating an account https://app.edenai.run/user/register  and heading here https://app.edenai.run/admin/account/settingsOnce we have a key we'll want to set it as an environment variable by running:export EDENAI_API_KEY="..."If you'd prefer not to set an environment variable you can pass the key in directly via the edenai_api_key named parameter when initiating the EdenAI LLM class:from langchain.llms import EdenAIAPI Reference:EdenAIllm = EdenAI(edenai_api_key="...",provider="openai", temperature=0.2, max_tokens=250)Calling a model‚ÄãThe EdenAI API brings together various providers, each offering multiple models.To access a specific model, you can simply add 'model' during instantiation.For instance, let's explore the models provided by OpenAI, such as GPT3.5 text generation‚Äãfrom langchain import PromptTemplate, LLMChainllm=EdenAI(feature="text",provider="openai",model="text-davinci-003",temperature=0.2, max_tokens=250)prompt = """User: Answer the following yes/no question by reasoning step by step. Can a dog drive a car?Assistant:"""llm(prompt)image generation‚Äãimport base64from io import BytesIOfrom PIL import Imageimport jsondef print_base64_image(base64_string):    # Decode the base64 string into binary data    decoded_data = base64.b64decode(base64_string)    # Create an in-memory stream to read the binary data    image_stream = BytesIO(decoded_data)    # Open the image using PIL    image = Image.open(image_stream)    # Display the image    image.show()text2image = EdenAI(    feature="image" ,    provider= "openai",    resolution="512x512")image_output = text2image("A cat riding a motorcycle by Picasso")print_base64_image(image_output)text generation with callback‚Äãfrom langchain.llms import EdenAIfrom langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandlerllm = EdenAI(    callbacks=[StreamingStdOutCallbackHandler()],    feature="text",provider="openai", temperature=0.2,max_tokens=250)prompt = """User: Answer the following yes/no question by reasoning step by step. Can a dog drive a car?Assistant:"""print(llm(prompt))API Reference:EdenAIStreamingStdOutCallbackHandlerChaining Calls‚Äãfrom langchain.chains import SimpleSequentialChainfrom langchain.prompts import PromptTemplatefrom langchain.chains import LLMChainAPI Reference:SimpleSequentialChainPromptTemplateLLMChainllm = EdenAI(feature="text", provider="openai", temperature=0.2, max_tokens=250)text2image = EdenAI(feature="image", provider="openai", resolution="512x512")prompt = PromptTemplate(    input_variables=["product"],    template="What is a good name for a company that makes {product}?",)chain = LLMChain(llm=llm, prompt=prompt)second_prompt = PromptTemplate(    input_variables=["company_name"],    template="Write a description of a logo for this company: {company_name}, the logo should not contain text at all ",)chain_two = LLMChain(llm=llm, prompt=second_prompt)third_prompt = PromptTemplate(    input_variables=["company_logo_description"],    template="{company_logo_description}",)chain_three = LLMChain(llm=text2image, prompt=third_prompt)# Run the chain specifying only the input variable for the first chain.overall_chain = SimpleSequentialChain(    chains=[chain, chain_two, chain_three],verbose=True)output = overall_chain.run("hats")#print the imageprint_base64_image(output)PreviousDeepSparseNextFireworks On this page Eden AIEden AI is revolutionizing the AI landscape by uniting the best AI providers, empowering users to unlock limitless possibilities and tap into the true potential of artificial intelligence. With an all-in-one comprehensive and hassle-free platform, it allows users to deploy AI features to production lightning fast, enabling effortless access to the full breadth of AI capabilities via a single API. (website: https://edenai.co/)This example goes over how to use LangChain to interact with Eden AI modelsAccessing the EDENAI's API requires an API key, which you can get by creating an account https://app.edenai.run/user/register  and heading here https://app.edenai.run/admin/account/settingsOnce we have a key we'll want to set it as an environment variable by running:export EDENAI_API_KEY="..."If you'd prefer not to set an environment variable you can pass the key in directly via the edenai_api_key named parameter when initiating the EdenAI LLM class:from langchain.llms import EdenAIAPI Reference:EdenAIllm = EdenAI(edenai_api_key="...",provider="openai", temperature=0.2, max_tokens=250)Calling a model‚ÄãThe EdenAI API brings together various providers, each offering multiple models.To access a specific model, you can simply add 'model' during instantiation.For instance, let's explore the models provided by OpenAI, such as GPT3.5 text generation‚Äãfrom langchain import PromptTemplate, LLMChainllm=EdenAI(feature="text",provider="openai",model="text-davinci-003",temperature=0.2, max_tokens=250)prompt = """User: Answer the following yes/no question by reasoning step by step. Can a dog drive a car?Assistant:"""llm(prompt)image generation‚Äãimport base64from io import BytesIOfrom PIL import Imageimport jsondef print_base64_image(base64_string):    # Decode the base64 string into binary data    decoded_data = base64.b64decode(base64_string)    # Create an in-memory stream to read the binary data    image_stream = BytesIO(decoded_data)    # Open the image using PIL    image = Image.open(image_stream)    # Display the image    image.show()text2image = EdenAI(    feature="image" ,    provider= "openai",    resolution="512x512")image_output = text2image("A cat riding a motorcycle by Picasso")print_base64_image(image_output)text generation with callback‚Äãfrom langchain.llms import EdenAIfrom langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandlerllm = EdenAI(    callbacks=[StreamingStdOutCallbackHandler()],    feature="text",provider="openai", temperature=0.2,max_tokens=250)prompt = """User: Answer the following yes/no question by reasoning step by step. Can a dog drive a car?Assistant:"""print(llm(prompt))API Reference:EdenAIStreamingStdOutCallbackHandlerChaining Calls‚Äãfrom langchain.chains import SimpleSequentialChainfrom langchain.prompts import PromptTemplatefrom langchain.chains import LLMChainAPI Reference:SimpleSequentialChainPromptTemplateLLMChainllm = EdenAI(feature="text", provider="openai", temperature=0.2, max_tokens=250)text2image = EdenAI(feature="image", provider="openai", resolution="512x512")prompt = PromptTemplate(    input_variables=["product"],    template="What is a good name for a company that makes {product}?",)chain = LLMChain(llm=llm, prompt=prompt)second_prompt = PromptTemplate(    input_variables=["company_name"],    template="Write a description of a logo for this company: {company_name}, the logo should not contain text at all ",)chain_two = LLMChain(llm=llm, prompt=second_prompt)third_prompt = PromptTemplate(    input_variables=["company_logo_description"],    template="{company_logo_description}",)chain_three = LLMChain(llm=text2image, prompt=third_prompt)# Run the chain specifying only the input variable for the first chain.overall_chain = SimpleSequentialChain(    chains=[chain, chain_two, chain_three],verbose=True)output = overall_chain.run("hats")#print the imageprint_base64_image(output) export EDENAI_API_KEY="..." export EDENAI_API_KEY="..."  from langchain.llms import EdenAI from langchain.llms import EdenAI  API Reference:EdenAI llm = EdenAI(edenai_api_key="...",provider="openai", temperature=0.2, max_tokens=250) llm = EdenAI(edenai_api_key="...",provider="openai", temperature=0.2, max_tokens=250)  from langchain import PromptTemplate, LLMChainllm=EdenAI(feature="text",provider="openai",model="text-davinci-003",temperature=0.2, max_tokens=250)prompt = """User: Answer the following yes/no question by reasoning step by step. Can a dog drive a car?Assistant:"""llm(prompt) from langchain import PromptTemplate, LLMChainllm=EdenAI(feature="text",provider="openai",model="text-davinci-003",temperature=0.2, max_tokens=250)prompt = """User: Answer the following yes/no question by reasoning step by step. Can a dog drive a car?Assistant:"""llm(prompt)  import base64from io import BytesIOfrom PIL import Imageimport jsondef print_base64_image(base64_string):    # Decode the base64 string into binary data    decoded_data = base64.b64decode(base64_string)    # Create an in-memory stream to read the binary data    image_stream = BytesIO(decoded_data)    # Open the image using PIL    image = Image.open(image_stream)    # Display the image    image.show() import base64from io import BytesIOfrom PIL import Imageimport jsondef print_base64_image(base64_string):    # Decode the base64 string into binary data    decoded_data = base64.b64decode(base64_string)    # Create an in-memory stream to read the binary data    image_stream = BytesIO(decoded_data)    # Open the image using PIL    image = Image.open(image_stream)    # Display the image    image.show()  text2image = EdenAI(    feature="image" ,    provider= "openai",    resolution="512x512") text2image = EdenAI(    feature="image" ,    provider= "openai",    resolution="512x512")  image_output = text2image("A cat riding a motorcycle by Picasso") image_output = text2image("A cat riding a motorcycle by Picasso")  print_base64_image(image_output) print_base64_image(image_output)  from langchain.llms import EdenAIfrom langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandlerllm = EdenAI(    callbacks=[StreamingStdOutCallbackHandler()],    feature="text",provider="openai", temperature=0.2,max_tokens=250)prompt = """User: Answer the following yes/no question by reasoning step by step. Can a dog drive a car?Assistant:"""print(llm(prompt)) from langchain.llms import EdenAIfrom langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandlerllm = EdenAI(    callbacks=[StreamingStdOutCallbackHandler()],    feature="text",provider="openai", temperature=0.2,max_tokens=250)prompt = """User: Answer the following yes/no question by reasoning step by step. Can a dog drive a car?Assistant:"""print(llm(prompt))  API Reference:EdenAIStreamingStdOutCallbackHandler from langchain.chains import SimpleSequentialChainfrom langchain.prompts import PromptTemplatefrom langchain.chains import LLMChain from langchain.chains import SimpleSequentialChainfrom langchain.prompts import PromptTemplatefrom langchain.chains import LLMChain  API Reference:SimpleSequentialChainPromptTemplateLLMChain llm = EdenAI(feature="text", provider="openai", temperature=0.2, max_tokens=250)text2image = EdenAI(feature="image", provider="openai", resolution="512x512") llm = EdenAI(feature="text", provider="openai", temperature=0.2, max_tokens=250)text2image = EdenAI(feature="image", provider="openai", resolution="512x512")  prompt = PromptTemplate(    input_variables=["product"],    template="What is a good name for a company that makes {product}?",)chain = LLMChain(llm=llm, prompt=prompt) prompt = PromptTemplate(    input_variables=["product"],    template="What is a good name for a company that makes {product}?",)chain = LLMChain(llm=llm, prompt=prompt)  second_prompt = PromptTemplate(    input_variables=["company_name"],    template="Write a description of a logo for this company: {company_name}, the logo should not contain text at all ",)chain_two = LLMChain(llm=llm, prompt=second_prompt) second_prompt = PromptTemplate(    input_variables=["company_name"],    template="Write a description of a logo for this company: {company_name}, the logo should not contain text at all ",)chain_two = LLMChain(llm=llm, prompt=second_prompt)  third_prompt = PromptTemplate(    input_variables=["company_logo_description"],    template="{company_logo_description}",)chain_three = LLMChain(llm=text2image, prompt=third_prompt) third_prompt = PromptTemplate(    input_variables=["company_logo_description"],    template="{company_logo_description}",)chain_three = LLMChain(llm=text2image, prompt=third_prompt)  # Run the chain specifying only the input variable for the first chain.overall_chain = SimpleSequentialChain(    chains=[chain, chain_two, chain_three],verbose=True)output = overall_chain.run("hats") # Run the chain specifying only the input variable for the first chain.overall_chain = SimpleSequentialChain(    chains=[chain, chain_two, chain_three],verbose=True)output = overall_chain.run("hats")  #print the imageprint_base64_image(output) #print the imageprint_base64_image(output)  Previous DeepSparse Next Fireworks Calling a modeltext generationimage generationtext generation with callbackChaining Calls Calling a modeltext generationimage generationtext generation with callbackChaining Calls CommunityDiscordTwitterGitHubPythonJS/TSMoreHomepageBlogCopyright ¬© 2023 LangChain, Inc. CommunityDiscordTwitterGitHubPythonJS/TSMoreHomepageBlog CommunityDiscordTwitter Community GitHubPythonJS/TS GitHub MoreHomepageBlog More Copyright ¬© 2023 LangChain, Inc. Copyright ¬© 2023 LangChain, Inc. Skip to main content (#docusaurus_skipToContent_fallback) ü¶úÔ∏èüîó LangChain (/) Docs (/docs/get_started/introduction) Use cases (/docs/use_cases) Integrations (/docs/integrations) API (https://api.python.langchain.com) LangSmith (https://smith.langchain.com) JS/TS Docs (https://js.langchain.com/docs)  (https://github.com/hwchase17/langchain) Integrations (/docs/integrations) Callbacks (/docs/integrations/callbacks/) Chat models (/docs/integrations/chat/) Chat loaders (/docs/integrations/chat_loaders/) Document loaders (/docs/integrations/document_loaders/) Document transformers (/docs/integrations/document_transformers/) LLMs (/docs/integrations/llms/) AI21 (/docs/integrations/llms/ai21) Aleph Alpha (/docs/integrations/llms/aleph_alpha) Amazon API Gateway (/docs/integrations/llms/amazon_api_gateway) Anyscale (/docs/integrations/llms/anyscale) Azure ML (/docs/integrations/llms/azure_ml) Azure OpenAI (/docs/integrations/llms/azure_openai) Banana (/docs/integrations/llms/banana) Baseten (/docs/integrations/llms/baseten) Beam (/docs/integrations/llms/beam) Bedrock (/docs/integrations/llms/bedrock) Bittensor (/docs/integrations/llms/bittensor) CerebriumAI (/docs/integrations/llms/cerebriumai) ChatGLM (/docs/integrations/llms/chatglm) Clarifai (/docs/integrations/llms/clarifai) Cohere (/docs/integrations/llms/cohere) C Transformers (/docs/integrations/llms/ctransformers) CTranslate2 (/docs/integrations/llms/ctranslate2) Databricks (/docs/integrations/llms/databricks) DeepInfra (/docs/integrations/llms/deepinfra) DeepSparse (/docs/integrations/llms/deepsparse) Eden AI (/docs/integrations/llms/edenai) Fireworks (/docs/integrations/llms/fireworks) ForefrontAI (/docs/integrations/llms/forefrontai) Google Vertex AI PaLM (/docs/integrations/llms/google_vertex_ai_palm) GooseAI (/docs/integrations/llms/gooseai) GPT4All (/docs/integrations/llms/gpt4all) Hugging Face Hub (/docs/integrations/llms/huggingface_hub) Hugging Face Local Pipelines (/docs/integrations/llms/huggingface_pipelines) Huggingface TextGen Inference (/docs/integrations/llms/huggingface_textgen_inference) JSONFormer (/docs/integrations/llms/jsonformer_experimental) KoboldAI API (/docs/integrations/llms/koboldai) Llama.cpp (/docs/integrations/llms/llamacpp) LLM Caching integrations (/docs/integrations/llms/llm_caching) Manifest (/docs/integrations/llms/manifest) Minimax (/docs/integrations/llms/minimax) Modal (/docs/integrations/llms/modal) MosaicML (/docs/integrations/llms/mosaicml) NLP Cloud (/docs/integrations/llms/nlpcloud) OctoAI (/docs/integrations/llms/octoai) Ollama (/docs/integrations/llms/ollama) OpaquePrompts (/docs/integrations/llms/opaqueprompts) OpenAI (/docs/integrations/llms/openai) OpenLLM (/docs/integrations/llms/openllm) OpenLM (/docs/integrations/llms/openlm) Petals (/docs/integrations/llms/petals) PipelineAI (/docs/integrations/llms/pipelineai) Predibase (/docs/integrations/llms/predibase) Prediction Guard (/docs/integrations/llms/predictionguard) PromptLayer OpenAI (/docs/integrations/llms/promptlayer_openai) RELLM (/docs/integrations/llms/rellm_experimental) Replicate (/docs/integrations/llms/replicate) Runhouse (/docs/integrations/llms/runhouse) SageMakerEndpoint (/docs/integrations/llms/sagemaker) StochasticAI (/docs/integrations/llms/stochasticai) Nebula (Symbl.ai) (/docs/integrations/llms/symblai_nebula) TextGen (/docs/integrations/llms/textgen) Titan Takeoff (/docs/integrations/llms/titan_takeoff) Tongyi Qwen (/docs/integrations/llms/tongyi) vLLM (/docs/integrations/llms/vllm) Writer (/docs/integrations/llms/writer) Xorbits Inference (Xinference) (/docs/integrations/llms/xinference) Memory (/docs/integrations/memory/) Retrievers (/docs/integrations/retrievers/) Text embedding models (/docs/integrations/text_embedding/) Agents & Toolkits (/docs/integrations/toolkits/) Tools (/docs/integrations/tools/) Vector stores (/docs/integrations/vectorstores/) Grouped by provider (/docs/integrations/providers/)  (/) Integrations (/docs/integrations) LLMs (/docs/integrations/llms/) https://edenai.co/ (https://edenai.co/) https://app.edenai.run/user/register (https://app.edenai.run/user/register) https://app.edenai.run/admin/account/settings (https://app.edenai.run/admin/account/settings) EdenAI (https://api.python.langchain.com/en/latest/llms/langchain.llms.edenai.EdenAI.html) ‚Äã (#calling-a-model) ‚Äã (#text-generation) ‚Äã (#image-generation) ‚Äã (#text-generation-with-callback) EdenAI (https://api.python.langchain.com/en/latest/llms/langchain.llms.edenai.EdenAI.html) StreamingStdOutCallbackHandler (https://api.python.langchain.com/en/latest/callbacks/langchain.callbacks.streaming_stdout.StreamingStdOutCallbackHandler.html) ‚Äã (#chaining-calls) SimpleSequentialChain (https://api.python.langchain.com/en/latest/chains/langchain.chains.sequential.SimpleSequentialChain.html) PromptTemplate (https://api.python.langchain.com/en/latest/prompts/langchain.prompts.prompt.PromptTemplate.html) LLMChain (https://api.python.langchain.com/en/latest/chains/langchain.chains.llm.LLMChain.html) PreviousDeepSparse (/docs/integrations/llms/deepsparse) NextFireworks (/docs/integrations/llms/fireworks) Calling a model (#calling-a-model) text generation (#text-generation) image generation (#image-generation) text generation with callback (#text-generation-with-callback) Chaining Calls (#chaining-calls) Discord (https://discord.gg/cU2adEyC7w) Twitter (https://twitter.com/LangChainAI) Python (https://github.com/hwchase17/langchain) JS/TS (https://github.com/hwchase17/langchainjs) Homepage (https://langchain.com) Blog (https://blog.langchain.dev)