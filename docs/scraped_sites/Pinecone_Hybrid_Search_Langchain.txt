Pinecone is a vector database with broad functionality. This notebook goes over how to use a retriever that under the hood uses Pinecone and Hybrid Search. The logic of this retriever is taken from this documentaion To use Pinecone, you must have an API key and an Environment. Here are the installation instructions. We want to use OpenAIEmbeddings so we have to get the OpenAI API Key. You should only have to do this part once. Note: it's important to make sure that the "context" field that holds the document text in the metadata is not indexed. Currently you need to specify explicitly the fields you do want to index. For more information checkout Pinecone's docs. Now that its created, we can use it Embeddings are used for the dense vectors, tokenizer is used for the sparse vector To encode the text to sparse values you can either choose SPLADE or BM25. For out of domain tasks we recommend using BM25. For more information about the sparse encoders you can checkout pinecone-text library docs. The above code is using default tfids values. It's highly recommended to fit the tf-idf values to your own corpus. You can do it as follow: We can now construct the retriever! We can optionally add texts to the retriever (if they aren't already in there) We can now use the retriever! IntegrationsCallbacksChat modelsChat loadersDocument loadersDocument transformersLLMsMemoryRetrieversAmazon KendraArxivAzure Cognitive SearchBM25ChaindeskChatGPT PluginCohere RerankerDocArray RetrieverElasticSearch BM25Google Cloud Enterprise SearchGoogle Drive RetrieverkNNLOTR (Merger Retriever)MetalPinecone Hybrid SearchPubMedRePhraseQueryRetrieverSVMTF-IDFVespaWeaviate Hybrid SearchWikipediaZepText embedding modelsAgents & ToolkitsToolsVector storesGrouped by provider Callbacks Chat models Chat loaders Document loaders Document transformers LLMs Memory RetrieversAmazon KendraArxivAzure Cognitive SearchBM25ChaindeskChatGPT PluginCohere RerankerDocArray RetrieverElasticSearch BM25Google Cloud Enterprise SearchGoogle Drive RetrieverkNNLOTR (Merger Retriever)MetalPinecone Hybrid SearchPubMedRePhraseQueryRetrieverSVMTF-IDFVespaWeaviate Hybrid SearchWikipediaZep Amazon Kendra Arxiv Azure Cognitive Search BM25 Chaindesk ChatGPT Plugin Cohere Reranker DocArray Retriever ElasticSearch BM25 Google Cloud Enterprise Search Google Drive Retriever kNN LOTR (Merger Retriever) Metal Pinecone Hybrid Search PubMed RePhraseQueryRetriever SVM TF-IDF Vespa Weaviate Hybrid Search Wikipedia Zep Text embedding models Agents & Toolkits Tools Vector stores Grouped by provider  Integrations Retrievers Pinecone Hybrid Search PineconeHybridSearchRetriever OpenAIEmbeddings Setup Pinecone Get embeddings and sparse encoders Load Retriever Add texts (if necessary) Use Retriever Discord Twitter Python JS/TS Homepage Blog Skip to main contentü¶úÔ∏èüîó LangChainDocsUse casesIntegrationsAPILangSmithJS/TS DocsCTRLKIntegrationsCallbacksChat modelsChat loadersDocument loadersDocument transformersLLMsMemoryRetrieversAmazon KendraArxivAzure Cognitive SearchBM25ChaindeskChatGPT PluginCohere RerankerDocArray RetrieverElasticSearch BM25Google Cloud Enterprise SearchGoogle Drive RetrieverkNNLOTR (Merger Retriever)MetalPinecone Hybrid SearchPubMedRePhraseQueryRetrieverSVMTF-IDFVespaWeaviate Hybrid SearchWikipediaZepText embedding modelsAgents & ToolkitsToolsVector storesGrouped by providerIntegrationsRetrieversPinecone Hybrid SearchOn this pagePinecone Hybrid SearchPinecone is a vector database with broad functionality.This notebook goes over how to use a retriever that under the hood uses Pinecone and Hybrid Search.The logic of this retriever is taken from this documentaionTo use Pinecone, you must have an API key and an Environment. Here are the installation instructions.#!pip install pinecone-client pinecone-textimport osimport getpassos.environ["PINECONE_API_KEY"] = getpass.getpass("Pinecone API Key:")from langchain.retrievers import PineconeHybridSearchRetrieverAPI Reference:PineconeHybridSearchRetrieveros.environ["PINECONE_ENVIRONMENT"] = getpass.getpass("Pinecone Environment:")We want to use OpenAIEmbeddings so we have to get the OpenAI API Key.os.environ["OPENAI_API_KEY"] = getpass.getpass("OpenAI API Key:")Setup Pinecone‚ÄãYou should only have to do this part once.Note: it's important to make sure that the "context" field that holds the document text in the metadata is not indexed. Currently you need to specify explicitly the fields you do want to index. For more information checkout Pinecone's docs.import osimport pineconeapi_key = os.getenv("PINECONE_API_KEY") or "PINECONE_API_KEY"# find environment next to your API key in the Pinecone consoleenv = os.getenv("PINECONE_ENVIRONMENT") or "PINECONE_ENVIRONMENT"index_name = "langchain-pinecone-hybrid-search"pinecone.init(api_key=api_key, environment=env)pinecone.whoami()    WhoAmIResponse(username='load', user_label='label', projectname='load-test')# create the indexpinecone.create_index(    name=index_name,    dimension=1536,  # dimensionality of dense model    metric="dotproduct",  # sparse values supported only for dotproduct    pod_type="s1",    metadata_config={"indexed": []},  # see explaination above)Now that its created, we can use itindex = pinecone.Index(index_name)Get embeddings and sparse encoders‚ÄãEmbeddings are used for the dense vectors, tokenizer is used for the sparse vectorfrom langchain.embeddings import OpenAIEmbeddingsembeddings = OpenAIEmbeddings()API Reference:OpenAIEmbeddingsTo encode the text to sparse values you can either choose SPLADE or BM25. For out of domain tasks we recommend using BM25.For more information about the sparse encoders you can checkout pinecone-text library docs.from pinecone_text.sparse import BM25Encoder# or from pinecone_text.sparse import SpladeEncoder if you wish to work with SPLADE# use default tf-idf valuesbm25_encoder = BM25Encoder().default()The above code is using default tfids values. It's highly recommended to fit the tf-idf values to your own corpus. You can do it as follow:corpus = ["foo", "bar", "world", "hello"]# fit tf-idf values on your corpusbm25_encoder.fit(corpus)# store the values to a json filebm25_encoder.dump("bm25_values.json")# load to your BM25Encoder objectbm25_encoder = BM25Encoder().load("bm25_values.json")Load Retriever‚ÄãWe can now construct the retriever!retriever = PineconeHybridSearchRetriever(    embeddings=embeddings, sparse_encoder=bm25_encoder, index=index)Add texts (if necessary)‚ÄãWe can optionally add texts to the retriever (if they aren't already in there)retriever.add_texts(["foo", "bar", "world", "hello"])    100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:02<00:00,  2.27s/it]Use Retriever‚ÄãWe can now use the retriever!result = retriever.get_relevant_documents("foo")result[0]    Document(page_content='foo', metadata={})PreviousMetalNextPubMedSetup PineconeGet embeddings and sparse encodersLoad RetrieverAdd texts (if necessary)Use RetrieverCommunityDiscordTwitterGitHubPythonJS/TSMoreHomepageBlogCopyright ¬© 2023 LangChain, Inc. Skip to main content ü¶úÔ∏èüîó LangChainDocsUse casesIntegrationsAPILangSmithJS/TS DocsCTRLK ü¶úÔ∏èüîó LangChainDocsUse casesIntegrationsAPI LangSmithJS/TS DocsCTRLK  CTRLK CTRLK  CTRLK   IntegrationsCallbacksChat modelsChat loadersDocument loadersDocument transformersLLMsMemoryRetrieversAmazon KendraArxivAzure Cognitive SearchBM25ChaindeskChatGPT PluginCohere RerankerDocArray RetrieverElasticSearch BM25Google Cloud Enterprise SearchGoogle Drive RetrieverkNNLOTR (Merger Retriever)MetalPinecone Hybrid SearchPubMedRePhraseQueryRetrieverSVMTF-IDFVespaWeaviate Hybrid SearchWikipediaZepText embedding modelsAgents & ToolkitsToolsVector storesGrouped by providerIntegrationsRetrieversPinecone Hybrid SearchOn this pagePinecone Hybrid SearchPinecone is a vector database with broad functionality.This notebook goes over how to use a retriever that under the hood uses Pinecone and Hybrid Search.The logic of this retriever is taken from this documentaionTo use Pinecone, you must have an API key and an Environment. Here are the installation instructions.#!pip install pinecone-client pinecone-textimport osimport getpassos.environ["PINECONE_API_KEY"] = getpass.getpass("Pinecone API Key:")from langchain.retrievers import PineconeHybridSearchRetrieverAPI Reference:PineconeHybridSearchRetrieveros.environ["PINECONE_ENVIRONMENT"] = getpass.getpass("Pinecone Environment:")We want to use OpenAIEmbeddings so we have to get the OpenAI API Key.os.environ["OPENAI_API_KEY"] = getpass.getpass("OpenAI API Key:")Setup Pinecone‚ÄãYou should only have to do this part once.Note: it's important to make sure that the "context" field that holds the document text in the metadata is not indexed. Currently you need to specify explicitly the fields you do want to index. For more information checkout Pinecone's docs.import osimport pineconeapi_key = os.getenv("PINECONE_API_KEY") or "PINECONE_API_KEY"# find environment next to your API key in the Pinecone consoleenv = os.getenv("PINECONE_ENVIRONMENT") or "PINECONE_ENVIRONMENT"index_name = "langchain-pinecone-hybrid-search"pinecone.init(api_key=api_key, environment=env)pinecone.whoami()    WhoAmIResponse(username='load', user_label='label', projectname='load-test')# create the indexpinecone.create_index(    name=index_name,    dimension=1536,  # dimensionality of dense model    metric="dotproduct",  # sparse values supported only for dotproduct    pod_type="s1",    metadata_config={"indexed": []},  # see explaination above)Now that its created, we can use itindex = pinecone.Index(index_name)Get embeddings and sparse encoders‚ÄãEmbeddings are used for the dense vectors, tokenizer is used for the sparse vectorfrom langchain.embeddings import OpenAIEmbeddingsembeddings = OpenAIEmbeddings()API Reference:OpenAIEmbeddingsTo encode the text to sparse values you can either choose SPLADE or BM25. For out of domain tasks we recommend using BM25.For more information about the sparse encoders you can checkout pinecone-text library docs.from pinecone_text.sparse import BM25Encoder# or from pinecone_text.sparse import SpladeEncoder if you wish to work with SPLADE# use default tf-idf valuesbm25_encoder = BM25Encoder().default()The above code is using default tfids values. It's highly recommended to fit the tf-idf values to your own corpus. You can do it as follow:corpus = ["foo", "bar", "world", "hello"]# fit tf-idf values on your corpusbm25_encoder.fit(corpus)# store the values to a json filebm25_encoder.dump("bm25_values.json")# load to your BM25Encoder objectbm25_encoder = BM25Encoder().load("bm25_values.json")Load Retriever‚ÄãWe can now construct the retriever!retriever = PineconeHybridSearchRetriever(    embeddings=embeddings, sparse_encoder=bm25_encoder, index=index)Add texts (if necessary)‚ÄãWe can optionally add texts to the retriever (if they aren't already in there)retriever.add_texts(["foo", "bar", "world", "hello"])    100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:02<00:00,  2.27s/it]Use Retriever‚ÄãWe can now use the retriever!result = retriever.get_relevant_documents("foo")result[0]    Document(page_content='foo', metadata={})PreviousMetalNextPubMedSetup PineconeGet embeddings and sparse encodersLoad RetrieverAdd texts (if necessary)Use Retriever IntegrationsCallbacksChat modelsChat loadersDocument loadersDocument transformersLLMsMemoryRetrieversAmazon KendraArxivAzure Cognitive SearchBM25ChaindeskChatGPT PluginCohere RerankerDocArray RetrieverElasticSearch BM25Google Cloud Enterprise SearchGoogle Drive RetrieverkNNLOTR (Merger Retriever)MetalPinecone Hybrid SearchPubMedRePhraseQueryRetrieverSVMTF-IDFVespaWeaviate Hybrid SearchWikipediaZepText embedding modelsAgents & ToolkitsToolsVector storesGrouped by providerIntegrationsRetrieversPinecone Hybrid SearchOn this pagePinecone Hybrid SearchPinecone is a vector database with broad functionality.This notebook goes over how to use a retriever that under the hood uses Pinecone and Hybrid Search.The logic of this retriever is taken from this documentaionTo use Pinecone, you must have an API key and an Environment. Here are the installation instructions.#!pip install pinecone-client pinecone-textimport osimport getpassos.environ["PINECONE_API_KEY"] = getpass.getpass("Pinecone API Key:")from langchain.retrievers import PineconeHybridSearchRetrieverAPI Reference:PineconeHybridSearchRetrieveros.environ["PINECONE_ENVIRONMENT"] = getpass.getpass("Pinecone Environment:")We want to use OpenAIEmbeddings so we have to get the OpenAI API Key.os.environ["OPENAI_API_KEY"] = getpass.getpass("OpenAI API Key:")Setup Pinecone‚ÄãYou should only have to do this part once.Note: it's important to make sure that the "context" field that holds the document text in the metadata is not indexed. Currently you need to specify explicitly the fields you do want to index. For more information checkout Pinecone's docs.import osimport pineconeapi_key = os.getenv("PINECONE_API_KEY") or "PINECONE_API_KEY"# find environment next to your API key in the Pinecone consoleenv = os.getenv("PINECONE_ENVIRONMENT") or "PINECONE_ENVIRONMENT"index_name = "langchain-pinecone-hybrid-search"pinecone.init(api_key=api_key, environment=env)pinecone.whoami()    WhoAmIResponse(username='load', user_label='label', projectname='load-test')# create the indexpinecone.create_index(    name=index_name,    dimension=1536,  # dimensionality of dense model    metric="dotproduct",  # sparse values supported only for dotproduct    pod_type="s1",    metadata_config={"indexed": []},  # see explaination above)Now that its created, we can use itindex = pinecone.Index(index_name)Get embeddings and sparse encoders‚ÄãEmbeddings are used for the dense vectors, tokenizer is used for the sparse vectorfrom langchain.embeddings import OpenAIEmbeddingsembeddings = OpenAIEmbeddings()API Reference:OpenAIEmbeddingsTo encode the text to sparse values you can either choose SPLADE or BM25. For out of domain tasks we recommend using BM25.For more information about the sparse encoders you can checkout pinecone-text library docs.from pinecone_text.sparse import BM25Encoder# or from pinecone_text.sparse import SpladeEncoder if you wish to work with SPLADE# use default tf-idf valuesbm25_encoder = BM25Encoder().default()The above code is using default tfids values. It's highly recommended to fit the tf-idf values to your own corpus. You can do it as follow:corpus = ["foo", "bar", "world", "hello"]# fit tf-idf values on your corpusbm25_encoder.fit(corpus)# store the values to a json filebm25_encoder.dump("bm25_values.json")# load to your BM25Encoder objectbm25_encoder = BM25Encoder().load("bm25_values.json")Load Retriever‚ÄãWe can now construct the retriever!retriever = PineconeHybridSearchRetriever(    embeddings=embeddings, sparse_encoder=bm25_encoder, index=index)Add texts (if necessary)‚ÄãWe can optionally add texts to the retriever (if they aren't already in there)retriever.add_texts(["foo", "bar", "world", "hello"])    100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:02<00:00,  2.27s/it]Use Retriever‚ÄãWe can now use the retriever!result = retriever.get_relevant_documents("foo")result[0]    Document(page_content='foo', metadata={})PreviousMetalNextPubMedSetup PineconeGet embeddings and sparse encodersLoad RetrieverAdd texts (if necessary)Use Retriever IntegrationsCallbacksChat modelsChat loadersDocument loadersDocument transformersLLMsMemoryRetrieversAmazon KendraArxivAzure Cognitive SearchBM25ChaindeskChatGPT PluginCohere RerankerDocArray RetrieverElasticSearch BM25Google Cloud Enterprise SearchGoogle Drive RetrieverkNNLOTR (Merger Retriever)MetalPinecone Hybrid SearchPubMedRePhraseQueryRetrieverSVMTF-IDFVespaWeaviate Hybrid SearchWikipediaZepText embedding modelsAgents & ToolkitsToolsVector storesGrouped by provider IntegrationsCallbacksChat modelsChat loadersDocument loadersDocument transformersLLMsMemoryRetrieversAmazon KendraArxivAzure Cognitive SearchBM25ChaindeskChatGPT PluginCohere RerankerDocArray RetrieverElasticSearch BM25Google Cloud Enterprise SearchGoogle Drive RetrieverkNNLOTR (Merger Retriever)MetalPinecone Hybrid SearchPubMedRePhraseQueryRetrieverSVMTF-IDFVespaWeaviate Hybrid SearchWikipediaZepText embedding modelsAgents & ToolkitsToolsVector storesGrouped by provider Integrations Callbacks Chat models Chat loaders Document loaders Document transformers LLMs Memory Retrievers Text embedding models Agents & Toolkits Tools Vector stores Grouped by provider IntegrationsRetrieversPinecone Hybrid SearchOn this pagePinecone Hybrid SearchPinecone is a vector database with broad functionality.This notebook goes over how to use a retriever that under the hood uses Pinecone and Hybrid Search.The logic of this retriever is taken from this documentaionTo use Pinecone, you must have an API key and an Environment. Here are the installation instructions.#!pip install pinecone-client pinecone-textimport osimport getpassos.environ["PINECONE_API_KEY"] = getpass.getpass("Pinecone API Key:")from langchain.retrievers import PineconeHybridSearchRetrieverAPI Reference:PineconeHybridSearchRetrieveros.environ["PINECONE_ENVIRONMENT"] = getpass.getpass("Pinecone Environment:")We want to use OpenAIEmbeddings so we have to get the OpenAI API Key.os.environ["OPENAI_API_KEY"] = getpass.getpass("OpenAI API Key:")Setup Pinecone‚ÄãYou should only have to do this part once.Note: it's important to make sure that the "context" field that holds the document text in the metadata is not indexed. Currently you need to specify explicitly the fields you do want to index. For more information checkout Pinecone's docs.import osimport pineconeapi_key = os.getenv("PINECONE_API_KEY") or "PINECONE_API_KEY"# find environment next to your API key in the Pinecone consoleenv = os.getenv("PINECONE_ENVIRONMENT") or "PINECONE_ENVIRONMENT"index_name = "langchain-pinecone-hybrid-search"pinecone.init(api_key=api_key, environment=env)pinecone.whoami()    WhoAmIResponse(username='load', user_label='label', projectname='load-test')# create the indexpinecone.create_index(    name=index_name,    dimension=1536,  # dimensionality of dense model    metric="dotproduct",  # sparse values supported only for dotproduct    pod_type="s1",    metadata_config={"indexed": []},  # see explaination above)Now that its created, we can use itindex = pinecone.Index(index_name)Get embeddings and sparse encoders‚ÄãEmbeddings are used for the dense vectors, tokenizer is used for the sparse vectorfrom langchain.embeddings import OpenAIEmbeddingsembeddings = OpenAIEmbeddings()API Reference:OpenAIEmbeddingsTo encode the text to sparse values you can either choose SPLADE or BM25. For out of domain tasks we recommend using BM25.For more information about the sparse encoders you can checkout pinecone-text library docs.from pinecone_text.sparse import BM25Encoder# or from pinecone_text.sparse import SpladeEncoder if you wish to work with SPLADE# use default tf-idf valuesbm25_encoder = BM25Encoder().default()The above code is using default tfids values. It's highly recommended to fit the tf-idf values to your own corpus. You can do it as follow:corpus = ["foo", "bar", "world", "hello"]# fit tf-idf values on your corpusbm25_encoder.fit(corpus)# store the values to a json filebm25_encoder.dump("bm25_values.json")# load to your BM25Encoder objectbm25_encoder = BM25Encoder().load("bm25_values.json")Load Retriever‚ÄãWe can now construct the retriever!retriever = PineconeHybridSearchRetriever(    embeddings=embeddings, sparse_encoder=bm25_encoder, index=index)Add texts (if necessary)‚ÄãWe can optionally add texts to the retriever (if they aren't already in there)retriever.add_texts(["foo", "bar", "world", "hello"])    100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:02<00:00,  2.27s/it]Use Retriever‚ÄãWe can now use the retriever!result = retriever.get_relevant_documents("foo")result[0]    Document(page_content='foo', metadata={})PreviousMetalNextPubMedSetup PineconeGet embeddings and sparse encodersLoad RetrieverAdd texts (if necessary)Use Retriever IntegrationsRetrieversPinecone Hybrid SearchOn this pagePinecone Hybrid SearchPinecone is a vector database with broad functionality.This notebook goes over how to use a retriever that under the hood uses Pinecone and Hybrid Search.The logic of this retriever is taken from this documentaionTo use Pinecone, you must have an API key and an Environment. Here are the installation instructions.#!pip install pinecone-client pinecone-textimport osimport getpassos.environ["PINECONE_API_KEY"] = getpass.getpass("Pinecone API Key:")from langchain.retrievers import PineconeHybridSearchRetrieverAPI Reference:PineconeHybridSearchRetrieveros.environ["PINECONE_ENVIRONMENT"] = getpass.getpass("Pinecone Environment:")We want to use OpenAIEmbeddings so we have to get the OpenAI API Key.os.environ["OPENAI_API_KEY"] = getpass.getpass("OpenAI API Key:")Setup Pinecone‚ÄãYou should only have to do this part once.Note: it's important to make sure that the "context" field that holds the document text in the metadata is not indexed. Currently you need to specify explicitly the fields you do want to index. For more information checkout Pinecone's docs.import osimport pineconeapi_key = os.getenv("PINECONE_API_KEY") or "PINECONE_API_KEY"# find environment next to your API key in the Pinecone consoleenv = os.getenv("PINECONE_ENVIRONMENT") or "PINECONE_ENVIRONMENT"index_name = "langchain-pinecone-hybrid-search"pinecone.init(api_key=api_key, environment=env)pinecone.whoami()    WhoAmIResponse(username='load', user_label='label', projectname='load-test')# create the indexpinecone.create_index(    name=index_name,    dimension=1536,  # dimensionality of dense model    metric="dotproduct",  # sparse values supported only for dotproduct    pod_type="s1",    metadata_config={"indexed": []},  # see explaination above)Now that its created, we can use itindex = pinecone.Index(index_name)Get embeddings and sparse encoders‚ÄãEmbeddings are used for the dense vectors, tokenizer is used for the sparse vectorfrom langchain.embeddings import OpenAIEmbeddingsembeddings = OpenAIEmbeddings()API Reference:OpenAIEmbeddingsTo encode the text to sparse values you can either choose SPLADE or BM25. For out of domain tasks we recommend using BM25.For more information about the sparse encoders you can checkout pinecone-text library docs.from pinecone_text.sparse import BM25Encoder# or from pinecone_text.sparse import SpladeEncoder if you wish to work with SPLADE# use default tf-idf valuesbm25_encoder = BM25Encoder().default()The above code is using default tfids values. It's highly recommended to fit the tf-idf values to your own corpus. You can do it as follow:corpus = ["foo", "bar", "world", "hello"]# fit tf-idf values on your corpusbm25_encoder.fit(corpus)# store the values to a json filebm25_encoder.dump("bm25_values.json")# load to your BM25Encoder objectbm25_encoder = BM25Encoder().load("bm25_values.json")Load Retriever‚ÄãWe can now construct the retriever!retriever = PineconeHybridSearchRetriever(    embeddings=embeddings, sparse_encoder=bm25_encoder, index=index)Add texts (if necessary)‚ÄãWe can optionally add texts to the retriever (if they aren't already in there)retriever.add_texts(["foo", "bar", "world", "hello"])    100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:02<00:00,  2.27s/it]Use Retriever‚ÄãWe can now use the retriever!result = retriever.get_relevant_documents("foo")result[0]    Document(page_content='foo', metadata={})PreviousMetalNextPubMed IntegrationsRetrieversPinecone Hybrid SearchOn this pagePinecone Hybrid SearchPinecone is a vector database with broad functionality.This notebook goes over how to use a retriever that under the hood uses Pinecone and Hybrid Search.The logic of this retriever is taken from this documentaionTo use Pinecone, you must have an API key and an Environment. Here are the installation instructions.#!pip install pinecone-client pinecone-textimport osimport getpassos.environ["PINECONE_API_KEY"] = getpass.getpass("Pinecone API Key:")from langchain.retrievers import PineconeHybridSearchRetrieverAPI Reference:PineconeHybridSearchRetrieveros.environ["PINECONE_ENVIRONMENT"] = getpass.getpass("Pinecone Environment:")We want to use OpenAIEmbeddings so we have to get the OpenAI API Key.os.environ["OPENAI_API_KEY"] = getpass.getpass("OpenAI API Key:")Setup Pinecone‚ÄãYou should only have to do this part once.Note: it's important to make sure that the "context" field that holds the document text in the metadata is not indexed. Currently you need to specify explicitly the fields you do want to index. For more information checkout Pinecone's docs.import osimport pineconeapi_key = os.getenv("PINECONE_API_KEY") or "PINECONE_API_KEY"# find environment next to your API key in the Pinecone consoleenv = os.getenv("PINECONE_ENVIRONMENT") or "PINECONE_ENVIRONMENT"index_name = "langchain-pinecone-hybrid-search"pinecone.init(api_key=api_key, environment=env)pinecone.whoami()    WhoAmIResponse(username='load', user_label='label', projectname='load-test')# create the indexpinecone.create_index(    name=index_name,    dimension=1536,  # dimensionality of dense model    metric="dotproduct",  # sparse values supported only for dotproduct    pod_type="s1",    metadata_config={"indexed": []},  # see explaination above)Now that its created, we can use itindex = pinecone.Index(index_name)Get embeddings and sparse encoders‚ÄãEmbeddings are used for the dense vectors, tokenizer is used for the sparse vectorfrom langchain.embeddings import OpenAIEmbeddingsembeddings = OpenAIEmbeddings()API Reference:OpenAIEmbeddingsTo encode the text to sparse values you can either choose SPLADE or BM25. For out of domain tasks we recommend using BM25.For more information about the sparse encoders you can checkout pinecone-text library docs.from pinecone_text.sparse import BM25Encoder# or from pinecone_text.sparse import SpladeEncoder if you wish to work with SPLADE# use default tf-idf valuesbm25_encoder = BM25Encoder().default()The above code is using default tfids values. It's highly recommended to fit the tf-idf values to your own corpus. You can do it as follow:corpus = ["foo", "bar", "world", "hello"]# fit tf-idf values on your corpusbm25_encoder.fit(corpus)# store the values to a json filebm25_encoder.dump("bm25_values.json")# load to your BM25Encoder objectbm25_encoder = BM25Encoder().load("bm25_values.json")Load Retriever‚ÄãWe can now construct the retriever!retriever = PineconeHybridSearchRetriever(    embeddings=embeddings, sparse_encoder=bm25_encoder, index=index)Add texts (if necessary)‚ÄãWe can optionally add texts to the retriever (if they aren't already in there)retriever.add_texts(["foo", "bar", "world", "hello"])    100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:02<00:00,  2.27s/it]Use Retriever‚ÄãWe can now use the retriever!result = retriever.get_relevant_documents("foo")result[0]    Document(page_content='foo', metadata={})PreviousMetalNextPubMed On this page Pinecone Hybrid SearchPinecone is a vector database with broad functionality.This notebook goes over how to use a retriever that under the hood uses Pinecone and Hybrid Search.The logic of this retriever is taken from this documentaionTo use Pinecone, you must have an API key and an Environment. Here are the installation instructions.#!pip install pinecone-client pinecone-textimport osimport getpassos.environ["PINECONE_API_KEY"] = getpass.getpass("Pinecone API Key:")from langchain.retrievers import PineconeHybridSearchRetrieverAPI Reference:PineconeHybridSearchRetrieveros.environ["PINECONE_ENVIRONMENT"] = getpass.getpass("Pinecone Environment:")We want to use OpenAIEmbeddings so we have to get the OpenAI API Key.os.environ["OPENAI_API_KEY"] = getpass.getpass("OpenAI API Key:")Setup Pinecone‚ÄãYou should only have to do this part once.Note: it's important to make sure that the "context" field that holds the document text in the metadata is not indexed. Currently you need to specify explicitly the fields you do want to index. For more information checkout Pinecone's docs.import osimport pineconeapi_key = os.getenv("PINECONE_API_KEY") or "PINECONE_API_KEY"# find environment next to your API key in the Pinecone consoleenv = os.getenv("PINECONE_ENVIRONMENT") or "PINECONE_ENVIRONMENT"index_name = "langchain-pinecone-hybrid-search"pinecone.init(api_key=api_key, environment=env)pinecone.whoami()    WhoAmIResponse(username='load', user_label='label', projectname='load-test')# create the indexpinecone.create_index(    name=index_name,    dimension=1536,  # dimensionality of dense model    metric="dotproduct",  # sparse values supported only for dotproduct    pod_type="s1",    metadata_config={"indexed": []},  # see explaination above)Now that its created, we can use itindex = pinecone.Index(index_name)Get embeddings and sparse encoders‚ÄãEmbeddings are used for the dense vectors, tokenizer is used for the sparse vectorfrom langchain.embeddings import OpenAIEmbeddingsembeddings = OpenAIEmbeddings()API Reference:OpenAIEmbeddingsTo encode the text to sparse values you can either choose SPLADE or BM25. For out of domain tasks we recommend using BM25.For more information about the sparse encoders you can checkout pinecone-text library docs.from pinecone_text.sparse import BM25Encoder# or from pinecone_text.sparse import SpladeEncoder if you wish to work with SPLADE# use default tf-idf valuesbm25_encoder = BM25Encoder().default()The above code is using default tfids values. It's highly recommended to fit the tf-idf values to your own corpus. You can do it as follow:corpus = ["foo", "bar", "world", "hello"]# fit tf-idf values on your corpusbm25_encoder.fit(corpus)# store the values to a json filebm25_encoder.dump("bm25_values.json")# load to your BM25Encoder objectbm25_encoder = BM25Encoder().load("bm25_values.json")Load Retriever‚ÄãWe can now construct the retriever!retriever = PineconeHybridSearchRetriever(    embeddings=embeddings, sparse_encoder=bm25_encoder, index=index)Add texts (if necessary)‚ÄãWe can optionally add texts to the retriever (if they aren't already in there)retriever.add_texts(["foo", "bar", "world", "hello"])    100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:02<00:00,  2.27s/it]Use Retriever‚ÄãWe can now use the retriever!result = retriever.get_relevant_documents("foo")result[0]    Document(page_content='foo', metadata={}) #!pip install pinecone-client pinecone-text #!pip install pinecone-client pinecone-text  import osimport getpassos.environ["PINECONE_API_KEY"] = getpass.getpass("Pinecone API Key:") import osimport getpassos.environ["PINECONE_API_KEY"] = getpass.getpass("Pinecone API Key:")  from langchain.retrievers import PineconeHybridSearchRetriever from langchain.retrievers import PineconeHybridSearchRetriever  API Reference:PineconeHybridSearchRetriever os.environ["PINECONE_ENVIRONMENT"] = getpass.getpass("Pinecone Environment:") os.environ["PINECONE_ENVIRONMENT"] = getpass.getpass("Pinecone Environment:")  os.environ["OPENAI_API_KEY"] = getpass.getpass("OpenAI API Key:") os.environ["OPENAI_API_KEY"] = getpass.getpass("OpenAI API Key:")  import osimport pineconeapi_key = os.getenv("PINECONE_API_KEY") or "PINECONE_API_KEY"# find environment next to your API key in the Pinecone consoleenv = os.getenv("PINECONE_ENVIRONMENT") or "PINECONE_ENVIRONMENT"index_name = "langchain-pinecone-hybrid-search"pinecone.init(api_key=api_key, environment=env)pinecone.whoami() import osimport pineconeapi_key = os.getenv("PINECONE_API_KEY") or "PINECONE_API_KEY"# find environment next to your API key in the Pinecone consoleenv = os.getenv("PINECONE_ENVIRONMENT") or "PINECONE_ENVIRONMENT"index_name = "langchain-pinecone-hybrid-search"pinecone.init(api_key=api_key, environment=env)pinecone.whoami()      WhoAmIResponse(username='load', user_label='label', projectname='load-test')     WhoAmIResponse(username='load', user_label='label', projectname='load-test')     WhoAmIResponse(username='load', user_label='label', projectname='load-test')  # create the indexpinecone.create_index(    name=index_name,    dimension=1536,  # dimensionality of dense model    metric="dotproduct",  # sparse values supported only for dotproduct    pod_type="s1",    metadata_config={"indexed": []},  # see explaination above) # create the indexpinecone.create_index(    name=index_name,    dimension=1536,  # dimensionality of dense model    metric="dotproduct",  # sparse values supported only for dotproduct    pod_type="s1",    metadata_config={"indexed": []},  # see explaination above)  index = pinecone.Index(index_name) index = pinecone.Index(index_name)  from langchain.embeddings import OpenAIEmbeddingsembeddings = OpenAIEmbeddings() from langchain.embeddings import OpenAIEmbeddingsembeddings = OpenAIEmbeddings()  API Reference:OpenAIEmbeddings from pinecone_text.sparse import BM25Encoder# or from pinecone_text.sparse import SpladeEncoder if you wish to work with SPLADE# use default tf-idf valuesbm25_encoder = BM25Encoder().default() from pinecone_text.sparse import BM25Encoder# or from pinecone_text.sparse import SpladeEncoder if you wish to work with SPLADE# use default tf-idf valuesbm25_encoder = BM25Encoder().default()  corpus = ["foo", "bar", "world", "hello"]# fit tf-idf values on your corpusbm25_encoder.fit(corpus)# store the values to a json filebm25_encoder.dump("bm25_values.json")# load to your BM25Encoder objectbm25_encoder = BM25Encoder().load("bm25_values.json") corpus = ["foo", "bar", "world", "hello"]# fit tf-idf values on your corpusbm25_encoder.fit(corpus)# store the values to a json filebm25_encoder.dump("bm25_values.json")# load to your BM25Encoder objectbm25_encoder = BM25Encoder().load("bm25_values.json")  retriever = PineconeHybridSearchRetriever(    embeddings=embeddings, sparse_encoder=bm25_encoder, index=index) retriever = PineconeHybridSearchRetriever(    embeddings=embeddings, sparse_encoder=bm25_encoder, index=index)  retriever.add_texts(["foo", "bar", "world", "hello"]) retriever.add_texts(["foo", "bar", "world", "hello"])      100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:02<00:00,  2.27s/it]     100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:02<00:00,  2.27s/it]     100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:02<00:00,  2.27s/it]  result = retriever.get_relevant_documents("foo") result = retriever.get_relevant_documents("foo")  result[0] result[0]      Document(page_content='foo', metadata={})     Document(page_content='foo', metadata={})     Document(page_content='foo', metadata={})  Previous Metal Next PubMed Setup PineconeGet embeddings and sparse encodersLoad RetrieverAdd texts (if necessary)Use Retriever Setup PineconeGet embeddings and sparse encodersLoad RetrieverAdd texts (if necessary)Use Retriever CommunityDiscordTwitterGitHubPythonJS/TSMoreHomepageBlogCopyright ¬© 2023 LangChain, Inc. CommunityDiscordTwitterGitHubPythonJS/TSMoreHomepageBlog CommunityDiscordTwitter Community GitHubPythonJS/TS GitHub MoreHomepageBlog More Copyright ¬© 2023 LangChain, Inc. Copyright ¬© 2023 LangChain, Inc. Skip to main content (#docusaurus_skipToContent_fallback) ü¶úÔ∏èüîó LangChain (/) Docs (/docs/get_started/introduction) Use cases (/docs/use_cases) Integrations (/docs/integrations) API (https://api.python.langchain.com) LangSmith (https://smith.langchain.com) JS/TS Docs (https://js.langchain.com/docs)  (https://github.com/hwchase17/langchain) Integrations (/docs/integrations) Callbacks (/docs/integrations/callbacks/) Chat models (/docs/integrations/chat/) Chat loaders (/docs/integrations/chat_loaders/) Document loaders (/docs/integrations/document_loaders/) Document transformers (/docs/integrations/document_transformers/) LLMs (/docs/integrations/llms/) Memory (/docs/integrations/memory/) Retrievers (/docs/integrations/retrievers/) Amazon Kendra (/docs/integrations/retrievers/amazon_kendra_retriever) Arxiv (/docs/integrations/retrievers/arxiv) Azure Cognitive Search (/docs/integrations/retrievers/azure_cognitive_search) BM25 (/docs/integrations/retrievers/bm25) Chaindesk (/docs/integrations/retrievers/chaindesk) ChatGPT Plugin (/docs/integrations/retrievers/chatgpt-plugin) Cohere Reranker (/docs/integrations/retrievers/cohere-reranker) DocArray Retriever (/docs/integrations/retrievers/docarray_retriever) ElasticSearch BM25 (/docs/integrations/retrievers/elastic_search_bm25) Google Cloud Enterprise Search (/docs/integrations/retrievers/google_cloud_enterprise_search) Google Drive Retriever (/docs/integrations/retrievers/google_drive) kNN (/docs/integrations/retrievers/knn) LOTR (Merger Retriever) (/docs/integrations/retrievers/merger_retriever) Metal (/docs/integrations/retrievers/metal) Pinecone Hybrid Search (/docs/integrations/retrievers/pinecone_hybrid_search) PubMed (/docs/integrations/retrievers/pubmed) RePhraseQueryRetriever (/docs/integrations/retrievers/re_phrase) SVM (/docs/integrations/retrievers/svm) TF-IDF (/docs/integrations/retrievers/tf_idf) Vespa (/docs/integrations/retrievers/vespa) Weaviate Hybrid Search (/docs/integrations/retrievers/weaviate-hybrid) Wikipedia (/docs/integrations/retrievers/wikipedia) Zep (/docs/integrations/retrievers/zep_memorystore) Text embedding models (/docs/integrations/text_embedding/) Agents & Toolkits (/docs/integrations/toolkits/) Tools (/docs/integrations/tools/) Vector stores (/docs/integrations/vectorstores/) Grouped by provider (/docs/integrations/providers/)  (/) Integrations (/docs/integrations) Retrievers (/docs/integrations/retrievers/) Pinecone (https://docs.pinecone.io/docs/overview) this documentaion (https://docs.pinecone.io/docs/hybrid-search) installation instructions (https://docs.pinecone.io/docs/quickstart) PineconeHybridSearchRetriever (https://api.python.langchain.com/en/latest/retrievers/langchain.retrievers.pinecone_hybrid_search.PineconeHybridSearchRetriever.html) ‚Äã (#setup-pinecone) docs (https://docs.pinecone.io/docs/manage-indexes#selective-metadata-indexing) ‚Äã (#get-embeddings-and-sparse-encoders) OpenAIEmbeddings (https://api.python.langchain.com/en/latest/embeddings/langchain.embeddings.openai.OpenAIEmbeddings.html) docs (https://pinecone-io.github.io/pinecone-text/pinecone_text.html) ‚Äã (#load-retriever) ‚Äã (#add-texts-if-necessary) ‚Äã (#use-retriever) PreviousMetal (/docs/integrations/retrievers/metal) NextPubMed (/docs/integrations/retrievers/pubmed) Setup Pinecone (#setup-pinecone) Get embeddings and sparse encoders (#get-embeddings-and-sparse-encoders) Load Retriever (#load-retriever) Add texts (if necessary) (#add-texts-if-necessary) Use Retriever (#use-retriever) Discord (https://discord.gg/cU2adEyC7w) Twitter (https://twitter.com/LangChainAI) Python (https://github.com/hwchase17/langchain) JS/TS (https://github.com/hwchase17/langchainjs) Homepage (https://langchain.com) Blog (https://blog.langchain.dev)