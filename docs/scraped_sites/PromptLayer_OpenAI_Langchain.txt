PromptLayer is the first platform that allows you to track, manage, and share your GPT prompt engineering. PromptLayer acts a middleware between your code and OpenAI‚Äôs python library. PromptLayer records all your OpenAI API requests, allowing you to search and explore request history in the PromptLayer dashboard. This example showcases how to connect to PromptLayer to start recording your OpenAI requests. Another example is here. The promptlayer package is required to use PromptLayer with OpenAI. Install promptlayer using pip. You can create a PromptLayer API Key at www.promptlayer.com by clicking the settings cog in the navbar. Set it as an environment variable called PROMPTLAYER_API_KEY. You also need an OpenAI Key, called OPENAI_API_KEY. You can optionally pass in pl_tags to track your requests with PromptLayer's tagging feature. The above request should now appear on your PromptLayer dashboard. If you would like to use any of the PromptLayer tracking features, you need to pass the argument return_pl_id when instantializing the PromptLayer LLM to get the request id.   Using this allows you to track the performance of your model in the PromptLayer dashboard. If you are using a prompt template, you can attach a template to a request as well. Overall, this gives you the opportunity to track the performance of different templates and models in the PromptLayer dashboard. IntegrationsCallbacksChat modelsChat loadersDocument loadersDocument transformersLLMsAI21Aleph AlphaAmazon API GatewayAnyscaleAzure MLAzure OpenAIBananaBasetenBeamBedrockBittensorCerebriumAIChatGLMClarifaiCohereC TransformersCTranslate2DatabricksDeepInfraDeepSparseEden AIFireworksForefrontAIGoogle Vertex AI PaLMGooseAIGPT4AllHugging Face HubHugging Face Local PipelinesHuggingface TextGen InferenceJSONFormerKoboldAI APILlama.cppLLM Caching integrationsManifestMinimaxModalMosaicMLNLP CloudOctoAIOllamaOpaquePromptsOpenAIOpenLLMOpenLMPetalsPipelineAIPredibasePrediction GuardPromptLayer OpenAIRELLMReplicateRunhouseSageMakerEndpointStochasticAINebula (Symbl.ai)TextGenTitan TakeoffTongyi QwenvLLMWriterXorbits Inference (Xinference)MemoryRetrieversText embedding modelsAgents & ToolkitsToolsVector storesGrouped by provider Callbacks Chat models Chat loaders Document loaders Document transformers LLMsAI21Aleph AlphaAmazon API GatewayAnyscaleAzure MLAzure OpenAIBananaBasetenBeamBedrockBittensorCerebriumAIChatGLMClarifaiCohereC TransformersCTranslate2DatabricksDeepInfraDeepSparseEden AIFireworksForefrontAIGoogle Vertex AI PaLMGooseAIGPT4AllHugging Face HubHugging Face Local PipelinesHuggingface TextGen InferenceJSONFormerKoboldAI APILlama.cppLLM Caching integrationsManifestMinimaxModalMosaicMLNLP CloudOctoAIOllamaOpaquePromptsOpenAIOpenLLMOpenLMPetalsPipelineAIPredibasePrediction GuardPromptLayer OpenAIRELLMReplicateRunhouseSageMakerEndpointStochasticAINebula (Symbl.ai)TextGenTitan TakeoffTongyi QwenvLLMWriterXorbits Inference (Xinference) AI21 Aleph Alpha Amazon API Gateway Anyscale Azure ML Azure OpenAI Banana Baseten Beam Bedrock Bittensor CerebriumAI ChatGLM Clarifai Cohere C Transformers CTranslate2 Databricks DeepInfra DeepSparse Eden AI Fireworks ForefrontAI Google Vertex AI PaLM GooseAI GPT4All Hugging Face Hub Hugging Face Local Pipelines Huggingface TextGen Inference JSONFormer KoboldAI API Llama.cpp LLM Caching integrations Manifest Minimax Modal MosaicML NLP Cloud OctoAI Ollama OpaquePrompts OpenAI OpenLLM OpenLM Petals PipelineAI Predibase Prediction Guard PromptLayer OpenAI RELLM Replicate Runhouse SageMakerEndpoint StochasticAI Nebula (Symbl.ai) TextGen Titan Takeoff Tongyi Qwen vLLM Writer Xorbits Inference (Xinference) Memory Retrievers Text embedding models Agents & Toolkits Tools Vector stores Grouped by provider  Integrations LLMs PromptLayer OpenAI PromptLayerOpenAI Install PromptLayer Imports Set the Environment API Key Use the PromptLayerOpenAI LLM like normal Using PromptLayer Track Discord Twitter Python JS/TS Homepage Blog Skip to main contentü¶úÔ∏èüîó LangChainDocsUse casesIntegrationsAPILangSmithJS/TS DocsCTRLKIntegrationsCallbacksChat modelsChat loadersDocument loadersDocument transformersLLMsAI21Aleph AlphaAmazon API GatewayAnyscaleAzure MLAzure OpenAIBananaBasetenBeamBedrockBittensorCerebriumAIChatGLMClarifaiCohereC TransformersCTranslate2DatabricksDeepInfraDeepSparseEden AIFireworksForefrontAIGoogle Vertex AI PaLMGooseAIGPT4AllHugging Face HubHugging Face Local PipelinesHuggingface TextGen InferenceJSONFormerKoboldAI APILlama.cppLLM Caching integrationsManifestMinimaxModalMosaicMLNLP CloudOctoAIOllamaOpaquePromptsOpenAIOpenLLMOpenLMPetalsPipelineAIPredibasePrediction GuardPromptLayer OpenAIRELLMReplicateRunhouseSageMakerEndpointStochasticAINebula (Symbl.ai)TextGenTitan TakeoffTongyi QwenvLLMWriterXorbits Inference (Xinference)MemoryRetrieversText embedding modelsAgents & ToolkitsToolsVector storesGrouped by providerIntegrationsLLMsPromptLayer OpenAIOn this pagePromptLayer OpenAIPromptLayer is the first platform that allows you to track, manage, and share your GPT prompt engineering. PromptLayer acts a middleware between your code and OpenAI‚Äôs python library.PromptLayer records all your OpenAI API requests, allowing you to search and explore request history in the PromptLayer dashboard.This example showcases how to connect to PromptLayer to start recording your OpenAI requests.Another example is here.Install PromptLayer‚ÄãThe promptlayer package is required to use PromptLayer with OpenAI. Install promptlayer using pip.pip install promptlayerImports‚Äãimport osfrom langchain.llms import PromptLayerOpenAIimport promptlayerAPI Reference:PromptLayerOpenAISet the Environment API Key‚ÄãYou can create a PromptLayer API Key at www.promptlayer.com by clicking the settings cog in the navbar.Set it as an environment variable called PROMPTLAYER_API_KEY.You also need an OpenAI Key, called OPENAI_API_KEY.from getpass import getpassPROMPTLAYER_API_KEY = getpass()     ¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑os.environ["PROMPTLAYER_API_KEY"] = PROMPTLAYER_API_KEYfrom getpass import getpassOPENAI_API_KEY = getpass()     ¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑os.environ["OPENAI_API_KEY"] = OPENAI_API_KEYUse the PromptLayerOpenAI LLM like normal‚ÄãYou can optionally pass in pl_tags to track your requests with PromptLayer's tagging feature.llm = PromptLayerOpenAI(pl_tags=["langchain"])llm("I am a cat and I want")The above request should now appear on your PromptLayer dashboard.Using PromptLayer Track‚ÄãIf you would like to use any of the PromptLayer tracking features, you need to pass the argument return_pl_id when instantializing the PromptLayer LLM to get the request id.  llm = PromptLayerOpenAI(return_pl_id=True)llm_results = llm.generate(["Tell me a joke"])for res in llm_results.generations:    pl_request_id = res[0].generation_info["pl_request_id"]    promptlayer.track.score(request_id=pl_request_id, score=100)Using this allows you to track the performance of your model in the PromptLayer dashboard. If you are using a prompt template, you can attach a template to a request as well. Overall, this gives you the opportunity to track the performance of different templates and models in the PromptLayer dashboard.PreviousPrediction GuardNextRELLMInstall PromptLayerImportsSet the Environment API KeyUse the PromptLayerOpenAI LLM like normalUsing PromptLayer TrackCommunityDiscordTwitterGitHubPythonJS/TSMoreHomepageBlogCopyright ¬© 2023 LangChain, Inc. Skip to main content ü¶úÔ∏èüîó LangChainDocsUse casesIntegrationsAPILangSmithJS/TS DocsCTRLK ü¶úÔ∏èüîó LangChainDocsUse casesIntegrationsAPI LangSmithJS/TS DocsCTRLK  CTRLK CTRLK  CTRLK   IntegrationsCallbacksChat modelsChat loadersDocument loadersDocument transformersLLMsAI21Aleph AlphaAmazon API GatewayAnyscaleAzure MLAzure OpenAIBananaBasetenBeamBedrockBittensorCerebriumAIChatGLMClarifaiCohereC TransformersCTranslate2DatabricksDeepInfraDeepSparseEden AIFireworksForefrontAIGoogle Vertex AI PaLMGooseAIGPT4AllHugging Face HubHugging Face Local PipelinesHuggingface TextGen InferenceJSONFormerKoboldAI APILlama.cppLLM Caching integrationsManifestMinimaxModalMosaicMLNLP CloudOctoAIOllamaOpaquePromptsOpenAIOpenLLMOpenLMPetalsPipelineAIPredibasePrediction GuardPromptLayer OpenAIRELLMReplicateRunhouseSageMakerEndpointStochasticAINebula (Symbl.ai)TextGenTitan TakeoffTongyi QwenvLLMWriterXorbits Inference (Xinference)MemoryRetrieversText embedding modelsAgents & ToolkitsToolsVector storesGrouped by providerIntegrationsLLMsPromptLayer OpenAIOn this pagePromptLayer OpenAIPromptLayer is the first platform that allows you to track, manage, and share your GPT prompt engineering. PromptLayer acts a middleware between your code and OpenAI‚Äôs python library.PromptLayer records all your OpenAI API requests, allowing you to search and explore request history in the PromptLayer dashboard.This example showcases how to connect to PromptLayer to start recording your OpenAI requests.Another example is here.Install PromptLayer‚ÄãThe promptlayer package is required to use PromptLayer with OpenAI. Install promptlayer using pip.pip install promptlayerImports‚Äãimport osfrom langchain.llms import PromptLayerOpenAIimport promptlayerAPI Reference:PromptLayerOpenAISet the Environment API Key‚ÄãYou can create a PromptLayer API Key at www.promptlayer.com by clicking the settings cog in the navbar.Set it as an environment variable called PROMPTLAYER_API_KEY.You also need an OpenAI Key, called OPENAI_API_KEY.from getpass import getpassPROMPTLAYER_API_KEY = getpass()     ¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑os.environ["PROMPTLAYER_API_KEY"] = PROMPTLAYER_API_KEYfrom getpass import getpassOPENAI_API_KEY = getpass()     ¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑os.environ["OPENAI_API_KEY"] = OPENAI_API_KEYUse the PromptLayerOpenAI LLM like normal‚ÄãYou can optionally pass in pl_tags to track your requests with PromptLayer's tagging feature.llm = PromptLayerOpenAI(pl_tags=["langchain"])llm("I am a cat and I want")The above request should now appear on your PromptLayer dashboard.Using PromptLayer Track‚ÄãIf you would like to use any of the PromptLayer tracking features, you need to pass the argument return_pl_id when instantializing the PromptLayer LLM to get the request id.  llm = PromptLayerOpenAI(return_pl_id=True)llm_results = llm.generate(["Tell me a joke"])for res in llm_results.generations:    pl_request_id = res[0].generation_info["pl_request_id"]    promptlayer.track.score(request_id=pl_request_id, score=100)Using this allows you to track the performance of your model in the PromptLayer dashboard. If you are using a prompt template, you can attach a template to a request as well. Overall, this gives you the opportunity to track the performance of different templates and models in the PromptLayer dashboard.PreviousPrediction GuardNextRELLMInstall PromptLayerImportsSet the Environment API KeyUse the PromptLayerOpenAI LLM like normalUsing PromptLayer Track IntegrationsCallbacksChat modelsChat loadersDocument loadersDocument transformersLLMsAI21Aleph AlphaAmazon API GatewayAnyscaleAzure MLAzure OpenAIBananaBasetenBeamBedrockBittensorCerebriumAIChatGLMClarifaiCohereC TransformersCTranslate2DatabricksDeepInfraDeepSparseEden AIFireworksForefrontAIGoogle Vertex AI PaLMGooseAIGPT4AllHugging Face HubHugging Face Local PipelinesHuggingface TextGen InferenceJSONFormerKoboldAI APILlama.cppLLM Caching integrationsManifestMinimaxModalMosaicMLNLP CloudOctoAIOllamaOpaquePromptsOpenAIOpenLLMOpenLMPetalsPipelineAIPredibasePrediction GuardPromptLayer OpenAIRELLMReplicateRunhouseSageMakerEndpointStochasticAINebula (Symbl.ai)TextGenTitan TakeoffTongyi QwenvLLMWriterXorbits Inference (Xinference)MemoryRetrieversText embedding modelsAgents & ToolkitsToolsVector storesGrouped by providerIntegrationsLLMsPromptLayer OpenAIOn this pagePromptLayer OpenAIPromptLayer is the first platform that allows you to track, manage, and share your GPT prompt engineering. PromptLayer acts a middleware between your code and OpenAI‚Äôs python library.PromptLayer records all your OpenAI API requests, allowing you to search and explore request history in the PromptLayer dashboard.This example showcases how to connect to PromptLayer to start recording your OpenAI requests.Another example is here.Install PromptLayer‚ÄãThe promptlayer package is required to use PromptLayer with OpenAI. Install promptlayer using pip.pip install promptlayerImports‚Äãimport osfrom langchain.llms import PromptLayerOpenAIimport promptlayerAPI Reference:PromptLayerOpenAISet the Environment API Key‚ÄãYou can create a PromptLayer API Key at www.promptlayer.com by clicking the settings cog in the navbar.Set it as an environment variable called PROMPTLAYER_API_KEY.You also need an OpenAI Key, called OPENAI_API_KEY.from getpass import getpassPROMPTLAYER_API_KEY = getpass()     ¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑os.environ["PROMPTLAYER_API_KEY"] = PROMPTLAYER_API_KEYfrom getpass import getpassOPENAI_API_KEY = getpass()     ¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑os.environ["OPENAI_API_KEY"] = OPENAI_API_KEYUse the PromptLayerOpenAI LLM like normal‚ÄãYou can optionally pass in pl_tags to track your requests with PromptLayer's tagging feature.llm = PromptLayerOpenAI(pl_tags=["langchain"])llm("I am a cat and I want")The above request should now appear on your PromptLayer dashboard.Using PromptLayer Track‚ÄãIf you would like to use any of the PromptLayer tracking features, you need to pass the argument return_pl_id when instantializing the PromptLayer LLM to get the request id.  llm = PromptLayerOpenAI(return_pl_id=True)llm_results = llm.generate(["Tell me a joke"])for res in llm_results.generations:    pl_request_id = res[0].generation_info["pl_request_id"]    promptlayer.track.score(request_id=pl_request_id, score=100)Using this allows you to track the performance of your model in the PromptLayer dashboard. If you are using a prompt template, you can attach a template to a request as well. Overall, this gives you the opportunity to track the performance of different templates and models in the PromptLayer dashboard.PreviousPrediction GuardNextRELLMInstall PromptLayerImportsSet the Environment API KeyUse the PromptLayerOpenAI LLM like normalUsing PromptLayer Track IntegrationsCallbacksChat modelsChat loadersDocument loadersDocument transformersLLMsAI21Aleph AlphaAmazon API GatewayAnyscaleAzure MLAzure OpenAIBananaBasetenBeamBedrockBittensorCerebriumAIChatGLMClarifaiCohereC TransformersCTranslate2DatabricksDeepInfraDeepSparseEden AIFireworksForefrontAIGoogle Vertex AI PaLMGooseAIGPT4AllHugging Face HubHugging Face Local PipelinesHuggingface TextGen InferenceJSONFormerKoboldAI APILlama.cppLLM Caching integrationsManifestMinimaxModalMosaicMLNLP CloudOctoAIOllamaOpaquePromptsOpenAIOpenLLMOpenLMPetalsPipelineAIPredibasePrediction GuardPromptLayer OpenAIRELLMReplicateRunhouseSageMakerEndpointStochasticAINebula (Symbl.ai)TextGenTitan TakeoffTongyi QwenvLLMWriterXorbits Inference (Xinference)MemoryRetrieversText embedding modelsAgents & ToolkitsToolsVector storesGrouped by provider IntegrationsCallbacksChat modelsChat loadersDocument loadersDocument transformersLLMsAI21Aleph AlphaAmazon API GatewayAnyscaleAzure MLAzure OpenAIBananaBasetenBeamBedrockBittensorCerebriumAIChatGLMClarifaiCohereC TransformersCTranslate2DatabricksDeepInfraDeepSparseEden AIFireworksForefrontAIGoogle Vertex AI PaLMGooseAIGPT4AllHugging Face HubHugging Face Local PipelinesHuggingface TextGen InferenceJSONFormerKoboldAI APILlama.cppLLM Caching integrationsManifestMinimaxModalMosaicMLNLP CloudOctoAIOllamaOpaquePromptsOpenAIOpenLLMOpenLMPetalsPipelineAIPredibasePrediction GuardPromptLayer OpenAIRELLMReplicateRunhouseSageMakerEndpointStochasticAINebula (Symbl.ai)TextGenTitan TakeoffTongyi QwenvLLMWriterXorbits Inference (Xinference)MemoryRetrieversText embedding modelsAgents & ToolkitsToolsVector storesGrouped by provider Integrations Callbacks Chat models Chat loaders Document loaders Document transformers LLMs Memory Retrievers Text embedding models Agents & Toolkits Tools Vector stores Grouped by provider IntegrationsLLMsPromptLayer OpenAIOn this pagePromptLayer OpenAIPromptLayer is the first platform that allows you to track, manage, and share your GPT prompt engineering. PromptLayer acts a middleware between your code and OpenAI‚Äôs python library.PromptLayer records all your OpenAI API requests, allowing you to search and explore request history in the PromptLayer dashboard.This example showcases how to connect to PromptLayer to start recording your OpenAI requests.Another example is here.Install PromptLayer‚ÄãThe promptlayer package is required to use PromptLayer with OpenAI. Install promptlayer using pip.pip install promptlayerImports‚Äãimport osfrom langchain.llms import PromptLayerOpenAIimport promptlayerAPI Reference:PromptLayerOpenAISet the Environment API Key‚ÄãYou can create a PromptLayer API Key at www.promptlayer.com by clicking the settings cog in the navbar.Set it as an environment variable called PROMPTLAYER_API_KEY.You also need an OpenAI Key, called OPENAI_API_KEY.from getpass import getpassPROMPTLAYER_API_KEY = getpass()     ¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑os.environ["PROMPTLAYER_API_KEY"] = PROMPTLAYER_API_KEYfrom getpass import getpassOPENAI_API_KEY = getpass()     ¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑os.environ["OPENAI_API_KEY"] = OPENAI_API_KEYUse the PromptLayerOpenAI LLM like normal‚ÄãYou can optionally pass in pl_tags to track your requests with PromptLayer's tagging feature.llm = PromptLayerOpenAI(pl_tags=["langchain"])llm("I am a cat and I want")The above request should now appear on your PromptLayer dashboard.Using PromptLayer Track‚ÄãIf you would like to use any of the PromptLayer tracking features, you need to pass the argument return_pl_id when instantializing the PromptLayer LLM to get the request id.  llm = PromptLayerOpenAI(return_pl_id=True)llm_results = llm.generate(["Tell me a joke"])for res in llm_results.generations:    pl_request_id = res[0].generation_info["pl_request_id"]    promptlayer.track.score(request_id=pl_request_id, score=100)Using this allows you to track the performance of your model in the PromptLayer dashboard. If you are using a prompt template, you can attach a template to a request as well. Overall, this gives you the opportunity to track the performance of different templates and models in the PromptLayer dashboard.PreviousPrediction GuardNextRELLMInstall PromptLayerImportsSet the Environment API KeyUse the PromptLayerOpenAI LLM like normalUsing PromptLayer Track IntegrationsLLMsPromptLayer OpenAIOn this pagePromptLayer OpenAIPromptLayer is the first platform that allows you to track, manage, and share your GPT prompt engineering. PromptLayer acts a middleware between your code and OpenAI‚Äôs python library.PromptLayer records all your OpenAI API requests, allowing you to search and explore request history in the PromptLayer dashboard.This example showcases how to connect to PromptLayer to start recording your OpenAI requests.Another example is here.Install PromptLayer‚ÄãThe promptlayer package is required to use PromptLayer with OpenAI. Install promptlayer using pip.pip install promptlayerImports‚Äãimport osfrom langchain.llms import PromptLayerOpenAIimport promptlayerAPI Reference:PromptLayerOpenAISet the Environment API Key‚ÄãYou can create a PromptLayer API Key at www.promptlayer.com by clicking the settings cog in the navbar.Set it as an environment variable called PROMPTLAYER_API_KEY.You also need an OpenAI Key, called OPENAI_API_KEY.from getpass import getpassPROMPTLAYER_API_KEY = getpass()     ¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑os.environ["PROMPTLAYER_API_KEY"] = PROMPTLAYER_API_KEYfrom getpass import getpassOPENAI_API_KEY = getpass()     ¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑os.environ["OPENAI_API_KEY"] = OPENAI_API_KEYUse the PromptLayerOpenAI LLM like normal‚ÄãYou can optionally pass in pl_tags to track your requests with PromptLayer's tagging feature.llm = PromptLayerOpenAI(pl_tags=["langchain"])llm("I am a cat and I want")The above request should now appear on your PromptLayer dashboard.Using PromptLayer Track‚ÄãIf you would like to use any of the PromptLayer tracking features, you need to pass the argument return_pl_id when instantializing the PromptLayer LLM to get the request id.  llm = PromptLayerOpenAI(return_pl_id=True)llm_results = llm.generate(["Tell me a joke"])for res in llm_results.generations:    pl_request_id = res[0].generation_info["pl_request_id"]    promptlayer.track.score(request_id=pl_request_id, score=100)Using this allows you to track the performance of your model in the PromptLayer dashboard. If you are using a prompt template, you can attach a template to a request as well. Overall, this gives you the opportunity to track the performance of different templates and models in the PromptLayer dashboard.PreviousPrediction GuardNextRELLM IntegrationsLLMsPromptLayer OpenAIOn this pagePromptLayer OpenAIPromptLayer is the first platform that allows you to track, manage, and share your GPT prompt engineering. PromptLayer acts a middleware between your code and OpenAI‚Äôs python library.PromptLayer records all your OpenAI API requests, allowing you to search and explore request history in the PromptLayer dashboard.This example showcases how to connect to PromptLayer to start recording your OpenAI requests.Another example is here.Install PromptLayer‚ÄãThe promptlayer package is required to use PromptLayer with OpenAI. Install promptlayer using pip.pip install promptlayerImports‚Äãimport osfrom langchain.llms import PromptLayerOpenAIimport promptlayerAPI Reference:PromptLayerOpenAISet the Environment API Key‚ÄãYou can create a PromptLayer API Key at www.promptlayer.com by clicking the settings cog in the navbar.Set it as an environment variable called PROMPTLAYER_API_KEY.You also need an OpenAI Key, called OPENAI_API_KEY.from getpass import getpassPROMPTLAYER_API_KEY = getpass()     ¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑os.environ["PROMPTLAYER_API_KEY"] = PROMPTLAYER_API_KEYfrom getpass import getpassOPENAI_API_KEY = getpass()     ¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑os.environ["OPENAI_API_KEY"] = OPENAI_API_KEYUse the PromptLayerOpenAI LLM like normal‚ÄãYou can optionally pass in pl_tags to track your requests with PromptLayer's tagging feature.llm = PromptLayerOpenAI(pl_tags=["langchain"])llm("I am a cat and I want")The above request should now appear on your PromptLayer dashboard.Using PromptLayer Track‚ÄãIf you would like to use any of the PromptLayer tracking features, you need to pass the argument return_pl_id when instantializing the PromptLayer LLM to get the request id.  llm = PromptLayerOpenAI(return_pl_id=True)llm_results = llm.generate(["Tell me a joke"])for res in llm_results.generations:    pl_request_id = res[0].generation_info["pl_request_id"]    promptlayer.track.score(request_id=pl_request_id, score=100)Using this allows you to track the performance of your model in the PromptLayer dashboard. If you are using a prompt template, you can attach a template to a request as well. Overall, this gives you the opportunity to track the performance of different templates and models in the PromptLayer dashboard.PreviousPrediction GuardNextRELLM On this page PromptLayer OpenAIPromptLayer is the first platform that allows you to track, manage, and share your GPT prompt engineering. PromptLayer acts a middleware between your code and OpenAI‚Äôs python library.PromptLayer records all your OpenAI API requests, allowing you to search and explore request history in the PromptLayer dashboard.This example showcases how to connect to PromptLayer to start recording your OpenAI requests.Another example is here.Install PromptLayer‚ÄãThe promptlayer package is required to use PromptLayer with OpenAI. Install promptlayer using pip.pip install promptlayerImports‚Äãimport osfrom langchain.llms import PromptLayerOpenAIimport promptlayerAPI Reference:PromptLayerOpenAISet the Environment API Key‚ÄãYou can create a PromptLayer API Key at www.promptlayer.com by clicking the settings cog in the navbar.Set it as an environment variable called PROMPTLAYER_API_KEY.You also need an OpenAI Key, called OPENAI_API_KEY.from getpass import getpassPROMPTLAYER_API_KEY = getpass()     ¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑os.environ["PROMPTLAYER_API_KEY"] = PROMPTLAYER_API_KEYfrom getpass import getpassOPENAI_API_KEY = getpass()     ¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑os.environ["OPENAI_API_KEY"] = OPENAI_API_KEYUse the PromptLayerOpenAI LLM like normal‚ÄãYou can optionally pass in pl_tags to track your requests with PromptLayer's tagging feature.llm = PromptLayerOpenAI(pl_tags=["langchain"])llm("I am a cat and I want")The above request should now appear on your PromptLayer dashboard.Using PromptLayer Track‚ÄãIf you would like to use any of the PromptLayer tracking features, you need to pass the argument return_pl_id when instantializing the PromptLayer LLM to get the request id.  llm = PromptLayerOpenAI(return_pl_id=True)llm_results = llm.generate(["Tell me a joke"])for res in llm_results.generations:    pl_request_id = res[0].generation_info["pl_request_id"]    promptlayer.track.score(request_id=pl_request_id, score=100)Using this allows you to track the performance of your model in the PromptLayer dashboard. If you are using a prompt template, you can attach a template to a request as well. Overall, this gives you the opportunity to track the performance of different templates and models in the PromptLayer dashboard. pip install promptlayer pip install promptlayer  import osfrom langchain.llms import PromptLayerOpenAIimport promptlayer import osfrom langchain.llms import PromptLayerOpenAIimport promptlayer  API Reference:PromptLayerOpenAI from getpass import getpassPROMPTLAYER_API_KEY = getpass() from getpass import getpassPROMPTLAYER_API_KEY = getpass()       ¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑      ¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑      ¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑  os.environ["PROMPTLAYER_API_KEY"] = PROMPTLAYER_API_KEY os.environ["PROMPTLAYER_API_KEY"] = PROMPTLAYER_API_KEY  from getpass import getpassOPENAI_API_KEY = getpass() from getpass import getpassOPENAI_API_KEY = getpass()       ¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑      ¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑      ¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑  os.environ["OPENAI_API_KEY"] = OPENAI_API_KEY os.environ["OPENAI_API_KEY"] = OPENAI_API_KEY  llm = PromptLayerOpenAI(pl_tags=["langchain"])llm("I am a cat and I want") llm = PromptLayerOpenAI(pl_tags=["langchain"])llm("I am a cat and I want")  llm = PromptLayerOpenAI(return_pl_id=True)llm_results = llm.generate(["Tell me a joke"])for res in llm_results.generations:    pl_request_id = res[0].generation_info["pl_request_id"]    promptlayer.track.score(request_id=pl_request_id, score=100) llm = PromptLayerOpenAI(return_pl_id=True)llm_results = llm.generate(["Tell me a joke"])for res in llm_results.generations:    pl_request_id = res[0].generation_info["pl_request_id"]    promptlayer.track.score(request_id=pl_request_id, score=100)  Previous Prediction Guard Next RELLM Install PromptLayerImportsSet the Environment API KeyUse the PromptLayerOpenAI LLM like normalUsing PromptLayer Track Install PromptLayerImportsSet the Environment API KeyUse the PromptLayerOpenAI LLM like normalUsing PromptLayer Track CommunityDiscordTwitterGitHubPythonJS/TSMoreHomepageBlogCopyright ¬© 2023 LangChain, Inc. CommunityDiscordTwitterGitHubPythonJS/TSMoreHomepageBlog CommunityDiscordTwitter Community GitHubPythonJS/TS GitHub MoreHomepageBlog More Copyright ¬© 2023 LangChain, Inc. Copyright ¬© 2023 LangChain, Inc. Skip to main content (#docusaurus_skipToContent_fallback) ü¶úÔ∏èüîó LangChain (/) Docs (/docs/get_started/introduction) Use cases (/docs/use_cases) Integrations (/docs/integrations) API (https://api.python.langchain.com) LangSmith (https://smith.langchain.com) JS/TS Docs (https://js.langchain.com/docs)  (https://github.com/hwchase17/langchain) Integrations (/docs/integrations) Callbacks (/docs/integrations/callbacks/) Chat models (/docs/integrations/chat/) Chat loaders (/docs/integrations/chat_loaders/) Document loaders (/docs/integrations/document_loaders/) Document transformers (/docs/integrations/document_transformers/) LLMs (/docs/integrations/llms/) AI21 (/docs/integrations/llms/ai21) Aleph Alpha (/docs/integrations/llms/aleph_alpha) Amazon API Gateway (/docs/integrations/llms/amazon_api_gateway) Anyscale (/docs/integrations/llms/anyscale) Azure ML (/docs/integrations/llms/azure_ml) Azure OpenAI (/docs/integrations/llms/azure_openai) Banana (/docs/integrations/llms/banana) Baseten (/docs/integrations/llms/baseten) Beam (/docs/integrations/llms/beam) Bedrock (/docs/integrations/llms/bedrock) Bittensor (/docs/integrations/llms/bittensor) CerebriumAI (/docs/integrations/llms/cerebriumai) ChatGLM (/docs/integrations/llms/chatglm) Clarifai (/docs/integrations/llms/clarifai) Cohere (/docs/integrations/llms/cohere) C Transformers (/docs/integrations/llms/ctransformers) CTranslate2 (/docs/integrations/llms/ctranslate2) Databricks (/docs/integrations/llms/databricks) DeepInfra (/docs/integrations/llms/deepinfra) DeepSparse (/docs/integrations/llms/deepsparse) Eden AI (/docs/integrations/llms/edenai) Fireworks (/docs/integrations/llms/fireworks) ForefrontAI (/docs/integrations/llms/forefrontai) Google Vertex AI PaLM (/docs/integrations/llms/google_vertex_ai_palm) GooseAI (/docs/integrations/llms/gooseai) GPT4All (/docs/integrations/llms/gpt4all) Hugging Face Hub (/docs/integrations/llms/huggingface_hub) Hugging Face Local Pipelines (/docs/integrations/llms/huggingface_pipelines) Huggingface TextGen Inference (/docs/integrations/llms/huggingface_textgen_inference) JSONFormer (/docs/integrations/llms/jsonformer_experimental) KoboldAI API (/docs/integrations/llms/koboldai) Llama.cpp (/docs/integrations/llms/llamacpp) LLM Caching integrations (/docs/integrations/llms/llm_caching) Manifest (/docs/integrations/llms/manifest) Minimax (/docs/integrations/llms/minimax) Modal (/docs/integrations/llms/modal) MosaicML (/docs/integrations/llms/mosaicml) NLP Cloud (/docs/integrations/llms/nlpcloud) OctoAI (/docs/integrations/llms/octoai) Ollama (/docs/integrations/llms/ollama) OpaquePrompts (/docs/integrations/llms/opaqueprompts) OpenAI (/docs/integrations/llms/openai) OpenLLM (/docs/integrations/llms/openllm) OpenLM (/docs/integrations/llms/openlm) Petals (/docs/integrations/llms/petals) PipelineAI (/docs/integrations/llms/pipelineai) Predibase (/docs/integrations/llms/predibase) Prediction Guard (/docs/integrations/llms/predictionguard) PromptLayer OpenAI (/docs/integrations/llms/promptlayer_openai) RELLM (/docs/integrations/llms/rellm_experimental) Replicate (/docs/integrations/llms/replicate) Runhouse (/docs/integrations/llms/runhouse) SageMakerEndpoint (/docs/integrations/llms/sagemaker) StochasticAI (/docs/integrations/llms/stochasticai) Nebula (Symbl.ai) (/docs/integrations/llms/symblai_nebula) TextGen (/docs/integrations/llms/textgen) Titan Takeoff (/docs/integrations/llms/titan_takeoff) Tongyi Qwen (/docs/integrations/llms/tongyi) vLLM (/docs/integrations/llms/vllm) Writer (/docs/integrations/llms/writer) Xorbits Inference (Xinference) (/docs/integrations/llms/xinference) Memory (/docs/integrations/memory/) Retrievers (/docs/integrations/retrievers/) Text embedding models (/docs/integrations/text_embedding/) Agents & Toolkits (/docs/integrations/toolkits/) Tools (/docs/integrations/tools/) Vector stores (/docs/integrations/vectorstores/) Grouped by provider (/docs/integrations/providers/)  (/) Integrations (/docs/integrations) LLMs (/docs/integrations/llms/) PromptLayer (https://www.promptlayer.com) here (https://python.langchain.com/en/latest/ecosystem/promptlayer.html) ‚Äã (#install-promptlayer) ‚Äã (#imports) PromptLayerOpenAI (https://api.python.langchain.com/en/latest/llms/langchain.llms.promptlayer_openai.PromptLayerOpenAI.html) ‚Äã (#set-the-environment-api-key) www.promptlayer.com (https://www.promptlayer.com) ‚Äã (#use-the-promptlayeropenai-llm-like-normal) PromptLayer dashboard (https://www.promptlayer.com) ‚Äã (#using-promptlayer-track) PromptLayer tracking features (https://magniv.notion.site/Track-4deee1b1f7a34c1680d085f82567dab9) PreviousPrediction Guard (/docs/integrations/llms/predictionguard) NextRELLM (/docs/integrations/llms/rellm_experimental) Install PromptLayer (#install-promptlayer) Imports (#imports) Set the Environment API Key (#set-the-environment-api-key) Use the PromptLayerOpenAI LLM like normal (#use-the-promptlayeropenai-llm-like-normal) Using PromptLayer Track (#using-promptlayer-track) Discord (https://discord.gg/cU2adEyC7w) Twitter (https://twitter.com/LangChainAI) Python (https://github.com/hwchase17/langchain) JS/TS (https://github.com/hwchase17/langchainjs) Homepage (https://langchain.com) Blog (https://blog.langchain.dev)