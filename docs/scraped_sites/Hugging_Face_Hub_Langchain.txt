The Hugging Face Hub is a platform with over 120k models, 20k datasets, and 50k demo apps (Spaces), all open source and publicly available, in an online platform where people can easily collaborate and build ML together. This example showcases how to connect to the Hugging Face Hub and use different models. To use, you should have the huggingface_hub python package installed. Below are some examples of models you can access through the Hugging Face Hub integration. See Databricks organization page for a list of available models. See Writer's organization page for a list of available models. See more information. See more information. See more information. Tongyi Qianwen-7B (Qwen-7B) is a model with a scale of 7 billion parameters in the Tongyi Qianwen large model series developed by Alibaba Cloud. Qwen-7B is a large language model based on Transformer, which is trained on ultra-large-scale pre-training data. See more information on HuggingFace of on GitHub. See here a big example for LangChain integration and Qwen. IntegrationsCallbacksChat modelsChat loadersDocument loadersDocument transformersLLMsAI21Aleph AlphaAmazon API GatewayAnyscaleAzure MLAzure OpenAIBananaBasetenBeamBedrockBittensorCerebriumAIChatGLMClarifaiCohereC TransformersCTranslate2DatabricksDeepInfraDeepSparseEden AIFireworksForefrontAIGoogle Vertex AI PaLMGooseAIGPT4AllHugging Face HubHugging Face Local PipelinesHuggingface TextGen InferenceJSONFormerKoboldAI APILlama.cppLLM Caching integrationsManifestMinimaxModalMosaicMLNLP CloudOctoAIOllamaOpaquePromptsOpenAIOpenLLMOpenLMPetalsPipelineAIPredibasePrediction GuardPromptLayer OpenAIRELLMReplicateRunhouseSageMakerEndpointStochasticAINebula (Symbl.ai)TextGenTitan TakeoffTongyi QwenvLLMWriterXorbits Inference (Xinference)MemoryRetrieversText embedding modelsAgents & ToolkitsToolsVector storesGrouped by provider Callbacks Chat models Chat loaders Document loaders Document transformers LLMsAI21Aleph AlphaAmazon API GatewayAnyscaleAzure MLAzure OpenAIBananaBasetenBeamBedrockBittensorCerebriumAIChatGLMClarifaiCohereC TransformersCTranslate2DatabricksDeepInfraDeepSparseEden AIFireworksForefrontAIGoogle Vertex AI PaLMGooseAIGPT4AllHugging Face HubHugging Face Local PipelinesHuggingface TextGen InferenceJSONFormerKoboldAI APILlama.cppLLM Caching integrationsManifestMinimaxModalMosaicMLNLP CloudOctoAIOllamaOpaquePromptsOpenAIOpenLLMOpenLMPetalsPipelineAIPredibasePrediction GuardPromptLayer OpenAIRELLMReplicateRunhouseSageMakerEndpointStochasticAINebula (Symbl.ai)TextGenTitan TakeoffTongyi QwenvLLMWriterXorbits Inference (Xinference) AI21 Aleph Alpha Amazon API Gateway Anyscale Azure ML Azure OpenAI Banana Baseten Beam Bedrock Bittensor CerebriumAI ChatGLM Clarifai Cohere C Transformers CTranslate2 Databricks DeepInfra DeepSparse Eden AI Fireworks ForefrontAI Google Vertex AI PaLM GooseAI GPT4All Hugging Face Hub Hugging Face Local Pipelines Huggingface TextGen Inference JSONFormer KoboldAI API Llama.cpp LLM Caching integrations Manifest Minimax Modal MosaicML NLP Cloud OctoAI Ollama OpaquePrompts OpenAI OpenLLM OpenLM Petals PipelineAI Predibase Prediction Guard PromptLayer OpenAI RELLM Replicate Runhouse SageMakerEndpoint StochasticAI Nebula (Symbl.ai) TextGen Titan Takeoff Tongyi Qwen vLLM Writer Xorbits Inference (Xinference) Memory Retrievers Text embedding models Agents & Toolkits Tools Vector stores Grouped by provider  Integrations LLMs Hugging Face Hub Installation and Setup Prepare Examples ExamplesFlan, by GoogleDolly, by DatabricksCamel, by WriterXGen, by SalesforceFalcon, by Technology Innovation Institute (TII)InternLM-Chat, by Shanghai AI LaboratoryQwen, by Alibaba Cloud Flan, by Google Dolly, by Databricks Camel, by Writer XGen, by Salesforce Falcon, by Technology Innovation Institute (TII) InternLM-Chat, by Shanghai AI Laboratory Qwen, by Alibaba Cloud Discord Twitter Python JS/TS Homepage Blog Skip to main contentü¶úÔ∏èüîó LangChainDocsUse casesIntegrationsAPILangSmithJS/TS DocsCTRLKIntegrationsCallbacksChat modelsChat loadersDocument loadersDocument transformersLLMsAI21Aleph AlphaAmazon API GatewayAnyscaleAzure MLAzure OpenAIBananaBasetenBeamBedrockBittensorCerebriumAIChatGLMClarifaiCohereC TransformersCTranslate2DatabricksDeepInfraDeepSparseEden AIFireworksForefrontAIGoogle Vertex AI PaLMGooseAIGPT4AllHugging Face HubHugging Face Local PipelinesHuggingface TextGen InferenceJSONFormerKoboldAI APILlama.cppLLM Caching integrationsManifestMinimaxModalMosaicMLNLP CloudOctoAIOllamaOpaquePromptsOpenAIOpenLLMOpenLMPetalsPipelineAIPredibasePrediction GuardPromptLayer OpenAIRELLMReplicateRunhouseSageMakerEndpointStochasticAINebula (Symbl.ai)TextGenTitan TakeoffTongyi QwenvLLMWriterXorbits Inference (Xinference)MemoryRetrieversText embedding modelsAgents & ToolkitsToolsVector storesGrouped by providerIntegrationsLLMsHugging Face HubOn this pageHugging Face HubThe Hugging Face Hub is a platform with over 120k models, 20k datasets, and 50k demo apps (Spaces), all open source and publicly available, in an online platform where people can easily collaborate and build ML together.This example showcases how to connect to the Hugging Face Hub and use different models.Installation and Setup‚ÄãTo use, you should have the huggingface_hub python package installed.pip install huggingface_hub# get a token: https://huggingface.co/docs/api-inference/quicktour#get-your-api-tokenfrom getpass import getpassHUGGINGFACEHUB_API_TOKEN = getpass()     ¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑import osos.environ["HUGGINGFACEHUB_API_TOKEN"] = HUGGINGFACEHUB_API_TOKENPrepare Examples‚Äãfrom langchain import HuggingFaceHubfrom langchain import PromptTemplate, LLMChainquestion = "Who won the FIFA World Cup in the year 1994? "template = """Question: {question}Answer: Let's think step by step."""prompt = PromptTemplate(template=template, input_variables=["question"])Examples‚ÄãBelow are some examples of models you can access through the Hugging Face Hub integration.Flan, by Google‚Äãrepo_id = "google/flan-t5-xxl"  # See https://huggingface.co/models?pipeline_tag=text-generation&sort=downloads for some other optionsllm = HuggingFaceHub(    repo_id=repo_id, model_kwargs={"temperature": 0.5, "max_length": 64})llm_chain = LLMChain(prompt=prompt, llm=llm)print(llm_chain.run(question))    The FIFA World Cup was held in the year 1994. West Germany won the FIFA World Cup in 1994Dolly, by Databricks‚ÄãSee Databricks organization page for a list of available models.repo_id = "databricks/dolly-v2-3b"llm = HuggingFaceHub(    repo_id=repo_id, model_kwargs={"temperature": 0.5, "max_length": 64})llm_chain = LLMChain(prompt=prompt, llm=llm)print(llm_chain.run(question))     First of all, the world cup was won by the Germany. Then the Argentina won the world cup in 2022. So, the Argentina won the world cup in 1994.            Question: WhoCamel, by Writer‚ÄãSee Writer's organization page for a list of available models.repo_id = "Writer/camel-5b-hf"  # See https://huggingface.co/Writer for other optionsllm = HuggingFaceHub(    repo_id=repo_id, model_kwargs={"temperature": 0.5, "max_length": 64})llm_chain = LLMChain(prompt=prompt, llm=llm)print(llm_chain.run(question))XGen, by Salesforce‚ÄãSee more information.repo_id = "Salesforce/xgen-7b-8k-base"llm = HuggingFaceHub(    repo_id=repo_id, model_kwargs={"temperature": 0.5, "max_length": 64})llm_chain = LLMChain(prompt=prompt, llm=llm)print(llm_chain.run(question))Falcon, by Technology Innovation Institute (TII)‚ÄãSee more information.repo_id = "tiiuae/falcon-40b"llm = HuggingFaceHub(    repo_id=repo_id, model_kwargs={"temperature": 0.5, "max_length": 64})llm_chain = LLMChain(prompt=prompt, llm=llm)print(llm_chain.run(question))InternLM-Chat, by Shanghai AI Laboratory‚ÄãSee more information.repo_id = "internlm/internlm-chat-7b"llm = HuggingFaceHub(    repo_id=repo_id, model_kwargs={"max_length": 128, "temperature": 0.8})llm_chain = LLMChain(prompt=prompt, llm=llm)print(llm_chain.run(question))Qwen, by Alibaba Cloud‚ÄãTongyi Qianwen-7B (Qwen-7B) is a model with a scale of 7 billion parameters in the Tongyi Qianwen large model series developed by Alibaba Cloud. Qwen-7B is a large language model based on Transformer, which is trained on ultra-large-scale pre-training data.See more information on HuggingFace of on GitHub.See here a big example for LangChain integration and Qwen.repo_id = "Qwen/Qwen-7B"llm = HuggingFaceHub(    repo_id=repo_id, model_kwargs={"max_length": 128, "temperature": 0.5})llm_chain = LLMChain(prompt=prompt, llm=llm)print(llm_chain.run(question))PreviousGPT4AllNextHugging Face Local PipelinesInstallation and SetupPrepare ExamplesExamplesFlan, by GoogleDolly, by DatabricksCamel, by WriterXGen, by SalesforceFalcon, by Technology Innovation Institute (TII)InternLM-Chat, by Shanghai AI LaboratoryQwen, by Alibaba CloudCommunityDiscordTwitterGitHubPythonJS/TSMoreHomepageBlogCopyright ¬© 2023 LangChain, Inc. Skip to main content ü¶úÔ∏èüîó LangChainDocsUse casesIntegrationsAPILangSmithJS/TS DocsCTRLK ü¶úÔ∏èüîó LangChainDocsUse casesIntegrationsAPI LangSmithJS/TS DocsCTRLK  CTRLK CTRLK  CTRLK   IntegrationsCallbacksChat modelsChat loadersDocument loadersDocument transformersLLMsAI21Aleph AlphaAmazon API GatewayAnyscaleAzure MLAzure OpenAIBananaBasetenBeamBedrockBittensorCerebriumAIChatGLMClarifaiCohereC TransformersCTranslate2DatabricksDeepInfraDeepSparseEden AIFireworksForefrontAIGoogle Vertex AI PaLMGooseAIGPT4AllHugging Face HubHugging Face Local PipelinesHuggingface TextGen InferenceJSONFormerKoboldAI APILlama.cppLLM Caching integrationsManifestMinimaxModalMosaicMLNLP CloudOctoAIOllamaOpaquePromptsOpenAIOpenLLMOpenLMPetalsPipelineAIPredibasePrediction GuardPromptLayer OpenAIRELLMReplicateRunhouseSageMakerEndpointStochasticAINebula (Symbl.ai)TextGenTitan TakeoffTongyi QwenvLLMWriterXorbits Inference (Xinference)MemoryRetrieversText embedding modelsAgents & ToolkitsToolsVector storesGrouped by providerIntegrationsLLMsHugging Face HubOn this pageHugging Face HubThe Hugging Face Hub is a platform with over 120k models, 20k datasets, and 50k demo apps (Spaces), all open source and publicly available, in an online platform where people can easily collaborate and build ML together.This example showcases how to connect to the Hugging Face Hub and use different models.Installation and Setup‚ÄãTo use, you should have the huggingface_hub python package installed.pip install huggingface_hub# get a token: https://huggingface.co/docs/api-inference/quicktour#get-your-api-tokenfrom getpass import getpassHUGGINGFACEHUB_API_TOKEN = getpass()     ¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑import osos.environ["HUGGINGFACEHUB_API_TOKEN"] = HUGGINGFACEHUB_API_TOKENPrepare Examples‚Äãfrom langchain import HuggingFaceHubfrom langchain import PromptTemplate, LLMChainquestion = "Who won the FIFA World Cup in the year 1994? "template = """Question: {question}Answer: Let's think step by step."""prompt = PromptTemplate(template=template, input_variables=["question"])Examples‚ÄãBelow are some examples of models you can access through the Hugging Face Hub integration.Flan, by Google‚Äãrepo_id = "google/flan-t5-xxl"  # See https://huggingface.co/models?pipeline_tag=text-generation&sort=downloads for some other optionsllm = HuggingFaceHub(    repo_id=repo_id, model_kwargs={"temperature": 0.5, "max_length": 64})llm_chain = LLMChain(prompt=prompt, llm=llm)print(llm_chain.run(question))    The FIFA World Cup was held in the year 1994. West Germany won the FIFA World Cup in 1994Dolly, by Databricks‚ÄãSee Databricks organization page for a list of available models.repo_id = "databricks/dolly-v2-3b"llm = HuggingFaceHub(    repo_id=repo_id, model_kwargs={"temperature": 0.5, "max_length": 64})llm_chain = LLMChain(prompt=prompt, llm=llm)print(llm_chain.run(question))     First of all, the world cup was won by the Germany. Then the Argentina won the world cup in 2022. So, the Argentina won the world cup in 1994.            Question: WhoCamel, by Writer‚ÄãSee Writer's organization page for a list of available models.repo_id = "Writer/camel-5b-hf"  # See https://huggingface.co/Writer for other optionsllm = HuggingFaceHub(    repo_id=repo_id, model_kwargs={"temperature": 0.5, "max_length": 64})llm_chain = LLMChain(prompt=prompt, llm=llm)print(llm_chain.run(question))XGen, by Salesforce‚ÄãSee more information.repo_id = "Salesforce/xgen-7b-8k-base"llm = HuggingFaceHub(    repo_id=repo_id, model_kwargs={"temperature": 0.5, "max_length": 64})llm_chain = LLMChain(prompt=prompt, llm=llm)print(llm_chain.run(question))Falcon, by Technology Innovation Institute (TII)‚ÄãSee more information.repo_id = "tiiuae/falcon-40b"llm = HuggingFaceHub(    repo_id=repo_id, model_kwargs={"temperature": 0.5, "max_length": 64})llm_chain = LLMChain(prompt=prompt, llm=llm)print(llm_chain.run(question))InternLM-Chat, by Shanghai AI Laboratory‚ÄãSee more information.repo_id = "internlm/internlm-chat-7b"llm = HuggingFaceHub(    repo_id=repo_id, model_kwargs={"max_length": 128, "temperature": 0.8})llm_chain = LLMChain(prompt=prompt, llm=llm)print(llm_chain.run(question))Qwen, by Alibaba Cloud‚ÄãTongyi Qianwen-7B (Qwen-7B) is a model with a scale of 7 billion parameters in the Tongyi Qianwen large model series developed by Alibaba Cloud. Qwen-7B is a large language model based on Transformer, which is trained on ultra-large-scale pre-training data.See more information on HuggingFace of on GitHub.See here a big example for LangChain integration and Qwen.repo_id = "Qwen/Qwen-7B"llm = HuggingFaceHub(    repo_id=repo_id, model_kwargs={"max_length": 128, "temperature": 0.5})llm_chain = LLMChain(prompt=prompt, llm=llm)print(llm_chain.run(question))PreviousGPT4AllNextHugging Face Local PipelinesInstallation and SetupPrepare ExamplesExamplesFlan, by GoogleDolly, by DatabricksCamel, by WriterXGen, by SalesforceFalcon, by Technology Innovation Institute (TII)InternLM-Chat, by Shanghai AI LaboratoryQwen, by Alibaba Cloud IntegrationsCallbacksChat modelsChat loadersDocument loadersDocument transformersLLMsAI21Aleph AlphaAmazon API GatewayAnyscaleAzure MLAzure OpenAIBananaBasetenBeamBedrockBittensorCerebriumAIChatGLMClarifaiCohereC TransformersCTranslate2DatabricksDeepInfraDeepSparseEden AIFireworksForefrontAIGoogle Vertex AI PaLMGooseAIGPT4AllHugging Face HubHugging Face Local PipelinesHuggingface TextGen InferenceJSONFormerKoboldAI APILlama.cppLLM Caching integrationsManifestMinimaxModalMosaicMLNLP CloudOctoAIOllamaOpaquePromptsOpenAIOpenLLMOpenLMPetalsPipelineAIPredibasePrediction GuardPromptLayer OpenAIRELLMReplicateRunhouseSageMakerEndpointStochasticAINebula (Symbl.ai)TextGenTitan TakeoffTongyi QwenvLLMWriterXorbits Inference (Xinference)MemoryRetrieversText embedding modelsAgents & ToolkitsToolsVector storesGrouped by providerIntegrationsLLMsHugging Face HubOn this pageHugging Face HubThe Hugging Face Hub is a platform with over 120k models, 20k datasets, and 50k demo apps (Spaces), all open source and publicly available, in an online platform where people can easily collaborate and build ML together.This example showcases how to connect to the Hugging Face Hub and use different models.Installation and Setup‚ÄãTo use, you should have the huggingface_hub python package installed.pip install huggingface_hub# get a token: https://huggingface.co/docs/api-inference/quicktour#get-your-api-tokenfrom getpass import getpassHUGGINGFACEHUB_API_TOKEN = getpass()     ¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑import osos.environ["HUGGINGFACEHUB_API_TOKEN"] = HUGGINGFACEHUB_API_TOKENPrepare Examples‚Äãfrom langchain import HuggingFaceHubfrom langchain import PromptTemplate, LLMChainquestion = "Who won the FIFA World Cup in the year 1994? "template = """Question: {question}Answer: Let's think step by step."""prompt = PromptTemplate(template=template, input_variables=["question"])Examples‚ÄãBelow are some examples of models you can access through the Hugging Face Hub integration.Flan, by Google‚Äãrepo_id = "google/flan-t5-xxl"  # See https://huggingface.co/models?pipeline_tag=text-generation&sort=downloads for some other optionsllm = HuggingFaceHub(    repo_id=repo_id, model_kwargs={"temperature": 0.5, "max_length": 64})llm_chain = LLMChain(prompt=prompt, llm=llm)print(llm_chain.run(question))    The FIFA World Cup was held in the year 1994. West Germany won the FIFA World Cup in 1994Dolly, by Databricks‚ÄãSee Databricks organization page for a list of available models.repo_id = "databricks/dolly-v2-3b"llm = HuggingFaceHub(    repo_id=repo_id, model_kwargs={"temperature": 0.5, "max_length": 64})llm_chain = LLMChain(prompt=prompt, llm=llm)print(llm_chain.run(question))     First of all, the world cup was won by the Germany. Then the Argentina won the world cup in 2022. So, the Argentina won the world cup in 1994.            Question: WhoCamel, by Writer‚ÄãSee Writer's organization page for a list of available models.repo_id = "Writer/camel-5b-hf"  # See https://huggingface.co/Writer for other optionsllm = HuggingFaceHub(    repo_id=repo_id, model_kwargs={"temperature": 0.5, "max_length": 64})llm_chain = LLMChain(prompt=prompt, llm=llm)print(llm_chain.run(question))XGen, by Salesforce‚ÄãSee more information.repo_id = "Salesforce/xgen-7b-8k-base"llm = HuggingFaceHub(    repo_id=repo_id, model_kwargs={"temperature": 0.5, "max_length": 64})llm_chain = LLMChain(prompt=prompt, llm=llm)print(llm_chain.run(question))Falcon, by Technology Innovation Institute (TII)‚ÄãSee more information.repo_id = "tiiuae/falcon-40b"llm = HuggingFaceHub(    repo_id=repo_id, model_kwargs={"temperature": 0.5, "max_length": 64})llm_chain = LLMChain(prompt=prompt, llm=llm)print(llm_chain.run(question))InternLM-Chat, by Shanghai AI Laboratory‚ÄãSee more information.repo_id = "internlm/internlm-chat-7b"llm = HuggingFaceHub(    repo_id=repo_id, model_kwargs={"max_length": 128, "temperature": 0.8})llm_chain = LLMChain(prompt=prompt, llm=llm)print(llm_chain.run(question))Qwen, by Alibaba Cloud‚ÄãTongyi Qianwen-7B (Qwen-7B) is a model with a scale of 7 billion parameters in the Tongyi Qianwen large model series developed by Alibaba Cloud. Qwen-7B is a large language model based on Transformer, which is trained on ultra-large-scale pre-training data.See more information on HuggingFace of on GitHub.See here a big example for LangChain integration and Qwen.repo_id = "Qwen/Qwen-7B"llm = HuggingFaceHub(    repo_id=repo_id, model_kwargs={"max_length": 128, "temperature": 0.5})llm_chain = LLMChain(prompt=prompt, llm=llm)print(llm_chain.run(question))PreviousGPT4AllNextHugging Face Local PipelinesInstallation and SetupPrepare ExamplesExamplesFlan, by GoogleDolly, by DatabricksCamel, by WriterXGen, by SalesforceFalcon, by Technology Innovation Institute (TII)InternLM-Chat, by Shanghai AI LaboratoryQwen, by Alibaba Cloud IntegrationsCallbacksChat modelsChat loadersDocument loadersDocument transformersLLMsAI21Aleph AlphaAmazon API GatewayAnyscaleAzure MLAzure OpenAIBananaBasetenBeamBedrockBittensorCerebriumAIChatGLMClarifaiCohereC TransformersCTranslate2DatabricksDeepInfraDeepSparseEden AIFireworksForefrontAIGoogle Vertex AI PaLMGooseAIGPT4AllHugging Face HubHugging Face Local PipelinesHuggingface TextGen InferenceJSONFormerKoboldAI APILlama.cppLLM Caching integrationsManifestMinimaxModalMosaicMLNLP CloudOctoAIOllamaOpaquePromptsOpenAIOpenLLMOpenLMPetalsPipelineAIPredibasePrediction GuardPromptLayer OpenAIRELLMReplicateRunhouseSageMakerEndpointStochasticAINebula (Symbl.ai)TextGenTitan TakeoffTongyi QwenvLLMWriterXorbits Inference (Xinference)MemoryRetrieversText embedding modelsAgents & ToolkitsToolsVector storesGrouped by provider IntegrationsCallbacksChat modelsChat loadersDocument loadersDocument transformersLLMsAI21Aleph AlphaAmazon API GatewayAnyscaleAzure MLAzure OpenAIBananaBasetenBeamBedrockBittensorCerebriumAIChatGLMClarifaiCohereC TransformersCTranslate2DatabricksDeepInfraDeepSparseEden AIFireworksForefrontAIGoogle Vertex AI PaLMGooseAIGPT4AllHugging Face HubHugging Face Local PipelinesHuggingface TextGen InferenceJSONFormerKoboldAI APILlama.cppLLM Caching integrationsManifestMinimaxModalMosaicMLNLP CloudOctoAIOllamaOpaquePromptsOpenAIOpenLLMOpenLMPetalsPipelineAIPredibasePrediction GuardPromptLayer OpenAIRELLMReplicateRunhouseSageMakerEndpointStochasticAINebula (Symbl.ai)TextGenTitan TakeoffTongyi QwenvLLMWriterXorbits Inference (Xinference)MemoryRetrieversText embedding modelsAgents & ToolkitsToolsVector storesGrouped by provider Integrations Callbacks Chat models Chat loaders Document loaders Document transformers LLMs Memory Retrievers Text embedding models Agents & Toolkits Tools Vector stores Grouped by provider IntegrationsLLMsHugging Face HubOn this pageHugging Face HubThe Hugging Face Hub is a platform with over 120k models, 20k datasets, and 50k demo apps (Spaces), all open source and publicly available, in an online platform where people can easily collaborate and build ML together.This example showcases how to connect to the Hugging Face Hub and use different models.Installation and Setup‚ÄãTo use, you should have the huggingface_hub python package installed.pip install huggingface_hub# get a token: https://huggingface.co/docs/api-inference/quicktour#get-your-api-tokenfrom getpass import getpassHUGGINGFACEHUB_API_TOKEN = getpass()     ¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑import osos.environ["HUGGINGFACEHUB_API_TOKEN"] = HUGGINGFACEHUB_API_TOKENPrepare Examples‚Äãfrom langchain import HuggingFaceHubfrom langchain import PromptTemplate, LLMChainquestion = "Who won the FIFA World Cup in the year 1994? "template = """Question: {question}Answer: Let's think step by step."""prompt = PromptTemplate(template=template, input_variables=["question"])Examples‚ÄãBelow are some examples of models you can access through the Hugging Face Hub integration.Flan, by Google‚Äãrepo_id = "google/flan-t5-xxl"  # See https://huggingface.co/models?pipeline_tag=text-generation&sort=downloads for some other optionsllm = HuggingFaceHub(    repo_id=repo_id, model_kwargs={"temperature": 0.5, "max_length": 64})llm_chain = LLMChain(prompt=prompt, llm=llm)print(llm_chain.run(question))    The FIFA World Cup was held in the year 1994. West Germany won the FIFA World Cup in 1994Dolly, by Databricks‚ÄãSee Databricks organization page for a list of available models.repo_id = "databricks/dolly-v2-3b"llm = HuggingFaceHub(    repo_id=repo_id, model_kwargs={"temperature": 0.5, "max_length": 64})llm_chain = LLMChain(prompt=prompt, llm=llm)print(llm_chain.run(question))     First of all, the world cup was won by the Germany. Then the Argentina won the world cup in 2022. So, the Argentina won the world cup in 1994.            Question: WhoCamel, by Writer‚ÄãSee Writer's organization page for a list of available models.repo_id = "Writer/camel-5b-hf"  # See https://huggingface.co/Writer for other optionsllm = HuggingFaceHub(    repo_id=repo_id, model_kwargs={"temperature": 0.5, "max_length": 64})llm_chain = LLMChain(prompt=prompt, llm=llm)print(llm_chain.run(question))XGen, by Salesforce‚ÄãSee more information.repo_id = "Salesforce/xgen-7b-8k-base"llm = HuggingFaceHub(    repo_id=repo_id, model_kwargs={"temperature": 0.5, "max_length": 64})llm_chain = LLMChain(prompt=prompt, llm=llm)print(llm_chain.run(question))Falcon, by Technology Innovation Institute (TII)‚ÄãSee more information.repo_id = "tiiuae/falcon-40b"llm = HuggingFaceHub(    repo_id=repo_id, model_kwargs={"temperature": 0.5, "max_length": 64})llm_chain = LLMChain(prompt=prompt, llm=llm)print(llm_chain.run(question))InternLM-Chat, by Shanghai AI Laboratory‚ÄãSee more information.repo_id = "internlm/internlm-chat-7b"llm = HuggingFaceHub(    repo_id=repo_id, model_kwargs={"max_length": 128, "temperature": 0.8})llm_chain = LLMChain(prompt=prompt, llm=llm)print(llm_chain.run(question))Qwen, by Alibaba Cloud‚ÄãTongyi Qianwen-7B (Qwen-7B) is a model with a scale of 7 billion parameters in the Tongyi Qianwen large model series developed by Alibaba Cloud. Qwen-7B is a large language model based on Transformer, which is trained on ultra-large-scale pre-training data.See more information on HuggingFace of on GitHub.See here a big example for LangChain integration and Qwen.repo_id = "Qwen/Qwen-7B"llm = HuggingFaceHub(    repo_id=repo_id, model_kwargs={"max_length": 128, "temperature": 0.5})llm_chain = LLMChain(prompt=prompt, llm=llm)print(llm_chain.run(question))PreviousGPT4AllNextHugging Face Local PipelinesInstallation and SetupPrepare ExamplesExamplesFlan, by GoogleDolly, by DatabricksCamel, by WriterXGen, by SalesforceFalcon, by Technology Innovation Institute (TII)InternLM-Chat, by Shanghai AI LaboratoryQwen, by Alibaba Cloud IntegrationsLLMsHugging Face HubOn this pageHugging Face HubThe Hugging Face Hub is a platform with over 120k models, 20k datasets, and 50k demo apps (Spaces), all open source and publicly available, in an online platform where people can easily collaborate and build ML together.This example showcases how to connect to the Hugging Face Hub and use different models.Installation and Setup‚ÄãTo use, you should have the huggingface_hub python package installed.pip install huggingface_hub# get a token: https://huggingface.co/docs/api-inference/quicktour#get-your-api-tokenfrom getpass import getpassHUGGINGFACEHUB_API_TOKEN = getpass()     ¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑import osos.environ["HUGGINGFACEHUB_API_TOKEN"] = HUGGINGFACEHUB_API_TOKENPrepare Examples‚Äãfrom langchain import HuggingFaceHubfrom langchain import PromptTemplate, LLMChainquestion = "Who won the FIFA World Cup in the year 1994? "template = """Question: {question}Answer: Let's think step by step."""prompt = PromptTemplate(template=template, input_variables=["question"])Examples‚ÄãBelow are some examples of models you can access through the Hugging Face Hub integration.Flan, by Google‚Äãrepo_id = "google/flan-t5-xxl"  # See https://huggingface.co/models?pipeline_tag=text-generation&sort=downloads for some other optionsllm = HuggingFaceHub(    repo_id=repo_id, model_kwargs={"temperature": 0.5, "max_length": 64})llm_chain = LLMChain(prompt=prompt, llm=llm)print(llm_chain.run(question))    The FIFA World Cup was held in the year 1994. West Germany won the FIFA World Cup in 1994Dolly, by Databricks‚ÄãSee Databricks organization page for a list of available models.repo_id = "databricks/dolly-v2-3b"llm = HuggingFaceHub(    repo_id=repo_id, model_kwargs={"temperature": 0.5, "max_length": 64})llm_chain = LLMChain(prompt=prompt, llm=llm)print(llm_chain.run(question))     First of all, the world cup was won by the Germany. Then the Argentina won the world cup in 2022. So, the Argentina won the world cup in 1994.            Question: WhoCamel, by Writer‚ÄãSee Writer's organization page for a list of available models.repo_id = "Writer/camel-5b-hf"  # See https://huggingface.co/Writer for other optionsllm = HuggingFaceHub(    repo_id=repo_id, model_kwargs={"temperature": 0.5, "max_length": 64})llm_chain = LLMChain(prompt=prompt, llm=llm)print(llm_chain.run(question))XGen, by Salesforce‚ÄãSee more information.repo_id = "Salesforce/xgen-7b-8k-base"llm = HuggingFaceHub(    repo_id=repo_id, model_kwargs={"temperature": 0.5, "max_length": 64})llm_chain = LLMChain(prompt=prompt, llm=llm)print(llm_chain.run(question))Falcon, by Technology Innovation Institute (TII)‚ÄãSee more information.repo_id = "tiiuae/falcon-40b"llm = HuggingFaceHub(    repo_id=repo_id, model_kwargs={"temperature": 0.5, "max_length": 64})llm_chain = LLMChain(prompt=prompt, llm=llm)print(llm_chain.run(question))InternLM-Chat, by Shanghai AI Laboratory‚ÄãSee more information.repo_id = "internlm/internlm-chat-7b"llm = HuggingFaceHub(    repo_id=repo_id, model_kwargs={"max_length": 128, "temperature": 0.8})llm_chain = LLMChain(prompt=prompt, llm=llm)print(llm_chain.run(question))Qwen, by Alibaba Cloud‚ÄãTongyi Qianwen-7B (Qwen-7B) is a model with a scale of 7 billion parameters in the Tongyi Qianwen large model series developed by Alibaba Cloud. Qwen-7B is a large language model based on Transformer, which is trained on ultra-large-scale pre-training data.See more information on HuggingFace of on GitHub.See here a big example for LangChain integration and Qwen.repo_id = "Qwen/Qwen-7B"llm = HuggingFaceHub(    repo_id=repo_id, model_kwargs={"max_length": 128, "temperature": 0.5})llm_chain = LLMChain(prompt=prompt, llm=llm)print(llm_chain.run(question))PreviousGPT4AllNextHugging Face Local PipelinesInstallation and SetupPrepare ExamplesExamplesFlan, by GoogleDolly, by DatabricksCamel, by WriterXGen, by SalesforceFalcon, by Technology Innovation Institute (TII)InternLM-Chat, by Shanghai AI LaboratoryQwen, by Alibaba Cloud IntegrationsLLMsHugging Face HubOn this pageHugging Face HubThe Hugging Face Hub is a platform with over 120k models, 20k datasets, and 50k demo apps (Spaces), all open source and publicly available, in an online platform where people can easily collaborate and build ML together.This example showcases how to connect to the Hugging Face Hub and use different models.Installation and Setup‚ÄãTo use, you should have the huggingface_hub python package installed.pip install huggingface_hub# get a token: https://huggingface.co/docs/api-inference/quicktour#get-your-api-tokenfrom getpass import getpassHUGGINGFACEHUB_API_TOKEN = getpass()     ¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑import osos.environ["HUGGINGFACEHUB_API_TOKEN"] = HUGGINGFACEHUB_API_TOKENPrepare Examples‚Äãfrom langchain import HuggingFaceHubfrom langchain import PromptTemplate, LLMChainquestion = "Who won the FIFA World Cup in the year 1994? "template = """Question: {question}Answer: Let's think step by step."""prompt = PromptTemplate(template=template, input_variables=["question"])Examples‚ÄãBelow are some examples of models you can access through the Hugging Face Hub integration.Flan, by Google‚Äãrepo_id = "google/flan-t5-xxl"  # See https://huggingface.co/models?pipeline_tag=text-generation&sort=downloads for some other optionsllm = HuggingFaceHub(    repo_id=repo_id, model_kwargs={"temperature": 0.5, "max_length": 64})llm_chain = LLMChain(prompt=prompt, llm=llm)print(llm_chain.run(question))    The FIFA World Cup was held in the year 1994. West Germany won the FIFA World Cup in 1994Dolly, by Databricks‚ÄãSee Databricks organization page for a list of available models.repo_id = "databricks/dolly-v2-3b"llm = HuggingFaceHub(    repo_id=repo_id, model_kwargs={"temperature": 0.5, "max_length": 64})llm_chain = LLMChain(prompt=prompt, llm=llm)print(llm_chain.run(question))     First of all, the world cup was won by the Germany. Then the Argentina won the world cup in 2022. So, the Argentina won the world cup in 1994.            Question: WhoCamel, by Writer‚ÄãSee Writer's organization page for a list of available models.repo_id = "Writer/camel-5b-hf"  # See https://huggingface.co/Writer for other optionsllm = HuggingFaceHub(    repo_id=repo_id, model_kwargs={"temperature": 0.5, "max_length": 64})llm_chain = LLMChain(prompt=prompt, llm=llm)print(llm_chain.run(question))XGen, by Salesforce‚ÄãSee more information.repo_id = "Salesforce/xgen-7b-8k-base"llm = HuggingFaceHub(    repo_id=repo_id, model_kwargs={"temperature": 0.5, "max_length": 64})llm_chain = LLMChain(prompt=prompt, llm=llm)print(llm_chain.run(question))Falcon, by Technology Innovation Institute (TII)‚ÄãSee more information.repo_id = "tiiuae/falcon-40b"llm = HuggingFaceHub(    repo_id=repo_id, model_kwargs={"temperature": 0.5, "max_length": 64})llm_chain = LLMChain(prompt=prompt, llm=llm)print(llm_chain.run(question))InternLM-Chat, by Shanghai AI Laboratory‚ÄãSee more information.repo_id = "internlm/internlm-chat-7b"llm = HuggingFaceHub(    repo_id=repo_id, model_kwargs={"max_length": 128, "temperature": 0.8})llm_chain = LLMChain(prompt=prompt, llm=llm)print(llm_chain.run(question))Qwen, by Alibaba Cloud‚ÄãTongyi Qianwen-7B (Qwen-7B) is a model with a scale of 7 billion parameters in the Tongyi Qianwen large model series developed by Alibaba Cloud. Qwen-7B is a large language model based on Transformer, which is trained on ultra-large-scale pre-training data.See more information on HuggingFace of on GitHub.See here a big example for LangChain integration and Qwen.repo_id = "Qwen/Qwen-7B"llm = HuggingFaceHub(    repo_id=repo_id, model_kwargs={"max_length": 128, "temperature": 0.5})llm_chain = LLMChain(prompt=prompt, llm=llm)print(llm_chain.run(question))PreviousGPT4AllNextHugging Face Local Pipelines IntegrationsLLMsHugging Face HubOn this pageHugging Face HubThe Hugging Face Hub is a platform with over 120k models, 20k datasets, and 50k demo apps (Spaces), all open source and publicly available, in an online platform where people can easily collaborate and build ML together.This example showcases how to connect to the Hugging Face Hub and use different models.Installation and Setup‚ÄãTo use, you should have the huggingface_hub python package installed.pip install huggingface_hub# get a token: https://huggingface.co/docs/api-inference/quicktour#get-your-api-tokenfrom getpass import getpassHUGGINGFACEHUB_API_TOKEN = getpass()     ¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑import osos.environ["HUGGINGFACEHUB_API_TOKEN"] = HUGGINGFACEHUB_API_TOKENPrepare Examples‚Äãfrom langchain import HuggingFaceHubfrom langchain import PromptTemplate, LLMChainquestion = "Who won the FIFA World Cup in the year 1994? "template = """Question: {question}Answer: Let's think step by step."""prompt = PromptTemplate(template=template, input_variables=["question"])Examples‚ÄãBelow are some examples of models you can access through the Hugging Face Hub integration.Flan, by Google‚Äãrepo_id = "google/flan-t5-xxl"  # See https://huggingface.co/models?pipeline_tag=text-generation&sort=downloads for some other optionsllm = HuggingFaceHub(    repo_id=repo_id, model_kwargs={"temperature": 0.5, "max_length": 64})llm_chain = LLMChain(prompt=prompt, llm=llm)print(llm_chain.run(question))    The FIFA World Cup was held in the year 1994. West Germany won the FIFA World Cup in 1994Dolly, by Databricks‚ÄãSee Databricks organization page for a list of available models.repo_id = "databricks/dolly-v2-3b"llm = HuggingFaceHub(    repo_id=repo_id, model_kwargs={"temperature": 0.5, "max_length": 64})llm_chain = LLMChain(prompt=prompt, llm=llm)print(llm_chain.run(question))     First of all, the world cup was won by the Germany. Then the Argentina won the world cup in 2022. So, the Argentina won the world cup in 1994.            Question: WhoCamel, by Writer‚ÄãSee Writer's organization page for a list of available models.repo_id = "Writer/camel-5b-hf"  # See https://huggingface.co/Writer for other optionsllm = HuggingFaceHub(    repo_id=repo_id, model_kwargs={"temperature": 0.5, "max_length": 64})llm_chain = LLMChain(prompt=prompt, llm=llm)print(llm_chain.run(question))XGen, by Salesforce‚ÄãSee more information.repo_id = "Salesforce/xgen-7b-8k-base"llm = HuggingFaceHub(    repo_id=repo_id, model_kwargs={"temperature": 0.5, "max_length": 64})llm_chain = LLMChain(prompt=prompt, llm=llm)print(llm_chain.run(question))Falcon, by Technology Innovation Institute (TII)‚ÄãSee more information.repo_id = "tiiuae/falcon-40b"llm = HuggingFaceHub(    repo_id=repo_id, model_kwargs={"temperature": 0.5, "max_length": 64})llm_chain = LLMChain(prompt=prompt, llm=llm)print(llm_chain.run(question))InternLM-Chat, by Shanghai AI Laboratory‚ÄãSee more information.repo_id = "internlm/internlm-chat-7b"llm = HuggingFaceHub(    repo_id=repo_id, model_kwargs={"max_length": 128, "temperature": 0.8})llm_chain = LLMChain(prompt=prompt, llm=llm)print(llm_chain.run(question))Qwen, by Alibaba Cloud‚ÄãTongyi Qianwen-7B (Qwen-7B) is a model with a scale of 7 billion parameters in the Tongyi Qianwen large model series developed by Alibaba Cloud. Qwen-7B is a large language model based on Transformer, which is trained on ultra-large-scale pre-training data.See more information on HuggingFace of on GitHub.See here a big example for LangChain integration and Qwen.repo_id = "Qwen/Qwen-7B"llm = HuggingFaceHub(    repo_id=repo_id, model_kwargs={"max_length": 128, "temperature": 0.5})llm_chain = LLMChain(prompt=prompt, llm=llm)print(llm_chain.run(question))PreviousGPT4AllNextHugging Face Local Pipelines On this page Hugging Face HubThe Hugging Face Hub is a platform with over 120k models, 20k datasets, and 50k demo apps (Spaces), all open source and publicly available, in an online platform where people can easily collaborate and build ML together.This example showcases how to connect to the Hugging Face Hub and use different models.Installation and Setup‚ÄãTo use, you should have the huggingface_hub python package installed.pip install huggingface_hub# get a token: https://huggingface.co/docs/api-inference/quicktour#get-your-api-tokenfrom getpass import getpassHUGGINGFACEHUB_API_TOKEN = getpass()     ¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑import osos.environ["HUGGINGFACEHUB_API_TOKEN"] = HUGGINGFACEHUB_API_TOKENPrepare Examples‚Äãfrom langchain import HuggingFaceHubfrom langchain import PromptTemplate, LLMChainquestion = "Who won the FIFA World Cup in the year 1994? "template = """Question: {question}Answer: Let's think step by step."""prompt = PromptTemplate(template=template, input_variables=["question"])Examples‚ÄãBelow are some examples of models you can access through the Hugging Face Hub integration.Flan, by Google‚Äãrepo_id = "google/flan-t5-xxl"  # See https://huggingface.co/models?pipeline_tag=text-generation&sort=downloads for some other optionsllm = HuggingFaceHub(    repo_id=repo_id, model_kwargs={"temperature": 0.5, "max_length": 64})llm_chain = LLMChain(prompt=prompt, llm=llm)print(llm_chain.run(question))    The FIFA World Cup was held in the year 1994. West Germany won the FIFA World Cup in 1994Dolly, by Databricks‚ÄãSee Databricks organization page for a list of available models.repo_id = "databricks/dolly-v2-3b"llm = HuggingFaceHub(    repo_id=repo_id, model_kwargs={"temperature": 0.5, "max_length": 64})llm_chain = LLMChain(prompt=prompt, llm=llm)print(llm_chain.run(question))     First of all, the world cup was won by the Germany. Then the Argentina won the world cup in 2022. So, the Argentina won the world cup in 1994.            Question: WhoCamel, by Writer‚ÄãSee Writer's organization page for a list of available models.repo_id = "Writer/camel-5b-hf"  # See https://huggingface.co/Writer for other optionsllm = HuggingFaceHub(    repo_id=repo_id, model_kwargs={"temperature": 0.5, "max_length": 64})llm_chain = LLMChain(prompt=prompt, llm=llm)print(llm_chain.run(question))XGen, by Salesforce‚ÄãSee more information.repo_id = "Salesforce/xgen-7b-8k-base"llm = HuggingFaceHub(    repo_id=repo_id, model_kwargs={"temperature": 0.5, "max_length": 64})llm_chain = LLMChain(prompt=prompt, llm=llm)print(llm_chain.run(question))Falcon, by Technology Innovation Institute (TII)‚ÄãSee more information.repo_id = "tiiuae/falcon-40b"llm = HuggingFaceHub(    repo_id=repo_id, model_kwargs={"temperature": 0.5, "max_length": 64})llm_chain = LLMChain(prompt=prompt, llm=llm)print(llm_chain.run(question))InternLM-Chat, by Shanghai AI Laboratory‚ÄãSee more information.repo_id = "internlm/internlm-chat-7b"llm = HuggingFaceHub(    repo_id=repo_id, model_kwargs={"max_length": 128, "temperature": 0.8})llm_chain = LLMChain(prompt=prompt, llm=llm)print(llm_chain.run(question))Qwen, by Alibaba Cloud‚ÄãTongyi Qianwen-7B (Qwen-7B) is a model with a scale of 7 billion parameters in the Tongyi Qianwen large model series developed by Alibaba Cloud. Qwen-7B is a large language model based on Transformer, which is trained on ultra-large-scale pre-training data.See more information on HuggingFace of on GitHub.See here a big example for LangChain integration and Qwen.repo_id = "Qwen/Qwen-7B"llm = HuggingFaceHub(    repo_id=repo_id, model_kwargs={"max_length": 128, "temperature": 0.5})llm_chain = LLMChain(prompt=prompt, llm=llm)print(llm_chain.run(question)) pip install huggingface_hub pip install huggingface_hub  # get a token: https://huggingface.co/docs/api-inference/quicktour#get-your-api-tokenfrom getpass import getpassHUGGINGFACEHUB_API_TOKEN = getpass() # get a token: https://huggingface.co/docs/api-inference/quicktour#get-your-api-tokenfrom getpass import getpassHUGGINGFACEHUB_API_TOKEN = getpass()       ¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑      ¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑      ¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑  import osos.environ["HUGGINGFACEHUB_API_TOKEN"] = HUGGINGFACEHUB_API_TOKEN import osos.environ["HUGGINGFACEHUB_API_TOKEN"] = HUGGINGFACEHUB_API_TOKEN  from langchain import HuggingFaceHub from langchain import HuggingFaceHub  from langchain import PromptTemplate, LLMChain from langchain import PromptTemplate, LLMChain  question = "Who won the FIFA World Cup in the year 1994? "template = """Question: {question}Answer: Let's think step by step."""prompt = PromptTemplate(template=template, input_variables=["question"]) question = "Who won the FIFA World Cup in the year 1994? "template = """Question: {question}Answer: Let's think step by step."""prompt = PromptTemplate(template=template, input_variables=["question"])  repo_id = "google/flan-t5-xxl"  # See https://huggingface.co/models?pipeline_tag=text-generation&sort=downloads for some other options repo_id = "google/flan-t5-xxl"  # See https://huggingface.co/models?pipeline_tag=text-generation&sort=downloads for some other options  llm = HuggingFaceHub(    repo_id=repo_id, model_kwargs={"temperature": 0.5, "max_length": 64})llm_chain = LLMChain(prompt=prompt, llm=llm)print(llm_chain.run(question)) llm = HuggingFaceHub(    repo_id=repo_id, model_kwargs={"temperature": 0.5, "max_length": 64})llm_chain = LLMChain(prompt=prompt, llm=llm)print(llm_chain.run(question))      The FIFA World Cup was held in the year 1994. West Germany won the FIFA World Cup in 1994     The FIFA World Cup was held in the year 1994. West Germany won the FIFA World Cup in 1994     The FIFA World Cup was held in the year 1994. West Germany won the FIFA World Cup in 1994  repo_id = "databricks/dolly-v2-3b" repo_id = "databricks/dolly-v2-3b"  llm = HuggingFaceHub(    repo_id=repo_id, model_kwargs={"temperature": 0.5, "max_length": 64})llm_chain = LLMChain(prompt=prompt, llm=llm)print(llm_chain.run(question)) llm = HuggingFaceHub(    repo_id=repo_id, model_kwargs={"temperature": 0.5, "max_length": 64})llm_chain = LLMChain(prompt=prompt, llm=llm)print(llm_chain.run(question))       First of all, the world cup was won by the Germany. Then the Argentina won the world cup in 2022. So, the Argentina won the world cup in 1994.            Question: Who      First of all, the world cup was won by the Germany. Then the Argentina won the world cup in 2022. So, the Argentina won the world cup in 1994.            Question: Who      First of all, the world cup was won by the Germany. Then the Argentina won the world cup in 2022. So, the Argentina won the world cup in 1994.            Question: Who  repo_id = "Writer/camel-5b-hf"  # See https://huggingface.co/Writer for other options repo_id = "Writer/camel-5b-hf"  # See https://huggingface.co/Writer for other options  llm = HuggingFaceHub(    repo_id=repo_id, model_kwargs={"temperature": 0.5, "max_length": 64})llm_chain = LLMChain(prompt=prompt, llm=llm)print(llm_chain.run(question)) llm = HuggingFaceHub(    repo_id=repo_id, model_kwargs={"temperature": 0.5, "max_length": 64})llm_chain = LLMChain(prompt=prompt, llm=llm)print(llm_chain.run(question))  repo_id = "Salesforce/xgen-7b-8k-base" repo_id = "Salesforce/xgen-7b-8k-base"  llm = HuggingFaceHub(    repo_id=repo_id, model_kwargs={"temperature": 0.5, "max_length": 64})llm_chain = LLMChain(prompt=prompt, llm=llm)print(llm_chain.run(question)) llm = HuggingFaceHub(    repo_id=repo_id, model_kwargs={"temperature": 0.5, "max_length": 64})llm_chain = LLMChain(prompt=prompt, llm=llm)print(llm_chain.run(question))  repo_id = "tiiuae/falcon-40b" repo_id = "tiiuae/falcon-40b"  llm = HuggingFaceHub(    repo_id=repo_id, model_kwargs={"temperature": 0.5, "max_length": 64})llm_chain = LLMChain(prompt=prompt, llm=llm)print(llm_chain.run(question)) llm = HuggingFaceHub(    repo_id=repo_id, model_kwargs={"temperature": 0.5, "max_length": 64})llm_chain = LLMChain(prompt=prompt, llm=llm)print(llm_chain.run(question))  repo_id = "internlm/internlm-chat-7b" repo_id = "internlm/internlm-chat-7b"  llm = HuggingFaceHub(    repo_id=repo_id, model_kwargs={"max_length": 128, "temperature": 0.8})llm_chain = LLMChain(prompt=prompt, llm=llm)print(llm_chain.run(question)) llm = HuggingFaceHub(    repo_id=repo_id, model_kwargs={"max_length": 128, "temperature": 0.8})llm_chain = LLMChain(prompt=prompt, llm=llm)print(llm_chain.run(question))  repo_id = "Qwen/Qwen-7B" repo_id = "Qwen/Qwen-7B"  llm = HuggingFaceHub(    repo_id=repo_id, model_kwargs={"max_length": 128, "temperature": 0.5})llm_chain = LLMChain(prompt=prompt, llm=llm)print(llm_chain.run(question)) llm = HuggingFaceHub(    repo_id=repo_id, model_kwargs={"max_length": 128, "temperature": 0.5})llm_chain = LLMChain(prompt=prompt, llm=llm)print(llm_chain.run(question))  Previous GPT4All Next Hugging Face Local Pipelines Installation and SetupPrepare ExamplesExamplesFlan, by GoogleDolly, by DatabricksCamel, by WriterXGen, by SalesforceFalcon, by Technology Innovation Institute (TII)InternLM-Chat, by Shanghai AI LaboratoryQwen, by Alibaba Cloud Installation and SetupPrepare ExamplesExamplesFlan, by GoogleDolly, by DatabricksCamel, by WriterXGen, by SalesforceFalcon, by Technology Innovation Institute (TII)InternLM-Chat, by Shanghai AI LaboratoryQwen, by Alibaba Cloud CommunityDiscordTwitterGitHubPythonJS/TSMoreHomepageBlogCopyright ¬© 2023 LangChain, Inc. CommunityDiscordTwitterGitHubPythonJS/TSMoreHomepageBlog CommunityDiscordTwitter Community GitHubPythonJS/TS GitHub MoreHomepageBlog More Copyright ¬© 2023 LangChain, Inc. Copyright ¬© 2023 LangChain, Inc. Skip to main content (#docusaurus_skipToContent_fallback) ü¶úÔ∏èüîó LangChain (/) Docs (/docs/get_started/introduction) Use cases (/docs/use_cases) Integrations (/docs/integrations) API (https://api.python.langchain.com) LangSmith (https://smith.langchain.com) JS/TS Docs (https://js.langchain.com/docs)  (https://github.com/hwchase17/langchain) Integrations (/docs/integrations) Callbacks (/docs/integrations/callbacks/) Chat models (/docs/integrations/chat/) Chat loaders (/docs/integrations/chat_loaders/) Document loaders (/docs/integrations/document_loaders/) Document transformers (/docs/integrations/document_transformers/) LLMs (/docs/integrations/llms/) AI21 (/docs/integrations/llms/ai21) Aleph Alpha (/docs/integrations/llms/aleph_alpha) Amazon API Gateway (/docs/integrations/llms/amazon_api_gateway) Anyscale (/docs/integrations/llms/anyscale) Azure ML (/docs/integrations/llms/azure_ml) Azure OpenAI (/docs/integrations/llms/azure_openai) Banana (/docs/integrations/llms/banana) Baseten (/docs/integrations/llms/baseten) Beam (/docs/integrations/llms/beam) Bedrock (/docs/integrations/llms/bedrock) Bittensor (/docs/integrations/llms/bittensor) CerebriumAI (/docs/integrations/llms/cerebriumai) ChatGLM (/docs/integrations/llms/chatglm) Clarifai (/docs/integrations/llms/clarifai) Cohere (/docs/integrations/llms/cohere) C Transformers (/docs/integrations/llms/ctransformers) CTranslate2 (/docs/integrations/llms/ctranslate2) Databricks (/docs/integrations/llms/databricks) DeepInfra (/docs/integrations/llms/deepinfra) DeepSparse (/docs/integrations/llms/deepsparse) Eden AI (/docs/integrations/llms/edenai) Fireworks (/docs/integrations/llms/fireworks) ForefrontAI (/docs/integrations/llms/forefrontai) Google Vertex AI PaLM (/docs/integrations/llms/google_vertex_ai_palm) GooseAI (/docs/integrations/llms/gooseai) GPT4All (/docs/integrations/llms/gpt4all) Hugging Face Hub (/docs/integrations/llms/huggingface_hub) Hugging Face Local Pipelines (/docs/integrations/llms/huggingface_pipelines) Huggingface TextGen Inference (/docs/integrations/llms/huggingface_textgen_inference) JSONFormer (/docs/integrations/llms/jsonformer_experimental) KoboldAI API (/docs/integrations/llms/koboldai) Llama.cpp (/docs/integrations/llms/llamacpp) LLM Caching integrations (/docs/integrations/llms/llm_caching) Manifest (/docs/integrations/llms/manifest) Minimax (/docs/integrations/llms/minimax) Modal (/docs/integrations/llms/modal) MosaicML (/docs/integrations/llms/mosaicml) NLP Cloud (/docs/integrations/llms/nlpcloud) OctoAI (/docs/integrations/llms/octoai) Ollama (/docs/integrations/llms/ollama) OpaquePrompts (/docs/integrations/llms/opaqueprompts) OpenAI (/docs/integrations/llms/openai) OpenLLM (/docs/integrations/llms/openllm) OpenLM (/docs/integrations/llms/openlm) Petals (/docs/integrations/llms/petals) PipelineAI (/docs/integrations/llms/pipelineai) Predibase (/docs/integrations/llms/predibase) Prediction Guard (/docs/integrations/llms/predictionguard) PromptLayer OpenAI (/docs/integrations/llms/promptlayer_openai) RELLM (/docs/integrations/llms/rellm_experimental) Replicate (/docs/integrations/llms/replicate) Runhouse (/docs/integrations/llms/runhouse) SageMakerEndpoint (/docs/integrations/llms/sagemaker) StochasticAI (/docs/integrations/llms/stochasticai) Nebula (Symbl.ai) (/docs/integrations/llms/symblai_nebula) TextGen (/docs/integrations/llms/textgen) Titan Takeoff (/docs/integrations/llms/titan_takeoff) Tongyi Qwen (/docs/integrations/llms/tongyi) vLLM (/docs/integrations/llms/vllm) Writer (/docs/integrations/llms/writer) Xorbits Inference (Xinference) (/docs/integrations/llms/xinference) Memory (/docs/integrations/memory/) Retrievers (/docs/integrations/retrievers/) Text embedding models (/docs/integrations/text_embedding/) Agents & Toolkits (/docs/integrations/toolkits/) Tools (/docs/integrations/tools/) Vector stores (/docs/integrations/vectorstores/) Grouped by provider (/docs/integrations/providers/)  (/) Integrations (/docs/integrations) LLMs (/docs/integrations/llms/) Hugging Face Hub (https://huggingface.co/docs/hub/index) ‚Äã (#installation-and-setup) package installed (https://huggingface.co/docs/huggingface_hub/installation) ‚Äã (#prepare-examples) ‚Äã (#examples) ‚Äã (#flan-by-google) ‚Äã (#dolly-by-databricks) Databricks (https://huggingface.co/databricks) ‚Äã (#camel-by-writer) Writer's (https://huggingface.co/Writer) ‚Äã (#xgen-by-salesforce) more information (https://github.com/salesforce/xgen) ‚Äã (#falcon-by-technology-innovation-institute-tii) more information (https://huggingface.co/tiiuae/falcon-40b) ‚Äã (#internlm-chat-by-shanghai-ai-laboratory) more information (https://huggingface.co/internlm/internlm-7b) ‚Äã (#qwen-by-alibaba-cloud) more information on HuggingFace (https://huggingface.co/Qwen/Qwen-7B) GitHub (https://github.com/QwenLM/Qwen-7B) big example for LangChain integration and Qwen (https://github.com/QwenLM/Qwen-7B/blob/main/examples/langchain_tooluse.ipynb) PreviousGPT4All (/docs/integrations/llms/gpt4all) NextHugging Face Local Pipelines (/docs/integrations/llms/huggingface_pipelines) Installation and Setup (#installation-and-setup) Prepare Examples (#prepare-examples) Examples (#examples) Flan, by Google (#flan-by-google) Dolly, by Databricks (#dolly-by-databricks) Camel, by Writer (#camel-by-writer) XGen, by Salesforce (#xgen-by-salesforce) Falcon, by Technology Innovation Institute (TII) (#falcon-by-technology-innovation-institute-tii) InternLM-Chat, by Shanghai AI Laboratory (#internlm-chat-by-shanghai-ai-laboratory) Qwen, by Alibaba Cloud (#qwen-by-alibaba-cloud) Discord (https://discord.gg/cU2adEyC7w) Twitter (https://twitter.com/LangChainAI) Python (https://github.com/hwchase17/langchain) JS/TS (https://github.com/hwchase17/langchainjs) Homepage (https://langchain.com) Blog (https://blog.langchain.dev)