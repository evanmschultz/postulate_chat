This notebook goes over how to connect to an Azure hosted OpenAI endpoint Azure OpenAI responses contain model property, which is name of the model used to generate the response. However unlike native OpenAI responses, it does not contain the version of the model, which is set on the deplyoment in Azure. This makes it tricky to know which version of the model was used to generate the response, which as result can lead to e.g. wrong total cost calculation with OpenAICallbackHandler. To solve this problem, you can pass model_version parameter to AzureChatOpenAI class, which will be added to the model name in the llm output. This way you can easily distinguish between different versions of the model. We can provide the model version to AzureChatOpenAI constructor. It will get appended to the model name returned by Azure OpenAI and cost will be counted correctly. IntegrationsCallbacksChat modelsAnthropicAnthropic FunctionsAnyscaleAzureAzureML Chat Online EndpointBedrock ChatERNIE-Bot ChatGoogle Cloud Platform Vertex AI PaLMJinaChatKonkoüöÖ LiteLLMLlama APIOllamaOpenAIPromptLayer ChatOpenAIChat loadersDocument loadersDocument transformersLLMsMemoryRetrieversText embedding modelsAgents & ToolkitsToolsVector storesGrouped by provider Callbacks Chat modelsAnthropicAnthropic FunctionsAnyscaleAzureAzureML Chat Online EndpointBedrock ChatERNIE-Bot ChatGoogle Cloud Platform Vertex AI PaLMJinaChatKonkoüöÖ LiteLLMLlama APIOllamaOpenAIPromptLayer ChatOpenAI Anthropic Anthropic Functions Anyscale Azure AzureML Chat Online Endpoint Bedrock Chat ERNIE-Bot Chat Google Cloud Platform Vertex AI PaLM JinaChat Konko üöÖ LiteLLM Llama API Ollama OpenAI PromptLayer ChatOpenAI Chat loaders Document loaders Document transformers LLMs Memory Retrievers Text embedding models Agents & Toolkits Tools Vector stores Grouped by provider  Integrations Chat models Azure AzureChatOpenAI HumanMessage get_openai_callback Model Version Discord Twitter Python JS/TS Homepage Blog Skip to main contentü¶úÔ∏èüîó LangChainDocsUse casesIntegrationsAPILangSmithJS/TS DocsCTRLKIntegrationsCallbacksChat modelsAnthropicAnthropic FunctionsAnyscaleAzureAzureML Chat Online EndpointBedrock ChatERNIE-Bot ChatGoogle Cloud Platform Vertex AI PaLMJinaChatKonkoüöÖ LiteLLMLlama APIOllamaOpenAIPromptLayer ChatOpenAIChat loadersDocument loadersDocument transformersLLMsMemoryRetrieversText embedding modelsAgents & ToolkitsToolsVector storesGrouped by providerIntegrationsChat modelsAzureOn this pageAzureThis notebook goes over how to connect to an Azure hosted OpenAI endpointfrom langchain.chat_models import AzureChatOpenAIfrom langchain.schema import HumanMessageAPI Reference:AzureChatOpenAIHumanMessageBASE_URL = "https://${TODO}.openai.azure.com"API_KEY = "..."DEPLOYMENT_NAME = "chat"model = AzureChatOpenAI(    openai_api_base=BASE_URL,    openai_api_version="2023-05-15",    deployment_name=DEPLOYMENT_NAME,    openai_api_key=API_KEY,    openai_api_type="azure",)model(    [        HumanMessage(            content="Translate this sentence from English to French. I love programming."        )    ])    AIMessage(content="\n\nJ'aime programmer.", additional_kwargs={})Model Version‚ÄãAzure OpenAI responses contain model property, which is name of the model used to generate the response. However unlike native OpenAI responses, it does not contain the version of the model, which is set on the deplyoment in Azure. This makes it tricky to know which version of the model was used to generate the response, which as result can lead to e.g. wrong total cost calculation with OpenAICallbackHandler.To solve this problem, you can pass model_version parameter to AzureChatOpenAI class, which will be added to the model name in the llm output. This way you can easily distinguish between different versions of the model.from langchain.callbacks import get_openai_callbackAPI Reference:get_openai_callbackBASE_URL = "https://{endpoint}.openai.azure.com"API_KEY = "..."DEPLOYMENT_NAME = "gpt-35-turbo" # in Azure, this deployment has version 0613 - input and output tokens are counted separatelymodel = AzureChatOpenAI(    openai_api_base=BASE_URL,    openai_api_version="2023-05-15",    deployment_name=DEPLOYMENT_NAME,    openai_api_key=API_KEY,    openai_api_type="azure",)with get_openai_callback() as cb:    model(        [            HumanMessage(                content="Translate this sentence from English to French. I love programming."            )        ]    )    print(f"Total Cost (USD): ${format(cb.total_cost, '.6f')}") # without specifying the model version, flat-rate 0.002 USD per 1k input and output tokens is used    Total Cost (USD): $0.000054We can provide the model version to AzureChatOpenAI constructor. It will get appended to the model name returned by Azure OpenAI and cost will be counted correctly.model0613 = AzureChatOpenAI(    openai_api_base=BASE_URL,    openai_api_version="2023-05-15",    deployment_name=DEPLOYMENT_NAME,    openai_api_key=API_KEY,    openai_api_type="azure",    model_version="0613")with get_openai_callback() as cb:    model0613(        [            HumanMessage(                content="Translate this sentence from English to French. I love programming."            )        ]    )    print(f"Total Cost (USD): ${format(cb.total_cost, '.6f')}")    Total Cost (USD): $0.000044PreviousAnyscaleNextAzureML Chat Online EndpointModel VersionCommunityDiscordTwitterGitHubPythonJS/TSMoreHomepageBlogCopyright ¬© 2023 LangChain, Inc. Skip to main content ü¶úÔ∏èüîó LangChainDocsUse casesIntegrationsAPILangSmithJS/TS DocsCTRLK ü¶úÔ∏èüîó LangChainDocsUse casesIntegrationsAPI LangSmithJS/TS DocsCTRLK  CTRLK CTRLK  CTRLK   IntegrationsCallbacksChat modelsAnthropicAnthropic FunctionsAnyscaleAzureAzureML Chat Online EndpointBedrock ChatERNIE-Bot ChatGoogle Cloud Platform Vertex AI PaLMJinaChatKonkoüöÖ LiteLLMLlama APIOllamaOpenAIPromptLayer ChatOpenAIChat loadersDocument loadersDocument transformersLLMsMemoryRetrieversText embedding modelsAgents & ToolkitsToolsVector storesGrouped by providerIntegrationsChat modelsAzureOn this pageAzureThis notebook goes over how to connect to an Azure hosted OpenAI endpointfrom langchain.chat_models import AzureChatOpenAIfrom langchain.schema import HumanMessageAPI Reference:AzureChatOpenAIHumanMessageBASE_URL = "https://${TODO}.openai.azure.com"API_KEY = "..."DEPLOYMENT_NAME = "chat"model = AzureChatOpenAI(    openai_api_base=BASE_URL,    openai_api_version="2023-05-15",    deployment_name=DEPLOYMENT_NAME,    openai_api_key=API_KEY,    openai_api_type="azure",)model(    [        HumanMessage(            content="Translate this sentence from English to French. I love programming."        )    ])    AIMessage(content="\n\nJ'aime programmer.", additional_kwargs={})Model Version‚ÄãAzure OpenAI responses contain model property, which is name of the model used to generate the response. However unlike native OpenAI responses, it does not contain the version of the model, which is set on the deplyoment in Azure. This makes it tricky to know which version of the model was used to generate the response, which as result can lead to e.g. wrong total cost calculation with OpenAICallbackHandler.To solve this problem, you can pass model_version parameter to AzureChatOpenAI class, which will be added to the model name in the llm output. This way you can easily distinguish between different versions of the model.from langchain.callbacks import get_openai_callbackAPI Reference:get_openai_callbackBASE_URL = "https://{endpoint}.openai.azure.com"API_KEY = "..."DEPLOYMENT_NAME = "gpt-35-turbo" # in Azure, this deployment has version 0613 - input and output tokens are counted separatelymodel = AzureChatOpenAI(    openai_api_base=BASE_URL,    openai_api_version="2023-05-15",    deployment_name=DEPLOYMENT_NAME,    openai_api_key=API_KEY,    openai_api_type="azure",)with get_openai_callback() as cb:    model(        [            HumanMessage(                content="Translate this sentence from English to French. I love programming."            )        ]    )    print(f"Total Cost (USD): ${format(cb.total_cost, '.6f')}") # without specifying the model version, flat-rate 0.002 USD per 1k input and output tokens is used    Total Cost (USD): $0.000054We can provide the model version to AzureChatOpenAI constructor. It will get appended to the model name returned by Azure OpenAI and cost will be counted correctly.model0613 = AzureChatOpenAI(    openai_api_base=BASE_URL,    openai_api_version="2023-05-15",    deployment_name=DEPLOYMENT_NAME,    openai_api_key=API_KEY,    openai_api_type="azure",    model_version="0613")with get_openai_callback() as cb:    model0613(        [            HumanMessage(                content="Translate this sentence from English to French. I love programming."            )        ]    )    print(f"Total Cost (USD): ${format(cb.total_cost, '.6f')}")    Total Cost (USD): $0.000044PreviousAnyscaleNextAzureML Chat Online EndpointModel Version IntegrationsCallbacksChat modelsAnthropicAnthropic FunctionsAnyscaleAzureAzureML Chat Online EndpointBedrock ChatERNIE-Bot ChatGoogle Cloud Platform Vertex AI PaLMJinaChatKonkoüöÖ LiteLLMLlama APIOllamaOpenAIPromptLayer ChatOpenAIChat loadersDocument loadersDocument transformersLLMsMemoryRetrieversText embedding modelsAgents & ToolkitsToolsVector storesGrouped by providerIntegrationsChat modelsAzureOn this pageAzureThis notebook goes over how to connect to an Azure hosted OpenAI endpointfrom langchain.chat_models import AzureChatOpenAIfrom langchain.schema import HumanMessageAPI Reference:AzureChatOpenAIHumanMessageBASE_URL = "https://${TODO}.openai.azure.com"API_KEY = "..."DEPLOYMENT_NAME = "chat"model = AzureChatOpenAI(    openai_api_base=BASE_URL,    openai_api_version="2023-05-15",    deployment_name=DEPLOYMENT_NAME,    openai_api_key=API_KEY,    openai_api_type="azure",)model(    [        HumanMessage(            content="Translate this sentence from English to French. I love programming."        )    ])    AIMessage(content="\n\nJ'aime programmer.", additional_kwargs={})Model Version‚ÄãAzure OpenAI responses contain model property, which is name of the model used to generate the response. However unlike native OpenAI responses, it does not contain the version of the model, which is set on the deplyoment in Azure. This makes it tricky to know which version of the model was used to generate the response, which as result can lead to e.g. wrong total cost calculation with OpenAICallbackHandler.To solve this problem, you can pass model_version parameter to AzureChatOpenAI class, which will be added to the model name in the llm output. This way you can easily distinguish between different versions of the model.from langchain.callbacks import get_openai_callbackAPI Reference:get_openai_callbackBASE_URL = "https://{endpoint}.openai.azure.com"API_KEY = "..."DEPLOYMENT_NAME = "gpt-35-turbo" # in Azure, this deployment has version 0613 - input and output tokens are counted separatelymodel = AzureChatOpenAI(    openai_api_base=BASE_URL,    openai_api_version="2023-05-15",    deployment_name=DEPLOYMENT_NAME,    openai_api_key=API_KEY,    openai_api_type="azure",)with get_openai_callback() as cb:    model(        [            HumanMessage(                content="Translate this sentence from English to French. I love programming."            )        ]    )    print(f"Total Cost (USD): ${format(cb.total_cost, '.6f')}") # without specifying the model version, flat-rate 0.002 USD per 1k input and output tokens is used    Total Cost (USD): $0.000054We can provide the model version to AzureChatOpenAI constructor. It will get appended to the model name returned by Azure OpenAI and cost will be counted correctly.model0613 = AzureChatOpenAI(    openai_api_base=BASE_URL,    openai_api_version="2023-05-15",    deployment_name=DEPLOYMENT_NAME,    openai_api_key=API_KEY,    openai_api_type="azure",    model_version="0613")with get_openai_callback() as cb:    model0613(        [            HumanMessage(                content="Translate this sentence from English to French. I love programming."            )        ]    )    print(f"Total Cost (USD): ${format(cb.total_cost, '.6f')}")    Total Cost (USD): $0.000044PreviousAnyscaleNextAzureML Chat Online EndpointModel Version IntegrationsCallbacksChat modelsAnthropicAnthropic FunctionsAnyscaleAzureAzureML Chat Online EndpointBedrock ChatERNIE-Bot ChatGoogle Cloud Platform Vertex AI PaLMJinaChatKonkoüöÖ LiteLLMLlama APIOllamaOpenAIPromptLayer ChatOpenAIChat loadersDocument loadersDocument transformersLLMsMemoryRetrieversText embedding modelsAgents & ToolkitsToolsVector storesGrouped by provider IntegrationsCallbacksChat modelsAnthropicAnthropic FunctionsAnyscaleAzureAzureML Chat Online EndpointBedrock ChatERNIE-Bot ChatGoogle Cloud Platform Vertex AI PaLMJinaChatKonkoüöÖ LiteLLMLlama APIOllamaOpenAIPromptLayer ChatOpenAIChat loadersDocument loadersDocument transformersLLMsMemoryRetrieversText embedding modelsAgents & ToolkitsToolsVector storesGrouped by provider Integrations Callbacks Chat models Chat loaders Document loaders Document transformers LLMs Memory Retrievers Text embedding models Agents & Toolkits Tools Vector stores Grouped by provider IntegrationsChat modelsAzureOn this pageAzureThis notebook goes over how to connect to an Azure hosted OpenAI endpointfrom langchain.chat_models import AzureChatOpenAIfrom langchain.schema import HumanMessageAPI Reference:AzureChatOpenAIHumanMessageBASE_URL = "https://${TODO}.openai.azure.com"API_KEY = "..."DEPLOYMENT_NAME = "chat"model = AzureChatOpenAI(    openai_api_base=BASE_URL,    openai_api_version="2023-05-15",    deployment_name=DEPLOYMENT_NAME,    openai_api_key=API_KEY,    openai_api_type="azure",)model(    [        HumanMessage(            content="Translate this sentence from English to French. I love programming."        )    ])    AIMessage(content="\n\nJ'aime programmer.", additional_kwargs={})Model Version‚ÄãAzure OpenAI responses contain model property, which is name of the model used to generate the response. However unlike native OpenAI responses, it does not contain the version of the model, which is set on the deplyoment in Azure. This makes it tricky to know which version of the model was used to generate the response, which as result can lead to e.g. wrong total cost calculation with OpenAICallbackHandler.To solve this problem, you can pass model_version parameter to AzureChatOpenAI class, which will be added to the model name in the llm output. This way you can easily distinguish between different versions of the model.from langchain.callbacks import get_openai_callbackAPI Reference:get_openai_callbackBASE_URL = "https://{endpoint}.openai.azure.com"API_KEY = "..."DEPLOYMENT_NAME = "gpt-35-turbo" # in Azure, this deployment has version 0613 - input and output tokens are counted separatelymodel = AzureChatOpenAI(    openai_api_base=BASE_URL,    openai_api_version="2023-05-15",    deployment_name=DEPLOYMENT_NAME,    openai_api_key=API_KEY,    openai_api_type="azure",)with get_openai_callback() as cb:    model(        [            HumanMessage(                content="Translate this sentence from English to French. I love programming."            )        ]    )    print(f"Total Cost (USD): ${format(cb.total_cost, '.6f')}") # without specifying the model version, flat-rate 0.002 USD per 1k input and output tokens is used    Total Cost (USD): $0.000054We can provide the model version to AzureChatOpenAI constructor. It will get appended to the model name returned by Azure OpenAI and cost will be counted correctly.model0613 = AzureChatOpenAI(    openai_api_base=BASE_URL,    openai_api_version="2023-05-15",    deployment_name=DEPLOYMENT_NAME,    openai_api_key=API_KEY,    openai_api_type="azure",    model_version="0613")with get_openai_callback() as cb:    model0613(        [            HumanMessage(                content="Translate this sentence from English to French. I love programming."            )        ]    )    print(f"Total Cost (USD): ${format(cb.total_cost, '.6f')}")    Total Cost (USD): $0.000044PreviousAnyscaleNextAzureML Chat Online EndpointModel Version IntegrationsChat modelsAzureOn this pageAzureThis notebook goes over how to connect to an Azure hosted OpenAI endpointfrom langchain.chat_models import AzureChatOpenAIfrom langchain.schema import HumanMessageAPI Reference:AzureChatOpenAIHumanMessageBASE_URL = "https://${TODO}.openai.azure.com"API_KEY = "..."DEPLOYMENT_NAME = "chat"model = AzureChatOpenAI(    openai_api_base=BASE_URL,    openai_api_version="2023-05-15",    deployment_name=DEPLOYMENT_NAME,    openai_api_key=API_KEY,    openai_api_type="azure",)model(    [        HumanMessage(            content="Translate this sentence from English to French. I love programming."        )    ])    AIMessage(content="\n\nJ'aime programmer.", additional_kwargs={})Model Version‚ÄãAzure OpenAI responses contain model property, which is name of the model used to generate the response. However unlike native OpenAI responses, it does not contain the version of the model, which is set on the deplyoment in Azure. This makes it tricky to know which version of the model was used to generate the response, which as result can lead to e.g. wrong total cost calculation with OpenAICallbackHandler.To solve this problem, you can pass model_version parameter to AzureChatOpenAI class, which will be added to the model name in the llm output. This way you can easily distinguish between different versions of the model.from langchain.callbacks import get_openai_callbackAPI Reference:get_openai_callbackBASE_URL = "https://{endpoint}.openai.azure.com"API_KEY = "..."DEPLOYMENT_NAME = "gpt-35-turbo" # in Azure, this deployment has version 0613 - input and output tokens are counted separatelymodel = AzureChatOpenAI(    openai_api_base=BASE_URL,    openai_api_version="2023-05-15",    deployment_name=DEPLOYMENT_NAME,    openai_api_key=API_KEY,    openai_api_type="azure",)with get_openai_callback() as cb:    model(        [            HumanMessage(                content="Translate this sentence from English to French. I love programming."            )        ]    )    print(f"Total Cost (USD): ${format(cb.total_cost, '.6f')}") # without specifying the model version, flat-rate 0.002 USD per 1k input and output tokens is used    Total Cost (USD): $0.000054We can provide the model version to AzureChatOpenAI constructor. It will get appended to the model name returned by Azure OpenAI and cost will be counted correctly.model0613 = AzureChatOpenAI(    openai_api_base=BASE_URL,    openai_api_version="2023-05-15",    deployment_name=DEPLOYMENT_NAME,    openai_api_key=API_KEY,    openai_api_type="azure",    model_version="0613")with get_openai_callback() as cb:    model0613(        [            HumanMessage(                content="Translate this sentence from English to French. I love programming."            )        ]    )    print(f"Total Cost (USD): ${format(cb.total_cost, '.6f')}")    Total Cost (USD): $0.000044PreviousAnyscaleNextAzureML Chat Online EndpointModel Version IntegrationsChat modelsAzureOn this pageAzureThis notebook goes over how to connect to an Azure hosted OpenAI endpointfrom langchain.chat_models import AzureChatOpenAIfrom langchain.schema import HumanMessageAPI Reference:AzureChatOpenAIHumanMessageBASE_URL = "https://${TODO}.openai.azure.com"API_KEY = "..."DEPLOYMENT_NAME = "chat"model = AzureChatOpenAI(    openai_api_base=BASE_URL,    openai_api_version="2023-05-15",    deployment_name=DEPLOYMENT_NAME,    openai_api_key=API_KEY,    openai_api_type="azure",)model(    [        HumanMessage(            content="Translate this sentence from English to French. I love programming."        )    ])    AIMessage(content="\n\nJ'aime programmer.", additional_kwargs={})Model Version‚ÄãAzure OpenAI responses contain model property, which is name of the model used to generate the response. However unlike native OpenAI responses, it does not contain the version of the model, which is set on the deplyoment in Azure. This makes it tricky to know which version of the model was used to generate the response, which as result can lead to e.g. wrong total cost calculation with OpenAICallbackHandler.To solve this problem, you can pass model_version parameter to AzureChatOpenAI class, which will be added to the model name in the llm output. This way you can easily distinguish between different versions of the model.from langchain.callbacks import get_openai_callbackAPI Reference:get_openai_callbackBASE_URL = "https://{endpoint}.openai.azure.com"API_KEY = "..."DEPLOYMENT_NAME = "gpt-35-turbo" # in Azure, this deployment has version 0613 - input and output tokens are counted separatelymodel = AzureChatOpenAI(    openai_api_base=BASE_URL,    openai_api_version="2023-05-15",    deployment_name=DEPLOYMENT_NAME,    openai_api_key=API_KEY,    openai_api_type="azure",)with get_openai_callback() as cb:    model(        [            HumanMessage(                content="Translate this sentence from English to French. I love programming."            )        ]    )    print(f"Total Cost (USD): ${format(cb.total_cost, '.6f')}") # without specifying the model version, flat-rate 0.002 USD per 1k input and output tokens is used    Total Cost (USD): $0.000054We can provide the model version to AzureChatOpenAI constructor. It will get appended to the model name returned by Azure OpenAI and cost will be counted correctly.model0613 = AzureChatOpenAI(    openai_api_base=BASE_URL,    openai_api_version="2023-05-15",    deployment_name=DEPLOYMENT_NAME,    openai_api_key=API_KEY,    openai_api_type="azure",    model_version="0613")with get_openai_callback() as cb:    model0613(        [            HumanMessage(                content="Translate this sentence from English to French. I love programming."            )        ]    )    print(f"Total Cost (USD): ${format(cb.total_cost, '.6f')}")    Total Cost (USD): $0.000044PreviousAnyscaleNextAzureML Chat Online Endpoint IntegrationsChat modelsAzureOn this pageAzureThis notebook goes over how to connect to an Azure hosted OpenAI endpointfrom langchain.chat_models import AzureChatOpenAIfrom langchain.schema import HumanMessageAPI Reference:AzureChatOpenAIHumanMessageBASE_URL = "https://${TODO}.openai.azure.com"API_KEY = "..."DEPLOYMENT_NAME = "chat"model = AzureChatOpenAI(    openai_api_base=BASE_URL,    openai_api_version="2023-05-15",    deployment_name=DEPLOYMENT_NAME,    openai_api_key=API_KEY,    openai_api_type="azure",)model(    [        HumanMessage(            content="Translate this sentence from English to French. I love programming."        )    ])    AIMessage(content="\n\nJ'aime programmer.", additional_kwargs={})Model Version‚ÄãAzure OpenAI responses contain model property, which is name of the model used to generate the response. However unlike native OpenAI responses, it does not contain the version of the model, which is set on the deplyoment in Azure. This makes it tricky to know which version of the model was used to generate the response, which as result can lead to e.g. wrong total cost calculation with OpenAICallbackHandler.To solve this problem, you can pass model_version parameter to AzureChatOpenAI class, which will be added to the model name in the llm output. This way you can easily distinguish between different versions of the model.from langchain.callbacks import get_openai_callbackAPI Reference:get_openai_callbackBASE_URL = "https://{endpoint}.openai.azure.com"API_KEY = "..."DEPLOYMENT_NAME = "gpt-35-turbo" # in Azure, this deployment has version 0613 - input and output tokens are counted separatelymodel = AzureChatOpenAI(    openai_api_base=BASE_URL,    openai_api_version="2023-05-15",    deployment_name=DEPLOYMENT_NAME,    openai_api_key=API_KEY,    openai_api_type="azure",)with get_openai_callback() as cb:    model(        [            HumanMessage(                content="Translate this sentence from English to French. I love programming."            )        ]    )    print(f"Total Cost (USD): ${format(cb.total_cost, '.6f')}") # without specifying the model version, flat-rate 0.002 USD per 1k input and output tokens is used    Total Cost (USD): $0.000054We can provide the model version to AzureChatOpenAI constructor. It will get appended to the model name returned by Azure OpenAI and cost will be counted correctly.model0613 = AzureChatOpenAI(    openai_api_base=BASE_URL,    openai_api_version="2023-05-15",    deployment_name=DEPLOYMENT_NAME,    openai_api_key=API_KEY,    openai_api_type="azure",    model_version="0613")with get_openai_callback() as cb:    model0613(        [            HumanMessage(                content="Translate this sentence from English to French. I love programming."            )        ]    )    print(f"Total Cost (USD): ${format(cb.total_cost, '.6f')}")    Total Cost (USD): $0.000044PreviousAnyscaleNextAzureML Chat Online Endpoint On this page AzureThis notebook goes over how to connect to an Azure hosted OpenAI endpointfrom langchain.chat_models import AzureChatOpenAIfrom langchain.schema import HumanMessageAPI Reference:AzureChatOpenAIHumanMessageBASE_URL = "https://${TODO}.openai.azure.com"API_KEY = "..."DEPLOYMENT_NAME = "chat"model = AzureChatOpenAI(    openai_api_base=BASE_URL,    openai_api_version="2023-05-15",    deployment_name=DEPLOYMENT_NAME,    openai_api_key=API_KEY,    openai_api_type="azure",)model(    [        HumanMessage(            content="Translate this sentence from English to French. I love programming."        )    ])    AIMessage(content="\n\nJ'aime programmer.", additional_kwargs={})Model Version‚ÄãAzure OpenAI responses contain model property, which is name of the model used to generate the response. However unlike native OpenAI responses, it does not contain the version of the model, which is set on the deplyoment in Azure. This makes it tricky to know which version of the model was used to generate the response, which as result can lead to e.g. wrong total cost calculation with OpenAICallbackHandler.To solve this problem, you can pass model_version parameter to AzureChatOpenAI class, which will be added to the model name in the llm output. This way you can easily distinguish between different versions of the model.from langchain.callbacks import get_openai_callbackAPI Reference:get_openai_callbackBASE_URL = "https://{endpoint}.openai.azure.com"API_KEY = "..."DEPLOYMENT_NAME = "gpt-35-turbo" # in Azure, this deployment has version 0613 - input and output tokens are counted separatelymodel = AzureChatOpenAI(    openai_api_base=BASE_URL,    openai_api_version="2023-05-15",    deployment_name=DEPLOYMENT_NAME,    openai_api_key=API_KEY,    openai_api_type="azure",)with get_openai_callback() as cb:    model(        [            HumanMessage(                content="Translate this sentence from English to French. I love programming."            )        ]    )    print(f"Total Cost (USD): ${format(cb.total_cost, '.6f')}") # without specifying the model version, flat-rate 0.002 USD per 1k input and output tokens is used    Total Cost (USD): $0.000054We can provide the model version to AzureChatOpenAI constructor. It will get appended to the model name returned by Azure OpenAI and cost will be counted correctly.model0613 = AzureChatOpenAI(    openai_api_base=BASE_URL,    openai_api_version="2023-05-15",    deployment_name=DEPLOYMENT_NAME,    openai_api_key=API_KEY,    openai_api_type="azure",    model_version="0613")with get_openai_callback() as cb:    model0613(        [            HumanMessage(                content="Translate this sentence from English to French. I love programming."            )        ]    )    print(f"Total Cost (USD): ${format(cb.total_cost, '.6f')}")    Total Cost (USD): $0.000044 from langchain.chat_models import AzureChatOpenAIfrom langchain.schema import HumanMessage from langchain.chat_models import AzureChatOpenAIfrom langchain.schema import HumanMessage  API Reference:AzureChatOpenAIHumanMessage BASE_URL = "https://${TODO}.openai.azure.com"API_KEY = "..."DEPLOYMENT_NAME = "chat"model = AzureChatOpenAI(    openai_api_base=BASE_URL,    openai_api_version="2023-05-15",    deployment_name=DEPLOYMENT_NAME,    openai_api_key=API_KEY,    openai_api_type="azure",) BASE_URL = "https://${TODO}.openai.azure.com"API_KEY = "..."DEPLOYMENT_NAME = "chat"model = AzureChatOpenAI(    openai_api_base=BASE_URL,    openai_api_version="2023-05-15",    deployment_name=DEPLOYMENT_NAME,    openai_api_key=API_KEY,    openai_api_type="azure",)  model(    [        HumanMessage(            content="Translate this sentence from English to French. I love programming."        )    ]) model(    [        HumanMessage(            content="Translate this sentence from English to French. I love programming."        )    ])      AIMessage(content="\n\nJ'aime programmer.", additional_kwargs={})     AIMessage(content="\n\nJ'aime programmer.", additional_kwargs={})     AIMessage(content="\n\nJ'aime programmer.", additional_kwargs={})  from langchain.callbacks import get_openai_callback from langchain.callbacks import get_openai_callback  API Reference:get_openai_callback BASE_URL = "https://{endpoint}.openai.azure.com"API_KEY = "..."DEPLOYMENT_NAME = "gpt-35-turbo" # in Azure, this deployment has version 0613 - input and output tokens are counted separately BASE_URL = "https://{endpoint}.openai.azure.com"API_KEY = "..."DEPLOYMENT_NAME = "gpt-35-turbo" # in Azure, this deployment has version 0613 - input and output tokens are counted separately  model = AzureChatOpenAI(    openai_api_base=BASE_URL,    openai_api_version="2023-05-15",    deployment_name=DEPLOYMENT_NAME,    openai_api_key=API_KEY,    openai_api_type="azure",)with get_openai_callback() as cb:    model(        [            HumanMessage(                content="Translate this sentence from English to French. I love programming."            )        ]    )    print(f"Total Cost (USD): ${format(cb.total_cost, '.6f')}") # without specifying the model version, flat-rate 0.002 USD per 1k input and output tokens is used model = AzureChatOpenAI(    openai_api_base=BASE_URL,    openai_api_version="2023-05-15",    deployment_name=DEPLOYMENT_NAME,    openai_api_key=API_KEY,    openai_api_type="azure",)with get_openai_callback() as cb:    model(        [            HumanMessage(                content="Translate this sentence from English to French. I love programming."            )        ]    )    print(f"Total Cost (USD): ${format(cb.total_cost, '.6f')}") # without specifying the model version, flat-rate 0.002 USD per 1k input and output tokens is used      Total Cost (USD): $0.000054     Total Cost (USD): $0.000054     Total Cost (USD): $0.000054  model0613 = AzureChatOpenAI(    openai_api_base=BASE_URL,    openai_api_version="2023-05-15",    deployment_name=DEPLOYMENT_NAME,    openai_api_key=API_KEY,    openai_api_type="azure",    model_version="0613")with get_openai_callback() as cb:    model0613(        [            HumanMessage(                content="Translate this sentence from English to French. I love programming."            )        ]    )    print(f"Total Cost (USD): ${format(cb.total_cost, '.6f')}") model0613 = AzureChatOpenAI(    openai_api_base=BASE_URL,    openai_api_version="2023-05-15",    deployment_name=DEPLOYMENT_NAME,    openai_api_key=API_KEY,    openai_api_type="azure",    model_version="0613")with get_openai_callback() as cb:    model0613(        [            HumanMessage(                content="Translate this sentence from English to French. I love programming."            )        ]    )    print(f"Total Cost (USD): ${format(cb.total_cost, '.6f')}")      Total Cost (USD): $0.000044     Total Cost (USD): $0.000044     Total Cost (USD): $0.000044  Previous Anyscale Next AzureML Chat Online Endpoint Model Version Model Version CommunityDiscordTwitterGitHubPythonJS/TSMoreHomepageBlogCopyright ¬© 2023 LangChain, Inc. CommunityDiscordTwitterGitHubPythonJS/TSMoreHomepageBlog CommunityDiscordTwitter Community GitHubPythonJS/TS GitHub MoreHomepageBlog More Copyright ¬© 2023 LangChain, Inc. Copyright ¬© 2023 LangChain, Inc. Skip to main content (#docusaurus_skipToContent_fallback) ü¶úÔ∏èüîó LangChain (/) Docs (/docs/get_started/introduction) Use cases (/docs/use_cases) Integrations (/docs/integrations) API (https://api.python.langchain.com) LangSmith (https://smith.langchain.com) JS/TS Docs (https://js.langchain.com/docs)  (https://github.com/hwchase17/langchain) Integrations (/docs/integrations) Callbacks (/docs/integrations/callbacks/) Chat models (/docs/integrations/chat/) Anthropic (/docs/integrations/chat/anthropic) Anthropic Functions (/docs/integrations/chat/anthropic_functions) Anyscale (/docs/integrations/chat/anyscale) Azure (/docs/integrations/chat/azure_chat_openai) AzureML Chat Online Endpoint (/docs/integrations/chat/azureml_chat_endpoint) Bedrock Chat (/docs/integrations/chat/bedrock) ERNIE-Bot Chat (/docs/integrations/chat/ernie) Google Cloud Platform Vertex AI PaLM (/docs/integrations/chat/google_vertex_ai_palm) JinaChat (/docs/integrations/chat/jinachat) Konko (/docs/integrations/chat/konko) üöÖ LiteLLM (/docs/integrations/chat/litellm) Llama API (/docs/integrations/chat/llama_api) Ollama (/docs/integrations/chat/ollama) OpenAI (/docs/integrations/chat/openai) PromptLayer ChatOpenAI (/docs/integrations/chat/promptlayer_chatopenai) Chat loaders (/docs/integrations/chat_loaders/) Document loaders (/docs/integrations/document_loaders/) Document transformers (/docs/integrations/document_transformers/) LLMs (/docs/integrations/llms/) Memory (/docs/integrations/memory/) Retrievers (/docs/integrations/retrievers/) Text embedding models (/docs/integrations/text_embedding/) Agents & Toolkits (/docs/integrations/toolkits/) Tools (/docs/integrations/tools/) Vector stores (/docs/integrations/vectorstores/) Grouped by provider (/docs/integrations/providers/)  (/) Integrations (/docs/integrations) Chat models (/docs/integrations/chat/) AzureChatOpenAI (https://api.python.langchain.com/en/latest/chat_models/langchain.chat_models.azure_openai.AzureChatOpenAI.html) HumanMessage (https://api.python.langchain.com/en/latest/schema/langchain.schema.messages.HumanMessage.html) ‚Äã (#model-version) get_openai_callback (https://api.python.langchain.com/en/latest/callbacks/langchain.callbacks.manager.get_openai_callback.html) PreviousAnyscale (/docs/integrations/chat/anyscale) NextAzureML Chat Online Endpoint (/docs/integrations/chat/azureml_chat_endpoint) Model Version (#model-version) Discord (https://discord.gg/cU2adEyC7w) Twitter (https://twitter.com/LangChainAI) Python (https://github.com/hwchase17/langchain) JS/TS (https://github.com/hwchase17/langchainjs) Homepage (https://langchain.com) Blog (https://blog.langchain.dev)