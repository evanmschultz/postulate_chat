Bittensor is a mining network, similar to Bitcoin, that includes built-in incentives designed to encourage miners to contribute compute + knowledge. NIBittensorLLM is developed by Neural Internet, powered by Bittensor. This LLM showcases true potential of decentralized AI by giving you the best response(s) from the Bittensor protocol, which consist of various AI models such as OpenAI, LLaMA2 etc. Users can view their logs, requests, and API keys on the Validator Endpoint Frontend. However, changes to the configuration are currently prohibited; otherwise, the user's queries will be blocked. If you encounter any difficulties or have any questions, please feel free to reach out to our developer on GitHub, Discord or join our discord server for latest update and queries Neural Internet. IntegrationsCallbacksChat modelsChat loadersDocument loadersDocument transformersLLMsAI21Aleph AlphaAmazon API GatewayAnyscaleAzure MLAzure OpenAIBananaBasetenBeamBedrockBittensorCerebriumAIChatGLMClarifaiCohereC TransformersCTranslate2DatabricksDeepInfraDeepSparseEden AIFireworksForefrontAIGoogle Vertex AI PaLMGooseAIGPT4AllHugging Face HubHugging Face Local PipelinesHuggingface TextGen InferenceJSONFormerKoboldAI APILlama.cppLLM Caching integrationsManifestMinimaxModalMosaicMLNLP CloudOctoAIOllamaOpaquePromptsOpenAIOpenLLMOpenLMPetalsPipelineAIPredibasePrediction GuardPromptLayer OpenAIRELLMReplicateRunhouseSageMakerEndpointStochasticAINebula (Symbl.ai)TextGenTitan TakeoffTongyi QwenvLLMWriterXorbits Inference (Xinference)MemoryRetrieversText embedding modelsAgents & ToolkitsToolsVector storesGrouped by provider Callbacks Chat models Chat loaders Document loaders Document transformers LLMsAI21Aleph AlphaAmazon API GatewayAnyscaleAzure MLAzure OpenAIBananaBasetenBeamBedrockBittensorCerebriumAIChatGLMClarifaiCohereC TransformersCTranslate2DatabricksDeepInfraDeepSparseEden AIFireworksForefrontAIGoogle Vertex AI PaLMGooseAIGPT4AllHugging Face HubHugging Face Local PipelinesHuggingface TextGen InferenceJSONFormerKoboldAI APILlama.cppLLM Caching integrationsManifestMinimaxModalMosaicMLNLP CloudOctoAIOllamaOpaquePromptsOpenAIOpenLLMOpenLMPetalsPipelineAIPredibasePrediction GuardPromptLayer OpenAIRELLMReplicateRunhouseSageMakerEndpointStochasticAINebula (Symbl.ai)TextGenTitan TakeoffTongyi QwenvLLMWriterXorbits Inference (Xinference) AI21 Aleph Alpha Amazon API Gateway Anyscale Azure ML Azure OpenAI Banana Baseten Beam Bedrock Bittensor CerebriumAI ChatGLM Clarifai Cohere C Transformers CTranslate2 Databricks DeepInfra DeepSparse Eden AI Fireworks ForefrontAI Google Vertex AI PaLM GooseAI GPT4All Hugging Face Hub Hugging Face Local Pipelines Huggingface TextGen Inference JSONFormer KoboldAI API Llama.cpp LLM Caching integrations Manifest Minimax Modal MosaicML NLP Cloud OctoAI Ollama OpaquePrompts OpenAI OpenLLM OpenLM Petals PipelineAI Predibase Prediction Guard PromptLayer OpenAI RELLM Replicate Runhouse SageMakerEndpoint StochasticAI Nebula (Symbl.ai) TextGen Titan Takeoff Tongyi Qwen vLLM Writer Xorbits Inference (Xinference) Memory Retrievers Text embedding models Agents & Toolkits Tools Vector stores Grouped by provider  Integrations LLMs Bittensor NIBittensorLLM NIBittensorLLM AgentType initialize_agent load_tools ZeroShotAgent Tool AgentExecutor ConversationBufferMemory GoogleSearchAPIWrapper SerpAPIWrapper NIBittensorLLM Different Parameter and response handling for NIBittensorLLM Using NIBittensorLLM with LLMChain and PromptTemplate Using NIBittensorLLM with Conversational Agent and Google Search Tool Discord Twitter Python JS/TS Homepage Blog Skip to main contentü¶úÔ∏èüîó LangChainDocsUse casesIntegrationsAPILangSmithJS/TS DocsCTRLKIntegrationsCallbacksChat modelsChat loadersDocument loadersDocument transformersLLMsAI21Aleph AlphaAmazon API GatewayAnyscaleAzure MLAzure OpenAIBananaBasetenBeamBedrockBittensorCerebriumAIChatGLMClarifaiCohereC TransformersCTranslate2DatabricksDeepInfraDeepSparseEden AIFireworksForefrontAIGoogle Vertex AI PaLMGooseAIGPT4AllHugging Face HubHugging Face Local PipelinesHuggingface TextGen InferenceJSONFormerKoboldAI APILlama.cppLLM Caching integrationsManifestMinimaxModalMosaicMLNLP CloudOctoAIOllamaOpaquePromptsOpenAIOpenLLMOpenLMPetalsPipelineAIPredibasePrediction GuardPromptLayer OpenAIRELLMReplicateRunhouseSageMakerEndpointStochasticAINebula (Symbl.ai)TextGenTitan TakeoffTongyi QwenvLLMWriterXorbits Inference (Xinference)MemoryRetrieversText embedding modelsAgents & ToolkitsToolsVector storesGrouped by providerIntegrationsLLMsBittensorOn this pageBittensorBittensor is a mining network, similar to Bitcoin, that includes built-in incentives designed to encourage miners to contribute compute + knowledge.NIBittensorLLM is developed by Neural Internet, powered by Bittensor.This LLM showcases true potential of decentralized AI by giving you the best response(s) from the Bittensor protocol, which consist of various AI models such as OpenAI, LLaMA2 etc.Users can view their logs, requests, and API keys on the Validator Endpoint Frontend. However, changes to the configuration are currently prohibited; otherwise, the user's queries will be blocked.If you encounter any difficulties or have any questions, please feel free to reach out to our developer on GitHub, Discord or join our discord server for latest update and queries Neural Internet.Different Parameter and response handling for NIBittensorLLM‚Äãimport langchainfrom langchain.llms import NIBittensorLLMimport jsonfrom pprint import pprintlangchain.debug = True# System parameter in NIBittensorLLM is optional but you can set whatever you want to perform with modelllm_sys = NIBittensorLLM(    system_prompt="Your task is to determine response based on user prompt.Explain me like I am technical lead of a project")sys_resp = llm_sys(    "What is bittensor and What are the potential benifits of decentralized AI?")print(f"Response provided by LLM with system prompt set is : {sys_resp}")# The top_responses parameter can give multiple responses based on its parameter value# This below code retrive top 10 miner's response all the response are in format of json# Json response structure is""" {    "choices":  [                    {"index": Bittensor's Metagraph index number,                    "uid": Unique Identifier of a miner,                    "responder_hotkey": Hotkey of a miner,                    "message":{"role":"assistant","content": Contains actual response},                    "response_ms": Time in millisecond required to fetch response from a miner}                 ]    } """multi_response_llm = NIBittensorLLM(top_responses=10)multi_resp = multi_response_llm("What is Neural Network Feeding Mechanism?")json_multi_resp = json.loads(multi_resp)pprint(json_multi_resp)API Reference:NIBittensorLLMUsing NIBittensorLLM with LLMChain and PromptTemplate‚Äãimport langchainfrom langchain import PromptTemplate, LLMChainfrom langchain.llms import NIBittensorLLMlangchain.debug = Truetemplate = """Question: {question}Answer: Let's think step by step."""prompt = PromptTemplate(template=template, input_variables=["question"])# System parameter in NIBittensorLLM is optional but you can set whatever you want to perform with modelllm = NIBittensorLLM(system_prompt="Your task is to determine response based on user prompt.")llm_chain = LLMChain(prompt=prompt, llm=llm)question = "What is bittensor?"llm_chain.run(question)API Reference:NIBittensorLLMUsing NIBittensorLLM with Conversational Agent and Google Search Tool‚Äãfrom langchain.agents import (    AgentType,    initialize_agent,    load_tools,    ZeroShotAgent,    Tool,    AgentExecutor,)from langchain.memory import ConversationBufferMemoryfrom langchain import LLMChain, PromptTemplatefrom langchain.utilities import GoogleSearchAPIWrapper, SerpAPIWrapperfrom langchain.llms import NIBittensorLLMmemory = ConversationBufferMemory(memory_key="chat_history")prefix = """Answer prompt based on LLM if there is need to search something then use internet and observe internet result and give accurate reply of user questions also try to use authenticated sources"""suffix = """Begin!            {chat_history}            Question: {input}            {agent_scratchpad}"""prompt = ZeroShotAgent.create_prompt(    tools,    prefix=prefix,    suffix=suffix,    input_variables=["input", "chat_history", "agent_scratchpad"],)llm = NIBittensorLLM(system_prompt="Your task is to determine response based on user prompt")llm_chain = LLMChain(llm=llm, prompt=prompt)memory = ConversationBufferMemory(memory_key="chat_history")agent = ZeroShotAgent(llm_chain=llm_chain, tools=tools, verbose=True)agent_chain = AgentExecutor.from_agent_and_tools(    agent=agent, tools=tools, verbose=True, memory=memory)response = agent_chain.run(input=prompt)API Reference:AgentTypeinitialize_agentload_toolsZeroShotAgentToolAgentExecutorConversationBufferMemoryGoogleSearchAPIWrapperSerpAPIWrapperNIBittensorLLMPreviousBedrockNextCerebriumAIDifferent Parameter and response handling for NIBittensorLLMUsing NIBittensorLLM with LLMChain and PromptTemplateUsing NIBittensorLLM with Conversational Agent and Google Search ToolCommunityDiscordTwitterGitHubPythonJS/TSMoreHomepageBlogCopyright ¬© 2023 LangChain, Inc. Skip to main content ü¶úÔ∏èüîó LangChainDocsUse casesIntegrationsAPILangSmithJS/TS DocsCTRLK ü¶úÔ∏èüîó LangChainDocsUse casesIntegrationsAPI LangSmithJS/TS DocsCTRLK  CTRLK CTRLK  CTRLK   IntegrationsCallbacksChat modelsChat loadersDocument loadersDocument transformersLLMsAI21Aleph AlphaAmazon API GatewayAnyscaleAzure MLAzure OpenAIBananaBasetenBeamBedrockBittensorCerebriumAIChatGLMClarifaiCohereC TransformersCTranslate2DatabricksDeepInfraDeepSparseEden AIFireworksForefrontAIGoogle Vertex AI PaLMGooseAIGPT4AllHugging Face HubHugging Face Local PipelinesHuggingface TextGen InferenceJSONFormerKoboldAI APILlama.cppLLM Caching integrationsManifestMinimaxModalMosaicMLNLP CloudOctoAIOllamaOpaquePromptsOpenAIOpenLLMOpenLMPetalsPipelineAIPredibasePrediction GuardPromptLayer OpenAIRELLMReplicateRunhouseSageMakerEndpointStochasticAINebula (Symbl.ai)TextGenTitan TakeoffTongyi QwenvLLMWriterXorbits Inference (Xinference)MemoryRetrieversText embedding modelsAgents & ToolkitsToolsVector storesGrouped by providerIntegrationsLLMsBittensorOn this pageBittensorBittensor is a mining network, similar to Bitcoin, that includes built-in incentives designed to encourage miners to contribute compute + knowledge.NIBittensorLLM is developed by Neural Internet, powered by Bittensor.This LLM showcases true potential of decentralized AI by giving you the best response(s) from the Bittensor protocol, which consist of various AI models such as OpenAI, LLaMA2 etc.Users can view their logs, requests, and API keys on the Validator Endpoint Frontend. However, changes to the configuration are currently prohibited; otherwise, the user's queries will be blocked.If you encounter any difficulties or have any questions, please feel free to reach out to our developer on GitHub, Discord or join our discord server for latest update and queries Neural Internet.Different Parameter and response handling for NIBittensorLLM‚Äãimport langchainfrom langchain.llms import NIBittensorLLMimport jsonfrom pprint import pprintlangchain.debug = True# System parameter in NIBittensorLLM is optional but you can set whatever you want to perform with modelllm_sys = NIBittensorLLM(    system_prompt="Your task is to determine response based on user prompt.Explain me like I am technical lead of a project")sys_resp = llm_sys(    "What is bittensor and What are the potential benifits of decentralized AI?")print(f"Response provided by LLM with system prompt set is : {sys_resp}")# The top_responses parameter can give multiple responses based on its parameter value# This below code retrive top 10 miner's response all the response are in format of json# Json response structure is""" {    "choices":  [                    {"index": Bittensor's Metagraph index number,                    "uid": Unique Identifier of a miner,                    "responder_hotkey": Hotkey of a miner,                    "message":{"role":"assistant","content": Contains actual response},                    "response_ms": Time in millisecond required to fetch response from a miner}                 ]    } """multi_response_llm = NIBittensorLLM(top_responses=10)multi_resp = multi_response_llm("What is Neural Network Feeding Mechanism?")json_multi_resp = json.loads(multi_resp)pprint(json_multi_resp)API Reference:NIBittensorLLMUsing NIBittensorLLM with LLMChain and PromptTemplate‚Äãimport langchainfrom langchain import PromptTemplate, LLMChainfrom langchain.llms import NIBittensorLLMlangchain.debug = Truetemplate = """Question: {question}Answer: Let's think step by step."""prompt = PromptTemplate(template=template, input_variables=["question"])# System parameter in NIBittensorLLM is optional but you can set whatever you want to perform with modelllm = NIBittensorLLM(system_prompt="Your task is to determine response based on user prompt.")llm_chain = LLMChain(prompt=prompt, llm=llm)question = "What is bittensor?"llm_chain.run(question)API Reference:NIBittensorLLMUsing NIBittensorLLM with Conversational Agent and Google Search Tool‚Äãfrom langchain.agents import (    AgentType,    initialize_agent,    load_tools,    ZeroShotAgent,    Tool,    AgentExecutor,)from langchain.memory import ConversationBufferMemoryfrom langchain import LLMChain, PromptTemplatefrom langchain.utilities import GoogleSearchAPIWrapper, SerpAPIWrapperfrom langchain.llms import NIBittensorLLMmemory = ConversationBufferMemory(memory_key="chat_history")prefix = """Answer prompt based on LLM if there is need to search something then use internet and observe internet result and give accurate reply of user questions also try to use authenticated sources"""suffix = """Begin!            {chat_history}            Question: {input}            {agent_scratchpad}"""prompt = ZeroShotAgent.create_prompt(    tools,    prefix=prefix,    suffix=suffix,    input_variables=["input", "chat_history", "agent_scratchpad"],)llm = NIBittensorLLM(system_prompt="Your task is to determine response based on user prompt")llm_chain = LLMChain(llm=llm, prompt=prompt)memory = ConversationBufferMemory(memory_key="chat_history")agent = ZeroShotAgent(llm_chain=llm_chain, tools=tools, verbose=True)agent_chain = AgentExecutor.from_agent_and_tools(    agent=agent, tools=tools, verbose=True, memory=memory)response = agent_chain.run(input=prompt)API Reference:AgentTypeinitialize_agentload_toolsZeroShotAgentToolAgentExecutorConversationBufferMemoryGoogleSearchAPIWrapperSerpAPIWrapperNIBittensorLLMPreviousBedrockNextCerebriumAIDifferent Parameter and response handling for NIBittensorLLMUsing NIBittensorLLM with LLMChain and PromptTemplateUsing NIBittensorLLM with Conversational Agent and Google Search Tool IntegrationsCallbacksChat modelsChat loadersDocument loadersDocument transformersLLMsAI21Aleph AlphaAmazon API GatewayAnyscaleAzure MLAzure OpenAIBananaBasetenBeamBedrockBittensorCerebriumAIChatGLMClarifaiCohereC TransformersCTranslate2DatabricksDeepInfraDeepSparseEden AIFireworksForefrontAIGoogle Vertex AI PaLMGooseAIGPT4AllHugging Face HubHugging Face Local PipelinesHuggingface TextGen InferenceJSONFormerKoboldAI APILlama.cppLLM Caching integrationsManifestMinimaxModalMosaicMLNLP CloudOctoAIOllamaOpaquePromptsOpenAIOpenLLMOpenLMPetalsPipelineAIPredibasePrediction GuardPromptLayer OpenAIRELLMReplicateRunhouseSageMakerEndpointStochasticAINebula (Symbl.ai)TextGenTitan TakeoffTongyi QwenvLLMWriterXorbits Inference (Xinference)MemoryRetrieversText embedding modelsAgents & ToolkitsToolsVector storesGrouped by providerIntegrationsLLMsBittensorOn this pageBittensorBittensor is a mining network, similar to Bitcoin, that includes built-in incentives designed to encourage miners to contribute compute + knowledge.NIBittensorLLM is developed by Neural Internet, powered by Bittensor.This LLM showcases true potential of decentralized AI by giving you the best response(s) from the Bittensor protocol, which consist of various AI models such as OpenAI, LLaMA2 etc.Users can view their logs, requests, and API keys on the Validator Endpoint Frontend. However, changes to the configuration are currently prohibited; otherwise, the user's queries will be blocked.If you encounter any difficulties or have any questions, please feel free to reach out to our developer on GitHub, Discord or join our discord server for latest update and queries Neural Internet.Different Parameter and response handling for NIBittensorLLM‚Äãimport langchainfrom langchain.llms import NIBittensorLLMimport jsonfrom pprint import pprintlangchain.debug = True# System parameter in NIBittensorLLM is optional but you can set whatever you want to perform with modelllm_sys = NIBittensorLLM(    system_prompt="Your task is to determine response based on user prompt.Explain me like I am technical lead of a project")sys_resp = llm_sys(    "What is bittensor and What are the potential benifits of decentralized AI?")print(f"Response provided by LLM with system prompt set is : {sys_resp}")# The top_responses parameter can give multiple responses based on its parameter value# This below code retrive top 10 miner's response all the response are in format of json# Json response structure is""" {    "choices":  [                    {"index": Bittensor's Metagraph index number,                    "uid": Unique Identifier of a miner,                    "responder_hotkey": Hotkey of a miner,                    "message":{"role":"assistant","content": Contains actual response},                    "response_ms": Time in millisecond required to fetch response from a miner}                 ]    } """multi_response_llm = NIBittensorLLM(top_responses=10)multi_resp = multi_response_llm("What is Neural Network Feeding Mechanism?")json_multi_resp = json.loads(multi_resp)pprint(json_multi_resp)API Reference:NIBittensorLLMUsing NIBittensorLLM with LLMChain and PromptTemplate‚Äãimport langchainfrom langchain import PromptTemplate, LLMChainfrom langchain.llms import NIBittensorLLMlangchain.debug = Truetemplate = """Question: {question}Answer: Let's think step by step."""prompt = PromptTemplate(template=template, input_variables=["question"])# System parameter in NIBittensorLLM is optional but you can set whatever you want to perform with modelllm = NIBittensorLLM(system_prompt="Your task is to determine response based on user prompt.")llm_chain = LLMChain(prompt=prompt, llm=llm)question = "What is bittensor?"llm_chain.run(question)API Reference:NIBittensorLLMUsing NIBittensorLLM with Conversational Agent and Google Search Tool‚Äãfrom langchain.agents import (    AgentType,    initialize_agent,    load_tools,    ZeroShotAgent,    Tool,    AgentExecutor,)from langchain.memory import ConversationBufferMemoryfrom langchain import LLMChain, PromptTemplatefrom langchain.utilities import GoogleSearchAPIWrapper, SerpAPIWrapperfrom langchain.llms import NIBittensorLLMmemory = ConversationBufferMemory(memory_key="chat_history")prefix = """Answer prompt based on LLM if there is need to search something then use internet and observe internet result and give accurate reply of user questions also try to use authenticated sources"""suffix = """Begin!            {chat_history}            Question: {input}            {agent_scratchpad}"""prompt = ZeroShotAgent.create_prompt(    tools,    prefix=prefix,    suffix=suffix,    input_variables=["input", "chat_history", "agent_scratchpad"],)llm = NIBittensorLLM(system_prompt="Your task is to determine response based on user prompt")llm_chain = LLMChain(llm=llm, prompt=prompt)memory = ConversationBufferMemory(memory_key="chat_history")agent = ZeroShotAgent(llm_chain=llm_chain, tools=tools, verbose=True)agent_chain = AgentExecutor.from_agent_and_tools(    agent=agent, tools=tools, verbose=True, memory=memory)response = agent_chain.run(input=prompt)API Reference:AgentTypeinitialize_agentload_toolsZeroShotAgentToolAgentExecutorConversationBufferMemoryGoogleSearchAPIWrapperSerpAPIWrapperNIBittensorLLMPreviousBedrockNextCerebriumAIDifferent Parameter and response handling for NIBittensorLLMUsing NIBittensorLLM with LLMChain and PromptTemplateUsing NIBittensorLLM with Conversational Agent and Google Search Tool IntegrationsCallbacksChat modelsChat loadersDocument loadersDocument transformersLLMsAI21Aleph AlphaAmazon API GatewayAnyscaleAzure MLAzure OpenAIBananaBasetenBeamBedrockBittensorCerebriumAIChatGLMClarifaiCohereC TransformersCTranslate2DatabricksDeepInfraDeepSparseEden AIFireworksForefrontAIGoogle Vertex AI PaLMGooseAIGPT4AllHugging Face HubHugging Face Local PipelinesHuggingface TextGen InferenceJSONFormerKoboldAI APILlama.cppLLM Caching integrationsManifestMinimaxModalMosaicMLNLP CloudOctoAIOllamaOpaquePromptsOpenAIOpenLLMOpenLMPetalsPipelineAIPredibasePrediction GuardPromptLayer OpenAIRELLMReplicateRunhouseSageMakerEndpointStochasticAINebula (Symbl.ai)TextGenTitan TakeoffTongyi QwenvLLMWriterXorbits Inference (Xinference)MemoryRetrieversText embedding modelsAgents & ToolkitsToolsVector storesGrouped by provider IntegrationsCallbacksChat modelsChat loadersDocument loadersDocument transformersLLMsAI21Aleph AlphaAmazon API GatewayAnyscaleAzure MLAzure OpenAIBananaBasetenBeamBedrockBittensorCerebriumAIChatGLMClarifaiCohereC TransformersCTranslate2DatabricksDeepInfraDeepSparseEden AIFireworksForefrontAIGoogle Vertex AI PaLMGooseAIGPT4AllHugging Face HubHugging Face Local PipelinesHuggingface TextGen InferenceJSONFormerKoboldAI APILlama.cppLLM Caching integrationsManifestMinimaxModalMosaicMLNLP CloudOctoAIOllamaOpaquePromptsOpenAIOpenLLMOpenLMPetalsPipelineAIPredibasePrediction GuardPromptLayer OpenAIRELLMReplicateRunhouseSageMakerEndpointStochasticAINebula (Symbl.ai)TextGenTitan TakeoffTongyi QwenvLLMWriterXorbits Inference (Xinference)MemoryRetrieversText embedding modelsAgents & ToolkitsToolsVector storesGrouped by provider Integrations Callbacks Chat models Chat loaders Document loaders Document transformers LLMs Memory Retrievers Text embedding models Agents & Toolkits Tools Vector stores Grouped by provider IntegrationsLLMsBittensorOn this pageBittensorBittensor is a mining network, similar to Bitcoin, that includes built-in incentives designed to encourage miners to contribute compute + knowledge.NIBittensorLLM is developed by Neural Internet, powered by Bittensor.This LLM showcases true potential of decentralized AI by giving you the best response(s) from the Bittensor protocol, which consist of various AI models such as OpenAI, LLaMA2 etc.Users can view their logs, requests, and API keys on the Validator Endpoint Frontend. However, changes to the configuration are currently prohibited; otherwise, the user's queries will be blocked.If you encounter any difficulties or have any questions, please feel free to reach out to our developer on GitHub, Discord or join our discord server for latest update and queries Neural Internet.Different Parameter and response handling for NIBittensorLLM‚Äãimport langchainfrom langchain.llms import NIBittensorLLMimport jsonfrom pprint import pprintlangchain.debug = True# System parameter in NIBittensorLLM is optional but you can set whatever you want to perform with modelllm_sys = NIBittensorLLM(    system_prompt="Your task is to determine response based on user prompt.Explain me like I am technical lead of a project")sys_resp = llm_sys(    "What is bittensor and What are the potential benifits of decentralized AI?")print(f"Response provided by LLM with system prompt set is : {sys_resp}")# The top_responses parameter can give multiple responses based on its parameter value# This below code retrive top 10 miner's response all the response are in format of json# Json response structure is""" {    "choices":  [                    {"index": Bittensor's Metagraph index number,                    "uid": Unique Identifier of a miner,                    "responder_hotkey": Hotkey of a miner,                    "message":{"role":"assistant","content": Contains actual response},                    "response_ms": Time in millisecond required to fetch response from a miner}                 ]    } """multi_response_llm = NIBittensorLLM(top_responses=10)multi_resp = multi_response_llm("What is Neural Network Feeding Mechanism?")json_multi_resp = json.loads(multi_resp)pprint(json_multi_resp)API Reference:NIBittensorLLMUsing NIBittensorLLM with LLMChain and PromptTemplate‚Äãimport langchainfrom langchain import PromptTemplate, LLMChainfrom langchain.llms import NIBittensorLLMlangchain.debug = Truetemplate = """Question: {question}Answer: Let's think step by step."""prompt = PromptTemplate(template=template, input_variables=["question"])# System parameter in NIBittensorLLM is optional but you can set whatever you want to perform with modelllm = NIBittensorLLM(system_prompt="Your task is to determine response based on user prompt.")llm_chain = LLMChain(prompt=prompt, llm=llm)question = "What is bittensor?"llm_chain.run(question)API Reference:NIBittensorLLMUsing NIBittensorLLM with Conversational Agent and Google Search Tool‚Äãfrom langchain.agents import (    AgentType,    initialize_agent,    load_tools,    ZeroShotAgent,    Tool,    AgentExecutor,)from langchain.memory import ConversationBufferMemoryfrom langchain import LLMChain, PromptTemplatefrom langchain.utilities import GoogleSearchAPIWrapper, SerpAPIWrapperfrom langchain.llms import NIBittensorLLMmemory = ConversationBufferMemory(memory_key="chat_history")prefix = """Answer prompt based on LLM if there is need to search something then use internet and observe internet result and give accurate reply of user questions also try to use authenticated sources"""suffix = """Begin!            {chat_history}            Question: {input}            {agent_scratchpad}"""prompt = ZeroShotAgent.create_prompt(    tools,    prefix=prefix,    suffix=suffix,    input_variables=["input", "chat_history", "agent_scratchpad"],)llm = NIBittensorLLM(system_prompt="Your task is to determine response based on user prompt")llm_chain = LLMChain(llm=llm, prompt=prompt)memory = ConversationBufferMemory(memory_key="chat_history")agent = ZeroShotAgent(llm_chain=llm_chain, tools=tools, verbose=True)agent_chain = AgentExecutor.from_agent_and_tools(    agent=agent, tools=tools, verbose=True, memory=memory)response = agent_chain.run(input=prompt)API Reference:AgentTypeinitialize_agentload_toolsZeroShotAgentToolAgentExecutorConversationBufferMemoryGoogleSearchAPIWrapperSerpAPIWrapperNIBittensorLLMPreviousBedrockNextCerebriumAIDifferent Parameter and response handling for NIBittensorLLMUsing NIBittensorLLM with LLMChain and PromptTemplateUsing NIBittensorLLM with Conversational Agent and Google Search Tool IntegrationsLLMsBittensorOn this pageBittensorBittensor is a mining network, similar to Bitcoin, that includes built-in incentives designed to encourage miners to contribute compute + knowledge.NIBittensorLLM is developed by Neural Internet, powered by Bittensor.This LLM showcases true potential of decentralized AI by giving you the best response(s) from the Bittensor protocol, which consist of various AI models such as OpenAI, LLaMA2 etc.Users can view their logs, requests, and API keys on the Validator Endpoint Frontend. However, changes to the configuration are currently prohibited; otherwise, the user's queries will be blocked.If you encounter any difficulties or have any questions, please feel free to reach out to our developer on GitHub, Discord or join our discord server for latest update and queries Neural Internet.Different Parameter and response handling for NIBittensorLLM‚Äãimport langchainfrom langchain.llms import NIBittensorLLMimport jsonfrom pprint import pprintlangchain.debug = True# System parameter in NIBittensorLLM is optional but you can set whatever you want to perform with modelllm_sys = NIBittensorLLM(    system_prompt="Your task is to determine response based on user prompt.Explain me like I am technical lead of a project")sys_resp = llm_sys(    "What is bittensor and What are the potential benifits of decentralized AI?")print(f"Response provided by LLM with system prompt set is : {sys_resp}")# The top_responses parameter can give multiple responses based on its parameter value# This below code retrive top 10 miner's response all the response are in format of json# Json response structure is""" {    "choices":  [                    {"index": Bittensor's Metagraph index number,                    "uid": Unique Identifier of a miner,                    "responder_hotkey": Hotkey of a miner,                    "message":{"role":"assistant","content": Contains actual response},                    "response_ms": Time in millisecond required to fetch response from a miner}                 ]    } """multi_response_llm = NIBittensorLLM(top_responses=10)multi_resp = multi_response_llm("What is Neural Network Feeding Mechanism?")json_multi_resp = json.loads(multi_resp)pprint(json_multi_resp)API Reference:NIBittensorLLMUsing NIBittensorLLM with LLMChain and PromptTemplate‚Äãimport langchainfrom langchain import PromptTemplate, LLMChainfrom langchain.llms import NIBittensorLLMlangchain.debug = Truetemplate = """Question: {question}Answer: Let's think step by step."""prompt = PromptTemplate(template=template, input_variables=["question"])# System parameter in NIBittensorLLM is optional but you can set whatever you want to perform with modelllm = NIBittensorLLM(system_prompt="Your task is to determine response based on user prompt.")llm_chain = LLMChain(prompt=prompt, llm=llm)question = "What is bittensor?"llm_chain.run(question)API Reference:NIBittensorLLMUsing NIBittensorLLM with Conversational Agent and Google Search Tool‚Äãfrom langchain.agents import (    AgentType,    initialize_agent,    load_tools,    ZeroShotAgent,    Tool,    AgentExecutor,)from langchain.memory import ConversationBufferMemoryfrom langchain import LLMChain, PromptTemplatefrom langchain.utilities import GoogleSearchAPIWrapper, SerpAPIWrapperfrom langchain.llms import NIBittensorLLMmemory = ConversationBufferMemory(memory_key="chat_history")prefix = """Answer prompt based on LLM if there is need to search something then use internet and observe internet result and give accurate reply of user questions also try to use authenticated sources"""suffix = """Begin!            {chat_history}            Question: {input}            {agent_scratchpad}"""prompt = ZeroShotAgent.create_prompt(    tools,    prefix=prefix,    suffix=suffix,    input_variables=["input", "chat_history", "agent_scratchpad"],)llm = NIBittensorLLM(system_prompt="Your task is to determine response based on user prompt")llm_chain = LLMChain(llm=llm, prompt=prompt)memory = ConversationBufferMemory(memory_key="chat_history")agent = ZeroShotAgent(llm_chain=llm_chain, tools=tools, verbose=True)agent_chain = AgentExecutor.from_agent_and_tools(    agent=agent, tools=tools, verbose=True, memory=memory)response = agent_chain.run(input=prompt)API Reference:AgentTypeinitialize_agentload_toolsZeroShotAgentToolAgentExecutorConversationBufferMemoryGoogleSearchAPIWrapperSerpAPIWrapperNIBittensorLLMPreviousBedrockNextCerebriumAIDifferent Parameter and response handling for NIBittensorLLMUsing NIBittensorLLM with LLMChain and PromptTemplateUsing NIBittensorLLM with Conversational Agent and Google Search Tool IntegrationsLLMsBittensorOn this pageBittensorBittensor is a mining network, similar to Bitcoin, that includes built-in incentives designed to encourage miners to contribute compute + knowledge.NIBittensorLLM is developed by Neural Internet, powered by Bittensor.This LLM showcases true potential of decentralized AI by giving you the best response(s) from the Bittensor protocol, which consist of various AI models such as OpenAI, LLaMA2 etc.Users can view their logs, requests, and API keys on the Validator Endpoint Frontend. However, changes to the configuration are currently prohibited; otherwise, the user's queries will be blocked.If you encounter any difficulties or have any questions, please feel free to reach out to our developer on GitHub, Discord or join our discord server for latest update and queries Neural Internet.Different Parameter and response handling for NIBittensorLLM‚Äãimport langchainfrom langchain.llms import NIBittensorLLMimport jsonfrom pprint import pprintlangchain.debug = True# System parameter in NIBittensorLLM is optional but you can set whatever you want to perform with modelllm_sys = NIBittensorLLM(    system_prompt="Your task is to determine response based on user prompt.Explain me like I am technical lead of a project")sys_resp = llm_sys(    "What is bittensor and What are the potential benifits of decentralized AI?")print(f"Response provided by LLM with system prompt set is : {sys_resp}")# The top_responses parameter can give multiple responses based on its parameter value# This below code retrive top 10 miner's response all the response are in format of json# Json response structure is""" {    "choices":  [                    {"index": Bittensor's Metagraph index number,                    "uid": Unique Identifier of a miner,                    "responder_hotkey": Hotkey of a miner,                    "message":{"role":"assistant","content": Contains actual response},                    "response_ms": Time in millisecond required to fetch response from a miner}                 ]    } """multi_response_llm = NIBittensorLLM(top_responses=10)multi_resp = multi_response_llm("What is Neural Network Feeding Mechanism?")json_multi_resp = json.loads(multi_resp)pprint(json_multi_resp)API Reference:NIBittensorLLMUsing NIBittensorLLM with LLMChain and PromptTemplate‚Äãimport langchainfrom langchain import PromptTemplate, LLMChainfrom langchain.llms import NIBittensorLLMlangchain.debug = Truetemplate = """Question: {question}Answer: Let's think step by step."""prompt = PromptTemplate(template=template, input_variables=["question"])# System parameter in NIBittensorLLM is optional but you can set whatever you want to perform with modelllm = NIBittensorLLM(system_prompt="Your task is to determine response based on user prompt.")llm_chain = LLMChain(prompt=prompt, llm=llm)question = "What is bittensor?"llm_chain.run(question)API Reference:NIBittensorLLMUsing NIBittensorLLM with Conversational Agent and Google Search Tool‚Äãfrom langchain.agents import (    AgentType,    initialize_agent,    load_tools,    ZeroShotAgent,    Tool,    AgentExecutor,)from langchain.memory import ConversationBufferMemoryfrom langchain import LLMChain, PromptTemplatefrom langchain.utilities import GoogleSearchAPIWrapper, SerpAPIWrapperfrom langchain.llms import NIBittensorLLMmemory = ConversationBufferMemory(memory_key="chat_history")prefix = """Answer prompt based on LLM if there is need to search something then use internet and observe internet result and give accurate reply of user questions also try to use authenticated sources"""suffix = """Begin!            {chat_history}            Question: {input}            {agent_scratchpad}"""prompt = ZeroShotAgent.create_prompt(    tools,    prefix=prefix,    suffix=suffix,    input_variables=["input", "chat_history", "agent_scratchpad"],)llm = NIBittensorLLM(system_prompt="Your task is to determine response based on user prompt")llm_chain = LLMChain(llm=llm, prompt=prompt)memory = ConversationBufferMemory(memory_key="chat_history")agent = ZeroShotAgent(llm_chain=llm_chain, tools=tools, verbose=True)agent_chain = AgentExecutor.from_agent_and_tools(    agent=agent, tools=tools, verbose=True, memory=memory)response = agent_chain.run(input=prompt)API Reference:AgentTypeinitialize_agentload_toolsZeroShotAgentToolAgentExecutorConversationBufferMemoryGoogleSearchAPIWrapperSerpAPIWrapperNIBittensorLLMPreviousBedrockNextCerebriumAI IntegrationsLLMsBittensorOn this pageBittensorBittensor is a mining network, similar to Bitcoin, that includes built-in incentives designed to encourage miners to contribute compute + knowledge.NIBittensorLLM is developed by Neural Internet, powered by Bittensor.This LLM showcases true potential of decentralized AI by giving you the best response(s) from the Bittensor protocol, which consist of various AI models such as OpenAI, LLaMA2 etc.Users can view their logs, requests, and API keys on the Validator Endpoint Frontend. However, changes to the configuration are currently prohibited; otherwise, the user's queries will be blocked.If you encounter any difficulties or have any questions, please feel free to reach out to our developer on GitHub, Discord or join our discord server for latest update and queries Neural Internet.Different Parameter and response handling for NIBittensorLLM‚Äãimport langchainfrom langchain.llms import NIBittensorLLMimport jsonfrom pprint import pprintlangchain.debug = True# System parameter in NIBittensorLLM is optional but you can set whatever you want to perform with modelllm_sys = NIBittensorLLM(    system_prompt="Your task is to determine response based on user prompt.Explain me like I am technical lead of a project")sys_resp = llm_sys(    "What is bittensor and What are the potential benifits of decentralized AI?")print(f"Response provided by LLM with system prompt set is : {sys_resp}")# The top_responses parameter can give multiple responses based on its parameter value# This below code retrive top 10 miner's response all the response are in format of json# Json response structure is""" {    "choices":  [                    {"index": Bittensor's Metagraph index number,                    "uid": Unique Identifier of a miner,                    "responder_hotkey": Hotkey of a miner,                    "message":{"role":"assistant","content": Contains actual response},                    "response_ms": Time in millisecond required to fetch response from a miner}                 ]    } """multi_response_llm = NIBittensorLLM(top_responses=10)multi_resp = multi_response_llm("What is Neural Network Feeding Mechanism?")json_multi_resp = json.loads(multi_resp)pprint(json_multi_resp)API Reference:NIBittensorLLMUsing NIBittensorLLM with LLMChain and PromptTemplate‚Äãimport langchainfrom langchain import PromptTemplate, LLMChainfrom langchain.llms import NIBittensorLLMlangchain.debug = Truetemplate = """Question: {question}Answer: Let's think step by step."""prompt = PromptTemplate(template=template, input_variables=["question"])# System parameter in NIBittensorLLM is optional but you can set whatever you want to perform with modelllm = NIBittensorLLM(system_prompt="Your task is to determine response based on user prompt.")llm_chain = LLMChain(prompt=prompt, llm=llm)question = "What is bittensor?"llm_chain.run(question)API Reference:NIBittensorLLMUsing NIBittensorLLM with Conversational Agent and Google Search Tool‚Äãfrom langchain.agents import (    AgentType,    initialize_agent,    load_tools,    ZeroShotAgent,    Tool,    AgentExecutor,)from langchain.memory import ConversationBufferMemoryfrom langchain import LLMChain, PromptTemplatefrom langchain.utilities import GoogleSearchAPIWrapper, SerpAPIWrapperfrom langchain.llms import NIBittensorLLMmemory = ConversationBufferMemory(memory_key="chat_history")prefix = """Answer prompt based on LLM if there is need to search something then use internet and observe internet result and give accurate reply of user questions also try to use authenticated sources"""suffix = """Begin!            {chat_history}            Question: {input}            {agent_scratchpad}"""prompt = ZeroShotAgent.create_prompt(    tools,    prefix=prefix,    suffix=suffix,    input_variables=["input", "chat_history", "agent_scratchpad"],)llm = NIBittensorLLM(system_prompt="Your task is to determine response based on user prompt")llm_chain = LLMChain(llm=llm, prompt=prompt)memory = ConversationBufferMemory(memory_key="chat_history")agent = ZeroShotAgent(llm_chain=llm_chain, tools=tools, verbose=True)agent_chain = AgentExecutor.from_agent_and_tools(    agent=agent, tools=tools, verbose=True, memory=memory)response = agent_chain.run(input=prompt)API Reference:AgentTypeinitialize_agentload_toolsZeroShotAgentToolAgentExecutorConversationBufferMemoryGoogleSearchAPIWrapperSerpAPIWrapperNIBittensorLLMPreviousBedrockNextCerebriumAI On this page BittensorBittensor is a mining network, similar to Bitcoin, that includes built-in incentives designed to encourage miners to contribute compute + knowledge.NIBittensorLLM is developed by Neural Internet, powered by Bittensor.This LLM showcases true potential of decentralized AI by giving you the best response(s) from the Bittensor protocol, which consist of various AI models such as OpenAI, LLaMA2 etc.Users can view their logs, requests, and API keys on the Validator Endpoint Frontend. However, changes to the configuration are currently prohibited; otherwise, the user's queries will be blocked.If you encounter any difficulties or have any questions, please feel free to reach out to our developer on GitHub, Discord or join our discord server for latest update and queries Neural Internet.Different Parameter and response handling for NIBittensorLLM‚Äãimport langchainfrom langchain.llms import NIBittensorLLMimport jsonfrom pprint import pprintlangchain.debug = True# System parameter in NIBittensorLLM is optional but you can set whatever you want to perform with modelllm_sys = NIBittensorLLM(    system_prompt="Your task is to determine response based on user prompt.Explain me like I am technical lead of a project")sys_resp = llm_sys(    "What is bittensor and What are the potential benifits of decentralized AI?")print(f"Response provided by LLM with system prompt set is : {sys_resp}")# The top_responses parameter can give multiple responses based on its parameter value# This below code retrive top 10 miner's response all the response are in format of json# Json response structure is""" {    "choices":  [                    {"index": Bittensor's Metagraph index number,                    "uid": Unique Identifier of a miner,                    "responder_hotkey": Hotkey of a miner,                    "message":{"role":"assistant","content": Contains actual response},                    "response_ms": Time in millisecond required to fetch response from a miner}                 ]    } """multi_response_llm = NIBittensorLLM(top_responses=10)multi_resp = multi_response_llm("What is Neural Network Feeding Mechanism?")json_multi_resp = json.loads(multi_resp)pprint(json_multi_resp)API Reference:NIBittensorLLMUsing NIBittensorLLM with LLMChain and PromptTemplate‚Äãimport langchainfrom langchain import PromptTemplate, LLMChainfrom langchain.llms import NIBittensorLLMlangchain.debug = Truetemplate = """Question: {question}Answer: Let's think step by step."""prompt = PromptTemplate(template=template, input_variables=["question"])# System parameter in NIBittensorLLM is optional but you can set whatever you want to perform with modelllm = NIBittensorLLM(system_prompt="Your task is to determine response based on user prompt.")llm_chain = LLMChain(prompt=prompt, llm=llm)question = "What is bittensor?"llm_chain.run(question)API Reference:NIBittensorLLMUsing NIBittensorLLM with Conversational Agent and Google Search Tool‚Äãfrom langchain.agents import (    AgentType,    initialize_agent,    load_tools,    ZeroShotAgent,    Tool,    AgentExecutor,)from langchain.memory import ConversationBufferMemoryfrom langchain import LLMChain, PromptTemplatefrom langchain.utilities import GoogleSearchAPIWrapper, SerpAPIWrapperfrom langchain.llms import NIBittensorLLMmemory = ConversationBufferMemory(memory_key="chat_history")prefix = """Answer prompt based on LLM if there is need to search something then use internet and observe internet result and give accurate reply of user questions also try to use authenticated sources"""suffix = """Begin!            {chat_history}            Question: {input}            {agent_scratchpad}"""prompt = ZeroShotAgent.create_prompt(    tools,    prefix=prefix,    suffix=suffix,    input_variables=["input", "chat_history", "agent_scratchpad"],)llm = NIBittensorLLM(system_prompt="Your task is to determine response based on user prompt")llm_chain = LLMChain(llm=llm, prompt=prompt)memory = ConversationBufferMemory(memory_key="chat_history")agent = ZeroShotAgent(llm_chain=llm_chain, tools=tools, verbose=True)agent_chain = AgentExecutor.from_agent_and_tools(    agent=agent, tools=tools, verbose=True, memory=memory)response = agent_chain.run(input=prompt)API Reference:AgentTypeinitialize_agentload_toolsZeroShotAgentToolAgentExecutorConversationBufferMemoryGoogleSearchAPIWrapperSerpAPIWrapperNIBittensorLLM import langchainfrom langchain.llms import NIBittensorLLMimport jsonfrom pprint import pprintlangchain.debug = True# System parameter in NIBittensorLLM is optional but you can set whatever you want to perform with modelllm_sys = NIBittensorLLM(    system_prompt="Your task is to determine response based on user prompt.Explain me like I am technical lead of a project")sys_resp = llm_sys(    "What is bittensor and What are the potential benifits of decentralized AI?")print(f"Response provided by LLM with system prompt set is : {sys_resp}")# The top_responses parameter can give multiple responses based on its parameter value# This below code retrive top 10 miner's response all the response are in format of json# Json response structure is""" {    "choices":  [                    {"index": Bittensor's Metagraph index number,                    "uid": Unique Identifier of a miner,                    "responder_hotkey": Hotkey of a miner,                    "message":{"role":"assistant","content": Contains actual response},                    "response_ms": Time in millisecond required to fetch response from a miner}                 ]    } """multi_response_llm = NIBittensorLLM(top_responses=10)multi_resp = multi_response_llm("What is Neural Network Feeding Mechanism?")json_multi_resp = json.loads(multi_resp)pprint(json_multi_resp) import langchainfrom langchain.llms import NIBittensorLLMimport jsonfrom pprint import pprintlangchain.debug = True# System parameter in NIBittensorLLM is optional but you can set whatever you want to perform with modelllm_sys = NIBittensorLLM(    system_prompt="Your task is to determine response based on user prompt.Explain me like I am technical lead of a project")sys_resp = llm_sys(    "What is bittensor and What are the potential benifits of decentralized AI?")print(f"Response provided by LLM with system prompt set is : {sys_resp}")# The top_responses parameter can give multiple responses based on its parameter value# This below code retrive top 10 miner's response all the response are in format of json# Json response structure is""" {    "choices":  [                    {"index": Bittensor's Metagraph index number,                    "uid": Unique Identifier of a miner,                    "responder_hotkey": Hotkey of a miner,                    "message":{"role":"assistant","content": Contains actual response},                    "response_ms": Time in millisecond required to fetch response from a miner}                 ]    } """multi_response_llm = NIBittensorLLM(top_responses=10)multi_resp = multi_response_llm("What is Neural Network Feeding Mechanism?")json_multi_resp = json.loads(multi_resp)pprint(json_multi_resp)  API Reference:NIBittensorLLM import langchainfrom langchain import PromptTemplate, LLMChainfrom langchain.llms import NIBittensorLLMlangchain.debug = Truetemplate = """Question: {question}Answer: Let's think step by step."""prompt = PromptTemplate(template=template, input_variables=["question"])# System parameter in NIBittensorLLM is optional but you can set whatever you want to perform with modelllm = NIBittensorLLM(system_prompt="Your task is to determine response based on user prompt.")llm_chain = LLMChain(prompt=prompt, llm=llm)question = "What is bittensor?"llm_chain.run(question) import langchainfrom langchain import PromptTemplate, LLMChainfrom langchain.llms import NIBittensorLLMlangchain.debug = Truetemplate = """Question: {question}Answer: Let's think step by step."""prompt = PromptTemplate(template=template, input_variables=["question"])# System parameter in NIBittensorLLM is optional but you can set whatever you want to perform with modelllm = NIBittensorLLM(system_prompt="Your task is to determine response based on user prompt.")llm_chain = LLMChain(prompt=prompt, llm=llm)question = "What is bittensor?"llm_chain.run(question)  API Reference:NIBittensorLLM from langchain.agents import (    AgentType,    initialize_agent,    load_tools,    ZeroShotAgent,    Tool,    AgentExecutor,)from langchain.memory import ConversationBufferMemoryfrom langchain import LLMChain, PromptTemplatefrom langchain.utilities import GoogleSearchAPIWrapper, SerpAPIWrapperfrom langchain.llms import NIBittensorLLMmemory = ConversationBufferMemory(memory_key="chat_history")prefix = """Answer prompt based on LLM if there is need to search something then use internet and observe internet result and give accurate reply of user questions also try to use authenticated sources"""suffix = """Begin!            {chat_history}            Question: {input}            {agent_scratchpad}"""prompt = ZeroShotAgent.create_prompt(    tools,    prefix=prefix,    suffix=suffix,    input_variables=["input", "chat_history", "agent_scratchpad"],)llm = NIBittensorLLM(system_prompt="Your task is to determine response based on user prompt")llm_chain = LLMChain(llm=llm, prompt=prompt)memory = ConversationBufferMemory(memory_key="chat_history")agent = ZeroShotAgent(llm_chain=llm_chain, tools=tools, verbose=True)agent_chain = AgentExecutor.from_agent_and_tools(    agent=agent, tools=tools, verbose=True, memory=memory)response = agent_chain.run(input=prompt) from langchain.agents import (    AgentType,    initialize_agent,    load_tools,    ZeroShotAgent,    Tool,    AgentExecutor,)from langchain.memory import ConversationBufferMemoryfrom langchain import LLMChain, PromptTemplatefrom langchain.utilities import GoogleSearchAPIWrapper, SerpAPIWrapperfrom langchain.llms import NIBittensorLLMmemory = ConversationBufferMemory(memory_key="chat_history")prefix = """Answer prompt based on LLM if there is need to search something then use internet and observe internet result and give accurate reply of user questions also try to use authenticated sources"""suffix = """Begin!            {chat_history}            Question: {input}            {agent_scratchpad}"""prompt = ZeroShotAgent.create_prompt(    tools,    prefix=prefix,    suffix=suffix,    input_variables=["input", "chat_history", "agent_scratchpad"],)llm = NIBittensorLLM(system_prompt="Your task is to determine response based on user prompt")llm_chain = LLMChain(llm=llm, prompt=prompt)memory = ConversationBufferMemory(memory_key="chat_history")agent = ZeroShotAgent(llm_chain=llm_chain, tools=tools, verbose=True)agent_chain = AgentExecutor.from_agent_and_tools(    agent=agent, tools=tools, verbose=True, memory=memory)response = agent_chain.run(input=prompt)  API Reference:AgentTypeinitialize_agentload_toolsZeroShotAgentToolAgentExecutorConversationBufferMemoryGoogleSearchAPIWrapperSerpAPIWrapperNIBittensorLLM Previous Bedrock Next CerebriumAI Different Parameter and response handling for NIBittensorLLMUsing NIBittensorLLM with LLMChain and PromptTemplateUsing NIBittensorLLM with Conversational Agent and Google Search Tool Different Parameter and response handling for NIBittensorLLMUsing NIBittensorLLM with LLMChain and PromptTemplateUsing NIBittensorLLM with Conversational Agent and Google Search Tool CommunityDiscordTwitterGitHubPythonJS/TSMoreHomepageBlogCopyright ¬© 2023 LangChain, Inc. CommunityDiscordTwitterGitHubPythonJS/TSMoreHomepageBlog CommunityDiscordTwitter Community GitHubPythonJS/TS GitHub MoreHomepageBlog More Copyright ¬© 2023 LangChain, Inc. Copyright ¬© 2023 LangChain, Inc. Skip to main content (#docusaurus_skipToContent_fallback) ü¶úÔ∏èüîó LangChain (/) Docs (/docs/get_started/introduction) Use cases (/docs/use_cases) Integrations (/docs/integrations) API (https://api.python.langchain.com) LangSmith (https://smith.langchain.com) JS/TS Docs (https://js.langchain.com/docs)  (https://github.com/hwchase17/langchain) Integrations (/docs/integrations) Callbacks (/docs/integrations/callbacks/) Chat models (/docs/integrations/chat/) Chat loaders (/docs/integrations/chat_loaders/) Document loaders (/docs/integrations/document_loaders/) Document transformers (/docs/integrations/document_transformers/) LLMs (/docs/integrations/llms/) AI21 (/docs/integrations/llms/ai21) Aleph Alpha (/docs/integrations/llms/aleph_alpha) Amazon API Gateway (/docs/integrations/llms/amazon_api_gateway) Anyscale (/docs/integrations/llms/anyscale) Azure ML (/docs/integrations/llms/azure_ml) Azure OpenAI (/docs/integrations/llms/azure_openai) Banana (/docs/integrations/llms/banana) Baseten (/docs/integrations/llms/baseten) Beam (/docs/integrations/llms/beam) Bedrock (/docs/integrations/llms/bedrock) Bittensor (/docs/integrations/llms/bittensor) CerebriumAI (/docs/integrations/llms/cerebriumai) ChatGLM (/docs/integrations/llms/chatglm) Clarifai (/docs/integrations/llms/clarifai) Cohere (/docs/integrations/llms/cohere) C Transformers (/docs/integrations/llms/ctransformers) CTranslate2 (/docs/integrations/llms/ctranslate2) Databricks (/docs/integrations/llms/databricks) DeepInfra (/docs/integrations/llms/deepinfra) DeepSparse (/docs/integrations/llms/deepsparse) Eden AI (/docs/integrations/llms/edenai) Fireworks (/docs/integrations/llms/fireworks) ForefrontAI (/docs/integrations/llms/forefrontai) Google Vertex AI PaLM (/docs/integrations/llms/google_vertex_ai_palm) GooseAI (/docs/integrations/llms/gooseai) GPT4All (/docs/integrations/llms/gpt4all) Hugging Face Hub (/docs/integrations/llms/huggingface_hub) Hugging Face Local Pipelines (/docs/integrations/llms/huggingface_pipelines) Huggingface TextGen Inference (/docs/integrations/llms/huggingface_textgen_inference) JSONFormer (/docs/integrations/llms/jsonformer_experimental) KoboldAI API (/docs/integrations/llms/koboldai) Llama.cpp (/docs/integrations/llms/llamacpp) LLM Caching integrations (/docs/integrations/llms/llm_caching) Manifest (/docs/integrations/llms/manifest) Minimax (/docs/integrations/llms/minimax) Modal (/docs/integrations/llms/modal) MosaicML (/docs/integrations/llms/mosaicml) NLP Cloud (/docs/integrations/llms/nlpcloud) OctoAI (/docs/integrations/llms/octoai) Ollama (/docs/integrations/llms/ollama) OpaquePrompts (/docs/integrations/llms/opaqueprompts) OpenAI (/docs/integrations/llms/openai) OpenLLM (/docs/integrations/llms/openllm) OpenLM (/docs/integrations/llms/openlm) Petals (/docs/integrations/llms/petals) PipelineAI (/docs/integrations/llms/pipelineai) Predibase (/docs/integrations/llms/predibase) Prediction Guard (/docs/integrations/llms/predictionguard) PromptLayer OpenAI (/docs/integrations/llms/promptlayer_openai) RELLM (/docs/integrations/llms/rellm_experimental) Replicate (/docs/integrations/llms/replicate) Runhouse (/docs/integrations/llms/runhouse) SageMakerEndpoint (/docs/integrations/llms/sagemaker) StochasticAI (/docs/integrations/llms/stochasticai) Nebula (Symbl.ai) (/docs/integrations/llms/symblai_nebula) TextGen (/docs/integrations/llms/textgen) Titan Takeoff (/docs/integrations/llms/titan_takeoff) Tongyi Qwen (/docs/integrations/llms/tongyi) vLLM (/docs/integrations/llms/vllm) Writer (/docs/integrations/llms/writer) Xorbits Inference (Xinference) (/docs/integrations/llms/xinference) Memory (/docs/integrations/memory/) Retrievers (/docs/integrations/retrievers/) Text embedding models (/docs/integrations/text_embedding/) Agents & Toolkits (/docs/integrations/toolkits/) Tools (/docs/integrations/tools/) Vector stores (/docs/integrations/vectorstores/) Grouped by provider (/docs/integrations/providers/)  (/) Integrations (/docs/integrations) LLMs (/docs/integrations/llms/) Bittensor (https://bittensor.com/) Neural Internet (https://neuralinternet.ai/) Validator Endpoint Frontend (https://api.neuralinternet.ai/) GitHub (https://github.com/Kunj-2206) Discord (https://discordapp.com/users/683542109248159777) Neural Internet (https://discord.gg/neuralinternet) ‚Äã (#different-parameter-and-response-handling-for-nibittensorllm) NIBittensorLLM (https://api.python.langchain.com/en/latest/llms/langchain.llms.bittensor.NIBittensorLLM.html) ‚Äã (#using-nibittensorllm-with-llmchain-and-prompttemplate) NIBittensorLLM (https://api.python.langchain.com/en/latest/llms/langchain.llms.bittensor.NIBittensorLLM.html) ‚Äã (#using-nibittensorllm-with-conversational-agent-and-google-search-tool) AgentType (https://api.python.langchain.com/en/latest/agents/langchain.agents.agent_types.AgentType.html) initialize_agent (https://api.python.langchain.com/en/latest/agents/langchain.agents.initialize.initialize_agent.html) load_tools (https://api.python.langchain.com/en/latest/agents/langchain.agents.load_tools.load_tools.html) ZeroShotAgent (https://api.python.langchain.com/en/latest/agents/langchain.agents.mrkl.base.ZeroShotAgent.html) Tool (https://api.python.langchain.com/en/latest/tools/langchain.tools.base.Tool.html) AgentExecutor (https://api.python.langchain.com/en/latest/agents/langchain.agents.agent.AgentExecutor.html) ConversationBufferMemory (https://api.python.langchain.com/en/latest/memory/langchain.memory.buffer.ConversationBufferMemory.html) GoogleSearchAPIWrapper (https://api.python.langchain.com/en/latest/utilities/langchain.utilities.google_search.GoogleSearchAPIWrapper.html) SerpAPIWrapper (https://api.python.langchain.com/en/latest/utilities/langchain.utilities.serpapi.SerpAPIWrapper.html) NIBittensorLLM (https://api.python.langchain.com/en/latest/llms/langchain.llms.bittensor.NIBittensorLLM.html) PreviousBedrock (/docs/integrations/llms/bedrock) NextCerebriumAI (/docs/integrations/llms/cerebriumai) Different Parameter and response handling for NIBittensorLLM (#different-parameter-and-response-handling-for-nibittensorllm) Using NIBittensorLLM with LLMChain and PromptTemplate (#using-nibittensorllm-with-llmchain-and-prompttemplate) Using NIBittensorLLM with Conversational Agent and Google Search Tool (#using-nibittensorllm-with-conversational-agent-and-google-search-tool) Discord (https://discord.gg/cU2adEyC7w) Twitter (https://twitter.com/LangChainAI) Python (https://github.com/hwchase17/langchain) JS/TS (https://github.com/hwchase17/langchainjs) Homepage (https://langchain.com) Blog (https://blog.langchain.dev)