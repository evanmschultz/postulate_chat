Amazon SageMaker is a system that can build, train, and deploy machine learning (ML) models for any use case with fully managed infrastructure, tools, and workflows. This notebooks goes over how to use an LLM hosted on a SageMaker endpoint. You have to set up following required parameters of the SagemakerEndpoint call: IntegrationsCallbacksChat modelsChat loadersDocument loadersDocument transformersLLMsAI21Aleph AlphaAmazon API GatewayAnyscaleAzure MLAzure OpenAIBananaBasetenBeamBedrockBittensorCerebriumAIChatGLMClarifaiCohereC TransformersCTranslate2DatabricksDeepInfraDeepSparseEden AIFireworksForefrontAIGoogle Vertex AI PaLMGooseAIGPT4AllHugging Face HubHugging Face Local PipelinesHuggingface TextGen InferenceJSONFormerKoboldAI APILlama.cppLLM Caching integrationsManifestMinimaxModalMosaicMLNLP CloudOctoAIOllamaOpaquePromptsOpenAIOpenLLMOpenLMPetalsPipelineAIPredibasePrediction GuardPromptLayer OpenAIRELLMReplicateRunhouseSageMakerEndpointStochasticAINebula (Symbl.ai)TextGenTitan TakeoffTongyi QwenvLLMWriterXorbits Inference (Xinference)MemoryRetrieversText embedding modelsAgents & ToolkitsToolsVector storesGrouped by provider Callbacks Chat models Chat loaders Document loaders Document transformers LLMsAI21Aleph AlphaAmazon API GatewayAnyscaleAzure MLAzure OpenAIBananaBasetenBeamBedrockBittensorCerebriumAIChatGLMClarifaiCohereC TransformersCTranslate2DatabricksDeepInfraDeepSparseEden AIFireworksForefrontAIGoogle Vertex AI PaLMGooseAIGPT4AllHugging Face HubHugging Face Local PipelinesHuggingface TextGen InferenceJSONFormerKoboldAI APILlama.cppLLM Caching integrationsManifestMinimaxModalMosaicMLNLP CloudOctoAIOllamaOpaquePromptsOpenAIOpenLLMOpenLMPetalsPipelineAIPredibasePrediction GuardPromptLayer OpenAIRELLMReplicateRunhouseSageMakerEndpointStochasticAINebula (Symbl.ai)TextGenTitan TakeoffTongyi QwenvLLMWriterXorbits Inference (Xinference) AI21 Aleph Alpha Amazon API Gateway Anyscale Azure ML Azure OpenAI Banana Baseten Beam Bedrock Bittensor CerebriumAI ChatGLM Clarifai Cohere C Transformers CTranslate2 Databricks DeepInfra DeepSparse Eden AI Fireworks ForefrontAI Google Vertex AI PaLM GooseAI GPT4All Hugging Face Hub Hugging Face Local Pipelines Huggingface TextGen Inference JSONFormer KoboldAI API Llama.cpp LLM Caching integrations Manifest Minimax Modal MosaicML NLP Cloud OctoAI Ollama OpaquePrompts OpenAI OpenLLM OpenLM Petals PipelineAI Predibase Prediction Guard PromptLayer OpenAI RELLM Replicate Runhouse SageMakerEndpoint StochasticAI Nebula (Symbl.ai) TextGen Titan Takeoff Tongyi Qwen vLLM Writer Xorbits Inference (Xinference) Memory Retrievers Text embedding models Agents & Toolkits Tools Vector stores Grouped by provider  Integrations LLMs SageMakerEndpoint endpoint_name: The name of the endpoint from the deployed Sagemaker model. Must be unique within an AWS Region. credentials_profile_name: The name of the profile in the ~/.aws/credentials or ~/.aws/config files, which has either access keys or role information specified. If not specified, the default credential profile or, if on an EC2 instance, credentials from IMDS will be used. See: https://boto3.amazonaws.com/v1/documentation/api/latest/guide/credentials.html Document LLMContentHandler load_qa_chain Set up Example Discord Twitter Python JS/TS Homepage Blog Skip to main contentü¶úÔ∏èüîó LangChainDocsUse casesIntegrationsAPILangSmithJS/TS DocsCTRLKIntegrationsCallbacksChat modelsChat loadersDocument loadersDocument transformersLLMsAI21Aleph AlphaAmazon API GatewayAnyscaleAzure MLAzure OpenAIBananaBasetenBeamBedrockBittensorCerebriumAIChatGLMClarifaiCohereC TransformersCTranslate2DatabricksDeepInfraDeepSparseEden AIFireworksForefrontAIGoogle Vertex AI PaLMGooseAIGPT4AllHugging Face HubHugging Face Local PipelinesHuggingface TextGen InferenceJSONFormerKoboldAI APILlama.cppLLM Caching integrationsManifestMinimaxModalMosaicMLNLP CloudOctoAIOllamaOpaquePromptsOpenAIOpenLLMOpenLMPetalsPipelineAIPredibasePrediction GuardPromptLayer OpenAIRELLMReplicateRunhouseSageMakerEndpointStochasticAINebula (Symbl.ai)TextGenTitan TakeoffTongyi QwenvLLMWriterXorbits Inference (Xinference)MemoryRetrieversText embedding modelsAgents & ToolkitsToolsVector storesGrouped by providerIntegrationsLLMsSageMakerEndpointOn this pageSageMakerEndpointAmazon SageMaker is a system that can build, train, and deploy machine learning (ML) models for any use case with fully managed infrastructure, tools, and workflows.This notebooks goes over how to use an LLM hosted on a SageMaker endpoint.pip3 install langchain boto3Set up‚ÄãYou have to set up following required parameters of the SagemakerEndpoint call:endpoint_name: The name of the endpoint from the deployed Sagemaker model. Must be unique within an AWS Region.credentials_profile_name: The name of the profile in the ~/.aws/credentials or ~/.aws/config files, which See: https://boto3.amazonaws.com/v1/documentation/api/latest/guide/credentials.htmlExample‚Äãfrom langchain.docstore.document import DocumentAPI Reference:Documentexample_doc_1 = """Peter and Elizabeth took a taxi to attend the night party in the city. While in the party, Elizabeth collapsed and was rushed to the hospital.Since she was diagnosed with a brain injury, the doctor told Peter to stay besides her until she gets well.Therefore, Peter stayed with her at the hospital for 3 days without leaving."""docs = [    Document(        page_content=example_doc_1,    )]from typing import Dictfrom langchain import PromptTemplate, SagemakerEndpointfrom langchain.llms.sagemaker_endpoint import LLMContentHandlerfrom langchain.chains.question_answering import load_qa_chainimport jsonquery = """How long was Elizabeth hospitalized?"""prompt_template = """Use the following pieces of context to answer the question at the end.{context}Question: {question}Answer:"""PROMPT = PromptTemplate(    template=prompt_template, input_variables=["context", "question"])class ContentHandler(LLMContentHandler):    content_type = "application/json"    accepts = "application/json"    def transform_input(self, prompt: str, model_kwargs: Dict) -> bytes:        input_str = json.dumps({prompt: prompt, **model_kwargs})        return input_str.encode("utf-8")    def transform_output(self, output: bytes) -> str:        response_json = json.loads(output.read().decode("utf-8"))        return response_json[0]["generated_text"]content_handler = ContentHandler()chain = load_qa_chain(    llm=SagemakerEndpoint(        endpoint_name="endpoint-name",        credentials_profile_name="credentials-profile-name",        region_name="us-west-2",        model_kwargs={"temperature": 1e-10},        content_handler=content_handler,    ),    prompt=PROMPT,)chain({"input_documents": docs, "question": query}, return_only_outputs=True)API Reference:LLMContentHandlerload_qa_chainPreviousRunhouseNextStochasticAISet upExampleCommunityDiscordTwitterGitHubPythonJS/TSMoreHomepageBlogCopyright ¬© 2023 LangChain, Inc. Skip to main content ü¶úÔ∏èüîó LangChainDocsUse casesIntegrationsAPILangSmithJS/TS DocsCTRLK ü¶úÔ∏èüîó LangChainDocsUse casesIntegrationsAPI LangSmithJS/TS DocsCTRLK  CTRLK CTRLK  CTRLK   IntegrationsCallbacksChat modelsChat loadersDocument loadersDocument transformersLLMsAI21Aleph AlphaAmazon API GatewayAnyscaleAzure MLAzure OpenAIBananaBasetenBeamBedrockBittensorCerebriumAIChatGLMClarifaiCohereC TransformersCTranslate2DatabricksDeepInfraDeepSparseEden AIFireworksForefrontAIGoogle Vertex AI PaLMGooseAIGPT4AllHugging Face HubHugging Face Local PipelinesHuggingface TextGen InferenceJSONFormerKoboldAI APILlama.cppLLM Caching integrationsManifestMinimaxModalMosaicMLNLP CloudOctoAIOllamaOpaquePromptsOpenAIOpenLLMOpenLMPetalsPipelineAIPredibasePrediction GuardPromptLayer OpenAIRELLMReplicateRunhouseSageMakerEndpointStochasticAINebula (Symbl.ai)TextGenTitan TakeoffTongyi QwenvLLMWriterXorbits Inference (Xinference)MemoryRetrieversText embedding modelsAgents & ToolkitsToolsVector storesGrouped by providerIntegrationsLLMsSageMakerEndpointOn this pageSageMakerEndpointAmazon SageMaker is a system that can build, train, and deploy machine learning (ML) models for any use case with fully managed infrastructure, tools, and workflows.This notebooks goes over how to use an LLM hosted on a SageMaker endpoint.pip3 install langchain boto3Set up‚ÄãYou have to set up following required parameters of the SagemakerEndpoint call:endpoint_name: The name of the endpoint from the deployed Sagemaker model. See: https://boto3.amazonaws.com/v1/documentation/api/latest/guide/credentials.htmlExample‚Äãfrom langchain.docstore.document import DocumentAPI Reference:Documentexample_doc_1 = """Peter and Elizabeth took a taxi to attend the night party in the city. While in the party, Elizabeth collapsed and was rushed to the hospital.Since she was diagnosed with a brain injury, the doctor told Peter to stay besides her until she gets well.Therefore, Peter stayed with her at the hospital for 3 days without leaving."""docs = [    Document(        page_content=example_doc_1,    )]from typing import Dictfrom langchain import PromptTemplate, SagemakerEndpointfrom langchain.llms.sagemaker_endpoint import LLMContentHandlerfrom langchain.chains.question_answering import load_qa_chainimport jsonquery = """How long was Elizabeth hospitalized?"""prompt_template = """Use the following pieces of context to answer the question at the end.{context}Question: {question}Answer:"""PROMPT = PromptTemplate(    template=prompt_template, input_variables=["context", "question"])class ContentHandler(LLMContentHandler):    content_type = "application/json"    accepts = "application/json"    def transform_input(self, prompt: str, model_kwargs: Dict) -> bytes:        input_str = json.dumps({prompt: prompt, **model_kwargs})        return input_str.encode("utf-8")    def transform_output(self, output: bytes) -> str:        response_json = json.loads(output.read().decode("utf-8"))        return response_json[0]["generated_text"]content_handler = ContentHandler()chain = load_qa_chain(    llm=SagemakerEndpoint(        endpoint_name="endpoint-name",        credentials_profile_name="credentials-profile-name",        region_name="us-west-2",        model_kwargs={"temperature": 1e-10},        content_handler=content_handler,    ),    prompt=PROMPT,)chain({"input_documents": docs, "question": query}, return_only_outputs=True)API Reference:LLMContentHandlerload_qa_chainPreviousRunhouseNextStochasticAISet upExample IntegrationsCallbacksChat modelsChat loadersDocument loadersDocument transformersLLMsAI21Aleph AlphaAmazon API GatewayAnyscaleAzure MLAzure OpenAIBananaBasetenBeamBedrockBittensorCerebriumAIChatGLMClarifaiCohereC TransformersCTranslate2DatabricksDeepInfraDeepSparseEden AIFireworksForefrontAIGoogle Vertex AI PaLMGooseAIGPT4AllHugging Face HubHugging Face Local PipelinesHuggingface TextGen InferenceJSONFormerKoboldAI APILlama.cppLLM Caching integrationsManifestMinimaxModalMosaicMLNLP CloudOctoAIOllamaOpaquePromptsOpenAIOpenLLMOpenLMPetalsPipelineAIPredibasePrediction GuardPromptLayer OpenAIRELLMReplicateRunhouseSageMakerEndpointStochasticAINebula (Symbl.ai)TextGenTitan TakeoffTongyi QwenvLLMWriterXorbits Inference (Xinference)MemoryRetrieversText embedding modelsAgents & ToolkitsToolsVector storesGrouped by providerIntegrationsLLMsSageMakerEndpointOn this pageSageMakerEndpointAmazon SageMaker is a system that can build, train, and deploy machine learning (ML) models for any use case with fully managed infrastructure, tools, and workflows.This notebooks goes over how to use an LLM hosted on a SageMaker endpoint.pip3 install langchain boto3Set up‚ÄãYou have to set up following required parameters of the SagemakerEndpoint call:endpoint_name: The name of the endpoint from the deployed Sagemaker model. See: https://boto3.amazonaws.com/v1/documentation/api/latest/guide/credentials.htmlExample‚Äãfrom langchain.docstore.document import DocumentAPI Reference:Documentexample_doc_1 = """Peter and Elizabeth took a taxi to attend the night party in the city. While in the party, Elizabeth collapsed and was rushed to the hospital.Since she was diagnosed with a brain injury, the doctor told Peter to stay besides her until she gets well.Therefore, Peter stayed with her at the hospital for 3 days without leaving."""docs = [    Document(        page_content=example_doc_1,    )]from typing import Dictfrom langchain import PromptTemplate, SagemakerEndpointfrom langchain.llms.sagemaker_endpoint import LLMContentHandlerfrom langchain.chains.question_answering import load_qa_chainimport jsonquery = """How long was Elizabeth hospitalized?"""prompt_template = """Use the following pieces of context to answer the question at the end.{context}Question: {question}Answer:"""PROMPT = PromptTemplate(    template=prompt_template, input_variables=["context", "question"])class ContentHandler(LLMContentHandler):    content_type = "application/json"    accepts = "application/json"    def transform_input(self, prompt: str, model_kwargs: Dict) -> bytes:        input_str = json.dumps({prompt: prompt, **model_kwargs})        return input_str.encode("utf-8")    def transform_output(self, output: bytes) -> str:        response_json = json.loads(output.read().decode("utf-8"))        return response_json[0]["generated_text"]content_handler = ContentHandler()chain = load_qa_chain(    llm=SagemakerEndpoint(        endpoint_name="endpoint-name",        credentials_profile_name="credentials-profile-name",        region_name="us-west-2",        model_kwargs={"temperature": 1e-10},        content_handler=content_handler,    ),    prompt=PROMPT,)chain({"input_documents": docs, "question": query}, return_only_outputs=True)API Reference:LLMContentHandlerload_qa_chainPreviousRunhouseNextStochasticAISet upExample IntegrationsCallbacksChat modelsChat loadersDocument loadersDocument transformersLLMsAI21Aleph AlphaAmazon API GatewayAnyscaleAzure MLAzure OpenAIBananaBasetenBeamBedrockBittensorCerebriumAIChatGLMClarifaiCohereC TransformersCTranslate2DatabricksDeepInfraDeepSparseEden AIFireworksForefrontAIGoogle Vertex AI PaLMGooseAIGPT4AllHugging Face HubHugging Face Local PipelinesHuggingface TextGen InferenceJSONFormerKoboldAI APILlama.cppLLM Caching integrationsManifestMinimaxModalMosaicMLNLP CloudOctoAIOllamaOpaquePromptsOpenAIOpenLLMOpenLMPetalsPipelineAIPredibasePrediction GuardPromptLayer OpenAIRELLMReplicateRunhouseSageMakerEndpointStochasticAINebula (Symbl.ai)TextGenTitan TakeoffTongyi QwenvLLMWriterXorbits Inference (Xinference)MemoryRetrieversText embedding modelsAgents & ToolkitsToolsVector storesGrouped by provider IntegrationsCallbacksChat modelsChat loadersDocument loadersDocument transformersLLMsAI21Aleph AlphaAmazon API GatewayAnyscaleAzure MLAzure OpenAIBananaBasetenBeamBedrockBittensorCerebriumAIChatGLMClarifaiCohereC TransformersCTranslate2DatabricksDeepInfraDeepSparseEden AIFireworksForefrontAIGoogle Vertex AI PaLMGooseAIGPT4AllHugging Face HubHugging Face Local PipelinesHuggingface TextGen InferenceJSONFormerKoboldAI APILlama.cppLLM Caching integrationsManifestMinimaxModalMosaicMLNLP CloudOctoAIOllamaOpaquePromptsOpenAIOpenLLMOpenLMPetalsPipelineAIPredibasePrediction GuardPromptLayer OpenAIRELLMReplicateRunhouseSageMakerEndpointStochasticAINebula (Symbl.ai)TextGenTitan TakeoffTongyi QwenvLLMWriterXorbits Inference (Xinference)MemoryRetrieversText embedding modelsAgents & ToolkitsToolsVector storesGrouped by provider Integrations Callbacks Chat models Chat loaders Document loaders Document transformers LLMs Memory Retrievers Text embedding models Agents & Toolkits Tools Vector stores Grouped by provider IntegrationsLLMsSageMakerEndpointOn this pageSageMakerEndpointAmazon SageMaker is a system that can build, train, and deploy machine learning (ML) models for any use case with fully managed infrastructure, tools, and workflows.This notebooks goes over how to use an LLM hosted on a SageMaker endpoint.pip3 install langchain boto3Set up‚ÄãYou have to set up following required parameters of the SagemakerEndpoint call:endpoint_name: The name of the endpoint from the deployed Sagemaker model. See: https://boto3.amazonaws.com/v1/documentation/api/latest/guide/credentials.htmlExample‚Äãfrom langchain.docstore.document import DocumentAPI Reference:Documentexample_doc_1 = """Peter and Elizabeth took a taxi to attend the night party in the city. While in the party, Elizabeth collapsed and was rushed to the hospital.Since she was diagnosed with a brain injury, the doctor told Peter to stay besides her until she gets well.Therefore, Peter stayed with her at the hospital for 3 days without leaving."""docs = [    Document(        page_content=example_doc_1,    )]from typing import Dictfrom langchain import PromptTemplate, SagemakerEndpointfrom langchain.llms.sagemaker_endpoint import LLMContentHandlerfrom langchain.chains.question_answering import load_qa_chainimport jsonquery = """How long was Elizabeth hospitalized?"""prompt_template = """Use the following pieces of context to answer the question at the end.{context}Question: {question}Answer:"""PROMPT = PromptTemplate(    template=prompt_template, input_variables=["context", "question"])class ContentHandler(LLMContentHandler):    content_type = "application/json"    accepts = "application/json"    def transform_input(self, prompt: str, model_kwargs: Dict) -> bytes:        input_str = json.dumps({prompt: prompt, **model_kwargs})        return input_str.encode("utf-8")    def transform_output(self, output: bytes) -> str:        response_json = json.loads(output.read().decode("utf-8"))        return response_json[0]["generated_text"]content_handler = ContentHandler()chain = load_qa_chain(    llm=SagemakerEndpoint(        endpoint_name="endpoint-name",        credentials_profile_name="credentials-profile-name",        region_name="us-west-2",        model_kwargs={"temperature": 1e-10},        content_handler=content_handler,    ),    prompt=PROMPT,)chain({"input_documents": docs, "question": query}, return_only_outputs=True)API Reference:LLMContentHandlerload_qa_chainPreviousRunhouseNextStochasticAISet upExample IntegrationsLLMsSageMakerEndpointOn this pageSageMakerEndpointAmazon SageMaker is a system that can build, train, and deploy machine learning (ML) models for any use case with fully managed infrastructure, tools, and workflows.This notebooks goes over how to use an LLM hosted on a SageMaker endpoint.pip3 install langchain boto3Set up‚ÄãYou have to set up following required parameters of the SagemakerEndpoint call:endpoint_name: The name of the endpoint from the deployed Sagemaker model. See: https://boto3.amazonaws.com/v1/documentation/api/latest/guide/credentials.htmlExample‚Äãfrom langchain.docstore.document import DocumentAPI Reference:Documentexample_doc_1 = """Peter and Elizabeth took a taxi to attend the night party in the city. While in the party, Elizabeth collapsed and was rushed to the hospital.Since she was diagnosed with a brain injury, the doctor told Peter to stay besides her until she gets well.Therefore, Peter stayed with her at the hospital for 3 days without leaving."""docs = [    Document(        page_content=example_doc_1,    )]from typing import Dictfrom langchain import PromptTemplate, SagemakerEndpointfrom langchain.llms.sagemaker_endpoint import LLMContentHandlerfrom langchain.chains.question_answering import load_qa_chainimport jsonquery = """How long was Elizabeth hospitalized?"""prompt_template = """Use the following pieces of context to answer the question at the end.{context}Question: {question}Answer:"""PROMPT = PromptTemplate(    template=prompt_template, input_variables=["context", "question"])class ContentHandler(LLMContentHandler):    content_type = "application/json"    accepts = "application/json"    def transform_input(self, prompt: str, model_kwargs: Dict) -> bytes:        input_str = json.dumps({prompt: prompt, **model_kwargs})        return input_str.encode("utf-8")    def transform_output(self, output: bytes) -> str:        response_json = json.loads(output.read().decode("utf-8"))        return response_json[0]["generated_text"]content_handler = ContentHandler()chain = load_qa_chain(    llm=SagemakerEndpoint(        endpoint_name="endpoint-name",        credentials_profile_name="credentials-profile-name",        region_name="us-west-2",        model_kwargs={"temperature": 1e-10},        content_handler=content_handler,    ),    prompt=PROMPT,)chain({"input_documents": docs, "question": query}, return_only_outputs=True)API Reference:LLMContentHandlerload_qa_chainPreviousRunhouseNextStochasticAI IntegrationsLLMsSageMakerEndpointOn this pageSageMakerEndpointAmazon SageMaker is a system that can build, train, and deploy machine learning (ML) models for any use case with fully managed infrastructure, tools, and workflows.This notebooks goes over how to use an LLM hosted on a SageMaker endpoint.pip3 install langchain boto3Set up‚ÄãYou have to set up following required parameters of the SagemakerEndpoint call:endpoint_name: The name of the endpoint from the deployed Sagemaker model. See: https://boto3.amazonaws.com/v1/documentation/api/latest/guide/credentials.htmlExample‚Äãfrom langchain.docstore.document import DocumentAPI Reference:Documentexample_doc_1 = """Peter and Elizabeth took a taxi to attend the night party in the city. While in the party, Elizabeth collapsed and was rushed to the hospital.Since she was diagnosed with a brain injury, the doctor told Peter to stay besides her until she gets well.Therefore, Peter stayed with her at the hospital for 3 days without leaving."""docs = [    Document(        page_content=example_doc_1,    )]from typing import Dictfrom langchain import PromptTemplate, SagemakerEndpointfrom langchain.llms.sagemaker_endpoint import LLMContentHandlerfrom langchain.chains.question_answering import load_qa_chainimport jsonquery = """How long was Elizabeth hospitalized?"""prompt_template = """Use the following pieces of context to answer the question at the end.{context}Question: {question}Answer:"""PROMPT = PromptTemplate(    template=prompt_template, input_variables=["context", "question"])class ContentHandler(LLMContentHandler):    content_type = "application/json"    accepts = "application/json"    def transform_input(self, prompt: str, model_kwargs: Dict) -> bytes:        input_str = json.dumps({prompt: prompt, **model_kwargs})        return input_str.encode("utf-8")    def transform_output(self, output: bytes) -> str:        response_json = json.loads(output.read().decode("utf-8"))        return response_json[0]["generated_text"]content_handler = ContentHandler()chain = load_qa_chain(    llm=SagemakerEndpoint(        endpoint_name="endpoint-name",        credentials_profile_name="credentials-profile-name",        region_name="us-west-2",        model_kwargs={"temperature": 1e-10},        content_handler=content_handler,    ),    prompt=PROMPT,)chain({"input_documents": docs, "question": query}, return_only_outputs=True)API Reference:LLMContentHandlerload_qa_chainPreviousRunhouseNextStochasticAI On this page SageMakerEndpointAmazon SageMaker is a system that can build, train, and deploy machine learning (ML) models for any use case with fully managed infrastructure, tools, and workflows.This notebooks goes over how to use an LLM hosted on a SageMaker endpoint.pip3 install langchain boto3Set up‚ÄãYou have to set up following required parameters of the SagemakerEndpoint call:endpoint_name: The name of the endpoint from the deployed Sagemaker model. See: https://boto3.amazonaws.com/v1/documentation/api/latest/guide/credentials.htmlExample‚Äãfrom langchain.docstore.document import DocumentAPI Reference:Documentexample_doc_1 = """Peter and Elizabeth took a taxi to attend the night party in the city. While in the party, Elizabeth collapsed and was rushed to the hospital.Since she was diagnosed with a brain injury, the doctor told Peter to stay besides her until she gets well.Therefore, Peter stayed with her at the hospital for 3 days without leaving."""docs = [    Document(        page_content=example_doc_1,    )]from typing import Dictfrom langchain import PromptTemplate, SagemakerEndpointfrom langchain.llms.sagemaker_endpoint import LLMContentHandlerfrom langchain.chains.question_answering import load_qa_chainimport jsonquery = """How long was Elizabeth hospitalized?"""prompt_template = """Use the following pieces of context to answer the question at the end.{context}Question: {question}Answer:"""PROMPT = PromptTemplate(    template=prompt_template, input_variables=["context", "question"])class ContentHandler(LLMContentHandler):    content_type = "application/json"    accepts = "application/json"    def transform_input(self, prompt: str, model_kwargs: Dict) -> bytes:        input_str = json.dumps({prompt: prompt, **model_kwargs})        return input_str.encode("utf-8")    def transform_output(self, output: bytes) -> str:        response_json = json.loads(output.read().decode("utf-8"))        return response_json[0]["generated_text"]content_handler = ContentHandler()chain = load_qa_chain(    llm=SagemakerEndpoint(        endpoint_name="endpoint-name",        credentials_profile_name="credentials-profile-name",        region_name="us-west-2",        model_kwargs={"temperature": 1e-10},        content_handler=content_handler,    ),    prompt=PROMPT,)chain({"input_documents": docs, "question": query}, return_only_outputs=True)API Reference:LLMContentHandlerload_qa_chain pip3 install langchain boto3 pip3 install langchain boto3  from langchain.docstore.document import Document from langchain.docstore.document import Document  API Reference:Document example_doc_1 = """Peter and Elizabeth took a taxi to attend the night party in the city. While in the party, Elizabeth collapsed and was rushed to the hospital.Since she was diagnosed with a brain injury, the doctor told Peter to stay besides her until she gets well.Therefore, Peter stayed with her at the hospital for 3 days without leaving."""docs = [    Document(        page_content=example_doc_1,    )] example_doc_1 = """Peter and Elizabeth took a taxi to attend the night party in the city. While in the party, Elizabeth collapsed and was rushed to the hospital.Since she was diagnosed with a brain injury, the doctor told Peter to stay besides her until she gets well.Therefore, Peter stayed with her at the hospital for 3 days without leaving."""docs = [    Document(        page_content=example_doc_1,    )]  from typing import Dictfrom langchain import PromptTemplate, SagemakerEndpointfrom langchain.llms.sagemaker_endpoint import LLMContentHandlerfrom langchain.chains.question_answering import load_qa_chainimport jsonquery = """How long was Elizabeth hospitalized?"""prompt_template = """Use the following pieces of context to answer the question at the end.{context}Question: {question}Answer:"""PROMPT = PromptTemplate(    template=prompt_template, input_variables=["context", "question"])class ContentHandler(LLMContentHandler):    content_type = "application/json"    accepts = "application/json"    def transform_input(self, prompt: str, model_kwargs: Dict) -> bytes:        input_str = json.dumps({prompt: prompt, **model_kwargs})        return input_str.encode("utf-8")    def transform_output(self, output: bytes) -> str:        response_json = json.loads(output.read().decode("utf-8"))        return response_json[0]["generated_text"]content_handler = ContentHandler()chain = load_qa_chain(    llm=SagemakerEndpoint(        endpoint_name="endpoint-name",        credentials_profile_name="credentials-profile-name",        region_name="us-west-2",        model_kwargs={"temperature": 1e-10},        content_handler=content_handler,    ),    prompt=PROMPT,)chain({"input_documents": docs, "question": query}, return_only_outputs=True) from typing import Dictfrom langchain import PromptTemplate, SagemakerEndpointfrom langchain.llms.sagemaker_endpoint import LLMContentHandlerfrom langchain.chains.question_answering import load_qa_chainimport jsonquery = """How long was Elizabeth hospitalized?"""prompt_template = """Use the following pieces of context to answer the question at the end.{context}Question: {question}Answer:"""PROMPT = PromptTemplate(    template=prompt_template, input_variables=["context", "question"])class ContentHandler(LLMContentHandler):    content_type = "application/json"    accepts = "application/json"    def transform_input(self, prompt: str, model_kwargs: Dict) -> bytes:        input_str = json.dumps({prompt: prompt, **model_kwargs})        return input_str.encode("utf-8")    def transform_output(self, output: bytes) -> str:        response_json = json.loads(output.read().decode("utf-8"))        return response_json[0]["generated_text"]content_handler = ContentHandler()chain = load_qa_chain(    llm=SagemakerEndpoint(        endpoint_name="endpoint-name",        credentials_profile_name="credentials-profile-name",        region_name="us-west-2",        model_kwargs={"temperature": 1e-10},        content_handler=content_handler,    ),    prompt=PROMPT,)chain({"input_documents": docs, "question": query}, return_only_outputs=True)  API Reference:LLMContentHandlerload_qa_chain Previous Runhouse Next StochasticAI Set upExample Set upExample CommunityDiscordTwitterGitHubPythonJS/TSMoreHomepageBlogCopyright ¬© 2023 LangChain, Inc. CommunityDiscordTwitterGitHubPythonJS/TSMoreHomepageBlog CommunityDiscordTwitter Community GitHubPythonJS/TS GitHub MoreHomepageBlog More Copyright ¬© 2023 LangChain, Inc. Copyright ¬© 2023 LangChain, Inc. Skip to main content (#docusaurus_skipToContent_fallback) ü¶úÔ∏èüîó LangChain (/) Docs (/docs/get_started/introduction) Use cases (/docs/use_cases) Integrations (/docs/integrations) API (https://api.python.langchain.com) LangSmith (https://smith.langchain.com) JS/TS Docs (https://js.langchain.com/docs)  (https://github.com/hwchase17/langchain) Integrations (/docs/integrations) Callbacks (/docs/integrations/callbacks/) Chat models (/docs/integrations/chat/) Chat loaders (/docs/integrations/chat_loaders/) Document loaders (/docs/integrations/document_loaders/) Document transformers (/docs/integrations/document_transformers/) LLMs (/docs/integrations/llms/) AI21 (/docs/integrations/llms/ai21) Aleph Alpha (/docs/integrations/llms/aleph_alpha) Amazon API Gateway (/docs/integrations/llms/amazon_api_gateway) Anyscale (/docs/integrations/llms/anyscale) Azure ML (/docs/integrations/llms/azure_ml) Azure OpenAI (/docs/integrations/llms/azure_openai) Banana (/docs/integrations/llms/banana) Baseten (/docs/integrations/llms/baseten) Beam (/docs/integrations/llms/beam) Bedrock (/docs/integrations/llms/bedrock) Bittensor (/docs/integrations/llms/bittensor) CerebriumAI (/docs/integrations/llms/cerebriumai) ChatGLM (/docs/integrations/llms/chatglm) Clarifai (/docs/integrations/llms/clarifai) Cohere (/docs/integrations/llms/cohere) C Transformers (/docs/integrations/llms/ctransformers) CTranslate2 (/docs/integrations/llms/ctranslate2) Databricks (/docs/integrations/llms/databricks) DeepInfra (/docs/integrations/llms/deepinfra) DeepSparse (/docs/integrations/llms/deepsparse) Eden AI (/docs/integrations/llms/edenai) Fireworks (/docs/integrations/llms/fireworks) ForefrontAI (/docs/integrations/llms/forefrontai) Google Vertex AI PaLM (/docs/integrations/llms/google_vertex_ai_palm) GooseAI (/docs/integrations/llms/gooseai) GPT4All (/docs/integrations/llms/gpt4all) Hugging Face Hub (/docs/integrations/llms/huggingface_hub) Hugging Face Local Pipelines (/docs/integrations/llms/huggingface_pipelines) Huggingface TextGen Inference (/docs/integrations/llms/huggingface_textgen_inference) JSONFormer (/docs/integrations/llms/jsonformer_experimental) KoboldAI API (/docs/integrations/llms/koboldai) Llama.cpp (/docs/integrations/llms/llamacpp) LLM Caching integrations (/docs/integrations/llms/llm_caching) Manifest (/docs/integrations/llms/manifest) Minimax (/docs/integrations/llms/minimax) Modal (/docs/integrations/llms/modal) MosaicML (/docs/integrations/llms/mosaicml) NLP Cloud (/docs/integrations/llms/nlpcloud) OctoAI (/docs/integrations/llms/octoai) Ollama (/docs/integrations/llms/ollama) OpaquePrompts (/docs/integrations/llms/opaqueprompts) OpenAI (/docs/integrations/llms/openai) OpenLLM (/docs/integrations/llms/openllm) OpenLM (/docs/integrations/llms/openlm) Petals (/docs/integrations/llms/petals) PipelineAI (/docs/integrations/llms/pipelineai) Predibase (/docs/integrations/llms/predibase) Prediction Guard (/docs/integrations/llms/predictionguard) PromptLayer OpenAI (/docs/integrations/llms/promptlayer_openai) RELLM (/docs/integrations/llms/rellm_experimental) Replicate (/docs/integrations/llms/replicate) Runhouse (/docs/integrations/llms/runhouse) SageMakerEndpoint (/docs/integrations/llms/sagemaker) StochasticAI (/docs/integrations/llms/stochasticai) Nebula (Symbl.ai) (/docs/integrations/llms/symblai_nebula) TextGen (/docs/integrations/llms/textgen) Titan Takeoff (/docs/integrations/llms/titan_takeoff) Tongyi Qwen (/docs/integrations/llms/tongyi) vLLM (/docs/integrations/llms/vllm) Writer (/docs/integrations/llms/writer) Xorbits Inference (Xinference) (/docs/integrations/llms/xinference) Memory (/docs/integrations/memory/) Retrievers (/docs/integrations/retrievers/) Text embedding models (/docs/integrations/text_embedding/) Agents & Toolkits (/docs/integrations/toolkits/) Tools (/docs/integrations/tools/) Vector stores (/docs/integrations/vectorstores/) Grouped by provider (/docs/integrations/providers/)  (/) Integrations (/docs/integrations) LLMs (/docs/integrations/llms/) Amazon SageMaker (https://aws.amazon.com/sagemaker/) ‚Äã (#set-up) https://boto3.amazonaws.com/v1/documentation/api/latest/guide/credentials.html (https://boto3.amazonaws.com/v1/documentation/api/latest/guide/credentials.html) ‚Äã (#example) Document (https://api.python.langchain.com/en/latest/schema/langchain.schema.document.Document.html) LLMContentHandler (https://api.python.langchain.com/en/latest/llms/langchain.llms.sagemaker_endpoint.LLMContentHandler.html) load_qa_chain (https://api.python.langchain.com/en/latest/chains/langchain.chains.question_answering.load_qa_chain.html) PreviousRunhouse (/docs/integrations/llms/runhouse) NextStochasticAI (/docs/integrations/llms/stochasticai) Set up (#set-up) Example (#example) Discord (https://discord.gg/cU2adEyC7w) Twitter (https://twitter.com/LangChainAI) Python (https://github.com/hwchase17/langchain) JS/TS (https://github.com/hwchase17/langchainjs) Homepage (https://langchain.com) Blog (https://blog.langchain.dev)